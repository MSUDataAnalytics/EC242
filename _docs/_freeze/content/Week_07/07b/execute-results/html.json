{
  "hash": "8e17323abb44832b5f6c631cce7d486b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Regression: Interpreting Coefficients\"\nlastmod: \"2024-10-04\"\noutput:\n  blogdown::html_page:\n    toc: true\n    code-overflow: scroll\n---\n\n\n\n\n::: {.callout-tip}\n\nToday's example will pivot between the content from this week and the example below.\n\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n\n- [<i class=\"fas fa-file-csv\"></i> `Ames.csv`](/projects/data/ames.csv)\n\n:::\n\n## Preliminaries\nSo far in each of our analyses, we have only used numeric variables as predictors. We have also only used *additive models*, meaning the effect any predictor had on the response was not dependent on the other predictors. In this chapter, we will remove both of these restrictions. We will fit models with multiple predictors, categorical predictors, and use models that allow predictors to *interact*. Instead of a single variable, will use *multiple* linear regression. The mathematics of multiple regression will remain largely unchanging, however, we will pay close attention to interpretation, as well as some difference in `R` usage.\n\n## Dummy Variables\n\nFor this example and discussion, we will briefly use the built in dataset `mtcars` before introducing the `autompg` dataset. The reason to use these easy, straightforward datasets is that they make visualization of the __entire dataset__ trivially easy. Accordingly, the `mtcars` dataset is small, so we'll quickly take a look at the entire dataset.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nmtcars\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n```\n\n\n:::\n:::\n\n\n\n\nWe will be interested in three of the variables: `mpg `, `hp`, and `am`.\n\n- `mpg`: fuel efficiency, in miles per gallon.\n- `hp`: horsepower, in foot-pounds per second.\n- `am`: transmission. Automatic or manual.\n\nAs we often do, we will start by plotting the data. We are interested in `mpg` as the response variable, and `hp` as a predictor.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mpg ~ hp, data = mtcars, cex = 2)\n```\n\n::: {.cell-output-display}\n![](07b_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\nSince we are also interested in the transmission type, we could also label the points accordingly.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)\nlegend(\"topright\", c(\"Automatic\", \"Manual\"), col = c(1, 2), pch = c(1, 2))\n```\n\n::: {.cell-output-display}\n![](07b_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\nWe now fit the SLR model\n\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\epsilon,\n\\]\n\nwhere $Y$ is `mpg` and $x_1$ is `hp`. For notational brevity, we drop the index $i$ for observations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpg_hp_slr = lm(mpg ~ hp, data = mtcars)\n```\n:::\n\n\n\n\nWe then re-plot the data and add the fitted line to the plot.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)\nabline(mpg_hp_slr, lwd = 3, col = \"grey\")\nlegend(\"topright\", c(\"Automatic\", \"Manual\"), col = c(1, 2), pch = c(1, 2))\n```\n\n::: {.cell-output-display}\n![](07b_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\nWe should notice a pattern here. The red, manual observations largely fall above the line, while the black, automatic observations are mostly below the line. This means our model underestimates the fuel efficiency of manual transmissions, and overestimates the fuel efficiency of automatic transmissions. To correct for this, we will add a predictor to our model, namely, `am` as $x_2$.\n\nOur new model is\n\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\n\\]\n\nwhere $x_1$ and $Y$ remain the same, but now\n\n\\[\nx_2 =\n  \\begin{cases}\n   1 & \\text{manual transmission} \\\\\n   0       & \\text{automatic transmission}\n  \\end{cases}.\n\\]\n\nIn this case, we call $x_2$ a **dummy variable**. A dummy variable (also called an \"indicator\" variable) is somewhat unfortunately named, as it is in no way \"dumb\". In fact, it is actually somewhat clever. A dummy variable is a numerical variable that is used in a regression analysis to \"code\" for a binary categorical variable. Let's see how this works.\n\nFirst, note that `am` is already a dummy variable, since it uses the values `0` and `1` to represent automatic and manual transmissions. Often, a variable like `am` would store the character values `auto` and `man` and we would either have to convert these to `0` and `1`, or, as we will see later, `R` will take care of creating dummy variables for us.\n\nSo, to fit the above model, we do so like any other multiple regression model we have seen before.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpg_hp_add = lm(mpg ~ hp + am, data = mtcars)\n```\n:::\n\n\n\n\nBriefly checking the output, we see that `R` has estimated the three $\\beta$ parameters.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpg_hp_add\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ hp + am, data = mtcars)\n\nCoefficients:\n(Intercept)           hp           am  \n   26.58491     -0.05889      5.27709  \n```\n\n\n:::\n:::\n\n\n\n\nSince $x_2$ can only take values `0` and `1`, we can effectively write two different models, one for manual and one for automatic transmissions.\n\nFor automatic transmissions, that is $x_2 = 0$, we have,\n\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\epsilon.\n\\]\n\nThen for manual transmissions, that is $x_2 = 1$, we have,\n\n\\[\nY = (\\beta_0 + \\beta_2) + \\beta_1 x_1 + \\epsilon.\n\\]\n\nNotice that these models share the same slope, $\\beta_1$, but have different intercepts, differing by $\\beta_2$. So the change in `mpg` is the same for both models, but on average `mpg` differs by $\\beta_2$ between the two transmission types.\n\nWe'll now calculate the estimated slope and intercept of these two models so that we can add them to a plot. Note that:\n\n- $\\hat{\\beta}_0$ = `coef(mpg_hp_add)[1]` = 26.5849137\n- $\\hat{\\beta}_1$ = `coef(mpg_hp_add)[2]` = -0.0588878\n- $\\hat{\\beta}_2$ = `coef(mpg_hp_add)[3]` = 5.2770853\n\nWe can then combine these to calculate the estimated slope and intercepts.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nint_auto = coef(mpg_hp_add)[1]\nint_manu = coef(mpg_hp_add)[1] + coef(mpg_hp_add)[3]\n\nslope_auto = coef(mpg_hp_add)[2]\nslope_manu = coef(mpg_hp_add)[2]\n```\n:::\n\n\n\n\nRe-plotting the data, we use these slopes and intercepts to add the \"two\" fitted models to the plot.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)\nabline(int_auto, slope_auto, col = 1, lty = 1, lwd = 2) # add line for auto\nabline(int_manu, slope_manu, col = 2, lty = 2, lwd = 2) # add line for manual\nlegend(\"topright\", c(\"Automatic\", \"Manual\"), col = c(1, 2), pch = c(1, 2))\n```\n\n::: {.cell-output-display}\n![](07b_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\nWe notice right away that the points are no longer systematically incorrect. The red, manual observations vary about the red line in no particular pattern without underestimating the observations as before. The black, automatic points vary about the black line, also without an obvious pattern.\n\nThey say a picture is worth a thousand words, but as a statistician, sometimes a picture is worth an entire analysis. The above picture makes it plainly obvious that $\\beta_2$ is significant, but let's verify mathematically. Essentially we would like to test:\n\n\\[\nH_0: \\beta_2 = 0 \\quad \\text{vs} \\quad H_1: \\beta_2 \\neq 0.\n\\]\n\nThis is nothing new. Again, the math is the same as the multiple regression analyses we have seen before. We could perform either a $t$ or $F$ test here. The only difference is a slight change in interpretation. We could think of this as testing a model with a single line ($H_0$) against a model that allows two lines ($H_1$).\n\nTo obtain the test statistic and p-value for the $t$-test, we would use\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mpg_hp_add)$coefficients[\"am\",]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Estimate   Std. Error      t value     Pr(>|t|) \n5.277085e+00 1.079541e+00 4.888270e+00 3.460318e-05 \n```\n\n\n:::\n:::\n\n\n\n\nTo do the same for the $F$ test, we would use a call to `anova`. One way of comparing the fit of models (which accounts for the fact that one model is more \"flexible\" than another) is to use `anova`. The null hypothesis of the `anova` is that the two models explain the same amount of variation in the dependent variable (`mpg`). If we reject the F-test in the `anova` (if the p-value is small), then the more flexible model is actually explaining more.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(mpg_hp_slr, mpg_hp_add)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: mpg ~ hp\nModel 2: mpg ~ hp + am\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)    \n1     30 447.67                                 \n2     29 245.44  1    202.24 23.895 3.46e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\nNotice that these are indeed testing the same thing, as the p-values are exactly equal. (And the $F$ test statistic is the $t$ test statistic squared.) This is only the case for different models that differ only by one additional variable. \n\nRecapping some interpretations:\n\n- $\\hat{\\beta}_0 = 26.5849137$ is the estimated average `mpg` for a car with an automatic transmission and **0** `hp`.\n- $\\hat{\\beta}_0 + \\hat{\\beta}_2 = 31.8619991$ is the estimated average `mpg` for a car with a manual transmission and **0** `hp`.\n\n- $\\hat{\\beta}_2 = 5.2770853$ is the estimated **difference** in average `mpg` for cars with manual transmissions as compared to those with automatic transmission, for **any** `hp`.\n- $\\hat{\\beta}_1 = -0.0588878$ is the estimated change in average `mpg` for an increase in one `hp`, for **either** transmission types.\n\nWe should take special notice of those last two. In the model,\n\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\n\\]\n\nwe see $\\beta_1$ is the average change in $Y$ for an increase in $x_1$, *no matter* the value of $x_2$. Also, $\\beta_2$ is always the difference in the average of $Y$ for *any* value of $x_1$. These are two restrictions we won't always want, so we need a way to specify a more flexible model.\n\nHere we restricted ourselves to a single numerical predictor $x_1$ and one dummy variable $x_2$. However, the concept of a dummy variable can be used with larger multiple regression models. We only use a single numerical predictor here for ease of visualization since we can think of the \"two lines\" interpretation. But in general, we can think of a dummy variable as creating \"two models,\" one for each category of a binary categorical variable.\n\n## Interactions\n\nTo remove the \"same slope\" restriction, we will now discuss **interactions**. To illustrate this concept, we will use the `autompg` dataset with a few modifications.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read data frame from the web\nautompg = read.table(\n  \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\",\n  quote = \"\\\"\",\n  comment.char = \"\",\n  stringsAsFactors = FALSE)\n# give the dataframe headers\ncolnames(autompg) = c(\"mpg\", \"cyl\", \"disp\", \"hp\", \"wt\", \"acc\", \"year\", \"origin\", \"name\")\n\n\nautompg = autompg %>%\n  filter(hp !='?' & name !='plymouth reliant') %>% # the reliant causes some issues\n  mutate(hp = as.numeric(hp),\n         domestic = as.numeric(origin==1)) %>%\n  filter(!cyl %in% c(5,3)) %>%\n  mutate(cyl = as.factor(cyl))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(autompg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t383 obs. of  10 variables:\n $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...\n $ cyl     : Factor w/ 3 levels \"4\",\"6\",\"8\": 3 3 3 3 3 3 3 3 3 3 ...\n $ disp    : num  307 350 318 304 302 429 454 440 455 390 ...\n $ hp      : num  130 165 150 150 140 198 220 215 225 190 ...\n $ wt      : num  3504 3693 3436 3433 3449 ...\n $ acc     : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n $ year    : int  70 70 70 70 70 70 70 70 70 70 ...\n $ origin  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ name    : chr  \"chevrolet chevelle malibu\" \"buick skylark 320\" \"plymouth satellite\" \"amc rebel sst\" ...\n $ domestic: num  1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n\n\n\nWe've removed cars with `3` and `5` cylinders , as well as created a new variable `domestic` which indicates whether or not a car was built in the United States. Removing the `3` and `5` cylinders is simply for ease of demonstration later in the chapter and would not be done in practice. The new variable `domestic` takes the value `1` if the car was built in the United States, and `0` otherwise, which we will refer to as \"foreign.\" (We are arbitrarily using the United States as the reference point here.) We have also made `cyl` and `origin` into factor variables, which we will discuss later.\n\nWe'll now be concerned with three variables: `mpg`, `disp`, and `domestic`. We will use `mpg` as the response. We can fit a model,\n\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\n\\]\n\nwhere\n\n- $Y$ is `mpg`, the fuel efficiency in miles per gallon,\n- $x_1$ is `disp`, the displacement in cubic inches,\n- $x_2$ is `domestic` as described above, which is a dummy variable.\n\n\\[\nx_2 =\n  \\begin{cases}\n   1 & \\text{Domestic} \\\\\n   0 & \\text{Foreign}\n  \\end{cases}\n\\]\n\nWe will fit this model, extract the slope and intercept for the \"two lines,\" plot the data and add the lines.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpg_disp_add = lm(mpg ~ disp + as.factor(domestic), data = autompg)\n\nint_for = coef(mpg_disp_add)[1]\nint_dom = coef(mpg_disp_add)[1] + coef(mpg_disp_add)[3]\n\nslope_for = coef(mpg_disp_add)[2]\nslope_dom = coef(mpg_disp_add)[2] #--> same slope!\n\nggplot(autompg, aes(x = disp, y = mpg, col = as.factor(domestic))) + \n  geom_point() +\n  geom_abline(intercept = int_dom, \n              slope = slope_dom, col = 'blue') +\n  geom_abline(intercept = int_for,\n              slope = slope_for, col = 'red') +\n  labs(color = 'Origin') +\n  scale_color_manual(labels = c('Foreign','Domestic'), values = c('red','blue')) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](07b_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\nThis is a model that allows for two *parallel* lines, meaning the `mpg` can be different on average between foreign and domestic cars of the same engine displacement, but the change in average `mpg` for an increase in displacement is the same for both. We can see this model isn't doing very well here. The red line fits the red points fairly well, but the black line isn't doing very well for the black points, it should clearly have a more negative slope. Essentially, we would like a model that allows for two different slopes.\n\nConsider the following model,\n\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\n\\]\n\nwhere $x_1$, $x_2$, and $Y$ are the same as before, but we have added a new **interaction** term $x_1 x_2$ which multiplies $x_1$ and $x_2$, so we also have an additional $\\beta$ parameter $\\beta_3$.\n\nThis model essentially creates two slopes and two intercepts, $\\beta_2$ being the difference in intercepts and $\\beta_3$ being the difference in slopes. To see this, we will break down the model into the two \"sub-models\" for foreign and domestic cars.\n\nFor foreign cars, that is $x_2 = 0$, we have\n\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\epsilon.\n\\]\n\nFor domestic cars, that is $x_2 = 1$, we have\n\n\\[\nY = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) x_1 + \\epsilon.\n\\]\n\nThese two models have both different slopes and intercepts.\n\n- $\\beta_0$ is the average `mpg` for a foreign car with **0** `disp`.\n- $\\beta_1$ is the change in average `mpg` for an increase of one `disp`, for **foreign** cars.\n- $\\beta_0 + \\beta_2$ is the average `mpg` for a domestic car with **0** `disp`.\n- $\\beta_1 + \\beta_3$ is the change in average `mpg` for an increase of one `disp`, for **domestic** cars.\n\nHow do we fit this model in `R`? There are a number of ways.\n\nOne method would be to simply create a new variable, then fit a model like any other.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautompg$x3 = autompg$disp * autompg$domestic # THIS CODE NOT RUN!\ndo_not_do_this = lm(mpg ~ disp + domestic + x3, data = autompg) # THIS CODE NOT RUN!\n```\n:::\n\n\n\n\nYou should only do this as a last resort. We greatly prefer not to have to modify our data simply to fit a model. Instead, we can tell `R` we would like to use the existing data with an interaction term, which it will create automatically when we use the `:` operator.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpg_disp_int = lm(mpg ~ disp + domestic + disp:domestic, data = autompg)\n```\n:::\n\n\n\n\nAn alternative method, which will fit the exact same model as above would be to use the `*` operator. This method automatically creates the interaction term, as well as any \"lower order terms,\" which in this case are the first order terms for `disp` and `domestic`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpg_disp_int2 = lm(mpg ~ disp * domestic, data = autompg)\n```\n:::\n\n\n\n\nWe can quickly verify that these are doing the same thing.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(mpg_disp_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)          disp      domestic disp:domestic \n   46.0548423    -0.1569239   -12.5754714     0.1025184 \n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(mpg_disp_int2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)          disp      domestic disp:domestic \n   46.0548423    -0.1569239   -12.5754714     0.1025184 \n```\n\n\n:::\n:::\n\n\n\n\nWe see that both the variables, and their coefficient estimates are indeed the same for both models.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mpg_disp_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ disp + domestic + disp:domestic, data = autompg)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8332  -2.8956  -0.8332   2.2828  18.7749 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    46.05484    1.80582  25.504  < 2e-16 ***\ndisp           -0.15692    0.01668  -9.407  < 2e-16 ***\ndomestic      -12.57547    1.95644  -6.428 3.90e-10 ***\ndisp:domestic   0.10252    0.01692   6.060 3.29e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.308 on 379 degrees of freedom\nMultiple R-squared:  0.7011,\tAdjusted R-squared:  0.6987 \nF-statistic: 296.3 on 3 and 379 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\nWe see that using `summary()` gives the usual output for a multiple regression model. We pay close attention to the row for `disp:domestic` which tests,\n\n\\[\nH_0: \\beta_3 = 0.\n\\]\n\nIn this case, testing for $\\beta_3 = 0$ is testing for two lines with parallel slopes versus two lines with possibly different slopes. The `disp:domestic` line in the `summary()` output uses a $t$-test to perform the test.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nint_for = coef(mpg_disp_int)[1]\nint_dom = coef(mpg_disp_int)[1] + coef(mpg_disp_int)[3]\n\nslope_for = coef(mpg_disp_int)[2]\nslope_dom = coef(mpg_disp_int)[2] + coef(mpg_disp_int)[4]\n```\n:::\n\n\n\n\nHere we again calculate the slope and intercepts for the two lines for use in plotting.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(autompg, aes(x = disp, y = mpg, col = as.factor(domestic))) + \n  geom_point() +\n  geom_abline(intercept = int_dom, \n              slope = slope_dom, col = 'blue') +\n  geom_abline(intercept = int_for,\n              slope = slope_for, col = 'red') +\n  labs(color = 'Origin') +\n  scale_color_manual(labels = c('Foreign','Domestic'), values = c('red','blue')) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](07b_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n\nWe see that these lines fit the data much better, which matches the result of our tests.\n\nSo far we have only seen interaction between a categorical variable (`domestic`) and a numerical variable (`disp`). While this is easy to visualize, since it allows for different slopes for two lines, it is not the only type of interaction we can use in a model. We can also consider interactions between two numerical variables.\n\nConsider the model,\n\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\n\\]\n\nwhere\n\n- $Y$ is `mpg`, the fuel efficiency in miles per gallon,\n- $x_1$ is `disp`, the displacement in cubic inches,\n- $x_2$ is `hp`, the horsepower, in foot-pounds per second.\n\nHow does `mpg` change based on `disp` in this model? We can rearrange some terms to see how.\n\n\\[\nY = \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon\n\\]\n\nSo, for a one unit increase in $x_1$ (`disp`), the mean of $Y$ (`mpg`) increases $\\beta_1 + \\beta_3 x_2$, which is a different value depending on the value of $x_2$ (`hp`)!\n\nSince we're now working in three dimensions, this model can't be easily justified via visualizations like the previous example. Instead, we will have to rely on a test.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpg_disp_add_hp = lm(mpg ~ disp + hp, data = autompg)\nmpg_disp_int_hp = lm(mpg ~ disp * hp, data = autompg)\nsummary(mpg_disp_int_hp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ disp * hp, data = autompg)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7849  -2.3104  -0.5699   2.1453  17.9211 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.241e+01  1.523e+00   34.42   <2e-16 ***\ndisp        -1.002e-01  6.638e-03  -15.09   <2e-16 ***\nhp          -2.198e-01  1.987e-02  -11.06   <2e-16 ***\ndisp:hp      5.658e-04  5.165e-05   10.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.896 on 379 degrees of freedom\nMultiple R-squared:  0.7554,\tAdjusted R-squared:  0.7535 \nF-statistic: 390.2 on 3 and 379 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\nUsing `summary()` we focus on the row for `disp:hp` which tests,\n\n\\[\nH_0: \\beta_3 = 0.\n\\]\n\nAgain, we see a very low p-value so we reject the null (additive model) in favor of the interaction model.\nWe can take a closer look at the coefficients of our fitted interaction model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(mpg_disp_int_hp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)          disp            hp       disp:hp \n52.4081997848 -0.1001737655 -0.2198199720  0.0005658269 \n```\n\n\n:::\n:::\n\n\n\n\n- $\\hat{\\beta}_0 = 52.4081998$ is the estimated average `mpg` for a car with 0 `disp` and 0 `hp`.\n- $\\hat{\\beta}_1 = -0.1001738$ is the estimated change in average `mpg` for an increase in 1 `disp`, **for a car with 0 `hp`**.\n- $\\hat{\\beta}_2 = -0.21982$ is the estimated change in average `mpg` for an increase in 1 `hp`, **for a car with 0 `disp`**.\n- $\\hat{\\beta}_3 = 5.658269\\times 10^{-4}$ is an estimate of the modification to the change in average `mpg` for an increase in `disp`, for a car of a certain `hp` (or vice versa).\n\nThat last coefficient needs further explanation. Recall the rearrangement we made earlier\n\n\\[\nY = \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon.\n\\]\n\nSo, our estimate for $\\beta_1 + \\beta_3 x_2$, is $\\hat{\\beta}_1 + \\hat{\\beta}_3 x_2$, which in this case is\n\n\\[\n-0.1001738 + 5.658269\\times 10^{-4} x_2.\n\\]\n\nThis says that, for an increase of one `disp` we see an estimated change in average `mpg` of $-0.1001738 + 5.658269\\times 10^{-4} x_2$. So how `disp` and `mpg` are related, depends on the `hp` of the car.\n\nSo for a car with 50 `hp`, the estimated change in average `mpg` for an increase of one `disp` is\n\n\\[\n-0.1001738 + 5.658269\\times 10^{-4} \\cdot 50 = -0.0718824\n\\]\n\nAnd for a car with 350 `hp`, the estimated change in average `mpg` for an increase of one `disp` is\n\n\\[\n-0.1001738 + 5.658269\\times 10^{-4} \\cdot 350 = 0.0978657\n\\]\n\nNotice the sign changed!\n\n## Factor Variables\n\nSo far in this chapter, we have limited our use of categorical variables to binary categorical variables. Specifically, we have limited ourselves to dummy variables which take a value of `0` or `1` and represent a categorical variable numerically.\n\nWe will now discuss **factor** variables, which is a special way that `R` deals with categorical variables. With factor variables, a human user can simply think about the categories of a variable, and `R` will take care of the necessary dummy variables without any 0/1 assignment being done by the user.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis.factor(autompg$domestic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\n\nEarlier when we used the `domestic` variable, it was **not** a factor variable. It was simply a numerical variable that only took two possible values, `1` for domestic, and `0` for foreign. Let's create a new variable `origin` that stores the same information, but in a different way. First, we create an empty (all-`NA`) variable of type `character`. Then, we update it. Yes, we could also do this with `ifelse` or `case_when`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautompg$origin = as.character(NA)\nautompg$origin[autompg$domestic == 1] = \"domestic\"\nautompg$origin[autompg$domestic == 0] = \"foreign\"\nhead(autompg$origin)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"domestic\" \"domestic\" \"domestic\" \"domestic\" \"domestic\" \"domestic\"\n```\n\n\n:::\n:::\n\n\n\n\nNow the `origin` variable stores `\"domestic\"` for domestic cars and `\"foreign\"` for foreign cars.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis.factor(autompg$origin)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\n\nHowever, this is simply a vector of character values. A vector of car models is a character variable in `R`. A vector of Vehicle Identification Numbers (VINs) is a character variable as well. But those don't represent a short list of levels that might influence a response variable. We will want to **coerce** this origin variable to be something more: a factor variable.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautompg$origin = as.factor(autompg$origin)\n```\n:::\n\n\n\n\nNow when we check the structure of the `autompg` dataset, we see that `origin` is a factor variable.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(autompg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t383 obs. of  10 variables:\n $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...\n $ cyl     : Factor w/ 3 levels \"4\",\"6\",\"8\": 3 3 3 3 3 3 3 3 3 3 ...\n $ disp    : num  307 350 318 304 302 429 454 440 455 390 ...\n $ hp      : num  130 165 150 150 140 198 220 215 225 190 ...\n $ wt      : num  3504 3693 3436 3433 3449 ...\n $ acc     : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n $ year    : int  70 70 70 70 70 70 70 70 70 70 ...\n $ origin  : Factor w/ 2 levels \"domestic\",\"foreign\": 1 1 1 1 1 1 1 1 1 1 ...\n $ name    : chr  \"chevrolet chevelle malibu\" \"buick skylark 320\" \"plymouth satellite\" \"amc rebel sst\" ...\n $ domestic: num  1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n\n\n\nFactor variables have **levels** which are the possible values (categories) that the variable may take, in this case foreign or domestic.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlevels(autompg$origin)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"domestic\" \"foreign\" \n```\n\n\n:::\n:::\n\n\n\n\nRecall that previously we have fit the model\n\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\n\\]\n\nwhere\n\n- $Y$ is `mpg`, the fuel efficiency in miles per gallon,\n- $x_1$ is `disp`, the displacement in cubic inches,\n- $x_2$ is `domestic` a dummy variable where `1` indicates a domestic car.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(mod_dummy = lm(mpg ~ disp * domestic, data = autompg))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ disp * domestic, data = autompg)\n\nCoefficients:\n  (Intercept)           disp       domestic  disp:domestic  \n      46.0548        -0.1569       -12.5755         0.1025  \n```\n\n\n:::\n:::\n\n\n\n\nSo here we see that\n\n\\[\n\\hat{\\beta}_0 + \\hat{\\beta}_2 = 46.0548423 + -12.5754714 = 33.4793709\n\\]\n\nis the estimated average `mpg` for a **domestic** car with 0 `disp`.\n\nNow let's try to do the same, but using our new factor variable.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(mod_factor = lm(mpg ~ disp * origin, data = autompg))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ disp * origin, data = autompg)\n\nCoefficients:\n       (Intercept)                disp       originforeign  disp:originforeign  \n          33.47937            -0.05441            12.57547            -0.10252  \n```\n\n\n:::\n:::\n\n\n\n\nIt seems that it doesn't produce the same results. Right away we notice that the intercept is different, as is the the coefficient in front of `disp`. We also notice that the remaining two coefficients are of the same magnitude as their respective counterparts using the domestic variable, but with a different sign. Why is this happening?\n\nIt turns out, that by using a factor variable, `R` is automatically creating a dummy variable for us. However, it is not the dummy variable that we had originally used ourselves.\n\n`R` is fitting the model\n\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\n\\]\n\nwhere\n\n- $Y$ is `mpg`, the fuel efficiency in miles per gallon,\n- $x_1$ is `disp`, the displacement in cubic inches,\n- $x_2$ **is a dummy variable created by `R`.** It uses `1` to represent a **foreign car**.\n\nYou may recall that we saw this in action in Assignment 05 when we used `model.matrix` on `GarageType`.\n\nSo now,\n\n\\[\n\\hat{\\beta}_0 = 33.4793709\n\\]\n\nis the estimated average `mpg` for a **domestic** car with 0 `disp`, which is indeed the same as before.\n\nWhen `R` created $x_2$, the dummy variable, it used domestic cars as the **reference** level, that is the default value of the factor variable. So when the dummy variable is `0`, the model represents this reference level, which is domestic. (`R` makes this choice because domestic comes before foreign alphabetically.)\n\nSo the two models have different estimated coefficients, but due to the different model representations, they are actually the same model.\n\n### Factors with More Than Two Levels\n\nLet's now consider a factor variable with more than two levels. In this dataset, `cyl` is an example.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis.factor(autompg$cyl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nlevels(autompg$cyl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"4\" \"6\" \"8\"\n```\n\n\n:::\n:::\n\n\n\n\nHere the `cyl` variable has three possible levels: `4`, `6`, and `8`. You may wonder, why not simply use `cyl` as a numerical variable? You certainly could.\n\nHowever, that would force the difference in average `mpg` between `4` and `6` cylinders to be the same as the difference in average mpg between `6` and `8` cylinders. That usually make senses for a continuous variable, but not for a discrete variable with so few possible values. In the case of this variable, there is no such thing as a 7-cylinder engine or a 6.23-cylinder engine in personal vehicles. For these reasons, we will simply consider `cyl` to be categorical. This is a decision that will commonly need to be made with ordinal variables. Often, with a large number of categories, the decision to treat them as numerical variables is appropriate because, otherwise, a large number of dummy variables are then needed to represent these variables.\n\nLet's define three dummy variables related to the `cyl` factor variable.\n\n\\[\nv_1 =\n  \\begin{cases}\n   1 & \\text{4 cylinder} \\\\\n   0       & \\text{not 4 cylinder}\n  \\end{cases}\n\\]\n\n\\[\nv_2 =\n  \\begin{cases}\n   1 & \\text{6 cylinder} \\\\\n   0       & \\text{not 6 cylinder}\n  \\end{cases}\n\\]\n\n\\[\nv_3 =\n  \\begin{cases}\n   1 & \\text{8 cylinder} \\\\\n   0       & \\text{not 8 cylinder}\n  \\end{cases}\n\\]\n\nNow, let's fit an additive model in `R`, using `mpg` as the response, and `disp` and `cyl` as predictors. This should be a model that uses \"three regression lines\" to model `mpg`, one for each of the possible `cyl` levels. They will all have the same slope (since it is an additive model), but each will have its own intercept.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(mpg_disp_add_cyl = lm(mpg ~ disp + cyl, data = autompg))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ disp + cyl, data = autompg)\n\nCoefficients:\n(Intercept)         disp         cyl6         cyl8  \n   34.99929     -0.05217     -3.63325     -2.03603  \n```\n\n\n:::\n:::\n\n\n\n\nThe question is, what is the model that `R` has fit here? It has chosen to use the model\n\n\\[\nY = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon,\n\\]\n\nwhere\n\n- $Y$ is `mpg`, the fuel efficiency in miles per gallon,\n- $x$ is `disp`, the displacement in cubic inches,\n- $v_2$ and $v_3$ are the dummy variables define above.\n\nWhy doesn't `R` use $v_1$? Essentially because it doesn't need to. To create three lines, it only needs two dummy variables since it is using a reference level, which in this case is a 4 cylinder car. The three \"sub models\" are then:\n\n- 4 Cylinder: $Y = \\beta_0 + \\beta_1 x + \\epsilon$\n- 6 Cylinder: $Y = (\\beta_0 + \\beta_2) + \\beta_1 x + \\epsilon$\n- 8 Cylinder: $Y = (\\beta_0 + \\beta_3) + \\beta_1 x + \\epsilon$\n\nNotice that they all have the same slope. However, using the two dummy variables, we achieve the three intercepts.\n\n- $\\beta_0$ is the average `mpg` for a 4 cylinder car with 0 `disp`.\n- $\\beta_0 + \\beta_2$ is the average `mpg` for a 6 cylinder car with 0 `disp`.\n- $\\beta_0 + \\beta_3$ is the average `mpg` for a 8 cylinder car with 0 `disp`.\n\nSo because 4 cylinder is the reference level, $\\beta_0$ is specific to 4 cylinders, but $\\beta_2$ and $\\beta_3$ are used to represent quantities relative to 4 cylinders.\n\nAs we have done before, we can extract these intercepts and slopes for the three lines, and plot them accordingly.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nint_4cyl = coef(mpg_disp_add_cyl)[1]\nint_6cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[3]\nint_8cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[4]\n\nslope_all_cyl = coef(mpg_disp_add_cyl)[2]\n\nplot_colors = c(\"Darkorange\", \"Darkgrey\", \"Dodgerblue\")\nplot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))\nabline(int_4cyl, slope_all_cyl, col = plot_colors[1], lty = 1, lwd = 2)\nabline(int_6cyl, slope_all_cyl, col = plot_colors[2], lty = 2, lwd = 2)\nabline(int_8cyl, slope_all_cyl, col = plot_colors[3], lty = 3, lwd = 2)\nlegend(\"topright\", c(\"4 Cylinder\", \"6 Cylinder\", \"8 Cylinder\"),\n       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))\n```\n\n::: {.cell-output-display}\n![](07b_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\n\nOn this plot, we have\n\n- 4 Cylinder: orange dots, solid orange line.\n- 6 Cylinder: grey dots, dashed grey line.\n- 8 Cylinder: blue dots, dotted blue line.\n\nThe odd result here is that we're estimating that 8 cylinder cars have better fuel efficiency than 6 cylinder cars at **any** displacement! The dotted blue line is always above the dashed grey line. That doesn't seem right. Maybe for very large displacement engines that could be true, but that seems wrong for medium to low displacement.\n\nTo attempt to fix this, we will try using an interaction model, that is, instead of simply three intercepts and one slope, we will allow for three slopes. Again, we'll let `R` take the wheel, (no pun intended) then figure out what model it has applied.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(mpg_disp_int_cyl = lm(mpg ~ disp * cyl, data = autompg))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ disp * cyl, data = autompg)\n\nCoefficients:\n(Intercept)         disp         cyl6         cyl8    disp:cyl6    disp:cyl8  \n   43.59052     -0.13069    -13.20026    -20.85706      0.08299      0.10817  \n```\n\n\n:::\n\n```{.r .cell-code}\n# could also use mpg ~ disp + cyl + disp:cyl\n```\n:::\n\n\n\n\n`R` has again chosen to use 4 cylinder cars as the reference level, but this also now has an effect on the interaction terms. `R` has fit the model.\n\n\\[\nY = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\gamma_2 x v_2 + \\gamma_3 x v_3 + \\epsilon\n\\]\n\nWe're using $\\gamma$ like a $\\beta$ parameter for simplicity, so that, for example $\\beta_2$ and $\\gamma_2$ are both associated with $v_2$.\n\nNow, the three \"sub models\" are:\n\n- 4 Cylinder: $Y = \\beta_0 + \\beta_1 x + \\epsilon$.\n- 6 Cylinder: $Y = (\\beta_0 + \\beta_2) + (\\beta_1 + \\gamma_2) x + \\epsilon$.\n- 8 Cylinder: $Y = (\\beta_0 + \\beta_3) + (\\beta_1 + \\gamma_3) x + \\epsilon$.\n\nInterpreting some parameters and coefficients then:\n\n- $(\\beta_0 + \\beta_2)$ is the average `mpg` of a 6 cylinder car with 0 `disp`\n- $(\\hat{\\beta}_1 + \\hat{\\gamma}_3) = -0.1306935 + 0.1081714 = -0.0225221$ is the estimated change in average `mpg` for an increase of one `disp`, for an 8 cylinder car.\n\nSo, as we have seen before $\\beta_2$ and $\\beta_3$ change the intercepts for 6 and 8 cylinder cars relative to the reference level of $\\beta_0$ for 4 cylinder cars.\n\nNow, similarly $\\gamma_2$ and $\\gamma_3$ change the slopes for 6 and 8 cylinder cars relative to the reference level of $\\beta_1$ for 4 cylinder cars.\n\nOnce again, we extract the coefficients and plot the results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nint_4cyl = coef(mpg_disp_int_cyl)[1]\nint_6cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[3]\nint_8cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[4]\n\nslope_4cyl = coef(mpg_disp_int_cyl)[2]\nslope_6cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[5]\nslope_8cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[6]\n\nplot_colors = c(\"Darkorange\", \"Darkgrey\", \"Dodgerblue\")\nplot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))\nabline(int_4cyl, slope_4cyl, col = plot_colors[1], lty = 1, lwd = 2)\nabline(int_6cyl, slope_6cyl, col = plot_colors[2], lty = 2, lwd = 2)\nabline(int_8cyl, slope_8cyl, col = plot_colors[3], lty = 3, lwd = 2)\nlegend(\"topright\", c(\"4 Cylinder\", \"6 Cylinder\", \"8 Cylinder\"),\n       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))\n```\n\n::: {.cell-output-display}\n![](07b_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n\n\nThis looks much better! We can see that for medium displacement cars, 6 cylinder cars now perform better than 8 cylinder cars, which seems much more reasonable than before.\n\nTo completely justify the interaction model (i.e., a unique slope for each `cyl` level) compared to the additive model (single slope), we can perform an $F$-test. Notice first, that there is no $t$-test that will be able to do this since the difference between the two models is not a single parameter.\n\nWe will test,\n\n\\[\nH_0: \\gamma_2 = \\gamma_3 = 0\n\\]\n\nwhich represents the parallel regression lines we saw before,\n\n\\[\nY = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon.\n\\]\n\nAgain, this is a difference of two parameters, thus no $t$-test will be useful.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(mpg_disp_add_cyl, mpg_disp_int_cyl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + cyl\nModel 2: mpg ~ disp * cyl\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1    379 7299.5                                  \n2    377 6551.7  2    747.79 21.515 1.419e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\nAs expected, we see a very low p-value, and thus reject the null. We prefer the interaction model over the additive model.\n\nRecapping a bit:\n\n- Null Model: $Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon$\n    - Number of parameters: $q = 4$\n- Full Model: $Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\gamma_2 x v_2 + \\gamma_3 x v_3 + \\epsilon$\n    - Number of parameters: $p = 6$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(coef(mpg_disp_int_cyl)) - length(coef(mpg_disp_add_cyl))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n:::\n\n\n\n\nWe see there is a difference of two parameters, which is also displayed in the resulting ANOVA table from `R`. Notice that the following two values also appear on the ANOVA table.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(autompg) - length(coef(mpg_disp_int_cyl))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 377\n```\n\n\n:::\n\n```{.r .cell-code}\nnrow(autompg) - length(coef(mpg_disp_add_cyl))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 379\n```\n\n\n:::\n:::\n\n\n\n\n## Parameterization\n\nSo far we have been simply letting `R` decide how to create the dummy variables, and thus `R` has been deciding the parameterization of the models. To illustrate the ability to use alternative parameterizations, we will recreate the data, but directly creating the dummy variables ourselves.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_param_data = data.frame(\n  y = autompg$mpg,\n  x = autompg$disp,\n  v1 = 1 * as.numeric(autompg$cyl == 4),\n  v2 = 1 * as.numeric(autompg$cyl == 6),\n  v3 = 1 * as.numeric(autompg$cyl == 8))\n\nhead(new_param_data, 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    y   x v1 v2 v3\n1  18 307  0  0  1\n2  15 350  0  0  1\n3  18 318  0  0  1\n4  16 304  0  0  1\n5  17 302  0  0  1\n6  15 429  0  0  1\n7  14 454  0  0  1\n8  14 440  0  0  1\n9  14 455  0  0  1\n10 15 390  0  0  1\n11 15 383  0  0  1\n12 14 340  0  0  1\n13 15 400  0  0  1\n14 14 455  0  0  1\n15 24 113  1  0  0\n16 22 198  0  1  0\n17 18 199  0  1  0\n18 21 200  0  1  0\n19 27  97  1  0  0\n20 26  97  1  0  0\n```\n\n\n:::\n:::\n\n\n\n\nNow,\n\n- `y` is `mpg`\n- `x` is `disp`, the displacement in cubic inches,\n- `v1`, `v2`, and `v3` are dummy variables as defined above.\n\nFirst let's try to fit an additive model using `x` as well as the three dummy variables.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y ~ x + v1 + v2 + v3, data = new_param_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x + v1 + v2 + v3, data = new_param_data)\n\nCoefficients:\n(Intercept)            x           v1           v2           v3  \n   32.96326     -0.05217      2.03603     -1.59722           NA  \n```\n\n\n:::\n:::\n\n\n\n\nWhat is happening here? Notice that `R` is essentially ignoring `v3`, but why? Well, because `R` uses an intercept, it cannot also use `v3`. This is because\n\n\\[\n\\boldsymbol{1} = v_1 + v_2 + v_3\n\\]\n\nwhich means that $\\boldsymbol{1}$, $v_1$, $v_2$, and $v_3$ are linearly dependent. This would make the $X^\\top X$ matrix singular, but we need to be able to invert it to solve the normal equations and obtain $\\hat{\\beta}.$ With the intercept, `v1`, and `v2`, `R` can make the necessary \"three intercepts\". So, in this case `v3` is the reference level.\n\nIf we remove the intercept, then we can directly obtain all \"three intercepts\" without a reference level.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y ~ 0 + x + v1 + v2 + v3, data = new_param_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ 0 + x + v1 + v2 + v3, data = new_param_data)\n\nCoefficients:\n       x        v1        v2        v3  \n-0.05217  34.99929  31.36604  32.96326  \n```\n\n\n:::\n:::\n\n\n\n\nHere, we are fitting the model\n\n\\[\nY = \\mu_1 v_1 + \\mu_2 v_2 + \\mu_3 v_3 + \\beta x +\\epsilon.\n\\]\n\nThus we have:\n\n- 4 Cylinder: $Y = \\mu_1 + \\beta x + \\epsilon$\n- 6 Cylinder: $Y = \\mu_2 + \\beta x + \\epsilon$\n- 8 Cylinder: $Y = \\mu_3 + \\beta x + \\epsilon$\n\nWe could also do something similar with the interaction model, and give each line an intercept and slope, without the need for a reference level.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)\n\nCoefficients:\n      v1        v2        v3      v1:x      v2:x      v3:x  \n43.59052  30.39026  22.73346  -0.13069  -0.04770  -0.02252  \n```\n\n\n:::\n:::\n\n\n\n\n\\[\nY = \\mu_1 v_1 + \\mu_2 v_2 + \\mu_3 v_3 + \\beta_1 x v_1 + \\beta_2 x v_2 + \\beta_3 x v_3 +\\epsilon\n\\]\n\n- 4 Cylinder: $Y = \\mu_1 + \\beta_1 x + \\epsilon$\n- 6 Cylinder: $Y = \\mu_2 + \\beta_2 x + \\epsilon$\n- 8 Cylinder: $Y = \\mu_3 + \\beta_3 x + \\epsilon$\n\nUsing the original data, we have (at least) three equivalent ways to specify the interaction model with `R`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(mpg ~ disp * cyl, data = autompg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ disp * cyl, data = autompg)\n\nCoefficients:\n(Intercept)         disp         cyl6         cyl8    disp:cyl6    disp:cyl8  \n   43.59052     -0.13069    -13.20026    -20.85706      0.08299      0.10817  \n```\n\n\n:::\n\n```{.r .cell-code}\nlm(mpg ~ 0 + cyl + disp : cyl, data = autompg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ 0 + cyl + disp:cyl, data = autompg)\n\nCoefficients:\n     cyl4       cyl6       cyl8  cyl4:disp  cyl6:disp  cyl8:disp  \n 43.59052   30.39026   22.73346   -0.13069   -0.04770   -0.02252  \n```\n\n\n:::\n\n```{.r .cell-code}\nlm(mpg ~ 0 + disp + cyl + disp : cyl, data = autompg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ 0 + disp + cyl + disp:cyl, data = autompg)\n\nCoefficients:\n     disp       cyl4       cyl6       cyl8  disp:cyl6  disp:cyl8  \n -0.13069   43.59052   30.39026   22.73346    0.08299    0.10817  \n```\n\n\n:::\n:::\n\n\n\n\nThey all fit the same model, importantly each using six parameters, but the coefficients mean slightly different things in each. However, once they are interpreted as slopes and intercepts for the \"three lines\" they will have the same result.\n\nUse `?all.equal` to learn about the `all.equal()` function, and think about how the following code verifies that the residuals of the two models are the same.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall.equal(fitted(lm(mpg ~ disp * cyl, data = autompg)),\n          fitted(lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n\n\n:::fyi\n\n__TRY IT__\n\nLet's try some interaction terms using our `Ames` data from before (linked at the top of this page). For now, we'll stick to interacting square footage (`GrLivArea`) with dummy or factor variables. As usual, we're trying to predict `SalePrice`.\n\nWhat factor variables should we use? Here are some options:\n\n1. `CentralAir` (binary, but stored as \"Y\"/\"N\")\n2. `Neighborhood` (25 different neighborhoods, stored as `chr`)\n3. `YearBuilt` (not a factor, but `case_when` can be used to make a few bins based on year)\n4. `OverallCond` (subjective 1-9 scale, but stored as an integer -- use `as.factor`!)\n5. `???` Anything else interesting you see?\n\nI'd like you to choose one of the above and run two models: one with `GrLivArea` and your chosen factor variable, and one with `GrLivArea`, your chosen factor variable, *and the interaction of the two*. Once you have run these:\n\n1. Be prepared to interpret the coefficients on the \"baseline\" coefficient on `GrLivArea`, as well as one of the coefficients on the interaction.\n2. Test the two models using `anova(mod1, mod2)` and tell us if your more flexible model is statistically better than the less flexible model.\n\n:::\n\n\n## Building Larger Models\n\nNow that we have seen how to incorporate categorical predictors as well as interaction terms, we can start to build much larger, much more flexible models which can potentially fit data better.\n\nLet's define a \"big\" model,\n\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\beta_7 x_1 x_2 x_3 + \\epsilon.\n\\]\n\nHere,\n\n- $Y$ is `mpg`.\n- $x_1$ is `disp`.\n- $x_2$ is `hp`.\n- $x_3$ is `domestic`, which is a dummy variable we defined, where `1` is a domestic vehicle.\n\nFirst thing to note here, we have included a new term $x_1 x_2 x_3$ which is a three-way interaction. Interaction terms can be larger and larger, up to the number of predictors in the model.\n\nSince we are using the three-way interaction term, we also use all possible two-way interactions, as well as each of the first order (**main effect**) terms. This is the concept of a **hierarchy**. Any time a \"higher-order\" term is in a model, the related \"lower-order\" terms should also be included. Mathematically their inclusion or exclusion is sometimes irrelevant, but from an interpretation standpoint, it is best to follow the hierarchy rules.\n\nLet's do some rearrangement to obtain a \"coefficient\" in front of $x_1$.\n\n\\[\nY = \\beta_0 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_6 x_2 x_3 + (\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3)x_1 + \\epsilon.\n\\]\n\nSpecifically, the \"coefficient\" in front of $x_1$ is\n\n\\[\n(\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3).\n\\]\n\nLet's discuss this \"coefficient\" to help us understand the idea of the *flexibility* of a model. Recall that,\n\n- $\\beta_1$ is the coefficient for a first order term,\n- $\\beta_4$ and $\\beta_5$ are coefficients for two-way interactions,\n- $\\beta_7$ is the coefficient for the three-way interaction.\n\nIf the two and three way interactions were not in the model, the whole \"coefficient\" would simply be\n\n\\[\n\\beta_1.\n\\]\n\nThus, no matter the values of $x_2$ and $x_3$, $\\beta_1$ would determine the relationship between $x_1$ (`disp`) and $Y$ (`mpg`).\n\nWith the addition of the two-way interactions, now the \"coefficient\" would be\n\n\\[\n(\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3).\n\\]\n\nNow, changing $x_1$ (`disp`) has a different effect on $Y$ (`mpg`), depending on the values of $x_2$ and $x_3$.\n\nLastly, adding the three-way interaction gives the whole \"coefficient\"\n\n\\[\n(\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3)\n\\]\n\nwhich is even more flexible. Now changing $x_1$ (`disp`) has a different effect on $Y$ (`mpg`), depending on the values of $x_2$ and $x_3$, but in a more flexible way which we can see with some more rearrangement. Now the \"coefficient\" in front of $x_3$ in this \"coefficient\" is dependent on $x_2$.\n\n\\[\n(\\beta_1 + \\beta_4 x_2 + (\\beta_5 + \\beta_7 x_2) x_3)\n\\]\n\nIt is so flexible, it is becoming hard to interpret!\n\nLet's fit this three-way interaction model in `R`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbig_model = lm(mpg ~ disp * hp * domestic, data = autompg)\nsummary(big_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ disp * hp * domestic, data = autompg)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.9410  -2.2147  -0.4008   1.9430  18.4094 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       6.065e+01  6.600e+00   9.189  < 2e-16 ***\ndisp             -1.416e-01  6.344e-02  -2.232   0.0262 *  \nhp               -3.545e-01  8.123e-02  -4.364 1.65e-05 ***\ndomestic         -1.257e+01  7.064e+00  -1.780   0.0759 .  \ndisp:hp           1.369e-03  6.727e-04   2.035   0.0426 *  \ndisp:domestic     4.933e-02  6.400e-02   0.771   0.4414    \nhp:domestic       1.852e-01  8.709e-02   2.126   0.0342 *  \ndisp:hp:domestic -9.163e-04  6.768e-04  -1.354   0.1766    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.88 on 375 degrees of freedom\nMultiple R-squared:   0.76,\tAdjusted R-squared:  0.7556 \nF-statistic: 169.7 on 7 and 375 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\nDo we actually need this large of a model? Let's first test for the necessity of the three-way interaction term. That is,\n\n\\[\nH_0: \\beta_7 = 0.\n\\]\n\nSo,\n\n- Full Model: $Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\beta_7 x_1 x_2 x_3 + \\epsilon$\n- Null Model: $Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon$\n\nWe fit the null model in `R` as `two_way_int_mod`. Again, we check to see if the big model is any more explanatory:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntwo_way_int_mod = lm(mpg ~ disp * hp + disp * domestic + hp * domestic, data = autompg)\n#two_way_int_mod = lm(mpg ~ (disp + hp + domestic) ^ 2, data = autompg)\nanova(two_way_int_mod, big_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp * hp + disp * domestic + hp * domestic\nModel 2: mpg ~ disp * hp * domestic\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    376 5673.2                           \n2    375 5645.6  1    27.599 1.8332 0.1766\n```\n\n\n:::\n:::\n\n\n\n\nWe see the p-value is somewhat large, so we would fail to reject. We prefer the smaller, less flexible, null model, without the three-way interaction.\n\nA quick note here: the full model does still \"fit better.\" Notice that it has a smaller RMSE than the null model, which means the full model makes smaller (squared) errors on average.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(resid(big_model) ^ 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14.74053\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(resid(two_way_int_mod) ^ 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14.81259\n```\n\n\n:::\n:::\n\n\n\n\nHowever, it is not much smaller. We could even say that, the difference is insignificant. This is an idea we will return to later in greater detail.\n\nNow that we have chosen the model without the three-way interaction, can we go further? Do we need the two-way interactions? Let's test\n\n\\[\nH_0: \\beta_4 = \\beta_5 = \\beta_6 = 0.\n\\]\n\nRemember we already chose $\\beta_7 = 0$, so,\n\n- Full Model: $Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon$\n- Null Model: $Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon$\n\nWe fit the null model in `R` as `additive_mod`, then use `anova()` to perform an $F$-test as usual.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadditive_mod = lm(mpg ~ disp + hp + domestic, data = autompg)\nanova(additive_mod, two_way_int_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + hp + domestic\nModel 2: mpg ~ disp * hp + disp * domestic + hp * domestic\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1    379 7369.7                                  \n2    376 5673.2  3    1696.5 37.478 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\nHere the p-value is small, so we reject the null, and we prefer the full (alternative) model. Of the models we have considered, our final preference is for\n\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon.\n\\]\n\n\n",
    "supporting": [
      "07b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}