{
  "hash": "a6a8e63fe0ddfe36a42d614d3123cefe",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Regression II\"\nlastmod: \"2024-10-04\"\noutput:\n  blogdown::html_page:\n    toc: true\n    code-overflow: scroll\n---\n\n\n\n\n## Required Reading\n\n- This page.\n\n### Supplemental Readings\n\n- <i class=\"fas fa-book\"></i> Chapter 3 in [Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)\n\n### Guiding Questions\n\n- How do we use R's `lm` for regression?\n- How do we interpret linear regression outputs?\n- How are the standard errors derived?\n- When should we turn to linear regression versus alternative approaches?\n- Why do we use linear regression so often in data analytics?\n\n\n\n# Linear Models II\n\nSince Galton's original development, regression has become one of the most widely used tools in data science. One reason has to do with the fact that regression permits us to find relationships between two variables taking into account the effects of other variables that affect both. This has been particularly popular in fields where randomized experiments are hard to run, such as economics and epidemiology.\n\nWhen we are not able to randomly assign each individual to a treatment or control group, confounding is particularly prevalent. For example, consider estimating the effect of eating fast foods on life expectancy using data collected from a random sample of people in a jurisdiction. Fast food consumers are more likely to be smokers, drinkers, and have lower incomes. Therefore, a naive regression model may lead to an overestimate of the negative health effect of fast food. So how do we account for confounding in practice? In this lecture we learn how linear models can help with such situations and can be used to describe how one or more variables affect an outcome variable.\n\n## Case study: Moneyball\n\n_Moneyball: The Art of Winning an Unfair Game_ is a book by Michael Lewis about the Oakland Athletics (A's) baseball team and its general manager, the person tasked with building the team, Billy Beane.\n\nTraditionally, baseball teams use _scouts_ to help them decide what players to hire. These scouts evaluate players by observing them perform. Scouts tend to favor athletic players with observable physical abilities. For this reason, scouts tend to agree on who the best players are and, as a result, these players tend to be in high demand. This in turn drives up their salaries.\n\nFrom 1989 to 1991, the A's had one of the highest payrolls in baseball. They were able to buy the best players and, during that time, they were one of the best teams. However, in 1995 the A's team owner changed and the new management cut the budget drastically, leaving then general manager, Sandy Alderson, with one of the lowest payrolls in baseball. He could no longer afford the most sought-after players. Alderson began using a statistical approach to find inefficiencies in the market. Alderson was a mentor to Billy Beane, who succeeded him in 1998 and fully embraced data science, as opposed to scouts, as a method for finding low-cost players that data predicted would help the team win. Today, this strategy has been adapted by most baseball teams. As we will see, regression plays a large role in this approach.\n\nAs motivation for this lecture, we will pretend it is 2002 (holy sh*t I'm old) and try to build a baseball team with a limited budget, just like the A's had to do. To appreciate what you are up against, note that in 2002 the Yankees' payroll of \\$125,928,583 more than tripled the Oakland A's \\$39,679,746:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07a_files/figure-html/mlb-2002-payroll-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n### Sabermetrics\n\nStatistics have been used in baseball since its beginnings. The dataset we will be using, included in the __Lahman__ library, goes back to the 19th century. For example, a summary statistic we will describe soon, the _batting average_, has been used for decades to summarize a batter's success. Other statistics^[http://mlb.mlb.com/stats/league_leaders.jsp] such as home runs (HR), runs batted in (RBI), and stolen bases (SB) are reported for each player in the game summaries included in the sports section of newspapers, with players rewarded for high numbers. Although summary statistics such as these were widely used in baseball, data analysis per se was not. These statistics were arbitrarily decided on without much thought as to whether they actually predicted anything or were related to helping a team win.\n\nThis changed with Bill James^[https://en.wikipedia.org/wiki/Bill_James]. In the late 1970s, this aspiring writer and baseball fan started publishing articles describing more in-depth analysis of baseball data. He named the approach of using data to predict what outcomes best predicted if a team would win _sabermetrics_^[https://en.wikipedia.org/wiki/Sabermetrics]. Until Billy Beane made sabermetrics the center of his baseball operation, Bill James' work was mostly ignored by the baseball world. Currently, sabermetrics popularity is no longer limited to just baseball; other sports have started to use this approach as well.\n\n\nIn this lecture, to simplify the exercise, we will focus on scoring runs and ignore the two other important aspects of the game: pitching and fielding. We will see how regression analysis can help develop strategies to build a competitive baseball team with a constrained budget. The approach can be divided into two separate data analyses. In the first, we determine which recorded player-specific statistics predict runs. In the second, we examine if players were undervalued based on what our first analysis predicts.\n\n### Baseball basics\n\nTo see how regression will help us find undervalued players, we actually don't need to understand all the details about the game of baseball, which has over 100 rules. Here, we distill the sport to the basic knowledge one needs to know how to effectively attack the data science problem.\n\nThe goal of a baseball game is to score more runs (points) than the other team. Each team has 9 batters that have an opportunity to hit a ball with a bat in a predetermined order. After the 9th batter has had their turn, the first batter bats again, then the second, and so on. Each time a batter has an opportunity to bat, we call it a plate appearance (PA). At each PA, the other team's _pitcher_ throws the ball and the batter tries to hit it. The PA ends with an binary outcome: the batter either makes an _out_ (failure) and returns to the bench or the batter doesn't (success) and can run around the bases, and potentially score a run (reach all 4 bases). Each team gets nine tries, referred to as _innings_, to score runs and each inning ends after three outs (three failures).\n\nHere is a video showing a success: [https://www.youtube.com/watch?v=HL-XjMCPfio](https://www.youtube.com/watch?v=HL-XjMCPfio). And here is one showing a failure: [https://www.youtube.com/watch?v=NeloljCx-1g](https://www.youtube.com/watch?v=NeloljCx-1g). In these videos, we see how luck is involved in the process. When at bat, the batter wants to hit the ball hard. If the batter hits it hard enough, it is a HR, the best possible outcome as the batter gets at least one automatic run. But sometimes, due to chance, the batter hits the ball very hard and a defender catches it, resulting in an out. In contrast, sometimes the batter hits the ball softly, but it lands just in the right place. The fact that there is chance involved hints at why probability models will be involved.\n\nNow there are several ways to succeed. Understanding this distinction will be important for our analysis. When the batter hits the ball, the batter wants to pass as many _bases_ as possible. There are four bases with the fourth one called _home plate_. Home plate is where batters start by trying to hit, so the bases form a cycle.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![(Courtesy of Cburnett^[https://en.wikipedia.org/wiki/User:Cburnett]. CC BY-SA 3.0 license^[https://creativecommons.org/licenses/by-sa/3.0/deed.en].)](../../images/Baseball_Diamond1.png){width=325}\n:::\n:::\n\n\n\n<!--Source: [Wikipedia Commons](https://commons.wikimedia.org/wiki/File:Baseball_diamond_simplified.svg))-->\n\nA batter who _goes around the bases_ and arrives home, scores a run.\n\n\nWe are simplifying a bit, but there are five ways a batter can succeed, that is, not make an out:\n\n- Bases on balls (BB) - the pitcher fails to throw the ball through a predefined area considered to be hittable (the strikezone), so the batter is permitted to go to first base.\n- Single - Batter hits the ball and gets to first base.\n- Double (2B) - Batter hits the ball and gets to second base.\n- Triple  (3B) - Batter hits the ball and gets to third base.\n- Home Run (HR) - Batter hits the ball and goes all the way home and scores a run.\n\nHere  is an example of a HR:\n  [https://www.youtube.com/watch?v=xYxSZJ9GZ-w](https://www.youtube.com/watch?v=xYxSZJ9GZ-w).\nIf a batter gets to a base, the batter still has a chance of getting home and scoring a run if the next batter hits successfully. While the batter is _on base_, the batter can also try to steal a base (SB). If a batter runs fast enough, the batter can try to go from one base to the next without the other team tagging the runner. [Here] is an example of a stolen base: [https://www.youtube.com/watch?v=JSE5kfxkzfk](https://www.youtube.com/watch?v=JSE5kfxkzfk).\n\nAll these events are kept track of during the season and are available to us through the __Lahman__ package. Now we will start discussing how data analysis can help us decide how to use these statistics to evaluate players.\n\n### No awards for BB\n\nHistorically, the _batting average_ has been considered the most important offensive statistic. To define this average, we define a _hit_ (H) and an _at bat_ (AB). Singles, doubles, triples, and home runs are hits. The fifth way to be successful, BB, is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate. Today this success rate ranges from 20% to 38%. We refer to the batting average in thousands so, for example, if your success rate is 28%, we call it _batting 280_.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![(Picture courtesy of Keith Allison^[https://www.flickr.com/people/27003603@N00]. CC BY-SA 2.0 license^[https://creativecommons.org/licenses/by-sa/2.0].)](../../images/JumboTron.png){width=360}\n:::\n:::\n\n\n\n\n\nOne of Bill James' first important insights is that the batting average ignores BB, but a BB is a success.  He proposed we use the _on base percentage_ (OBP) instead of batting average. He defined OBP as (H+BB)/(AB+BB) which is simply the proportion of plate appearances that don't result in an out, a very intuitive measure. He noted that a player that gets many more BB than the average player might not be recognized if the batter does not excel in batting average. But is this player not helping produce runs? No award is given to the player with the most BB. However, bad habits are hard to break and baseball did not immediately adopt OBP as an important statistic. In contrast, total stolen bases were considered important and an award^[http://www.baseball-almanac.com/awards/lou_brock_award.shtml] given to the player with the most. But players with high totals of SB also made more outs as they did not always succeed. Does a player with high SB total help produce runs? Can we use data science to determine if it's better to pay for players with high BB or SB?\n\n### Base on balls or stolen bases?\n\nOne of the challenges in this analysis is that it is not obvious how to determine if a player produces runs because so much depends on his teammates. We do keep track of the number of runs scored by a player. However, remember that if a player X bats right before someone who hits many HRs, batter X will score many runs. But these runs don't necessarily happen if we hire player X but not his HR hitting teammate.  However, we can examine team-level statistics. How do teams with many SB compare to teams with few? How about BB? We have data! Let's examine some.\n\nLet's start with an obvious one: HRs. Do teams that hit more home runs score more runs? We examine data from 1961 to 2001. The visualization of choice when exploring the relationship between two variables, such as HRs and wins, is a scatterplot:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Lahman)\n\nTeams %>% filter(yearID %in% 1961:2001) %>%\n  mutate(HR_per_game = HR / G, R_per_game = R / G) %>%\n  ggplot(aes(HR_per_game, R_per_game)) +\n  geom_point(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/runs-vs-hrs-1.png){width=672}\n:::\n:::\n\n\n\n\nThe plot shows a strong association: teams with more HRs tend to score more runs. Now let's examine the relationship between stolen bases and runs:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTeams %>% filter(yearID %in% 1961:2001) %>%\n  mutate(SB_per_game = SB / G, R_per_game = R / G) %>%\n  ggplot(aes(SB_per_game, R_per_game)) +\n  geom_point(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/runs-vs-sb-1.png){width=672}\n:::\n:::\n\n\n\n\nHere the relationship is not as clear. Finally, let's examine the relationship between BB and runs:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTeams %>% filter(yearID %in% 1961:2001) %>%\n  mutate(BB_per_game = BB/G, R_per_game = R/G) %>%\n  ggplot(aes(BB_per_game, R_per_game)) +\n  geom_point(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/runs-vs-bb-1.png){width=672}\n:::\n:::\n\n\n\n\nHere again we see a clear association. But does this mean that increasing a team's BBs **causes** an increase in runs? One of the most important lessons you learn in this book is that **association is not causation.**\n\nIn fact, it looks like BBs and HRs are also associated:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTeams %>% filter(yearID %in% 1961:2001 ) %>%\n  mutate(HR_per_game = HR/G, BB_per_game = BB/G) %>%\n  ggplot(aes(HR_per_game, BB_per_game)) +\n  geom_point(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/bb-vs-hrs-1.png){width=672}\n:::\n:::\n\n\n\n\nWe know that HRs cause runs because, as the name \"home run\" implies, when a player hits a HR they are guaranteed at least one run. Could it be that HRs also cause BB and this makes it appear as if BB cause runs? When this happens we say there is _confounding_, an important concept we will learn more about throughout this lecture.\n\nLinear regression will help us parse all this out and quantify the associations. This will then help us determine what players to recruit. Specifically, we will try to predict things like how many more runs will a team score if we increase the number of BBs, but keep the HRs fixed? Regression will help us answer questions like this one.\n\n\n### Regression applied to baseball statistics\n\nCan we use regression with these data? First, notice that the HR and Run data appear to be bivariate normal. We may have skipped the bivariate normal from before, but it just means that two variables share a joint normal distribution -- each has it's own mean, but conditional on some value of one, the other is normally distributed around it's conditional value. We save the plot into the object `p` as we will use it again later.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Lahman)\np <- Teams %>% filter(yearID %in% 1961:2001 ) %>%\n  mutate(HR_per_game = HR/G, R_per_game = R/G) %>%\n  ggplot(aes(HR_per_game, R_per_game)) +\n  geom_point(alpha = 0.5)\np\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/hr-runs-bivariate-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThe qq-plots confirm that the normal approximation is useful here:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTeams %>% filter(yearID %in% 1961:2001 ) %>%\n  mutate(z_HR = round((HR - mean(HR))/sd(HR)),\n         R_per_game = R/G) %>%\n  filter(z_HR %in% -2:3) %>%\n  ggplot() +\n  stat_qq(aes(sample=R_per_game)) +\n  facet_wrap(~z_HR)\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/hr-by-runs-qq-1.png){width=672}\n:::\n:::\n\n\n\n\nNow we are ready to use linear regression to predict the number of runs a team will score if we know how many home runs the team hits. All we need to do is compute the five summary statistics:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary_stats <- Teams %>%\n  filter(yearID %in% 1961:2001 ) %>%\n  mutate(HR_per_game = HR/G, R_per_game = R/G) %>%\n  summarize(avg_HR = mean(HR_per_game),\n            s_HR = sd(HR_per_game),\n            avg_R = mean(R_per_game),\n            s_R = sd(R_per_game),\n            r = cor(HR_per_game, R_per_game))\nsummary_stats\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     avg_HR      s_HR    avg_R       s_R         r\n1 0.8547104 0.2429707 4.355262 0.5885791 0.7615597\n```\n\n\n:::\n:::\n\n\n\n\nand use the formulas given above to create the regression lines, as we did in Week 5's Content, and adding the line to our plot `p` created earlier:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg_line <- summary_stats %>% summarize(slope = r*s_R/s_HR,\n                            intercept = avg_R - slope*avg_HR)\n\np + geom_abline(intercept = reg_line$intercept, slope = reg_line$slope)\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/hr-versus-runs-regression-1.png){width=672}\n:::\n:::\n\n\n\n\nFor plotting, we can also use the argument `method = \"lm\"` which stands for _linear model_, the title of an upcoming section. So we can simplify the code above like this:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np + geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/hr-versus-runs-regression-easy-1.png){width=672}\n:::\n:::\n\n\n\n\n\nIn the example above, the slope is 1.8448241. So this tells us that teams that hit 1 more HR per game than the average team, score 1.8448241 more runs per game than the average team. Given that the most common final score is a difference of a run, this can certainly lead to a large increase in wins. Not surprisingly, HR hitters are very expensive. Because we are working on a budget, we will need to find some other way to increase wins. So in the next section we move our attention to BB.\n\n## Confounding\n\nPreviously, we noted a strong relationship between Runs and BB. If we find the regression line for predicting runs from bases on balls, we a get slope of:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(Lahman)\nget_slope <- function(x, y) cor(x, y) * sd(y) / sd(x)\n\nbb_slope <- Teams %>%\n  filter(yearID %in% 1961:2001 ) %>%\n  mutate(BB_per_game = BB/G, R_per_game = R/G) %>%\n  summarize(slope = get_slope(BB_per_game, R_per_game))\n\nbb_slope\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      slope\n1 0.7353288\n```\n\n\n:::\n:::\n\n\n\nSo does this mean that if we go and hire low salary players with many BB, and who therefore increase the number of walks per game by 2, our team will score 1.5 more runs per game?\n\nWe are again reminded that association is not causation. The data does provide strong evidence that a team with two more BB per game than the average team, scores 1.5 runs per game. But this does not mean that BB are the cause.\n\nNote that if we compute the regression line slope for singles we get:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsingles_slope <- Teams %>%\n  filter(yearID %in% 1961:2001 ) %>%\n  mutate(Singles_per_game = (H-HR-X2B-X3B)/G, R_per_game = R/G) %>%\n  summarize(slope = get_slope(Singles_per_game, R_per_game))\n\nsingles_slope\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      slope\n1 0.4494253\n```\n\n\n:::\n:::\n\n\n\n\nwhich is a lower value than what we obtain for BB.\n\nAlso, notice that a single gets you to first base just like a BB. Those that know about baseball will tell you that with a single, runners on base have a better chance of scoring than with a BB. So how can BB be more predictive of runs? The reason this happen is because of confounding. Here we show the correlation between HR, BB, and singles:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTeams %>%\n  filter(yearID %in% 1961:2001 ) %>%\n  mutate(Singles = (H-HR-X2B-X3B)/G, BB = BB/G, HR = HR/G) %>%\n  summarize(cor(BB, HR), cor(Singles, HR), cor(BB, Singles))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  cor(BB, HR) cor(Singles, HR) cor(BB, Singles)\n1   0.4039313       -0.1737435      -0.05603822\n```\n\n\n:::\n:::\n\n\n\n\nIt turns out that pitchers, afraid of HRs, will sometimes avoid throwing strikes to HR hitters. As a result, HR hitters tend to have more BBs and a team with many HRs will also have more BBs. Although it may appear that BBs cause runs, it is actually the HRs that cause most of these runs. We say that BBs are _confounded_ with HRs. Nonetheless, could it be that BBs still help? To find out, we somehow have to adjust for the HR effect. Regression can help with this as well.\n\n### Understanding confounding through stratification\n\nA first approach is to keep HRs fixed at a certain value and then examine the relationship between BB and runs. As we did when we stratified fathers by rounding to the closest inch, here we can stratify HR per game to the closest ten. We filter out the strata with few points to avoid highly variable estimates:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- Teams %>% filter(yearID %in% 1961:2001) %>%\n  mutate(HR_strata = round(HR/G, 1),\n         BB_per_game = BB / G,\n         R_per_game = R / G) %>%\n  filter(HR_strata >= 0.4 & HR_strata <=1.2)\n```\n:::\n\n\n\n\nand then make a scatterplot for each strata:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat %>%\n  ggplot(aes(BB_per_game, R_per_game)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  facet_wrap( ~ HR_strata)\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/runs-vs-bb-by-hr-strata-1.png){width=80%}\n:::\n\n```{.r .cell-code}\n# Note: we'll get a \"warning\"\n# telling us that ggplot has\n# used lm(y ~ x) where\n# y refers to the aesthetic mapping of y\n# x refers to the aesthetic mapping of x\n```\n:::\n\n\n\n\nRemember that the regression slope for predicting runs with BB was 0.7. Once we stratify by HR, these slopes are substantially reduced:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat %>%\n  group_by(HR_strata) %>%\n  summarize(slope = get_slope(BB_per_game, R_per_game))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 2\n  HR_strata slope\n      <dbl> <dbl>\n1       0.4 0.734\n2       0.5 0.566\n3       0.6 0.412\n4       0.7 0.285\n5       0.8 0.365\n6       0.9 0.261\n7       1   0.512\n8       1.1 0.454\n9       1.2 0.440\n```\n\n\n:::\n:::\n\n\n\n\nThe slopes are reduced, but they are not 0, which indicates that BBs are helpful for producing runs, just not as much as previously thought.\nIn fact, the values above are closer to the slope we obtained from singles, 0.45, which is more consistent with our intuition. Since both singles and BB get us to first base, they should have about the same predictive power.\n\nAlthough our understanding of the application tells us that HR cause BB but not the other way around, we can still check if stratifying by BB makes the effect of HR go down. To do this, we use the same code except that we swap HR and BBs to get this plot:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07a_files/figure-html/runs-vs-hr-by-bb-strata-1.png){width=100%}\n:::\n:::\n\n\n\n\nIn this case, the slopes do not change much from the original:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat %>% group_by(BB_strata) %>%\n   summarize(slope = get_slope(HR_per_game, R_per_game))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 × 2\n   BB_strata slope\n       <dbl> <dbl>\n 1       2.8  1.52\n 2       2.9  1.57\n 3       3    1.52\n 4       3.1  1.49\n 5       3.2  1.58\n 6       3.3  1.56\n 7       3.4  1.48\n 8       3.5  1.63\n 9       3.6  1.83\n10       3.7  1.45\n11       3.8  1.70\n12       3.9  1.30\n```\n\n\n:::\n:::\n\n\n\n\nThey are reduced a bit, which is consistent with the fact that BB do in fact cause some runs.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhr_slope <- Teams %>%\n  filter(yearID %in% 1961:2001 ) %>%\n  mutate(HR_per_game = HR/G, R_per_game = R/G) %>%\n  summarize(slope = get_slope(HR_per_game, R_per_game))\n\nhr_slope\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     slope\n1 1.844824\n```\n\n\n:::\n:::\n\n\n\n\nRegardless, it seems that if we stratify by HR, we have bivariate distributions for runs versus BB. Similarly, if we stratify by BB, we have approximate bivariate normal distributions for HR versus runs.\n\n###  Multivariate regression\n\nIt is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this:\n\n$$\n\\mbox{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1(x_2) x_1 + \\beta_2(x_1) x_2\n$$\n\nwith the slopes for $x_1$ changing for different values of $x_2$ and vice versa (think of $\\beta_1(x_2)$ as a function that gives a different slope depending on $x_2$). But is there an easier approach?\n\nIf we take random variability into account, the slopes in the strata don't appear to change much. If these slopes are in fact the same, this implies that $\\beta_1(x_2)$ and $\\beta_2(x_1)$ are constants. This in turn implies that the expectation of runs conditioned on HR and BB can be written like this:\n\n$$\n\\mbox{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n$$\n\nThis model suggests that if the number of HR is fixed at $x_2$, we observe a linear relationship between runs and BB with an intercept of $\\beta_0 + \\beta_2 x_2$. Our exploratory data analysis suggested this. The model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by $\\beta_1 x_1$.\n\nIn this analysis, referred to as _multivariate regression_, you will often hear people say that the BB slope $\\beta_1$ is _adjusted_ for the HR effect. If the model is correct then confounding has been accounted for. But how do we estimate $\\beta_1$ and $\\beta_2$ from the data? For this, we learn about linear models and least squares estimates.\n\n## Least squares estimates {#lse}\n\nWe have described how if data is bivariate normal then the conditional expectations follow the regression line. The fact that the conditional expectation is a line is not an extra assumption but rather a derived result. However, in practice it is common to explicitly write down a model that describes the relationship between two or more variables using a _linear model_.\n\nWe note that \"linear\" here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combination of known quantities. In mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables. For example, $3x - 4y + 5z$ is a linear combination of $x$, $y$, and $z$. We can also add a constant so $2 + 3x - 4y + 5z$ is also linear combination of $x$, $y$, and $z$.\n\nSo $\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$, is a linear combination of $x_1$ and $x_2$.\nThe simplest linear model is a constant $\\beta_0$; the second simplest is a line $\\beta_0 + \\beta_1 x$. If we were to specify a linear model for Galton's data, we would denote the $N$ observed father heights with $x_1, \\dots, x_n$, then we model the $N$ son heights we are trying to predict with:\n\n$$\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\, i=1,\\dots,N.\n$$\n\nHere $x_i$ is the father's height, which is fixed (not random) due to the conditioning, and $Y_i$ is the random son's height that we want to predict. We further assume that $\\varepsilon_i$ are independent from each other, have expected value 0 and the standard deviation of $\\varepsilon_i$, call it $\\sigma$, does not depend on $i$.\n\nIn the above model, we know the $x_i$, but to have a useful model for prediction, we need $\\beta_0$ and $\\beta_1$. We estimate these from the data. Once we do this, we can predict son's heights for any father's height $x$. We show how to do this in the next section.\n\nNote that if we further assume that the $\\varepsilon$ is normally distributed, then this model is exactly the same one we derived earlier by assuming bivariate normal data. A somewhat nuanced difference is that in the first approach we assumed the data was bivariate normal and that the linear model was derived, not assumed. In practice, linear models are just assumed without necessarily assuming normality: the distribution of the $\\varepsilon$'s is not specified.  Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model.\n\n### Interpreting linear models\n\nOne reason linear models are popular is that they are interpretable. In the case of Galton's data, we can interpret the data like this: due to inherited genes, the son's height prediction grows by $\\beta_1$ for each inch we increase the father's height $x$. Because not all sons with fathers of height $x$ are of equal height, we need the term $\\varepsilon$, which explains the remaining variability. This remaining variability includes the mother's genetic effect, environmental factors, and other biological randomness.\n\nGiven how we wrote the model above, the intercept $\\beta_0$ is not very interpretable as it is the predicted height of a son with a father with no height. Due to regression to the mean, the prediction will usually be a bit larger than 0. To make the slope parameter more interpretable, we can rewrite the model slightly as:\n\n$$\nY_i = \\beta_0 + \\beta_1 (x_i - \\bar{x}) + \\varepsilon_i, \\, i=1,\\dots,N\n$$\n\n\nwith $\\bar{x} = 1/N \\sum_{i=1}^N x_i$ the average of the $x$. In this case $\\beta_0$ represents the height when $x_i = \\bar{x}$, which is the height of the son of an average father.\n\n### Least Squares Estimates (LSE)\n\nFor linear models to be useful, we have to estimate the unknown $\\beta$s. The standard approach in science is to find the values that minimize the distance of the fitted model to the data. The following is called the least squares (LS) equation and we will see it often in this lecture. For Galton's data, we would write:\n\n$$\nRSS = \\sum_{i=1}^n \\left\\{  y_i - \\left(\\beta_0 + \\beta_1 x_i \\right)\\right\\}^2\n$$\n\nThis quantity is called the residual sum of squares (RSS). Once we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them with $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$. Let's demonstrate this with the previously defined dataset:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(HistData)\ndata(\"GaltonFamilies\")\nset.seed(1983)\ngalton_heights <- GaltonFamilies %>%\n  filter(gender == \"male\") %>%\n  group_by(family) %>%\n  sample_n(1) %>%\n  ungroup() %>%\n  select(father, childHeight) %>%\n  rename(son = childHeight)\n```\n:::\n\n\n\n\nLet's write a function that computes the RSS for any pair of values $\\beta_0$ and $\\beta_1$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrss <- function(beta0, beta1, data){\n  resid <- galton_heights$son - (beta0+beta1*galton_heights$father)\n  return(sum(resid^2))\n}\n```\n:::\n\n\n\n\nSo for any pair of values, we get an RSS. Here is a plot of the RSS as a function of $\\beta_1$ when we keep the $\\beta_0$ fixed at 25.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta1 = seq(0, 1, len=nrow(galton_heights))\nresults <- data.frame(beta1 = beta1,\n                      rss = sapply(beta1, rss, beta0 = 25))\nresults %>% ggplot(aes(beta1, rss)) + geom_line() +\n  geom_line(aes(beta1, rss))\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/rss-versus-estimate-1.png){width=672}\n:::\n:::\n\n\n\n\nWe can see a clear minimum for $\\beta_1$ at around 0.65. However, this minimum for $\\beta_1$ is for when $\\beta_0 = 25$, a value we arbitrarily picked. We don't know if  (25, 0.65) is the pair that minimizes the equation across all possible pairs.\n\nTrial and error is not going to work in this case. We could search for a minimum within a fine grid of $\\beta_0$ and $\\beta_1$ values, but this is unnecessarily time-consuming since we can use calculus: take the partial derivatives, set them to 0 and solve for $\\beta_1$ and $\\beta_2$. Of course, if we have many parameters, these equations can get rather complex. But there are functions in R that do these calculations for us. We will learn these next. To learn the mathematics behind this, you can consult a book on linear models.\n\n### The `lm` function\n\nThe `lm` function is the workhorse for linear models in R. We want to fit the model:\n\n$$\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n$$\n\nwith $Y_i$ the son's height and $x_i$ the father's height as in our Galton Heights dataset. We can use `lm()` to obtain the least squares estimates.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(son ~ father, data = galton_heights)\nfit$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)      father \n  37.287605    0.461392 \n```\n\n\n:::\n:::\n\n\n\n\nThere are two arguments here. The first is specifying the regression formula. We use the character `~` to let `lm` know which is the variable we are predicting (left of `~`) and which we are using to predict (right of `~`). The intercept is added *automatically* to the model that will be fit. Note that we just use the column names here -- `son` not `galton_heights$son`. And note that we aren't putting quotes around the names, even though R doesn't have an object called `son` or `father` in its memory (it does have `galton_heights$son` and `galton_heights$father` of course).\n\nThe second argument is the key here. It is the `data=` argoument. Here, you're telling R where to find the variables `son` and `father`. \n\nNote that while `lm(galton_heights$son ~ galton_heights$father)` will work, it will cause problems later when we try to predict using our model. Do not write your regressions like this. Use the formula and `data = ` notation. \n\nThe object we named `fit` here is a \"lm\" object. Not a data frame or a tibble. It includes more information about the fit and lots of other stuff. We can use the function `summary` to extract more of this information. `summary` is usually how we'll look at regression results:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = son ~ father, data = galton_heights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3543 -1.5657 -0.0078  1.7263  9.4150 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 37.28761    4.98618   7.478 3.37e-12 ***\nfather       0.46139    0.07211   6.398 1.36e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.45 on 177 degrees of freedom\nMultiple R-squared:  0.1878,\tAdjusted R-squared:  0.1833 \nF-statistic: 40.94 on 1 and 177 DF,  p-value: 1.36e-09\n```\n\n\n:::\n:::\n\n\n\n\nTo understand some of the information included in this summary we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables\n\n\n### LSE are random variables\n\nThe LSE is derived from the data $y_1,\\dots,y_N$, which are a realization of random variables $Y_1, \\dots, Y_N$.  This implies that our estimates are random variables. To see this, we can run a Monte Carlo simulation in which we assume the son and father height data defines a population, take a random sample of size $N=50$, and compute the regression slope coefficient for each one:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 1000\nN <- 50\nlse <- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) %>%\n    lm(son ~ father, data = .) %>%\n    .$coef\n})\nlse <- data.frame(beta_0 = lse[1,], beta_1 = lse[2,])\n```\n:::\n\n\n\n\nWe can see the variability of the estimates by plotting their distributions:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07a_files/figure-html/lse-distributions-1.png){width=100%}\n:::\n:::\n\n\n\n\nThe reason these look normal is because the central limit theorem applies here as well: for large enough $N$, the least squares estimates will be approximately normal with expected value $\\beta_0$ and $\\beta_1$, respectively. The standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the `lm` function. Here it is for one of our simulated data sets:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n sample_n(galton_heights, N, replace = TRUE) %>%\n  lm(son ~ father, data = .) %>%\n  summary %>% .$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Estimate Std. Error  t value     Pr(>|t|)\n(Intercept) 19.2791952 11.6564590 1.653950 0.1046637693\nfather       0.7198756  0.1693834 4.249977 0.0000979167\n```\n\n\n:::\n:::\n\n\n\n\nYou can see that the standard errors estimates reported by the `summary` are close to the standard errors from the simulation:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlse %>% summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     se_0      se_1\n1 8.83591 0.1278812\n```\n\n\n:::\n:::\n\n\n\n\nThe `summary` function also reports t-statistics (`t value`) and p-values (`Pr(>|t|)`). The t-statistic is not actually based on the central limit theorem but rather on the assumption that the $\\varepsilon$s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, $\\hat{\\beta}_0 / \\hat{\\mbox{SE}}(\\hat{\\beta}_0 )$ and $\\hat{\\beta}_1 / \\hat{\\mbox{SE}}(\\hat{\\beta}_1 )$, follow a t-distribution with $N-p$ degrees of freedom, with $p$ the number of parameters in our model. In the case of height $p=2$, the two p-values are testing the null hypothesis that $\\beta_0 = 0$ and $\\beta_1=0$, respectively.\n\nRemember that, as we described in the section on t-tests for large enough $N$, the CLT works and the t-distribution becomes almost the same as the normal distribution. Also, notice that we can construct confidence intervals, but we will soon learn about __broom__, an add-on package that makes this easy.\n\nAlthough we do not show examples in this section, hypothesis testing with regression models is commonly used in epidemiology and economics to make statements such as \"the effect of A on B was statistically significant after adjusting for X, Y, and Z\". However, several assumptions have to hold for these statements to be true.\n\n\n### Predicted values are random variables\n\nOnce we fit our model, we can obtain prediction of $Y$ by plugging in the estimates into the regression model. For example, if the father's height is $x$, then our prediction $\\hat{Y}$ for the son's height will be:\n\n$$\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$$\n\nWhen we plot $\\hat{Y}$ versus $x$, we see the regression line.\n\nKeep in mind that the prediction $\\hat{Y}$ is also a random variable and mathematical theory tells us what the standard errors are. If we assume the errors are normal, or have a large enough sample size, we can use theory to construct confidence intervals as well. In fact, the __ggplot2__ layer `geom_smooth(method = \"lm\")` that we previously used plots $\\hat{Y}$ and surrounds it by confidence intervals:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton_heights %>% ggplot(aes(son, father)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/father-son-regression-1.png){width=672}\n:::\n:::\n\n\n\n\nThe R function `predict` takes an `lm` object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- galton_heights %>% lm(son ~ father, data = .)\n\ny_hat <- predict(fit, se.fit = TRUE)\n\nnames(y_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n### The broom package\n\nOur original task was to provide an estimate and confidence interval for the slope estimates of each strata. The __broom__ package will make this quite easy. While the most useful way of looking at results from a regression with `lm` is to use `summary` on the `lm` object, __broom__ provides tools to extract info from the regression.\n\nThe __broom__ package has three main functions, all of which extract information from the object returned by `lm` and return it in a __tidyverse__ friendly data frame. These functions are `tidy`, `glance`, and `augment`. The `tidy` function returns estimates and related information as a data frame:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nfit <- lm(R ~ BB, data = dat)\ntidy(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  214.      23.5         9.10 6.46e-19\n2 BB             0.910    0.0447     20.3  5.30e-75\n```\n\n\n:::\n:::\n\n\n\n\nWe can add other important summaries, such as confidence intervals:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(fit, conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  214.      23.5         9.10 6.46e-19  168.      260.   \n2 BB             0.910    0.0447     20.3  5.30e-75    0.822     0.998\n```\n\n\n:::\n:::\n\n\n\n\nBecause the outcome is a data frame, we can immediately use it with `do` to string together the commands that produce the table we are after. Because a data frame is returned, we can filter and select the rows and columns we want, which facilitates working with __ggplot2__:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat %>%\n  group_by(HR) %>%\n  do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) %>%\n  filter(term == \"BB\") %>%\n  select(HR, estimate, conf.low, conf.high) %>%\n  ggplot(aes(HR, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_errorbar() +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/do-tidy-example-1.png){width=672}\n:::\n:::\n\n\n\n\nNow we return to discussing our original task of determining if slopes changed. The plot we just made, using `do` and `tidy`, shows that the confidence intervals overlap, which provides a nice visual confirmation that our assumption that the slope does not change is safe.\n\nThe other functions provided by __broom__, `glance`, and `augment`, relate to model-specific and observation-specific outcomes, respectively. Here, we can see the model fit summaries `glance` returns:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.331         0.330  79.9      413. 5.30e-75     1 -4865. 9735. 9750.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\n\nYou can learn more about these summaries in any regression text book.\n\nWe will see an example of `augment` in the next section (and you used it already to predict values in an earlier assignment).\n\n\n## Case study: Multiple Regression and Moneyball (continued)\n\nIn trying to answer how well BBs predict runs, data exploration led us to a model, one with perhaps multiple variables:\n\n$$\n\\mbox{E}[R \\mid BB = x_1, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n$$\n\nHere, the data is approximately normal and conditional distributions were also normal. Thus, we are justified in using a linear model:\n\n$$\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i\n$$\n\nwith $Y_i$ runs per game for team $i$, $x_{i,1}$ walks per game, and $x_{i,2}$. To use `lm` here, we need to let the function know we have two predictor variables. So we use the `+` symbol as follows:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- Teams %>%\n  filter(yearID %in% 1961:2001) %>%\n  mutate(BB = BB/G, HR = HR/G,  R = R/G) %>%\n  lm(R ~ BB + HR, data = .)\n```\n:::\n\n\n\n\nWe can use `tidy` to see a nice summary:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(fit, conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)    1.74     0.0824      21.2 7.62e- 83    1.58      1.91 \n2 BB             0.387    0.0270      14.3 1.20e- 42    0.334     0.440\n3 HR             1.56     0.0490      31.9 1.78e-155    1.47      1.66 \n```\n\n\n:::\n:::\n\n\n\n\n\nWhen we fit the model with only one variable, the estimated slopes were 0.7353288 and 1.8448241 for BB and HR, respectively. Note that when fitting the multivariate model both go down, with the BB effect decreasing much more.\n\n### Interpreting Multiple Regression Coefficients\n\nHow do we now interpret the coefficients from the regression? We are using regression to condition on values of the $x$ variables (of which there are now two). So, the coefficient on $BB$ is interpreted as the associated change in the $E[R]$ per 1-unit increase in $BB$, **holding all else equal**. That means holding the $\\varepsilon$ present in every regression constant, and it means holding $HR$ (the other variable) constant. The latin phrase *ceteris paribus* is often used, meaning \"holding all else equal\".\n\n### Constructing a metric to pick players\n\nNow we want to construct a metric to pick players, we need to consider singles, doubles, and triples as well. Can we build a model that predicts runs based on all these outcomes?\n\nWe now are going to take somewhat of a \"leap of faith\" and assume that these five variables are jointly normal. This means that if we pick any one of them, and hold the other four fixed, the relationship with the outcome is linear and the slope does not depend on the four values held constant. If this is true, then a linear model for our data is:\n\n$$\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\beta_3 x_{i,3}+ \\beta_4 x_{i,4} + \\beta_5 x_{i,5} + \\varepsilon_i\n$$\n\nwith $x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}$ representing BB, singles, doubles, triples, and HR respectively.\n\nUsing `lm`, we can quickly find the LSE for the parameters using:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- Teams %>%\n  filter(yearID %in% 1961:2001) %>%\n  mutate(BB = BB / G,\n         singles = (H - X2B - X3B - HR) / G,\n         doubles = X2B / G,\n         triples = X3B / G,\n         HR = HR / G,\n         R = R / G) %>%\n  lm(R ~ BB + singles + doubles + triples + HR, data = .)\n```\n:::\n\n\n\n\nWe can see the coefficients using `tidy`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoefs <- tidy(fit, conf.int = TRUE)\n\ncoefs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   -2.77     0.0862     -32.1 4.76e-157   -2.94     -2.60 \n2 BB             0.371    0.0117      31.6 1.87e-153    0.348     0.394\n3 singles        0.519    0.0127      40.8 8.67e-217    0.494     0.544\n4 doubles        0.771    0.0226      34.1 8.44e-171    0.727     0.816\n5 triples        1.24     0.0768      16.1 2.12e- 52    1.09      1.39 \n6 HR             1.44     0.0243      59.3 0            1.40      1.49 \n```\n\n\n:::\n:::\n\n\n\n\nTo see how well our metric actually predicts runs, we can predict the number of runs for each team in 2002 using the function `predict`, then make a plot:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTeams %>%\n  filter(yearID %in% 2002) %>%\n  mutate(BB = BB/G,\n         singles = (H-X2B-X3B-HR)/G,\n         doubles = X2B/G,\n         triples =X3B/G,\n         HR=HR/G,\n         R=R/G)  %>%\n  mutate(R_hat = predict(fit, newdata = .)) %>%\n  ggplot(aes(R_hat, R, label = teamID)) +\n  geom_point() +\n  geom_text(nudge_x=0.1, cex = 2) +\n  geom_abline()\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/model-predicts-runs-1.png){width=672}\n:::\n:::\n\n\n\n\nOur model does quite a good job as demonstrated by the fact that points from the observed versus predicted plot fall close to the identity line.\n\nSo instead of using batting average, or just number of HR, as a measure of picking players, we can use our fitted model to form a metric that relates more directly to run production. Specifically, to define a metric for player A, we imagine a team made up of players just like player A and use our fitted regression model to predict how many runs this team would produce. The formula would look like this:\n-2.7691857 +\n0.3712147 $\\times$ BB +\n0.5193923 $\\times$ singles +\n0.7711444 $\\times$ doubles +\n1.2399696 $\\times$ triples +\n1.4433701 $\\times$ HR.\n\nTo define a player-specific metric, we have a bit more work to do. A challenge here is that we derived the metric for teams, based on team-level summary statistics. For example, the HR value that is entered into the equation is HR per game for the entire team. If we compute the HR per game for a player, it will be much lower since the total is accumulated by 9 batters. Furthermore, if a player only plays part of the game and gets fewer opportunities than average, it is still considered a game played. For players, a rate that takes into account opportunities is the per-plate-appearance rate.\n\nTo make the per-game team rate comparable to the per-plate-appearance player rate, we compute the average number of team plate appearances per game:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npa_per_game <- Batting %>% filter(yearID == 2002) %>%\n  group_by(teamID) %>%\n  summarize(pa_per_game = sum(AB+BB)/max(G)) %>%\n  pull(pa_per_game) %>%\n  mean\n```\n:::\n\n\n\n\nWe compute the per-plate-appearance rates for players available in 2002 on data from 1997-2001. To avoid small sample artifacts, we filter players with less than 1,000 plate appearances per year. Here is the entire calculation in one line:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplayers <- Batting %>% filter(yearID %in% 1997:2001) %>%\n  group_by(playerID) %>%\n  mutate(PA = BB + AB) %>%\n  summarize(G = sum(PA)/pa_per_game,\n    BB = sum(BB)/G,\n    singles = sum(H-X2B-X3B-HR)/G,\n    doubles = sum(X2B)/G,\n    triples = sum(X3B)/G,\n    HR = sum(HR)/G,\n    AVG = sum(H)/sum(AB),\n    PA = sum(PA)) %>%\n  filter(PA >= 1000) %>%\n  select(-G) %>%\n  mutate(R_hat = predict(fit, newdata = .))\n```\n:::\n\n\n\n\nThe player-specific predicted runs computed here can be interpreted as the number of runs we predict a team will score if all batters are exactly like that player. The distribution shows that there is wide variability across players:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqplot(R_hat, data = players, binwidth = 0.5, color = I(\"black\"))\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/r-hat-hist-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n::: {.callout-note}\n\n## TRY IT\n\nSince the 1980s, sabermetricians have used a summary statistic different from batting average to evaluate players. They realized walks were important and that doubles, triples, and HRs, should be weighed more than singles. As a result, they proposed the following metric:\n\n$$\n\\frac{\\mbox{BB}}{\\mbox{PA}} + \\frac{\\mbox{Singles} + 2 \\mbox{Doubles} + 3 \\mbox{Triples} + 4\\mbox{HR}}{\\mbox{AB}}\n$$\n\nThey called this on-base-percentage plus slugging percentage (OPS). Although the sabermetricians probably did not use regression, here we show how this metric is close to what one gets with regression.\n\n1. Compute the OPS for each team in the 2001 season. Then plot Runs per game versus OPS.\n\n\n2. For every year since 1961, compute the correlation between runs per game and OPS; then plot these correlations as a function of year.\n\n3. Note that we can rewrite OPS as a weighted average of BBs, singles, doubles, triples, and HRs. We know that the weights for doubles, triples, and HRs are 2, 3, and 4 times that of singles. But what about BB? What is the weight for BB relative to singles? Hint: the weight for BB relative to singles will be a function of AB and PA.\n\n4. Note that the weight for BB, $\\frac{\\mbox{AB}}{\\mbox{PA}}$, will change from team to team. To see how variable it is, compute and plot this quantity for each team for each year since 1961. Then plot it again, but instead of computing it for every team, compute and plot the ratio for the entire year. Then, once you are convinced that there is not much of a time or team trend, report the overall average.\n\n5. So now we know that the formula for OPS is proportional to $0.91 \\times \\mbox{BB} + \\mbox{singles} + 2 \\times \\mbox{doubles} + 3 \\times \\mbox{triples} + 4 \\times \\mbox{HR}$. Let's see how these coefficients compare to those obtained with regression. Fit a regression model to the data after 1961, as done earlier: using per game statistics for each year for each team. After fitting this model, report the coefficients as weights relative to the coefficient for singles.\n\n6. We see that our linear regression model coefficients follow the same general trend as those used by OPS, but with slightly less weight for metrics other than singles. For each team in years after 1961, compute the OPS, the predicted runs with the regression model and compute the correlation between the two as well as the correlation with runs per game.\n\n:::\n\n\n\n\n## BONUS: But I'm really into moneyball!\nIf you're interested in how we might build the best team for the dollar, keep reading. \n\n### Adding salary and position information\n\nTo actually build the team, we will need to know their salaries as well as their defensive position. For this, we join the `players` data frame we just created with the player information data frame included in some of the other Lahman data tables. We will learn more about the join function (and we will discuss this further in a later lecture). For now, we just need to know that a join matches a \"key\" field that is shared between the two datasets. Here, it is `playerID`.\n\nEach join consists of two datasets, a shared key (or keys), and a type of join. A **right join** takes any rows in X that match rows in Y, and *all* rows in Y. A **left join** takes all rows of X and any rows of Y that match. With *left* and *right* joins, you may end up with observations where some columns do not have data. R will give these an \"NA\". \n\nAn **inner join** takes only the rows in X and Y that match, so all observations will have data. A **full join** takes all rows in X and Y (and will give you lots of NAs if they don't all match).\n\nIf more than one observation in Y matches the key field in X (or vice-versa), then you can get duplicated observations. We'll cover joins more later. For now, we'll just use **right** and **left** joins on data we know is safe to merge.\n\nStart by adding the 2002 salary of each player:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplayers <- Salaries %>%\n  filter(yearID == 2002) %>%\n  select(playerID, salary) %>%\n  right_join(players, by=\"playerID\")\n```\n:::\n\n\n\n\nNext, we add their defensive position. This is a somewhat complicated task because players play more than one position each year. The __Lahman__ package table `Appearances` tells how many games each player played in each position, so we can pick the position that was most played using `which.max` on each row. We use `apply` to do this. However, because some players are traded, they appear more than once on the table, so we first sum their appearances across teams.\nHere, we pick the one position the player most played using the `top_n` function. To make sure we only pick one position, in the case of ties, we pick the first row of the resulting data frame. We also remove the `OF` position which stands for outfielder, a generalization of three positions: left field (LF), center field (CF), and right field (RF). We also remove pitchers since they don't bat in the league in which the A's play.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposition_names <-\n  paste0(\"G_\", c(\"p\",\"c\",\"1b\",\"2b\",\"3b\",\"ss\",\"lf\",\"cf\",\"rf\", \"dh\"))\n\ntmp <- Appearances %>%\n  filter(yearID == 2002) %>%\n  group_by(playerID) %>%\n  summarize_at(position_names, sum) %>%\n  ungroup()\n\npos <- tmp %>%\n  select(all_of(position_names)) %>% # all_of lets us use an external vector of position names to select\n  apply(., MAR = 1, which.max) # which.max gives us the column number of the position that is the max\n\nplayers <- tibble(playerID = tmp$playerID, POS = position_names[pos]) %>%\n  mutate(POS = str_to_upper(str_remove(POS, \"G_\"))) %>%\n  filter(POS != \"P\") %>%\n  right_join(players, by=\"playerID\") %>%\n  filter(!is.na(POS)  & !is.na(salary))\n```\n:::\n\n\n\n\nFinally, we add their first and last name:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplayers <- People %>%\n  select(playerID, nameFirst, nameLast, debut) %>%\n  mutate(debut = as.Date(debut)) %>%\n  right_join(players, by=\"playerID\")\n```\n:::\n\n\n\n\nIf you are a baseball fan (or were years ago), you will recognize the top 10 players:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplayers %>% select(nameFirst, nameLast, POS, salary, R_hat) %>%\n  arrange(desc(R_hat)) %>% top_n(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   nameFirst nameLast POS   salary    R_hat\n1      Barry    Bonds  LF 15000000 8.441480\n2      Larry   Walker  RF 12666667 8.344316\n3       Todd   Helton  1B  5000000 7.764649\n4      Manny  Ramirez  LF 15462727 7.714582\n5      Sammy     Sosa  RF 15000000 7.559582\n6       Jeff  Bagwell  1B 11000000 7.405572\n7       Mike   Piazza   C 10571429 7.343984\n8      Jason   Giambi  1B 10428571 7.263690\n9      Edgar Martinez  DH  7086668 7.259399\n10       Jim    Thome  1B  8000000 7.231955\n```\n\n\n:::\n:::\n\n\n\n\n\n### Picking nine players\n\nOn average, players with a higher metric have higher salaries:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplayers %>% ggplot(aes(salary, R_hat, color = POS)) +\n  geom_point() +\n  scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/predicted-runs-vs-salary-1.png){width=672}\n:::\n:::\n\n\n\n\n<!--Notice the very high salaries for most players. We do see some low-cost players with very high metrics. These will be great for our team. Some of these are likely young players that have not yet been able to negotiate a salary and are unavailable.\n\nHere we remake plot without players that debuted before 1998. We use the __lubridate__ function `year`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate)\nplayers %>% filter(year(debut) < 1998) %>%\n ggplot(aes(salary, R_hat, color = POS)) +\n  geom_point() +\n  scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](07a_files/figure-html/predicted-runs-vs-salary-no-rookies-1.png){width=672}\n:::\n:::\n\n\n\n-->\n\n\nWe can search for good deals by looking at players who produce many more runs than others with similar salaries. We can use this table to decide what players to pick and keep our total salary below the 40 million dollars Billy Beane had to work with. This can be done using what computer scientists call linear programming. This is not something we teach, but here are the position players selected with this approach:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> nameFirst </th>\n   <th style=\"text-align:left;\"> nameLast </th>\n   <th style=\"text-align:left;\"> POS </th>\n   <th style=\"text-align:right;\"> salary </th>\n   <th style=\"text-align:right;\"> R_hat </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Todd </td>\n   <td style=\"text-align:left;\"> Helton </td>\n   <td style=\"text-align:left;\"> 1B </td>\n   <td style=\"text-align:right;\"> 5000000 </td>\n   <td style=\"text-align:right;\"> 7.764649 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Mike </td>\n   <td style=\"text-align:left;\"> Piazza </td>\n   <td style=\"text-align:left;\"> C </td>\n   <td style=\"text-align:right;\"> 10571429 </td>\n   <td style=\"text-align:right;\"> 7.343984 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Edgar </td>\n   <td style=\"text-align:left;\"> Martinez </td>\n   <td style=\"text-align:left;\"> DH </td>\n   <td style=\"text-align:right;\"> 7086668 </td>\n   <td style=\"text-align:right;\"> 7.259399 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Jim </td>\n   <td style=\"text-align:left;\"> Edmonds </td>\n   <td style=\"text-align:left;\"> CF </td>\n   <td style=\"text-align:right;\"> 7333333 </td>\n   <td style=\"text-align:right;\"> 6.552456 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Jeff </td>\n   <td style=\"text-align:left;\"> Kent </td>\n   <td style=\"text-align:left;\"> 2B </td>\n   <td style=\"text-align:right;\"> 6000000 </td>\n   <td style=\"text-align:right;\"> 6.391614 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Phil </td>\n   <td style=\"text-align:left;\"> Nevin </td>\n   <td style=\"text-align:left;\"> 3B </td>\n   <td style=\"text-align:right;\"> 2600000 </td>\n   <td style=\"text-align:right;\"> 6.163936 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Matt </td>\n   <td style=\"text-align:left;\"> Stairs </td>\n   <td style=\"text-align:left;\"> RF </td>\n   <td style=\"text-align:right;\"> 500000 </td>\n   <td style=\"text-align:right;\"> 6.062372 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Henry </td>\n   <td style=\"text-align:left;\"> Rodriguez </td>\n   <td style=\"text-align:left;\"> LF </td>\n   <td style=\"text-align:right;\"> 300000 </td>\n   <td style=\"text-align:right;\"> 5.938315 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> John </td>\n   <td style=\"text-align:left;\"> Valentin </td>\n   <td style=\"text-align:left;\"> SS </td>\n   <td style=\"text-align:right;\"> 550000 </td>\n   <td style=\"text-align:right;\"> 5.273441 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nWe see that all these players have above average BB and most have above average HR rates, while the same is not true for singles. Here is a table with statistics standardized across players so that, for example, above average HR hitters have values above 0.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> nameLast </th>\n   <th style=\"text-align:right;\"> BB </th>\n   <th style=\"text-align:right;\"> singles </th>\n   <th style=\"text-align:right;\"> doubles </th>\n   <th style=\"text-align:right;\"> triples </th>\n   <th style=\"text-align:right;\"> HR </th>\n   <th style=\"text-align:right;\"> AVG </th>\n   <th style=\"text-align:right;\"> R_hat </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Helton </td>\n   <td style=\"text-align:right;\"> 0.9088340 </td>\n   <td style=\"text-align:right;\"> -0.2147828 </td>\n   <td style=\"text-align:right;\"> 2.6489997 </td>\n   <td style=\"text-align:right;\"> -0.3105275 </td>\n   <td style=\"text-align:right;\"> 1.5221254 </td>\n   <td style=\"text-align:right;\"> 2.6704562 </td>\n   <td style=\"text-align:right;\"> 2.5316660 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Piazza </td>\n   <td style=\"text-align:right;\"> 0.3281058 </td>\n   <td style=\"text-align:right;\"> 0.4231217 </td>\n   <td style=\"text-align:right;\"> 0.2037161 </td>\n   <td style=\"text-align:right;\"> -1.4181571 </td>\n   <td style=\"text-align:right;\"> 1.8253653 </td>\n   <td style=\"text-align:right;\"> 2.1990055 </td>\n   <td style=\"text-align:right;\"> 2.0890701 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Martinez </td>\n   <td style=\"text-align:right;\"> 2.1352215 </td>\n   <td style=\"text-align:right;\"> -0.0051702 </td>\n   <td style=\"text-align:right;\"> 1.2649044 </td>\n   <td style=\"text-align:right;\"> -1.2242578 </td>\n   <td style=\"text-align:right;\"> 0.8079817 </td>\n   <td style=\"text-align:right;\"> 2.2032836 </td>\n   <td style=\"text-align:right;\"> 2.0000756 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Edmonds </td>\n   <td style=\"text-align:right;\"> 1.0706548 </td>\n   <td style=\"text-align:right;\"> -0.5579104 </td>\n   <td style=\"text-align:right;\"> 0.7912381 </td>\n   <td style=\"text-align:right;\"> -1.1517126 </td>\n   <td style=\"text-align:right;\"> 0.9730052 </td>\n   <td style=\"text-align:right;\"> 0.8543566 </td>\n   <td style=\"text-align:right;\"> 1.2562767 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Kent </td>\n   <td style=\"text-align:right;\"> 0.2316321 </td>\n   <td style=\"text-align:right;\"> -0.7322902 </td>\n   <td style=\"text-align:right;\"> 2.0113988 </td>\n   <td style=\"text-align:right;\"> 0.4483097 </td>\n   <td style=\"text-align:right;\"> 0.7658693 </td>\n   <td style=\"text-align:right;\"> 0.7871932 </td>\n   <td style=\"text-align:right;\"> 1.0870488 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Nevin </td>\n   <td style=\"text-align:right;\"> 0.3066863 </td>\n   <td style=\"text-align:right;\"> -0.9051225 </td>\n   <td style=\"text-align:right;\"> 0.4787634 </td>\n   <td style=\"text-align:right;\"> -1.1908955 </td>\n   <td style=\"text-align:right;\"> 1.1927055 </td>\n   <td style=\"text-align:right;\"> 0.1048721 </td>\n   <td style=\"text-align:right;\"> 0.8475017 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Stairs </td>\n   <td style=\"text-align:right;\"> 1.0996635 </td>\n   <td style=\"text-align:right;\"> -1.5127562 </td>\n   <td style=\"text-align:right;\"> -0.0460876 </td>\n   <td style=\"text-align:right;\"> -1.1285395 </td>\n   <td style=\"text-align:right;\"> 1.1209081 </td>\n   <td style=\"text-align:right;\"> -0.5608456 </td>\n   <td style=\"text-align:right;\"> 0.7406428 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Rodriguez </td>\n   <td style=\"text-align:right;\"> 0.2011513 </td>\n   <td style=\"text-align:right;\"> -1.5963595 </td>\n   <td style=\"text-align:right;\"> 0.3324557 </td>\n   <td style=\"text-align:right;\"> -0.7823620 </td>\n   <td style=\"text-align:right;\"> 1.3202734 </td>\n   <td style=\"text-align:right;\"> -0.6723416 </td>\n   <td style=\"text-align:right;\"> 0.6101181 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Valentin </td>\n   <td style=\"text-align:right;\"> 0.1802855 </td>\n   <td style=\"text-align:right;\"> -0.9287069 </td>\n   <td style=\"text-align:right;\"> 1.7940379 </td>\n   <td style=\"text-align:right;\"> -0.4348410 </td>\n   <td style=\"text-align:right;\"> -0.0452462 </td>\n   <td style=\"text-align:right;\"> -0.4717038 </td>\n   <td style=\"text-align:right;\"> -0.0894187 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n## The regression fallacy\n\nWikipedia defines the _sophomore slump_ as:\n\n> A sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels).\n\nIn Major League Baseball, the rookie of the year (ROY) award is given to the first-year player who is judged to have performed the best. The _sophmore slump_ phrase is used to describe the observation that ROY award winners don't do as well during their second year. For example, this Fox Sports article^[http://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715] asks \"Will MLB's tremendous rookie class of 2015 suffer a sophomore slump?\".\n\nDoes the data confirm the existence of a sophomore slump? Let's take a look. Examining the data for batting average, we see that this observation holds true for the top performing ROYs:\n\n<!--The data is available in the Lahman library, but we have to do some work to create a table with the statistics for all the ROY. First we create a table with player ID, their names, and their most played position.-->\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n<!--\nNow, we will create a table with only the ROY award winners and add their batting statistics. We filter out pitchers, since pitchers are not given awards for batting and we are going to focus on offense. Specifically, we will focus on batting average since it is the summary that most pundits talk about when discussing the sophomore slump:\n-->\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n<!--\nWe also will keep only the rookie and sophomore seasons and remove players that did not play sophomore seasons:\n-->\n\n\n\n::: {.cell}\n\n:::\n\n\n\n<!--\nFinally, we will use the `spread` function to have one column for the rookie and sophomore years batting averages:\n-->\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> nameFirst </th>\n   <th style=\"text-align:left;\"> nameLast </th>\n   <th style=\"text-align:right;\"> rookie_year </th>\n   <th style=\"text-align:right;\"> rookie </th>\n   <th style=\"text-align:right;\"> sophomore </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Willie </td>\n   <td style=\"text-align:left;\"> McCovey </td>\n   <td style=\"text-align:right;\"> 1959 </td>\n   <td style=\"text-align:right;\"> 0.3541667 </td>\n   <td style=\"text-align:right;\"> 0.2384615 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Ichiro </td>\n   <td style=\"text-align:left;\"> Suzuki </td>\n   <td style=\"text-align:right;\"> 2001 </td>\n   <td style=\"text-align:right;\"> 0.3497110 </td>\n   <td style=\"text-align:right;\"> 0.3214838 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Al </td>\n   <td style=\"text-align:left;\"> Bumbry </td>\n   <td style=\"text-align:right;\"> 1973 </td>\n   <td style=\"text-align:right;\"> 0.3370787 </td>\n   <td style=\"text-align:right;\"> 0.2333333 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Fred </td>\n   <td style=\"text-align:left;\"> Lynn </td>\n   <td style=\"text-align:right;\"> 1975 </td>\n   <td style=\"text-align:right;\"> 0.3314394 </td>\n   <td style=\"text-align:right;\"> 0.3136095 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Albert </td>\n   <td style=\"text-align:left;\"> Pujols </td>\n   <td style=\"text-align:right;\"> 2001 </td>\n   <td style=\"text-align:right;\"> 0.3288136 </td>\n   <td style=\"text-align:right;\"> 0.3135593 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nIn fact, the proportion of players that have a lower batting average their sophomore year is 0.7090909.\n\nSo is it \"jitters\" or \"jinx\"? To answer this question, let's turn our attention to all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year).\n\n<!--We perform similar operations to what we did above: -->\n\n\n\n\n::: {.cell warnings='false'}\n\n:::\n\n\n\n\nThe same pattern arises when we look at the top performers: batting averages go down for most of the top performers.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> nameFirst </th>\n   <th style=\"text-align:left;\"> nameLast </th>\n   <th style=\"text-align:right;\"> 2013 </th>\n   <th style=\"text-align:right;\"> 2014 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Miguel </td>\n   <td style=\"text-align:left;\"> Cabrera </td>\n   <td style=\"text-align:right;\"> 0.3477477 </td>\n   <td style=\"text-align:right;\"> 0.3126023 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Hanley </td>\n   <td style=\"text-align:left;\"> Ramirez </td>\n   <td style=\"text-align:right;\"> 0.3453947 </td>\n   <td style=\"text-align:right;\"> 0.2828508 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Michael </td>\n   <td style=\"text-align:left;\"> Cuddyer </td>\n   <td style=\"text-align:right;\"> 0.3312883 </td>\n   <td style=\"text-align:right;\"> 0.3315789 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Scooter </td>\n   <td style=\"text-align:left;\"> Gennett </td>\n   <td style=\"text-align:right;\"> 0.3239437 </td>\n   <td style=\"text-align:right;\"> 0.2886364 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Joe </td>\n   <td style=\"text-align:left;\"> Mauer </td>\n   <td style=\"text-align:right;\"> 0.3235955 </td>\n   <td style=\"text-align:right;\"> 0.2769231 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nBut these are not rookies! Also, look at what happens to the worst performers of 2013:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> nameFirst </th>\n   <th style=\"text-align:left;\"> nameLast </th>\n   <th style=\"text-align:right;\"> 2013 </th>\n   <th style=\"text-align:right;\"> 2014 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Danny </td>\n   <td style=\"text-align:left;\"> Espinosa </td>\n   <td style=\"text-align:right;\"> 0.1582278 </td>\n   <td style=\"text-align:right;\"> 0.2192192 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Dan </td>\n   <td style=\"text-align:left;\"> Uggla </td>\n   <td style=\"text-align:right;\"> 0.1785714 </td>\n   <td style=\"text-align:right;\"> 0.1489362 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Jeff </td>\n   <td style=\"text-align:left;\"> Mathis </td>\n   <td style=\"text-align:right;\"> 0.1810345 </td>\n   <td style=\"text-align:right;\"> 0.2000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> B. J. </td>\n   <td style=\"text-align:left;\"> Upton </td>\n   <td style=\"text-align:right;\"> 0.1841432 </td>\n   <td style=\"text-align:right;\"> 0.2080925 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Adam </td>\n   <td style=\"text-align:left;\"> Rosales </td>\n   <td style=\"text-align:right;\"> 0.1904762 </td>\n   <td style=\"text-align:right;\"> 0.2621951 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nTheir batting averages mostly go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as the sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07a_files/figure-html/regression-fallacy-1.png){width=40%}\n:::\n:::\n\n\n\n\nThe correlation is 0.460254 and\nthe data look very much like a bivariate normal distribution, which means we predict a 2014 batting average $Y$ for any given player that had a 2013 batting average $X$ with:\n\n$$ \\frac{Y - .255}{.032} = 0.46 \\left( \\frac{X - .261}{.023}\\right) $$\n\nBecause the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It's not a jinx; it's just due to chance. The ROY are selected from the top values of $X$ so it is expected that $Y$ will regress to the mean.\n\n\n",
    "supporting": [
      "07a_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}