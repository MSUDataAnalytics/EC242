{
  "hash": "092ea1cae75e8c3913ef4b1e39d70b4f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Illustrating Classification\"\ntype: docs\nweight: 1\neditor_options:\n  chunk_output_type: console\n---\n\n\n\n\n::: {.callout-note}\n\nToday's example will build on material from the Principles lecture (earlier this week).\n\n:::\n\n\n\n## Predicting Defaults\n\nToday, we will continue to use the `ISLR` data on defaults:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR)\nlibrary(tibble)\nDefault = ISLR::Default\n```\n:::\n\n\n\n\n::: {.callout-note}\nIn our first breakout:\n\n1. Clean the data so that the `default` column is a binary indicator for default\n\n2. Build a logistic model to predict `default` using any combination of variables and interactions in the data. For now, just use your best judgement for choosing the variables and interactions -- your lab for this week will have you formally tune a logistic model to optimize results.\n\n3. Use a Bayes Classifier cutoff of .50 to generate your classifier output. Do you need to alter the cutoff?\n\n:::\n\n\nBack in class, let's look at how we did. What variables were most useful in explaining `default`? \n\n\n::: {.callout-note}\nIn our second breakout, we will create a ROC curve manually. To do this\n\n1. Take your model from the first breakout, and using a loop (or `sapply`), step through a large number of possible cutoffs for classification ranging from 0 to 1.\n\n2. For each cutoff, generate a confusion matrix with *accuracy*, *sensitivity* and *specificity*.\n\n3. Combine the cutoff with the *sensitivity* and *specificity* results and make a ROC plot. Use `ggplot` for your plot and map the color aesthetic to the cutoff value.\n\n4. Calculate the AUC (the *area under the curve*). This is a little tricky but can be done with your data.\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}