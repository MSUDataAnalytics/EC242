{
  "hash": "7cb52b6c2af08d2955771069428a0980",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Regression\"\nlastmod: \"2025-02-18\"\noutput:\n  blogdown::html_page:\n    toc: true\n    code-overflow: scroll\n---\n\n\n\n\n\n## Sharing Screen \n\nAs we've discussed, we'll be using the share screen feature to discuss folks' output. Please see link on screen. \n\n## Preliminaries\n\nFor this example, we're going to use historical weather data from [Dark Sky](https://darksky.net/forecast/33.7546,-84.39/us12/en) about wind speed and temperature trends for downtown Atlanta ([specifically `33.754557, -84.390009`](https://www.google.com/maps/place/33°45'16.4\"N+84°23'24.0\"W/@33.754557,-84.3921977,17z/)) in 2019. We downloaded this data using Dark Sky's (about-to-be-retired-because-they-were-bought-by-Apple) API using the [ **darksky** package](https://github.com/hrbrmstr/darksky).\n\nIf you want to follow along with this example, you can download the data below (you'll likely need to right click and choose \"Save Link As…\" and place it in a `data` folder next to your working R script):\n\n- [<i class=\"fas fa-file-csv\"></i> `atl-weather-2019.csv`](/data/atl-weather-2019.csv)\n\n\n## Code\n\n\n\n\n\n\n\n\n\n### Load the data\n\nFirst, we load the libraries we'll be using:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(patchwork)  # For combining ggplot plots\nlibrary(GGally)     # New one, for scatterplot matrices\nlibrary(broom)      # For converting model objects to data frames\n```\n:::\n\n\n\n\nThen we load the data with `read_csv()`. This is `tidyverse`'s CSV reader (base `R` is `read.csv`). Here we assume that the CSV file was downloaded and lives in a subfolder named `data`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nweather_atl <- read_csv(\"data/atl-weather-2019.csv\")\n```\n:::\n\n\n\n\n\n\n\n\nOf course, you can also load it directly with the URL from above (as we showed before).\n\n\n### Scatterplot matrices\n\nThe foundation of linear regression is corrleation. We can visualize the correlations between pairs of variables with the `ggpairs()` function in the **GGally** package. For instance, how correlated are high and low temperatures, humidity, wind speed, and the chance of precipitation? We first make a smaller dataset with just those columns, and then we feed that dataset into `ggpairs()` to see all the correlation information:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(GGally)\n\nweather_correlations <- weather_atl %>%\n  select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability)\n\nggpairs(weather_correlations)\n```\n\n::: {.cell-output-display}\n![](06b_files/figure-html/ggpairs-1.png){fig-align='center' width=864}\n:::\n:::\n\n\n\n\nIt looks like high and low temperatures are extremely highly positively correlated (r = 0.92). Wind speed and temperature are moderately negatively correlated, with low temperatures having a stronger negative correlation (r = -0.45). There's no correlation whatsoever between low temperatures and the precipitation probability (r = -0.03) or humidity and high temperatures (r = -0.03).\n\nEven though `ggpairs()` doesn't use the standard `ggplot(...) + geom_whatever()` syntax we're familiar with, it does behind the scenes, so you can add regular ggplot layers to it:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggpairs(weather_correlations) + \n  labs(title = \"Correlations!\") +\n  theme_dark()\n```\n\n::: {.cell-output-display}\n![](06b_files/figure-html/ggpairs-layers-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n\n::: {.callout-note}\n\n## TRY IT\n\nHere's some code to load data from the Residential Energy Consumption Survey (RECS), a [once-a-decade survey of household energy consumption done by the Energy Information Administration.](https://www.eia.gov/consumption/residential/data/2020/). Use the code here to load the data. I've selected a few variables and renamed them to something intuitive. Each observation is a household surveyed by the EIA. The variable named `EnergyUsed` is the total BTU's of energy consumed by that household.\n\nMost of the variables are self-explanatory except the following:\n\n- `AgeOfHome` is on a 1-9 scale where 1 is the oldest and 9 is the newest. This is \"discretized\" age.\n\n- `HeaterAge` is the age of the home's heater from 1 to 6 where 6 is the oldest and 1 is the newest.\n\n- `TVSize` is on a scale of 1 (smallest) to 4 (largest)\n\n- `TempHome` tells us the home's thermostat setting when home (in degrees farenheit)\n\n\nYour task is to make a ggpairs plot for `EnergyUsed` and five of the variables in the RECS data. What variables do you think are correlated with total energy use `EnergyUsed`? And **what can we learn about energy consumption from these correlations?**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nRECS = read.csv('https://www.eia.gov/consumption/residential/data/2009/csv/recs2009_public.csv', stringsAsFactors = F) %>%\n  as_tibble() %>%  \n  dplyr::select(EnergyUsed = TOTALBTU,\n                ColdDays = HDD65, HotDayss = CDD65, SquareFeet = TOTHSQFT, CarsGarage = SIZEOFGARAGE, \n                AgeOfHome = YEARMADERANGE, TreeShade = TREESHAD, TVSize = TVSIZE1, HeaterAge = EQUIPAGE, HasAC = AIRCOND,\n                TempHome = TEMPHOME) %>%\n  dplyr::filter(HeaterAge != -2 & TempHome !=-2)  # these are NAs\n```\n:::\n\n\n\n\n\n\n:::\n\n### Correlograms\n\nScatterplot matrices typically include way too much information to be used in actual publications. I use them when doing my own analysis just to see how different variables are related, but I rarely polish them up for public consumption. In some interesting supplemental material [located here](https://clauswilke.com/dataviz/visualizing-associations.html#associations-correlograms), Claus Wilke showed a type of plot called a [*correlogram*](https://serialmentor.com/dataviz/visualizing-associations.html#associations-correlograms) which *is* more appropriate for publication.\n\nThese are essentially heatmaps of the different correlation coefficients. To make these with ggplot, we need to do a little bit of extra data processing, mostly to reshape data into a long, tidy format that we can plot. Here's how.\n\nFirst we need to build a correlation matrix of the main variables we care about. Ordinarily the `cor()` function in R takes two arguments—x and y—and it will return a single correlation value. If you feed a data frame into `cor()` though, it'll calculate the correlation between each pair of columns. <span style=\"     color: red !important;\" >But be careful - don't feed in hundreds of variables by accident!</span>\n\n\n\n\n::: {.cell layout-align=\"center\" width='150'}\n\n```{.r .cell-code}\n# Create a correlation matrix\nthings_to_correlate <- weather_atl %>%\n  select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) %>%\n  cor()\n\nthings_to_correlate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  temperatureHigh temperatureLow humidity windSpeed precipProbability\ntemperatureHigh              1.00          0.920   -0.030    -0.377            -0.124\ntemperatureLow               0.92          1.000    0.112    -0.450            -0.026\nhumidity                    -0.03          0.112    1.000     0.011             0.722\nwindSpeed                   -0.38         -0.450    0.011     1.000             0.196\nprecipProbability           -0.12         -0.026    0.722     0.196             1.000\n```\n\n\n:::\n:::\n\n\n\n\nThe two halves of this matrix (split along the diagonal line) are identical, so we can remove the lower triangle with this code (which will set all the cells in the lower triangle to `NA`):\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Get rid of the lower triangle\nthings_to_correlate[lower.tri(things_to_correlate)] <- NA\nthings_to_correlate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  temperatureHigh temperatureLow humidity windSpeed precipProbability\ntemperatureHigh                 1           0.92    -0.03    -0.377            -0.124\ntemperatureLow                 NA           1.00     0.11    -0.450            -0.026\nhumidity                       NA             NA     1.00     0.011             0.722\nwindSpeed                      NA             NA       NA     1.000             0.196\nprecipProbability              NA             NA       NA        NA             1.000\n```\n\n\n:::\n:::\n\n\n\n\nFinally, in order to plot this, the data needs to be in tidy (or long) format. Here we convert the `things_to_correlate` matrix into a data frame, add a column for the row names, take all the columns and put them into a single column named `measure1`, and take all the correlation numbers and put them in a column named `cor` In the end, we make sure the measure variables are ordered by their order of appearance (otherwise they plot alphabetically and don't make a triangle)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nthings_to_correlate_long <- things_to_correlate %>%\n  # Convert from a matrix to a data frame\n  as.data.frame() %>%\n  # Matrixes have column names that don't get converted to columns when using\n  # as.data.frame(), so this adds those names as a column\n  rownames_to_column(\"measure2\") %>%\n  # Make this long. Take all the columns except measure2 and put their names in\n  # a column named measure1 and their values in a column named cor\n  pivot_longer(cols = -measure2,\n               names_to = \"measure1\",\n               values_to = \"cor\") %>%\n  # Make a new column with the rounded version of the correlation value\n  mutate(nice_cor = round(cor, 2)) %>%\n  # Remove rows where the two measures are the same (like the correlation\n  # between humidity and humidity)\n  filter(measure2 != measure1) %>%\n  # Get rid of the empty triangle\n  filter(!is.na(cor)) %>%\n  # Put these categories in order\n  mutate(measure1 = fct_inorder(measure1),\n         measure2 = fct_inorder(measure2))\n\nthings_to_correlate_long\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 4\n   measure2        measure1              cor nice_cor\n   <fct>           <fct>               <dbl>    <dbl>\n 1 temperatureHigh temperatureLow     0.920      0.92\n 2 temperatureHigh humidity          -0.0301    -0.03\n 3 temperatureHigh windSpeed         -0.377     -0.38\n 4 temperatureHigh precipProbability -0.124     -0.12\n 5 temperatureLow  humidity           0.112      0.11\n 6 temperatureLow  windSpeed         -0.450     -0.45\n 7 temperatureLow  precipProbability -0.0255    -0.03\n 8 humidity        windSpeed          0.0108     0.01\n 9 humidity        precipProbability  0.722      0.72\n10 windSpeed       precipProbability  0.196      0.2 \n```\n\n\n:::\n:::\n\n\n\n\nPhew. With the data all tidied like that, we can make a correlogram with a heatmap. We have manipulated the fill scale a little so that it's diverging with three colors: a high value, a midpoint value, and a low value.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(things_to_correlate_long,\n       aes(x = measure2, y = measure1, fill = cor)) +\n  geom_tile() +\n  geom_text(aes(label = nice_cor)) +\n  scale_fill_gradient2(low = \"#E16462\", mid = \"white\", high = \"#0D0887\",\n                       limits = c(-1, 1)) +\n  labs(x = NULL, y = NULL, fill = 'Corr.') +\n  coord_equal() +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](06b_files/figure-html/cor-heatmap-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n\nInstead of using a heatmap, we can also use points, which encode the correlation information both as color *and* as size. To do that, we just need to switch `geom_tile()` to `geom_point()` and set the `size = cor` mapping (note the use of the absolute value function):\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(things_to_correlate_long,\n       aes(x = measure2, y = measure1, color = cor)) +\n  # Size by the absolute value so that -0.7 and 0.7 are the same size\n  geom_point(aes(size = abs(cor))) +\n  scale_color_gradient2(low = \"#E16462\", mid = \"white\", high = \"#0D0887\",\n                        limits = c(-1, 1)) +\n  scale_size_area(max_size = 15, limits = c(-1, 1), guide = 'none') +\n  labs(x = NULL, y = NULL, color = 'Corr.') +\n  coord_equal() +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank())\n```\n\n::: {.cell-output-display}\n![](06b_files/figure-html/cor-points-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n## Simple regression\nWe finally get to this week's material. Although correlation is nice, we eventually may want to visualize regression. The lecture has shown us some very intuitive, straightforward ways to think about regression (aka, a line). Simple regression is easy to visualize, since you're only working with an $X$ and a $Y$. For instance, what's the relationship between humidity and high temperatures during the summer?\n\nFirst, let's filter the Atlanta weather data to only look at the summer. We also add a column to scale up the humidity value—right now it's on a scale of 0-1 (for percentages), but when interpreting regression we talk about increases in whole units, so we'd talk about moving from 0% humidity to 100% humidity, which isn't helpful, so instead we multiply everything by 100 so we can talk about moving from 50% humidity to 51% humidity. We also scale up a couple other variables that we'll use in the larger model later.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nweather_atl_summer <- weather_atl %>%\n  filter(time >= \"2019-05-01\", time <= \"2019-09-30\") %>%\n  mutate(humidity_scaled = humidity * 100,\n         moonPhase_scaled = moonPhase * 100,\n         precipProbability_scaled = precipProbability * 100,\n         cloudCover_scaled = cloudCover * 100)\n```\n:::\n\n\n\n\nThen we can build a simple, single-variable regression model for the high temperature \"regressed on\" humidity. We would write this regression as:\n\n$$HighTempScaled_i = \\beta_0 + \\beta_1 HumidityScaled_i + \\varepsilon_i$$\n\nEach of our 153 observations is indexed by $i$. Since it will almost never be the case that $\\beta_0 + \\beta_1 HumidityScaled_i = HighTempScaled$, we include the error term $\\varepsilon_i$ to account for things that aren't in our regression. \n\nOur estimated parameters $\\beta_0, \\beta_1$ define the line of best fit, or the \"regression line\". That regression line is our conditional expectation function:\n\n$$E[HighTempScaled|HumidityScaled] = \\beta_0 + \\beta_1 HumidityScaled$$\n\nNote that we do not have an error term here -- the $E[]$ doesn't have an error term in it, only the regression function does.\n\nTo do this in R, we'll first use the formula from [Principles 06](content/Week_06/06a.html#the-regression-line-1):\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nweather_atl_results = weather_atl_summer %>%\n  dplyr::summarize(mu_x = mean(humidity_scaled),\n                   mu_y = mean(temperatureHigh),\n                   sd_x = sd(humidity_scaled),\n                   sd_y = sd(temperatureHigh),\n                   rho = sum(((temperatureHigh-mu_y)/sd_y)*((humidity_scaled-mu_x)/sd_x))/(n()-1) ) %>%\n                  # Note in rho we had to use that same \"N-1\" correction. cor() does this for us automatically\n  dplyr::mutate(beta_1 = rho*(sd_y/sd_x),\n                beta_0 = mu_y - beta_1*mu_x)\n\nprint(weather_atl_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 7\n   mu_x  mu_y  sd_x  sd_y    rho beta_1 beta_0\n  <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>  <dbl>\n1  64.8  88.5  10.6  5.33 -0.481 -0.241   104.\n```\n\n\n:::\n:::\n\n\n\n\nNote that the formulas from  [Principles 06](content/Week_06/06a.html#the-regression-line-1) have a direct translation into the code. Also note that we used `summarize` on the data so that we get only **one** row of data (no more 1,000+ rows, we've summarized them using `mean` and `sd`).\n\n### Interpretation of coefficients\n\nOur estimates of the $\\beta_0, \\beta_1$ are our \"coefficients\". It's important we interpret them correctly. We can interpret these coefficients like so:\n\n- `beta_0` (or $\\beta_0$) is the intercept. By definition of an intercept, it shows that the average temperature when there's **0% humidity** is 104°. There are no days with 0% humidity though, so we can ignore the intercept—it's really just here so that we can draw the line.\n\n- `beta_1` (or $\\beta_1$) is the coefficient for `HumidityScaled`. It is the slope of the regression line, and thus the change in E[HighTempScaled] per 1-unit increase in $HumidityScaled$. It shows that a one percent increase in humidity is **associated with** a 0.241° decrease in temperature, on average.\n\n  - This (and $\\beta_0$) are random variables -- if we re-draw the sample, we'll see a slightly different relationship. We'll talk about the standard error of this (and how to see it) in a little bit.\n\nVisualizing this model is simple, since there are only two variables. We visualized the regression line in the Galton Heights data during Content 05, so let's just use that code.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(weather_atl_summer, aes(x = humidity_scaled, y = temperatureHigh)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = weather_atl_results$beta_0, slope = weather_atl_results$beta_1)\n```\n\n::: {.cell-output-display}\n![](06b_files/figure-html/plot-simple-formula-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n\nEvery point along the regression line is the $E[HighTempScaled|HumidityScaled]$. Put in whatever value of humidity you want, and the line tells you what the expectation of the high temperature will be.\n\nAnother way we can add a regression line in a 2-D plot is using `geom_smooth`. `geom_smooth` is a `ggplot` geometry that adds a line of best fit. This can be flexible, or this can be based on a linear relationship -- the regression line. We'll see how to use built-in functions in R to do regression later, but for now, we'll just ask ggplot to make a **l**inear **m**odel (\"lm\") for a line of best fit. The slope of that line is going to be exactly our estimate of $\\beta_1$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(weather_atl_summer,\n       aes(x = humidity_scaled, y = temperatureHigh)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](06b_files/figure-html/plot-simple-model-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n\nAnd indeed, we see the same line as we made \"manually\" before.\n\n\n\n::: {.callout-note}\n\n## TRY IT\n\nLet's use that `RECS` data to regress `EnergyUsed` on one of the variables from your scatterplot matrix. Note that we say the left-hand-side variable (the \"Y\") is *regressed on* the right-hand-side (the \"X\") variable. \n\nUsing the formulas and codes for regression, choose the variable you want to use in the regression, run it, and visualize the result.\n\nWe'll have you share your work and interpret the results when done. [https://bit.ly/EC242](https://bit.ly/EC242)\n\n:::\n\n\n\n\n",
    "supporting": [
      "06b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}