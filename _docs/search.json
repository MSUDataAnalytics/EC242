[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Innovations in statistical learning have created many engineering breakthroughs. From real time voice recognition to automatic categorization (and in some cases production) of news stories, machine learning is transforming the way we live our lives. These techniques are, at their heart, novel ways to work with data, and therefore they should have implications for social science. This course explores the intersection of statistical learning (or machine learning) and social science and aims to answer two primary questions about these new techniques:\n\nHow does statistical learning work and what kinds of statistical guarantees can be made about the performance of statistical-learning algorithms?\nHow can statistical learning be used to answer questions that interest social science researchers, such as testing theories or improving social policy?\n\nIn order to address these questions, we will cover so-called “standard” techniques such as supervised and unsupervised learning, statistical learning theory and nonparametric and Bayesian approaches. If it were up to me, this course would be titled “Statistical Learning for Social Scientists”—I believe this provides a more appropriate guide to the content of this course. And while this class will cover these novel statistical methodologies in some detail, it is not a substitute for the appropriate class in Computer Science or Statistics. Nor is this a class that teaches specific skills for the job market. Rather, this class will teach you to think about data analytics broadly. We will spend a great deal of time learning how to interpret the output of statistical learning algorithms and approaches, and will also spend a great deal of time on better understanding the basic ideas in statistical learning. This, of course, comes at some cost in terms of time spent on learning computational and/or programming skills.\nEnrollment for credit in this course is simply not suitable for those unprepared in or uninterested in elementary statistical theory no matter the intensity of interest in machine learning or “Big Data”. Really.\nYou will be required to understand elementary mathematics in this course and should have at least some exposure to statistical theory. The class is front-loaded technically: early lectures are more mathematically oriented, while later lectures are more applied.\nThe topics covered in this course are listed later in this document. I will assign readings sparingly from  Introduction to Statistical Learning, henceforth referred to as ISL. This text is available for free online and, for those who like physical books, can be purchased for about $25. Importantly, the lectures deviate a fair bit from the reading, and thus you will rely on your course notes much more than you might in other classes.\nIf—after you have read this document and preferably after attending the first lecture—you have any questions about whether this course is appropriate for you, please come talk to me.\n\n\n\nThe focus of this course is conceptual. The goal is to create a working understanding of when and how tools from computer science and statistics can be profitably applied to problems in social science. Though students will be required to apply some of these techniques themselves, this course is not…\n…a replacement for EC420, EC422, or a course in causal inference.\nAs social scientists, we are most often concerned with causal inference in order to anaOpelyze and write policies. Statistical learning and the other methods we will discuss in this course are generally not well-suited to these problems, and while I’ll give a short overview of standard methods, this is only to build intuitions. Ultimately, this course has a different focus and you should still pursue standard methodological insights from your home departments.\n…a course on the computational aspects of the underlying methods.\nThere are many important innovations that have made machine learning techniques computationally feasible. We will not discuss these, as there are computer science courses better equipped to cover them. When appropriate, we will discuss whether something is computable, and we will even give rough approximations of the amount of time required (e.g. P vs NP). But we will not discuss how optimizers work or best practices in programming.\n…a primer on the nitty-gritty of how to use these tools or a way to pad your resume.\nThe mechanics of implementation, whether it be programming languages or learning to use APIs, will not be covered in any satisfying level of depth. Students will be expected to learn most of the programming skills on their own. Specifically, while there will be some material to remind you of basic R commands, this is not a good course for people who are simply looking to learn the mechanics of programming. This course is designed to get you to use both traditional analytics and, eventually, machine learning tools. We will do some review of basic programming, and you will have the opportunity to explore topics that interest you through a final project, but ultimately this is a course that largely focuses on the theoretical and practical aspects of statistical learning as applied to social science and not a class on programming.\nPerhaps most importantly, this course is an attempt to push undergraduate education toward the frontiers in social science. Accordingly, please allow some messiness. Some topics may be underdeveloped for a given person’s passions, but given the wide variety of technical skills and overall interests, this is a near certainty. Both the challenge and opportunity of this area comes from the fact that there is no fully developed, wholly unifying framework. Our collective struggle—me from teaching, you from learning—will ultimately bear fruit."
  },
  {
    "objectID": "syllabus.html#what-is-this-course",
    "href": "syllabus.html#what-is-this-course",
    "title": "Syllabus",
    "section": "",
    "text": "Innovations in statistical learning have created many engineering breakthroughs. From real time voice recognition to automatic categorization (and in some cases production) of news stories, machine learning is transforming the way we live our lives. These techniques are, at their heart, novel ways to work with data, and therefore they should have implications for social science. This course explores the intersection of statistical learning (or machine learning) and social science and aims to answer two primary questions about these new techniques:\n\nHow does statistical learning work and what kinds of statistical guarantees can be made about the performance of statistical-learning algorithms?\nHow can statistical learning be used to answer questions that interest social science researchers, such as testing theories or improving social policy?\n\nIn order to address these questions, we will cover so-called “standard” techniques such as supervised and unsupervised learning, statistical learning theory and nonparametric and Bayesian approaches. If it were up to me, this course would be titled “Statistical Learning for Social Scientists”—I believe this provides a more appropriate guide to the content of this course. And while this class will cover these novel statistical methodologies in some detail, it is not a substitute for the appropriate class in Computer Science or Statistics. Nor is this a class that teaches specific skills for the job market. Rather, this class will teach you to think about data analytics broadly. We will spend a great deal of time learning how to interpret the output of statistical learning algorithms and approaches, and will also spend a great deal of time on better understanding the basic ideas in statistical learning. This, of course, comes at some cost in terms of time spent on learning computational and/or programming skills.\nEnrollment for credit in this course is simply not suitable for those unprepared in or uninterested in elementary statistical theory no matter the intensity of interest in machine learning or “Big Data”. Really.\nYou will be required to understand elementary mathematics in this course and should have at least some exposure to statistical theory. The class is front-loaded technically: early lectures are more mathematically oriented, while later lectures are more applied.\nThe topics covered in this course are listed later in this document. I will assign readings sparingly from  Introduction to Statistical Learning, henceforth referred to as ISL. This text is available for free online and, for those who like physical books, can be purchased for about $25. Importantly, the lectures deviate a fair bit from the reading, and thus you will rely on your course notes much more than you might in other classes.\nIf—after you have read this document and preferably after attending the first lecture—you have any questions about whether this course is appropriate for you, please come talk to me.\n\n\n\nThe focus of this course is conceptual. The goal is to create a working understanding of when and how tools from computer science and statistics can be profitably applied to problems in social science. Though students will be required to apply some of these techniques themselves, this course is not…\n…a replacement for EC420, EC422, or a course in causal inference.\nAs social scientists, we are most often concerned with causal inference in order to anaOpelyze and write policies. Statistical learning and the other methods we will discuss in this course are generally not well-suited to these problems, and while I’ll give a short overview of standard methods, this is only to build intuitions. Ultimately, this course has a different focus and you should still pursue standard methodological insights from your home departments.\n…a course on the computational aspects of the underlying methods.\nThere are many important innovations that have made machine learning techniques computationally feasible. We will not discuss these, as there are computer science courses better equipped to cover them. When appropriate, we will discuss whether something is computable, and we will even give rough approximations of the amount of time required (e.g. P vs NP). But we will not discuss how optimizers work or best practices in programming.\n…a primer on the nitty-gritty of how to use these tools or a way to pad your resume.\nThe mechanics of implementation, whether it be programming languages or learning to use APIs, will not be covered in any satisfying level of depth. Students will be expected to learn most of the programming skills on their own. Specifically, while there will be some material to remind you of basic R commands, this is not a good course for people who are simply looking to learn the mechanics of programming. This course is designed to get you to use both traditional analytics and, eventually, machine learning tools. We will do some review of basic programming, and you will have the opportunity to explore topics that interest you through a final project, but ultimately this is a course that largely focuses on the theoretical and practical aspects of statistical learning as applied to social science and not a class on programming.\nPerhaps most importantly, this course is an attempt to push undergraduate education toward the frontiers in social science. Accordingly, please allow some messiness. Some topics may be underdeveloped for a given person’s passions, but given the wide variety of technical skills and overall interests, this is a near certainty. Both the challenge and opportunity of this area comes from the fact that there is no fully developed, wholly unifying framework. Our collective struggle—me from teaching, you from learning—will ultimately bear fruit."
  },
  {
    "objectID": "syllabus.html#course-times-structure-and-office-hours",
    "href": "syllabus.html#course-times-structure-and-office-hours",
    "title": "Syllabus",
    "section": "Course Times, Structure, and Office Hours",
    "text": "Course Times, Structure, and Office Hours\n\nLecture times and location\nThis course meets Tu/Th 1:00 - 2:20pm in NatSci 204. This class is in-person and will not have an online component.\n\n\nCourse Structure\nHere’s how each week will work: before class on Tuesday, you’ll read the first entry under content for the week – the first is more geared towards “principles” and the second toward “applications” and are labeled as such on our course schedule. On Tuesday, you’ll come to lecture. Before Thursday, you’ll read the second content entry for the week, and on Thursday you’ll come to lecture ready to participate with a charged laptop. On Saturday night by 11:59pm, you’ll turn in your Weekly Writing assignment responding to the weekly writing prompt given during class on Tuesday or Thursday, rendered to PDF by RMarkdown and using the proper template (see assignments). By Monday at 11:59pm, you’ll turn in your Lab assignment, also using the RMarkdown template.\nYou’ll repeat this each week until we’re out of labs. If a holiday occurs on a due date, then the assignment or lab is due the next non-holiday day at 11:59pm or as otherwise noted. You’ll also work with your assigned group on the Group Project. We’ll assign groups after the drop deadline has passed.\nOur material is laid out on each page, one for Tuesday and one for Thursday. This means we will conduct our class meetings by moving through the material and discussing it. Scrolling through together can feel a bit odd, but having one document means it is more searchable than, say, slides. Along the way, we’ll hit callout boxes instructing us to try some coding, time permitting.\nThis class is totally, unapologetically a work in progress. Material is a mish-mash of stuff from courses offered at Caltech, Stanford, Harvard, and Duke…so, yeah, it will be challenging. Hopefully, you’ll find it fun! Because this is an ever-evolving field, there may be more hiccups than you might otherwise expect:\n\nSome of the lectures will be too long or too short.\nSome of the lectures won’t make sense at first\nSome of the time I’ll forget what I intended to say and awkwardly stare at you for a few moments (sorry).\n\nI promise to improve the course with feedback, so if you have comments please speak up.\n\n\nOffice hours - Prof. Bushong\n\nProf. Bushong office hours: Tuesday and Thursday 4:00-5:00 PM\nProf. Bushong office hours Zoom link: Zoom, (Passcode: GODUCKS).\n\nMy office hours start immediately following class on Thursday, so I will stay in the classroom and answer questions, or we can walk-and-talk back to my office if you have a longer question. If you prefer to meet at my office located at Marshall-Adams Hall #25E, allow a little time for me to return from class. I’ll log onto Zoom when I arrive at the office for anyone attending remotely.\nI also hold dedicated Sunday evening office hours via Slack between 6:30 PM until about 8:00 PM. In addition, I will check Slack throughout the week and use it as an always-on avenue of communication and help.\nIt would be remarkable if you didn’t need some assistance with the material, and I am here to help. One of the benefits of open office hours is to accommodate many students at once; if fellow students are in my office, please join in and feel very free to show up in groups. As a general rule, please first seek course-related help from the course website. However, if my scheduled office hours do not work for you please let me know. I may encourage you to make appointments with me. I ask that you schedule your studying so that you are prepared to ask questions during office hours – office hours are not a lecture and if you’re not prepared with questions we will end up awkwardly staring at each other for an hour until you leave.\nSome gentle requests regarding office hours and on contacting me. First, my office hours end sharply at the end, so don’t arrive 10 minutes before the scheduled end and expect a full session. Please arrive early if you have lengthy questions, or if you don’t want to risk not having time due to others’ questions. You are free to ask me some stuff by e-mail, (e.g. a typo or something on a handout), but please know e-mail sucks for answering many types of questions. “How do I do this lab?” or “What’s wrong with my R setup?” are short questions with long answers. Come to office hours or ask on Slack.\n\n\nOffice Hours - Teaching Assistant\n\nTA Sierra Smith: smithsi6@msu.edu\nTA Office Hours via Zoom: Fridays 9:30-11AM\nTA Office Hours Zoom link: zoomlink\n\n\n\nSlack\nWe will use Slack as a forum for asking questions about the course, including course policies and, primarily, help questions with R. Students are encouraged to help answer each others’ questions, and to use the forum as a first-step for seeking help. Myself and the TA will monitor slack and answer questions regularly. You can join our Slack channel with this link: Join EC242 Slack. Once you have joined, bookmark our Slack.\nOne of the biggest advantages of using Slack is that you can take screenshots and paste them directly into your post (using screenshot on mac or snipping tool on Windows, set it to capture to your “clipboard”).\n\n\n\n\n\n\nMy DM’s are closed\n\n\n\nPlease ask your question in one of the coursewide channels so that our TA can answer when I am unavailable. You will receive a faster reply that way.\n\n\nJoining Slack counts towards course participation. I highly recommend you join."
  },
  {
    "objectID": "syllabus.html#about-me",
    "href": "syllabus.html#about-me",
    "title": "Syllabus",
    "section": "About Me",
    "text": "About Me\nMe: My primary area of expertise is economics. In brief, I am a behavioral economist whose research examines how cognitive biases and erroneous social beliefs influence decision-making and our interactions with others. In my research, I utilize a mix of theoretical modeling and experimental studies. These approaches guide the work in the Spartan Psychology and Economics Advanced Research (SPEAR) lab, which I formed in 2023. My teaching emphasizes how psychological factors shape economic choices (EC404; EC895) and the importance of rigorous empirical methods (EC242).\nWhile my research occasionally touches the topics in the course, it mostly utilizes tools and techniques from this course as tools."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course materials",
    "text": "Course materials\nThe course website can be found at https://ec242.netlify.app (but you know that. You’re on it right now.)\nThe second required reading is the  Introduction to Statistical Learning (2nd Ed), which is available free online (you can buy a paper copy if you want, and you hate trees).\nAll of the readings and software in this class are free. There are free online version of all the texts including  Introduction to Statistical Learning (2nd Ed) and R / RStudio are free (don’t pay for RStudio). We will reference outside readings and there exist paper versions of some “books” but you won’t need to buy anything1\n\nR and RStudio/Posit\nYou will do all of your analysis with the open source (and free!) programming language R. You will use RStudio (which is undergoing a slow-mo rebrand to “Posit” while the functionality remains the same) as the main program to access R. Think of R as an engine and RStudio as a car—R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code.\nR is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free Posit.cloud (formerly Rstudio.cloud) service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R. We recommend this for those who may be switching between computers and are trying to get some work done. That said, while Posit.cloud is convenient, it can be slow and it is not designed to be able to handle larger datasets or more complicated analysis and graphics. You also can’t use your own custom fonts with RStudio.cloud.2 And, generally speaking, you should have (from the prerequisite course) sufficient experience to make your R work. If not, over the course of the semester, you’ll probably want to get around to installing R, RStudio, and other R packages on your computer and wean yourself off of RStudio.cloud. If you plan on making a career out of data science, you should consider this a necessary step.\nYou can find instructions for installing R, RStudio/Posit, and all the tidyverse packages here. And you may find some other goodies.\n\n\nOnline help\nData science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately there are tons of online resources to help you with this beyond our course Slack. Two of the most important are StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Likewise, whenever using a specific package, try searching for that package name instead of the letter “r” (e.g. “ggplot scatterplot”). Good, concise searches are generally more effective.\nHelp with Using R: There are some excellent additional tutorials on R available through Rstudio/Posit Clould Primers."
  },
  {
    "objectID": "syllabus.html#evaluations-and-grades",
    "href": "syllabus.html#evaluations-and-grades",
    "title": "Syllabus",
    "section": "Evaluations and Grades",
    "text": "Evaluations and Grades\nYour grade in this course will be based on attendance/participation, labs, weekly writings, and a final project.\nThe general breakdown will be approximately 55% for labs, participation, and weekly writings, and 45% for projects (see below for specific details). The primary focus of the course is a final project; this requires two “mini-projects” to ensure you’re making satisfactory progress. Assignment of numeric grades will follow the standard, where ties (e.g., 91.5%) are rounded to favor the student. Evaluations (read: grades) are designed not to deter anyone from taking this course who might otherwise be interested, but will be taken seriously.\nWeekly writings are intended to be an easy way to get some points. Labs will be short homework assignments that require you to do something practical using R. You must have access to computing resources and the ability to program basic statistical analyses. If you are unprepared to implement basic statistical coding, please take (or retake) PLS202. I highly encourage seeking coding advice from those who instruct computer science courses – it’s their job and they are better at it than I am. I’ll try to provide a good service, but I’m really not an expert in computer science.\nMore in-depth descriptions for all the assignments are on the assignments page. As the course progresses, the assignments themselves will be posted within that page.\n\nDropping your lowest scores\nI will automatically drop your two lowest weekly writings and your one lowest lab assignment score. This allowance absorbs any personal issues, travel problems, computing issues, or non-excused medical issues that may preclude you from completing your assignment on time. If you request an extension or exemption, I will politely point you here.\n\n\nGrade Rubric\n\n\n\n\n\n\n\n\n\nAssignment\nPoints\nPercent\n\n\n\n\nClass Participation\n25\n5%\n\n\nWeekly Writings (14-2 x 8 ea), drop two lowest\n96\n17%\n\n\nLabs (14-1 x 15 ea), drop one lowest\n195\n35%\n\n\nMini project 1\n50\n9%\n\n\nMini project 2\n50\n9%\n\n\nFinal project\n135\n25%\n\n\nTotal\n551\n—\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\n4.0\n92-100%\n2.0\n72-76%\n\n\n3.5\n87-91%\n1.5\n67-72%\n\n\n3.0\n82-87%\n1.0\n62-67%\n\n\n2.5\n77-81%\n0.0\nbad-66%\n\n\n\n\n\nGrading, in general\nGrading: come to class.\nIf you complete all assignments and attend all class dates, I suspect you will do very well. Given the way the syllabus is structured, I conjecture that the following is a loose guide to grades:\n4.0 Turned in all assignments with good effort, worked hard on the projects and was proud of final product.\n3.5 Turned in all assignments with good effort, worked a bit on the projects and was indifferent to final product.\n3.0 Turned in all assignments with some effort, worked a bit on the projects and was shy about final product.\n&lt; 3.0 Very little effort, or did not turn in all assignments, worked very little on the projects and was embarassed by final product.\n…of course, failing to turn in assignments can lead to a grade dramatically lower than just a 3.0.\n\n\n\nGrading Appeals\nAll grades are considered final. Any request for a re-grade beyond simple point-tallying mistakes will require that the entire assignment be re-graded. Any points previously awarded may be changed in either direction in the re-grade.\n\n\nClass Participation\n\nParticipation can take many forms. Most preferred is active participation during class – asking clarifying questions or responding to lecture questions. In the latter half of the semester, we will work in groups in class on small coding exercises and will share results at the end. Sharing your group’s results and code will always count towards participation. Finally, I will often bribe give extra credit points for answering specific questions in class, which I will clearly state as extra credit. Wrong answers get the same credit as right answers. We are here to learn . If you knew everything already, you wouldn’t be in the class.\nWe are a sizable class, so participation points will be awarded using EC slips which you will be able to turn into me (with your name on it) at the end of class. That way, I don’t slow down the class trying to write down everyone’s names.\n\n\nSuccess in this Course\nI promise, you are equipped to succeed in this course. It will provide two challenges: the conceptual challenge of understanding and linking statistical theory to social science problems; and the implementation challenge of learning to program in R.\nLearning R can be difficult at first. Like learning a new language—Spanish, French, or Chinese—it takes dedication and perseverance. Hadley Wickham (the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like) ggplot2—made this wise observation:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\nEven experienced programmers (like me) find themselves bashing their heads against seemingly intractable errors.3 If you’re finding yourself bashing your head against a wall and not making progress, try the following. First, take a break. Sometimes you just need space to see an error. Next, talk to classmates. Finally, if you genuinely cannot see the solution, e-mail the TA. But, honestly, it’s probably just a typo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcademic honesty\nViolation of MSU’s Spartan Code of Honor will result in a grade of 0.0 in the course. Moreover, I am required by MSU policy to report suspected cases of academic dishonesty for possible disciplinary action.4\n\n\nGenerative AI\nGenerative AI is both a computing resource and potential avenue for cheating in violation of the Spartan Code of Honor (see Academic Integrity, below). For this course, some use of generative AI is permitted or forbidden as follows:\n\nFor the purposes of learning R coding, data cleaning and processing, visualization, and other coding applications of R, you are fully permitted to use ChatGPT or other generative AI models provided you indicate such a use at the top of your problem set, and include a comment #Used Generative AI in your code. When “stuck” on coding, I have always encouraged students to use Stack Overflow to find similar problems and then translate the found solutions into the problem at hand. It is one of the most important skills in coding. Generative AI facilitates this process, and thus is a tool. Since R itself is a tool to help you learn and apply the material, I view using generative AI for help in specific coding tasks as a suitable use. You must, in all cases, understand what your code is doing, and be able to justify its use. If you cannot tell me, on review, what a line of code is used for, then you have not used generative AI properly."
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\nIf you need a special accommodation for a disability, religious observance, or have any other concerns about your ability to perform well in this course, please contact me immediately so that we can discuss the issue and make appropriate arrangements. MSU has a specific policy for religious observance available here.\nMichigan State University is committed to providing equal opportunity for participation in all programs, services and activities. Requests for accommodations by persons with disabilities may be made by contacting the Resource Center for Persons with Disabilities at 517-884-RCPD or on the web at rcpd.msu.edu. Once your eligibility for an accommodation has been determined, you will be issued a verified individual services accommodation (“VISA”) form. Please present this form to me at the start of the term and/or two weeks prior to the accommodation date (test, project, etc). Requests received after this date will be honored whenever possible."
  },
  {
    "objectID": "syllabus.html#resources",
    "href": "syllabus.html#resources",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nMental health concerns or stressful events may lead to diminished academic performance or reduce a student’s ability to participate in daily activities. Services are available to assist you with addressing these and other concerns you may be experiencing. You can learn more about the broad range of confidential mental health services available on campus via the Counseling & Psychiatric Services (CAPS) website at www.caps.msu.edu."
  },
  {
    "objectID": "syllabus.html#mandated-reporting",
    "href": "syllabus.html#mandated-reporting",
    "title": "Syllabus",
    "section": "Mandated Reporting",
    "text": "Mandated Reporting\nWritings, labs, and projects, and other materials submitted for this class are generally considered confidential pursuant to the University’s student record policies. However, students should be aware that University employees, including instructors, may not be able to maintain confidentiality when it conflicts with their responsibility to report certain issues to protect the health and safety of MSU community members and others. As the instructor, I must report the following information to other University offices (including the Department of Police and Public Safety) if you share it with me: - Suspected child abuse/neglect, even if this maltreatment happened when you were a child; - Allegations of sexual assault, relationship violence, stalking, or sexual harassment; and - Credible threats of harm to oneself or to others. These reports may trigger contact from a campus official who will want to talk with you about the incident that you have shared. In almost all cases, it will be your decision whether you wish to speak with that individual. If you would like to talk about these events in a more confidential setting, you are encouraged to make an appointment with the MSU Counseling and Psychiatric Services."
  },
  {
    "objectID": "syllabus.html#acknowledgements",
    "href": "syllabus.html#acknowledgements",
    "title": "Syllabus",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course structure and content has been improved greatly by Prof. Kirkpatrick. All remaining errors are my own."
  },
  {
    "objectID": "syllabus.html#miscellanea",
    "href": "syllabus.html#miscellanea",
    "title": "Syllabus",
    "section": "Miscellanea",
    "text": "Miscellanea\nAll class material will be posted on https://ec242.netlify.app. D2L will be used sparingly for submission of weekly writings and assignments and distribution of grades."
  },
  {
    "objectID": "syllabus.html#contacting-me",
    "href": "syllabus.html#contacting-me",
    "title": "Syllabus",
    "section": "Contacting Me",
    "text": "Contacting Me\nEmail is a blessing and a curse. Instant communication is wonderful, but often email is the wrong medium to have a productive conversation about course material. Moreover, I get a lot of emails. This means that I am frequently triaging emails into two piles: “my house is burning down” and “everything else”. Your email is unlikely to make the former pile. So… asking questions about course material is always best done in-class or in office hours. Students always roll their eyes when professors say things like that, but it’s true that if you have a question, it’s very likely someone else has the same question.\nThat said, email is still useful. If you’re going to use it, you should at least use if effectively. There’s a running joke in academia that professors only read an email until they find a question. They then respond to that question and ignore the rest of the email. I won’t do this, but I do think it is helpful to assume that the person on the receiving end of an email will operate this way. By keeping this in mind, you will write a much more concise and easy to understand email.\nSome general tips:\n\nAlways include [EC242] in your subject line (brackets included).\nUse a short but informative subject line. For example: [EC242] Final Project Grading\nUse your University-supplied email for University business. This helps me know who you are.\nOne topic, one email. If you have multiple things to discuss, and you anticipate followup replies, it is best to split them into two emails so that the threads do not get cluttered.\nAsk direct questions. If you’re asking multiple questions in one email, use a bulleted list.\nDon’t ask questions that are answered by reading the syllabus! This drives me nuts.\nI’ve also found that students are overly polite in emails. I suppose it may be intimidating to email a professor, and you should try to match the style that the professor prefers, but I view email for a course as a casual form of communication. Said another way: get to the point. Students often send an entire paragraph introducing themselves, but if you use your University email address, and add the course name in the subject, I will already know who you are. Here’s an example of a perfectly reasonable email:\n\n\nSubject: [EC242] Lab, Question 2, Typo\nHi Prof. Bushong,\nThere seems to be a typo in the Lab on Question 2. The problem says to use a column of data that doesn’t seem to exist. Can you correct this or which should we use?\nThanks, Student McStudentFace\n\n\nLetters of Recommendation / References\nAt this time, I am essentially not writing letters of recommendation. If you want to convince me to do so, you’ll likely need to work exceptionally hard. Consider this a warning."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’ve got money to burn, you can buy me a burrito.↩︎\nThis bothers me way more than it should.↩︎\nBy the end of the course, you will realize that 1) I make many many many errors; 2) that I frequently cannot remember a command or the correct syntax; and 3) that none of this matters too much in the big picture because I know the broad approaches I’m trying to take and I know how to Google stuff. Learn from my idiocy.↩︎\nSo just don’t cheat or plagiarize. This is an easy problem to avoid.↩︎"
  },
  {
    "objectID": "resource/visualization.html",
    "href": "resource/visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "A good alternative to using two y-axes is to use two plots instead. The patchwork package makes this really easy to do with R. There are other similar packages that do this, like cowplot and gridExtra, but I’ve found that patchwork is the easiest to use and it actually aligns the different plot elements like axis lines and legends. The documentation for patchwork is really great and full of examples—you should check it out to see all the things you can do with it!\nFirst, we load the libraries and data we’ll be using. We loaded the Atlanta weather data in Example 05:\n\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(patchwork)  # For combining ggplot plots\nweather_atl &lt;- read_csv(\"/data/atl-weather-2019.csv\")\n\n\nTo use **patchwork**, we need to (1) save our plots as objects and (2) add them together with `+`.\n\nFor instance, is there a relationship between temperature and humidity in Atlanta? We can plot both:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Temperature in Atlanta\ntemp_plot &lt;- ggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\n  geom_line() +\n  geom_smooth() +\n  scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,\n                                         name = \"Celsius\")) +\n  labs(x = NULL, y = \"Fahrenheit\") +\n  theme_minimal()\ntemp_plot\n\n\n\n\n\n\n\n# Humidity in Atlanta\nhumidity_plot &lt;- ggplot(weather_atl, aes(x = time, y = humidity)) +\n  geom_line() +\n  geom_smooth() +\n  labs(x = NULL, y = \"Humidity\") +\n  theme_minimal()\nhumidity_plot\n\n\n\n\n\n\n\n:::\nRight now, these are two separate plots, but we can combine them with + if we load patchwork:\n\nlibrary(patchwork)\n\ntemp_plot + humidity_plot\n\n\n\n\n\n\n\n\nBy default, patchwork will put these side-by-side. We can specify that we want the plots to be oriented over/under:\n\ntemp_plot / humidity_plot\n\n\n\n\n\n\n\n\nOr we can change the orientation with the plot_layout() function:\n\ntemp_plot + humidity_plot +\n  plot_layout(ncol = 1)\n\n\n\n\n\n\n\n\nWe can also play with other arguments in plot_layout(). If we want to make the temperature plot taller and shrink the humidity section, we can specify the proportions for the plot heights. Here, the temperature plot is 70% of the height and the humidity plot is 30%:\n\ntemp_plot + humidity_plot +\n  plot_layout(ncol = 1, heights = c(0.7, 0.3))\n\n\n\n\n\n\n\n\n\n\n\nIt is fine (and often helpful) to use two y-axes if the two different scales measure the same thing, like counts and percentages, Fahrenheit and Celsius, pounds and kilograms, inches and centimeters, etc.\nTo do this, you need to add an argument (sec.axis) to scale_y_continuous() to tell it to use a second axis. This sec.axis argument takes a sec_axis() function that tells ggplot how to transform the scale. You need to specify a formula or function that defines how the original axis gets transformed. This formula uses a special syntax. It needs to start with a ~, which indicates that it’s a function, and it needs to use . to stand in for the original value in the original axis.\nSince the equation for converting Fahrenheit to Celsius is this…\n\\[\n\\text{C} = (32 - \\text{F}) \\times -\\frac{5}{9}\n\\]\n…we can specify this with code like so (where . stands for the Fahrenheit value):\n~ (32 - .) * -5 / 9\nHere’s a plot of daily high temperatures in Atlanta throughout 2019, with a second axis:\n\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\n  geom_line() +\n  scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,\n                                         name = \"Celsius\")) +\n  labs(x = NULL, y = \"Fahrenheit\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor fun, we could also convert it to Kelvin, which uses this formula:\n\\[\n\\text{K} = (\\text{F} - 32) \\times \\frac{5}{9} + 273.15\n\\]\n\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\n  geom_line() +\n  scale_y_continuous(sec.axis = sec_axis(trans = ~ (. - 32) * 5/9 + 273.15,\n                                         name = \"Kelvin\")) +\n  labs(x = NULL, y = \"Fahrenheit\") +\n  theme_minimal()",
    "crumbs": [
      "Syllabus",
      "Other Useful Stuff",
      "Visualization"
    ]
  },
  {
    "objectID": "resource/visualization.html#supplemental-visualization",
    "href": "resource/visualization.html#supplemental-visualization",
    "title": "Visualization",
    "section": "",
    "text": "A good alternative to using two y-axes is to use two plots instead. The patchwork package makes this really easy to do with R. There are other similar packages that do this, like cowplot and gridExtra, but I’ve found that patchwork is the easiest to use and it actually aligns the different plot elements like axis lines and legends. The documentation for patchwork is really great and full of examples—you should check it out to see all the things you can do with it!\nFirst, we load the libraries and data we’ll be using. We loaded the Atlanta weather data in Example 05:\n\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(patchwork)  # For combining ggplot plots\nweather_atl &lt;- read_csv(\"/data/atl-weather-2019.csv\")\n\n\nTo use **patchwork**, we need to (1) save our plots as objects and (2) add them together with `+`.\n\nFor instance, is there a relationship between temperature and humidity in Atlanta? We can plot both:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Temperature in Atlanta\ntemp_plot &lt;- ggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\n  geom_line() +\n  geom_smooth() +\n  scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,\n                                         name = \"Celsius\")) +\n  labs(x = NULL, y = \"Fahrenheit\") +\n  theme_minimal()\ntemp_plot\n\n\n\n\n\n\n\n# Humidity in Atlanta\nhumidity_plot &lt;- ggplot(weather_atl, aes(x = time, y = humidity)) +\n  geom_line() +\n  geom_smooth() +\n  labs(x = NULL, y = \"Humidity\") +\n  theme_minimal()\nhumidity_plot\n\n\n\n\n\n\n\n:::\nRight now, these are two separate plots, but we can combine them with + if we load patchwork:\n\nlibrary(patchwork)\n\ntemp_plot + humidity_plot\n\n\n\n\n\n\n\n\nBy default, patchwork will put these side-by-side. We can specify that we want the plots to be oriented over/under:\n\ntemp_plot / humidity_plot\n\n\n\n\n\n\n\n\nOr we can change the orientation with the plot_layout() function:\n\ntemp_plot + humidity_plot +\n  plot_layout(ncol = 1)\n\n\n\n\n\n\n\n\nWe can also play with other arguments in plot_layout(). If we want to make the temperature plot taller and shrink the humidity section, we can specify the proportions for the plot heights. Here, the temperature plot is 70% of the height and the humidity plot is 30%:\n\ntemp_plot + humidity_plot +\n  plot_layout(ncol = 1, heights = c(0.7, 0.3))\n\n\n\n\n\n\n\n\n\n\n\nIt is fine (and often helpful) to use two y-axes if the two different scales measure the same thing, like counts and percentages, Fahrenheit and Celsius, pounds and kilograms, inches and centimeters, etc.\nTo do this, you need to add an argument (sec.axis) to scale_y_continuous() to tell it to use a second axis. This sec.axis argument takes a sec_axis() function that tells ggplot how to transform the scale. You need to specify a formula or function that defines how the original axis gets transformed. This formula uses a special syntax. It needs to start with a ~, which indicates that it’s a function, and it needs to use . to stand in for the original value in the original axis.\nSince the equation for converting Fahrenheit to Celsius is this…\n\\[\n\\text{C} = (32 - \\text{F}) \\times -\\frac{5}{9}\n\\]\n…we can specify this with code like so (where . stands for the Fahrenheit value):\n~ (32 - .) * -5 / 9\nHere’s a plot of daily high temperatures in Atlanta throughout 2019, with a second axis:\n\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\n  geom_line() +\n  scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,\n                                         name = \"Celsius\")) +\n  labs(x = NULL, y = \"Fahrenheit\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor fun, we could also convert it to Kelvin, which uses this formula:\n\\[\n\\text{K} = (\\text{F} - 32) \\times \\frac{5}{9} + 273.15\n\\]\n\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\n  geom_line() +\n  scale_y_continuous(sec.axis = sec_axis(trans = ~ (. - 32) * 5/9 + 273.15,\n                                         name = \"Kelvin\")) +\n  labs(x = NULL, y = \"Fahrenheit\") +\n  theme_minimal()",
    "crumbs": [
      "Syllabus",
      "Other Useful Stuff",
      "Visualization"
    ]
  },
  {
    "objectID": "resource/visualization.html#interesting-and-excellent-real-world-examples",
    "href": "resource/visualization.html#interesting-and-excellent-real-world-examples",
    "title": "Visualization",
    "section": "Interesting and excellent real world examples",
    "text": "Interesting and excellent real world examples\n\nThe Stories Behind a Line\nAustralia as 100 people: You can make something like this with d3 and the potato project.\nMarrying Later, Staying Single Longer",
    "crumbs": [
      "Syllabus",
      "Other Useful Stuff",
      "Visualization"
    ]
  },
  {
    "objectID": "resource/visualization.html#how-to-select-the-appropriate-chart-type",
    "href": "resource/visualization.html#how-to-select-the-appropriate-chart-type",
    "title": "Visualization",
    "section": "How to select the appropriate chart type",
    "text": "How to select the appropriate chart type\nMany people have created many useful tools for selecting the correct chart type for a given dataset or question. Here are some of the best:\n\nThe Data Visualisation Catalogue: Descriptions, explanations, examples, and tools for creating 60 different types of visualizations.\nThe Data Viz Project: Descriptions and examples for 150 different types of visualizations. Also allows you to search by data shape and chart function (comparison, correlation, distribution, geographical, part to whole, trend over time, etc.).\nFrom Data to Viz: A decision tree for dozens of chart types with links to R and Python code.\nThe Chartmaker Directory: Examples of how to create 51 different types of visualizations in 31 different software packages, including Excel, Tableau, and R.\nR Graph Catalog: R code for 124 ggplot graphs.\nEmery’s Essentials: Descriptions and examples of 26 different chart types.",
    "crumbs": [
      "Syllabus",
      "Other Useful Stuff",
      "Visualization"
    ]
  },
  {
    "objectID": "resource/visualization.html#general-resources",
    "href": "resource/visualization.html#general-resources",
    "title": "Visualization",
    "section": "General resources",
    "text": "General resources\n\nStorytelling with Data: Blog and site full of resources by Cole Nussbaumer Knaflic.\nAnn K. Emery’s blog: Blog and tutorials by Ann Emery.\nEvergreen Data: Helful resources by Stephanie Evergreen.\nPolicyViz: Regular podcast and site full of helpful resources by Jon Schwabisch.\nVisualising Data: Fantastic collection of visualization resources, articles, and tutorials by Andy Kirk.\nInfo We Trust: Detailed explorations of visualizations by RJ Andrews, including a beautiful visual history of the field.\nFlowingData: Blog by Nathan Yau.\nInformation is Beautiful: Blog by David McCandless.\nJunk Charts: Blog by Kaiser Fung.\nWTF Visualizations: Visualizations that make you ask “wtf?”\nThe Data Visualization Checklist: A helpful set of criteria for grading the effectiveness of a graphic.\nData Literacy Starter Kit: Compilation of resources to become data literate by Laura Calloway.\nSeeing Data: A series of research projects about perceptions and visualizations.",
    "crumbs": [
      "Syllabus",
      "Other Useful Stuff",
      "Visualization"
    ]
  },
  {
    "objectID": "resource/visualization.html#visualization-in-excel",
    "href": "resource/visualization.html#visualization-in-excel",
    "title": "Visualization",
    "section": "Visualization in Excel",
    "text": "Visualization in Excel\n\nHow to Build Data Visualizations in Excel: Detailed tutorials for creating 14 different visualizations in Excel.\nAnn Emery’s tutorials: Fantastic series of tutorials for creating charts in Excel.",
    "crumbs": [
      "Syllabus",
      "Other Useful Stuff",
      "Visualization"
    ]
  },
  {
    "objectID": "resource/visualization.html#visualization-in-tableau",
    "href": "resource/visualization.html#visualization-in-tableau",
    "title": "Visualization",
    "section": "Visualization in Tableau",
    "text": "Visualization in Tableau\nBecause it is focused entirely on visualization (and because it’s a well-supported commercial product), Tableau has a phenomenal library of tutorials and training videos. There’s a helpful collections of videos here, as well.",
    "crumbs": [
      "Syllabus",
      "Other Useful Stuff",
      "Visualization"
    ]
  },
  {
    "objectID": "resource/style.html",
    "href": "resource/style.html",
    "title": "R style suggestions",
    "section": "",
    "text": "R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\n\nmpg %&gt;% \n  filter(cty &gt; 10, class == \"compact\")\n\nmpg %&gt;% filter(cty &gt; 10, class == \"compact\")\n\nmpg %&gt;% \n  filter(cty &gt; 10, \n         class == \"compact\")\n\nmpg %&gt;% filter(cty&gt;10, class==\"compact\")\n\nfilter(mpg,cty&gt;10,class==\"compact\")\n\nmpg %&gt;% \nfilter(cty &gt; 10, \n                        class == \"compact\")\n\nfilter ( mpg,cty&gt;10,     class==\"compact\" )\n\nBut you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.",
    "crumbs": [
      "Syllabus",
      "Getting Started in R",
      "R style suggestions"
    ]
  },
  {
    "objectID": "resource/style.html#r-style-conventions",
    "href": "resource/style.html#r-style-conventions",
    "title": "R style suggestions",
    "section": "",
    "text": "R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\n\nmpg %&gt;% \n  filter(cty &gt; 10, class == \"compact\")\n\nmpg %&gt;% filter(cty &gt; 10, class == \"compact\")\n\nmpg %&gt;% \n  filter(cty &gt; 10, \n         class == \"compact\")\n\nmpg %&gt;% filter(cty&gt;10, class==\"compact\")\n\nfilter(mpg,cty&gt;10,class==\"compact\")\n\nmpg %&gt;% \nfilter(cty &gt; 10, \n                        class == \"compact\")\n\nfilter ( mpg,cty&gt;10,     class==\"compact\" )\n\nBut you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.",
    "crumbs": [
      "Syllabus",
      "Getting Started in R",
      "R style suggestions"
    ]
  },
  {
    "objectID": "resource/style.html#main-style-things-to-pay-attention-to-for-this-class",
    "href": "resource/style.html#main-style-things-to-pay-attention-to-for-this-class",
    "title": "R style suggestions",
    "section": "Main style things to pay attention to for this class",
    "text": "Main style things to pay attention to for this class\n\nImportant note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty&gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n\n\nSpacing\n\nSee the “Spacing” section in the tidyverse style guide.\n\nPut spaces after commas (like in regular English):\n\n# Good\nfilter(mpg, cty &gt; 10)\n\n# Bad\nfilter(mpg , cty &gt; 10)\nfilter(mpg ,cty &gt; 10)\nfilter(mpg,cty &gt; 10)\n\nPut spaces around operators like +, -, &gt;, =, etc.:\n\n# Good\nfilter(mpg, cty &gt; 10)\n\n# Bad\nfilter(mpg, cty&gt;10)\nfilter(mpg, cty&gt; 10)\nfilter(mpg, cty &gt;10)\n\nDon’t put spaces around parentheses that are parts of functions:\n\n# Good\nfilter(mpg, cty &gt; 10)\n\n# Bad\nfilter (mpg, cty &gt; 10)\nfilter ( mpg, cty &gt; 10)\nfilter( mpg, cty &gt; 10 )\n\n\n\nLong lines\n\nSee the “Long lines” section in the tidyverse style guide.\n\nIt’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” &gt; “Global Options” &gt; “Code” &gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n\n# Good\nfilter(mpg, cty &gt; 10, class == \"compact\")\n\n# Good\nfilter(mpg, cty &gt; 10, \n       class == \"compact\")\n\n# Good\nfilter(mpg,\n       cty &gt; 10,\n       class == \"compact\")\n\n# Bad\nfilter(mpg, cty &gt; 10, class %in% c(\"compact\", \"pickup\", \"midsize\", \"subcompact\", \"suv\", \"2seater\", \"minivan\"))\n\n# Good\nfilter(mpg, \n       cty &gt; 10, \n       class %in% c(\"compact\", \"pickup\", \"midsize\", \"subcompact\", \n                    \"suv\", \"2seater\", \"minivan\"))\n\n\n\nPipes (%&gt;%) and ggplot layers (+)\nPut each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n\n# Good\nggplot(mpg, aes(x = cty, y = hwy, color = class)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw()\n\n# Bad\nggplot(mpg, aes(x = cty, y = hwy, color = class)) +\n  geom_point() + geom_smooth() +\n  theme_bw()\n\n# Super bad\nggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw()\n\n# Super bad and won't even work\nggplot(mpg, aes(x = cty, y = hwy, color = class))\n  + geom_point()\n  + geom_smooth() \n  + theme_bw()\n\nPut each step in a dplyr pipeline on separate lines, with the %&gt;% at the end of the line, indented with two spaces:\n\n# Good\nmpg %&gt;% \n  filter(cty &gt; 10) %&gt;% \n  group_by(class) %&gt;% \n  summarize(avg_hwy = mean(hwy))\n\n# Bad\nmpg %&gt;% filter(cty &gt; 10) %&gt;% group_by(class) %&gt;% \n  summarize(avg_hwy = mean(hwy))\n\n# Super bad\nmpg %&gt;% filter(cty &gt; 10) %&gt;% group_by(class) %&gt;% summarize(avg_hwy = mean(hwy))\n\n# Super bad and won't even work\nmpg %&gt;% \n  filter(cty &gt; 10)\n  %&gt;% group_by(class)\n  %&gt;% summarize(avg_hwy = mean(hwy))\n\n\n\nComments\n\nSee the “Comments” section in the tidyverse style guide.\n\nComments should start with a comment symbol and a single space: #\n\n# Good\n\n#Bad\n\n    #Bad\n\nIf the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\n\nmpg %&gt;% \n  filter(cty &gt; 10) %&gt;%  # Only rows where cty is 10 +\n  group_by(class) %&gt;%  # Divide into class groups\n  summarize(avg_hwy = mean(hwy))  # Find the average hwy in each group\n\nYou can add extra spaces to get inline comments to align, if you want:\n\nmpg %&gt;% \n  filter(cty &gt; 10) %&gt;%            # Only rows where cty is 10 +\n  group_by(class) %&gt;%             # Divide into class groups\n  summarize(avg_hwy = mean(hwy))  # Find the average hwy in each group\n\nIf the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” &gt; “Reflow comment”\n\n# Good\n# Happy families are all alike; every unhappy family is unhappy in its own way.\n# Everything was in confusion in the Oblonskys’ house. The wife had discovered\n# that the husband was carrying on an intrigue with a French girl, who had been\n# a governess in their family, and she had announced to her husband that she\n# could not go on living in the same house with him. This position of affairs\n# had now lasted three days, and not only the husband and wife themselves, but\n# all the members of their family and household, were painfully conscious of it.\n\n# Bad\n# Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it.\n\nThough, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.",
    "crumbs": [
      "Syllabus",
      "Getting Started in R",
      "R style suggestions"
    ]
  },
  {
    "objectID": "resource/rmarkdown.html",
    "href": "resource/rmarkdown.html",
    "title": "Using R Markdown",
    "section": "",
    "text": "R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nI have created a video walkthrough for using R Markdown for another course, but it is useful here. You can see it here ]\nHere are the most important things you’ll need to know about R Markdown in this class:",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using R Markdown"
    ]
  },
  {
    "objectID": "resource/rmarkdown.html#key-terms",
    "href": "resource/rmarkdown.html#key-terms",
    "title": "Using R Markdown",
    "section": "Key terms",
    "text": "Key terms\n\nDocument: A Markdown file where you type stuff\nChunk: A piece of R code that is included in your document. It looks like this:\n\n```{r}\n# Code goes here\n```\nThere must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\nKnit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected). We will always use PDF for this course.\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n::: {.cell} ::: {.cell-output-display}  ::: :::",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using R Markdown"
    ]
  },
  {
    "objectID": "resource/rmarkdown.html#add-chunks",
    "href": "resource/rmarkdown.html#add-chunks",
    "title": "Using R Markdown",
    "section": "Add chunks",
    "text": "Add chunks\nThere are three ways to insert chunks:\n\nPress ⌘⌥I on macOS or control + alt + I on Windows\nClick on the “Insert” button at the top of the editor window\n::: {.cell} ::: {.cell-output-display}  ::: :::\nManually type all the backticks and curly braces (don’t do this)",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using R Markdown"
    ]
  },
  {
    "objectID": "resource/rmarkdown.html#chunk-names",
    "href": "resource/rmarkdown.html#chunk-names",
    "title": "Using R Markdown",
    "section": "Chunk names",
    "text": "Chunk names\nYou can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\n\n\n\n\n\n\n\n\n\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk}\n# Code goes here\n```",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using R Markdown"
    ]
  },
  {
    "objectID": "resource/rmarkdown.html#chunk-options",
    "href": "resource/rmarkdown.html#chunk-options",
    "title": "Using R Markdown",
    "section": "Chunk options",
    "text": "Chunk options\nThere are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE}\n# Code goes here\n```\nThe most common chunk options are these:\n\nfig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures\necho=FALSE: The code is not shown in the final document, but the results are\nmessage=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted\nwarning=FALSE: Any warnings that R generates are omitted\ninclude=FALSE: The chunk still runs, but the code and results are not included in the final document. Don’t use this on your labs as we need to see your work, but do use this for your final project when the output is to be polished and clean.\n\nYou can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\n\n\n\n\n\n\n\n\n\n\nChunk fig.width and fig.height\nWhen a code chunk includes a plot (like from ggplot), the “canvas” size used can affect the output. You may have noticed if you plot directly in Rstudio, the plot resizes when you change the pane size. Each chunk has its own canvas size, and you can change that. So, if you have a chunk with a large plot, you can change the fig.width=7.5 in the chunk options. Note that units default to inches, so no more than 7.5 will fit on a regular-sized sheet of paper (or appropriately sized PDF).",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using R Markdown"
    ]
  },
  {
    "objectID": "resource/rmarkdown.html#inline-chunks",
    "href": "resource/rmarkdown.html#inline-chunks",
    "title": "Using R Markdown",
    "section": "Inline chunks",
    "text": "Inline chunks\nYou can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE}\navg_mpg &lt;- mean(mtcars$mpg)\n```\n\nThe average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon.\n… would knit into this:\n\nThe average fuel efficiency for cars from 1974 was 20.1 miles per gallon.",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using R Markdown"
    ]
  },
  {
    "objectID": "resource/rmarkdown.html#output-formats",
    "href": "resource/rmarkdown.html#output-formats",
    "title": "Using R Markdown",
    "section": "Output formats",
    "text": "Output formats\nYou can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \"My document\"\noutput:\n  html_document: default\n  pdf_document: default\n  word_document: default\nYou can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\n\n\n\n\n\n\n\n\n\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n---\ntitle: \"My document\"\nauthor: \"My name\"\ndate: \"January 13, 2020\"\noutput: \n  html_document: \n    toc: yes\n    fig_caption: yes\n    fig_height: 8\n    fig_width: 10\n  pdf_document: \n    latex_engine: xelatex  # More modern PDF typesetting engine\n    toc: yes\n  word_document: \n    toc: yes\n    fig_caption: yes\n    fig_height: 4\n    fig_width: 5\n---",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using R Markdown"
    ]
  },
  {
    "objectID": "resource/install.html",
    "href": "resource/install.html",
    "title": "Installing R, RStudio (Posit), tidyverse, and tinytex",
    "section": "",
    "text": "As mentioned in the syllabus, you will do all of your work in this class with the open source programming language R. You will use RStudio/Posit as the main program to access R. Think of R as an engine and RStudio/Posit as a car dashboard—–R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nRStudio is in the process of rebranding as Posit. You’ll see both along the way.\nHopefully you’re well-versed in dealing with these things, but if you’re lost, here’s how you install the required software for the course.",
    "crumbs": [
      "Syllabus",
      "Getting Started in R",
      "Installing R, RStudio (Posit), tidyverse, and tinytex"
    ]
  },
  {
    "objectID": "resource/install.html#footnotes",
    "href": "resource/install.html#footnotes",
    "title": "Installing R, RStudio (Posit), tidyverse, and tinytex",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎\nPronounced “lay-tek” for those who are correct; or “lah-tex” to those who love goofy nerdy pronunciation. Technically speaking, the x is the “ch” sound in “Bach”, but most people just say it as “k”. While either saying “lay” or “lah” is correct, “layteks” is frowned upon because it clearly shows you’re not cool.↩︎",
    "crumbs": [
      "Syllabus",
      "Getting Started in R",
      "Installing R, RStudio (Posit), tidyverse, and tinytex"
    ]
  },
  {
    "objectID": "resource/design.html",
    "href": "resource/design.html",
    "title": "Design",
    "section": "",
    "text": "Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based)\nColor Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)",
    "crumbs": [
      "Other Useful Stuff",
      "Design"
    ]
  },
  {
    "objectID": "resource/design.html#accessibility",
    "href": "resource/design.html#accessibility",
    "title": "Design",
    "section": "",
    "text": "Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based)\nColor Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)",
    "crumbs": [
      "Other Useful Stuff",
      "Design"
    ]
  },
  {
    "objectID": "resource/design.html#colors",
    "href": "resource/design.html#colors",
    "title": "Design",
    "section": "Colors",
    "text": "Colors\n\nAdobe Color: Create, share, and explore rule-based and custom color palettes.\nColourLovers: Like Facebook for color palettes.\nviridis: Percetually uniform color scales.\nScientific Colour-Maps: Perceptually uniform color scales like viridis. Use them in R with scico.\nColorBrewer: Sequential, diverging, and qualitative color palettes that take accessibility into account.\nColorgorical: Create color palettes based on fancy mathematical rules for perceptual distance.\nColorpicker for data: More fancy mathematical rules for color palettes (explanation).\niWantHue: Yet another perceptual distance-based color palette builder.\nPhotochrome: Word-based color pallettes.\nPolicyViz Design Color Tools: Large collection of useful color resources",
    "crumbs": [
      "Other Useful Stuff",
      "Design"
    ]
  },
  {
    "objectID": "resource/design.html#fonts",
    "href": "resource/design.html#fonts",
    "title": "Design",
    "section": "Fonts",
    "text": "Fonts\n\nGoogle Fonts: Huge collection of free, well-made fonts.\nThe Ultimate Collection of Google Font Pairings: A list of great, well-designed font pairings from all those fonts hosted by Google (for when you’re looking for good contrasting or complementary fonts).",
    "crumbs": [
      "Other Useful Stuff",
      "Design"
    ]
  },
  {
    "objectID": "resource/design.html#graphic-assets",
    "href": "resource/design.html#graphic-assets",
    "title": "Design",
    "section": "Graphic assets",
    "text": "Graphic assets\n\nImages\n\nUse the Creative Commons filters on Google Images or Flickr\nUnsplash\nPexels\nPixabay\nStockSnap.io\nBurst\nfreephotos.cc\n\n\n\nVectors\n\nNoun Project: Thousands of free simple vector images\naiconica: 1,000+ vector icons\nVecteezy: Thousands of free vector images\n\n\n\nVectors, photos, videos, and other assets\n\nStockio",
    "crumbs": [
      "Other Useful Stuff",
      "Design"
    ]
  },
  {
    "objectID": "resource/RstudioCloud.html",
    "href": "resource/RstudioCloud.html",
    "title": "Using Posit (Rstudio) Cloud",
    "section": "",
    "text": "What is Rstudio.cloud Posit.cloud?\nInstalling R and RStudio/Posit locally (on your computer) is the preferred method. Additionally, most MSU computer labs have R and RStudio installed. However, if your computer isn’t able to run RStudio, or you run into errors that we can’t troubleshoot, using RStudio Cloud is a viable alternative. Hosted at Posit.cloud, RStudio Posit Cloud is an online, web-based version of R and RStudio. On the “pro” side, it doesn’t require anything of your system except a functioning browser and can be up and running in a few minutes. The “con”, though, is that you must have an internet connection to use it, and it is limited to 25 hours per month for the free account (and $5/month for up to 75 hours per month). You shouldn’t need more than 25 hours per month, but if you’re very new to coding, it’s possible you will.\nBoth Rstudio.cloud and Posit.cloud take you to the same site – posit.cloud. RStudio is in the process of rebranding to Posit. I have updated some, but not all, references here.\n\n\nSetting up Posit.cloud\n\nStart by going to Posit.cloud and create a free account.\nClick “Your Workspace”\nIn the top right corner, click on “New Project” and name it something like “EC242”\n\n\n\n\n\n\n\n\n\n\n\nThe “Files” tab in the lower-right pane shows your cloud project storage space. Let’s add a templates folder and store the two course assignment templates there.\n\n\nGo to assignments, scroll down, and right-click on the Weekly Writing template and select “Save As…” and save it to your desktop\nDo the same for the Lab Assignment template. Clicking on the link will open the text of the file in a browser window and you cannot just copy the text into an R file.\nWith the files on your computer, go back to RStudio.cloud and in the Files tab, click on “New Folder” and name it templates\nUpload the templates one at a time to the folder.\n\n\nUse the green up arrow to navigate the “Files” tab back to your main directory and create a folder for Labs and one for Weekly Writings.\nOpen the Lab Assignment template\nAt the top of the template in the upper-left pane, you’ll see a warning that RStudio needs to install some packages. Click “Install”\n\n\n\n\n\n\n\n\n\n\n\nAfter R finishes installing packages (less than a minute), click the “Knit” button on the toolbar.\nRMarkdown will need to install a few more packages. Click “Yes”.\n\n\n\n\n\n\n\n\n\n\n\nA pop-up window will open showing you the PDF of the rendered template. Congrats, you are ready to get to work!\n\n\n\n\n\n\n\n\n\n\nYour RStudio.cloud version of RStudio will work just like a “local” install on a computer. Make sure you “Save As…” when you do your Lab or Weekly Writing assignments so that you don’t have to re-upload the templates. Keep a separate folder for each Lab inside the Labs to keep things clean in your workspace.",
    "crumbs": [
      "Syllabus",
      "Getting Started in R",
      "Using Posit (Rstudio) Cloud"
    ]
  },
  {
    "objectID": "groupproject/project2.html",
    "href": "groupproject/project2.html",
    "title": "Project 2",
    "section": "",
    "text": "Note: this assignment was updated 11/26/2024. Some parts may be different from the assignment as it stood when due. All additions are clarifications.",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "groupproject/project2.html#predicting-energy-consumption",
    "href": "groupproject/project2.html#predicting-energy-consumption",
    "title": "Project 2",
    "section": "Predicting Energy Consumption",
    "text": "Predicting Energy Consumption\nThe goal of this project is to predict household energy consumption using the Department of Energy’s Residential Energy Consuption Survey for 2020 using one of the predictive models you have learned: LASSO, elastic net, ridge, KNN, or trees. The RECS is a sample of around 18,000 households that asks myriad questions about housing characteristics (age of house, size of house), appliances (heat pump, gas furnace, EV), and total energy consumption (measured in BTU’s such that electricity, gas, propane, etc. are all included). This is microdata, so each household is an observation in the data.\nThis survey exists to provide data to utilities (power companies, gas companies, etc.) and state government’s for planning future energy consumption so that grids and utilities can be prepared for trends in energy use. Usually, this sort of analysis would be done in an interpretable way (modeling consumption with, say, and EV versus without tells you something about how aggregate energy use will look if more people have EV’s). For this project, though, we just want a good and useful prediction of energy consumption. And so, we will bring our prediction model tools to bear on the data.\nYour task is to develop, estimate, and evaluate a predictive model of the variableTOTALBTU. To compare across groups, we will hold out around 5% of the sample that will be the “evaluation” sample. The group with the lowest evaluation sample RMSE wins fame and fortune beyond comprehension. Also, +1% of your total grade in extra credit.\n\nThe RECS Data\nPart of being a data analyst is learning your data. While our predictive models don’t necessarily need us to know every variable’s definition, we do need to use the data correctly. So part of your task will be to import, clean, process, and understand the data. Most categorical variables (like “Year Built”) are held in the data as integers, but represent ranges of years. You’ll need to make sure you’re handling these correctly. There are also many variables that likely aren’t going to help: for instance, “flags” probably shouldn’t be included (they are variables that indicate when an observation has some values that are missing or imputed). Use the “variable and response codebook” (also known as the data dictionary) understand the variables. Make sure you know the units you’re working in as well.\nEach observation has a TOTALBTU that you want to predict. There are also subcategories of TOTALBTU that you will need to drop right away: TOTALBTUSPH, TOTALBTUWTH, and TOTALBTUOTH. These are subsets of TOTALBTU representing space heating, water heating, and other, and will add up to TOTALBTU. Do not use them.\nIt will take you some time to digest the data, so you won’t start predicting right out of the gate. Take your time to understand the data, and explore it as you work your way through the data dictionary. For many of our predictive models, you have to specify (possibly many, many, many) potential interactions. You’ll need to have some idea of what might be an important interaction to do this. For instance, HDD65 tells you how many cold days the household had. An interaction with TOTSQFT is probably useful to include.\n\n\nData Details\nOn the microdata tab for RECS 2020 https://www.eia.gov/consumption/residential/data/2020/index.php?view=microdata, down at the bottom, you’ll find a link to the .csv file (ignore the SAS file), as well as an Excel “variable and response codebook”. The codebook has five columns: Variable contains the name of the variable exactly how you’ll find it in the .csv, Type tells you if the variable is numeric or character or logical. When it says “numeric” it doesn’t mean the variable itself is represented as a number – note that CELLAR (row 17) is represented as a numeric, but the numbers map to values of “1 Yes”, “0 No” and “-2 Not applicable”. Column 3 is the description of the data, and Column 4 contains all of the response codes which tell you what each value means. Note that including something like CELLAR without making it a factor variable will use it in a bizarre and unhelpful way (as a continuous number). Column 5 tells you what section of the survey the data comes from.\nWhile not super-helpful for this application, the survey itself can be viewed on the microdata page. It can help in undertanding the responses.\n\n\n\n\n\n\nClarifications\n\n\n\nThis is a new project, so there may be some unclear descriptions or details that are necessary. I’ll add questions and answers here as they come in.\n\nI added a short amount on 11/12 regarding the structure of the table I want you to include. If you have more than 100 values for your tuning parameters (lambda, k, etc.) then shorten the range you use so that you have under 200 rows on that table. Make sure your test-RMSE-minimizing level of complexity / tuning parameter value is in the middle of the range.",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "groupproject/project2.html#the-project",
    "href": "groupproject/project2.html#the-project",
    "title": "Project 2",
    "section": "The project",
    "text": "The project\nThis project will take the form of a short memo-style documentation and evaluation of your predictive model prepared using R and RMarkdown as usual. It will have five sections, each of which should be separated with a proper header.\n\nData Processing (10 points): Clean and process the RECS 2020 data starting by loading in the data .csv. Below, in the “Your approach should be…” section, I’ve included some important steps you must take. For this section, use echo=T in your code chunk(s) so that your cleaning and processing code is shown. It’s OK if it’s long (though try to be an efficient coder). All cleaning and processing should be shown here. If you find yourself taking extra steps later on with the data, move those steps up into this section. No write-up or description is required here, just your code.\nState your priors (6 points): In 1-2 paragraphs, discuss your prior beliefs about the relationship between three of the variables and the target variable, TOTALBTU. While our predictive models often become un-interpretable, we have to start with some set of beliefs about the relationship, starting with the existence of some relationship. You cannot choose HDD65 or CDD65, though. These are too obvious. Make one plot of the relationship between TOTALBTU and some variable(s) that you think will be important and include that plot in this section. The plot has to be clean and clear, but the analysis behind it can be very simple and straightforward (this assignment is about the prediction, not a re-hash of Project 1). Do not include any of the code here – use echo=F to ensure that your code is not showing. You will turn in your .RMD file on D2L as well should the grader need to see your code.\nEstimate a model (20 points): Estimate a robust and detailed predictive model that includes the relationship from (2) and anything and everything else you think will provide predictive power – the sky is the limit. Use one of the machine learning methods that we learned about (kNN, regression trees, LASSO/Ridge/Elastic Net, do not use regular OLS) and carefully use the train-test paradigm to arrive at your best prediction.\n\nSome rules apply here:\n\nDo not use the “canned” routines for finding the optimal values of your tuning parameter – you’ll need to set a range of complexity and use lapply or a loop to estimate over those complexity parameters. This means that you should not directly give glmnet (if using LASSO or Ridge) a vector of lambdas and should not use cv.glmnet, but rather should use lapply to estimate different models over a range of lambda (or k or whatever your tuning parameters are).\nDo not output your estimation code or underlying output. Your output should be clean, but you will need to upload your .RMD file for your assignment as well.\n\nSome hints may come in handy:\n\nMost of methods we learned start out with a specification formula that is highly complex – unlike when we added variables to change complexity, our machine learning methods take a very deatiled formula with polynomial terms and interactions, and use the tuning parameters to change the complexity of the model. So start with a very complex model in your formula or data.\nAs you tune your tuning parameters, you may find that a large part of the tuning parameter space (the values of the tuning parameters you’re trying) result in a flat, high RMSE. As we discussed in class, there is no “right” range for the tuning parameters, so you may need to adjust the range you cover so that you focus on the RMSE-minimizing values. If you’re in a tuning parameter range that results in very high RMSE, then your plot is going to “compress” the minimizing point down and you likely won’t be able to even tell there’s a minimum. Be smart and iterate the range for your tuning parameter estimates.\n\nIn the write-up for this section (section 3), include a table showing the range of complexity that was tried (e.g. the lambda from a LASSO), the training RMSE, and the testing RMSE. The table will be long, but that’s OK. If you have more than 200 different values, then shorten the range around the test-RMSE-minimizing point. Use round() to cut down on the number of digits showing, but make sure enough are showing to find the minimizing level of complexity. Make a plot showing the progression of both RMSE’s as complexity increases. The plot should show the nadir where the testing RMSE reaches it’s minimum. If you are using a method that has two complexity parameters, you’ll need to include test and train RMSE’s for every combination of parameter values. In that case, choose a coarse number of parameter values in the complexity parameters so that you are not searching over too many values. You can plot across one complexity parameter with the other complexity parameter held fixed. Your write-up should not include any code in this section, just the table and the plot. You can try using knitr::kable() to output a table - this function converts data.frame-like R objects into clean tables for RMarkdown. We haven’t covered it in class, but it should be easy to figure out.\nFinally, in a 2-3 sentence discussion, note which model you have selected as your optimal model and why.\n\nEvaluate the Model (8 points): Take your model to the evaluation sample (see below) and calculate the RMSE for your optimal model on this sample. Then, see if your priors were correct by changing the variable you hypothesized to be related, and see if your predictive model changes it’s prediction accordingly. For instance, I think an increase in HDD65 will increase the energy consumed when it increases. To see if this holds in my model, I can increase HDD65 by some amount (a percent or a fixed amount, or a change in category if a categorical/factor variable) across all the data, and compare the new prediction to the old. Do this for each of the three prior predictions you stated in (2).\nData and Hypotheses (6 points): Go back to the three datasets from your first project. Choose one that you think you, as a group, might want to study further. Write up three hypothesis (labeled H1, H2, and H3) that you might want to test or predictions you might want to make (you can mix if you’d like - two hypothesis and one prediction, etc.) and why those might be interesting to others. That is, tell me why you or others would be interested in testing each hypothesis, or predicting an outcome. This is unrelated to Sections 1-4.\n\n\nYour data processing approach should be:\n\nImport the data from RECS at https://www.eia.gov/consumption/residential/data/2020/csv/recs2020_public_v7.csv\nDrop TOTALBTUSPH, TOTALBTUWTH, TOTALBTUOTH, and DOEID\nMake sure all variables that are numeric but represent factor variables (e.g. CELLAR) are converted into factors. Do not convert variables like HDD65 that are meant to be numeric. You can use the function mutate(across(c(X,Y,Z), as.factor)) to convert columns X, Y, and Z (obviously, substitute your columns) to factor variables. Check to make sure you got it right using str!\nDigest and understand your data. The data is described in detail using codebooks found here: https://www.eia.gov/consumption/residential/data/2020/index.php?view=microdata.\nUse set.seed(2422024) and then immediately do the following:\n\nSample exactly 1,000 households to hold as the final evaluation data. This is neither test nor train, but a third holdout “evaluation” set with which we will evaluate the final model.\nOnce you have removed the 1,000 household final evaluation data, split the remaining sample into test and train. Stick to 50/50. This should give you equal samples of 8,748 in each. You’ll have a train, a test, and an evaluation sample, all mutually exclusive and adding up to 18,496 rows total.",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "groupproject/project2.html#evaluation",
    "href": "groupproject/project2.html#evaluation",
    "title": "Project 2",
    "section": "Evaluation",
    "text": "Evaluation\nThe project is worth a total of 50 points. I will evaluate according to the following rubric:\nSection 1 (10 points)\n\nDid you drop the stated variables?\nDid you convert factors correctly?\nDid you correctly sample your data into three sets?\nDid you show your processing code?\n\nPart 2 (6 points, 2 points per prior)\n\nDo your three priorsand their description make sense?\nDoes your plot show the relationship?\nDoes the plot meet the technical requirements (properly labeled, clear, colorblind-friendly, etc.)\n\nPart 3 (20 points)\n\nDid you estimate the predictive model properly without using a “canned” routing (e.g. manually testing across a range of complexity)?\nDoes your table show the relationship between complexity and the train and test RMSEs?\nIs it nicely formatted and clear to read?\nDid you suppress the code and model results so that only the RMSE table, plot, and discussion is showing?\nDoes the plot meet the technical requirements (properly labeled, clear, colorblind-friendly, etc.)?s\n\nPart 4 (8 points)\n\nDo you show the RMSE for the evaluation sample?\nDid you propertly test your three priors?\nIs the discussion clear and concise?\n\nPart 5 (6 points, 2 points per hypothesis/prediction)\n\nDid you state actual hypotheses – specific questions that are testable?\nFor proposed predictions, did you state the variables that would be predicted?\nIn both cases, did you properly describe why one might be interested in the prediction or hypothesis?",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "groupproject/project2.html#turning-it-in",
    "href": "groupproject/project2.html#turning-it-in",
    "title": "Project 2",
    "section": "Turning it in",
    "text": "Turning it in\nYou are required to turn in both your final, rendered project with no code showing (except as stated in Section 1) AND your .RMD file used to render the project. All code generated with generative AI (e.g. ChatGPT) must be labeled as such as required in the syllabus..\nOnly one group member should post on D2L. Please make sure you coordinate to ensure this is done.",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "groupproject/final-project.html",
    "href": "groupproject/final-project.html",
    "title": "Final project",
    "section": "",
    "text": "Due Date\n\n\n\nThis Project is due on Thursday, May 1st, 11:59PM. This is the absolute last day I can accept projects. No exceptions will be given.\nAs you may have guessed given Project 1 and Project 2, your final project will be a chance for you to work on an analysis pertaining to your own (group’s) interest, and with data you find, clean, and analyze.\nThis assignment requires three steps which I outline here and detail below. First, you are to choose a topic, likely one that you have been writing about in the second parts of Project 1 and Project 2, and assemble and clean a rich dataset on the topic from multiple sources. Second, present an analysis of the data that highlights a pattern in the data that is not easily expressed in a closed form in the spirit of Project 1, primarily using visuals. This section will focus on a specific outcome (or two) that is of particular interest. Third, design and estimate using cross-validation a predictive model of the outcome of interest that performs well on an out-of-sample slice of your data.\nYour final product will take the form of a short memo in two versions – one with all code turned off, and one with all code turned on. Details for each section and a rubric for scoring are below.",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Final project"
    ]
  },
  {
    "objectID": "groupproject/final-project.html#the-memo",
    "href": "groupproject/final-project.html#the-memo",
    "title": "Final project",
    "section": "The Memo",
    "text": "The Memo\nBegin your memo with a brief (1 paragraph max) discussion of your topic of interest and the outcome you are interested in understanding and predicting. Tell the reader what sort of questions or analysis you are going to perform, and very briefly (1-2 sentences max) discuss what your analysis finds.",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Final project"
    ]
  },
  {
    "objectID": "groupproject/final-project.html#section-1-of-3-data-40-points",
    "href": "groupproject/final-project.html#section-1-of-3-data-40-points",
    "title": "Final project",
    "section": "Section 1 of 3: Data (40 points)",
    "text": "Section 1 of 3: Data (40 points)\nFind and assemble a dataset on your topic of interest using existing data.1 You must incorporate at least two datasets, properly merging them, though more than two is encouraged. This will require careful examination of data types and key(s), as well as attention to handling missing data. Your topic and questions may be shaped in part by what you can find – there are many sources of publicly-available data, and wrestling with them is part of being a good data analyst. To aid in your 3rd section, try to find and use data with many observations.\nOnce merged, clean your data such that it is usable in your project. Address missing data appropriately. Avoid dropping observations whenever possible. If your data includes extensive categorical variables, consider re-grouping them to relevant, coarser aggregations (e.g. all the building_types in the Rats data from Project 1 could have been grouped into 4-8 more general categories for most analytical uses, though how fine/coarse your grouping should be depends on your analytical questions).\nIn your memo, make a Data section after your intro paragraph and, in 2-3 paragraphs max, cite the source datasets, and describe how the data was merged. Do not refer to specific column names, but rather speak generally (“we merge on County FIPS and year to link state spending to county-level average test scores….”) so that the reader has an idea of how the data was assembled. In a similar manner, state how missing data was handled.\nBriefly summarize the dataset including the number of observations and statistics about key variables (e.g. “the average test score was XX.XX, while the average per-pupil spending was $YYYY”).\n\nScoring Rubric Section 1\n\nData Source (15 10 points): Do you have 2 or more data sources, and does each data source provide a relevant contribution to the topic?\nMerging (10 points): Did you identify the proper key columns and correctly merge such that data is not distorted or unintentionally dropped? Does your work show technical mastery of the process?\nCleaning (15 points): Did you address missing data appropriately and with clear assumptions? Did you generate meaningful categories for data that contained too much detail? Does your work show technical mastery of the process? Was it efficiently coded?\nMemo (5 points): Did you describe the necessary parts of the data construction, merging, cleaning process, and final dataset in a readable and concise maner?",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Final project"
    ]
  },
  {
    "objectID": "groupproject/final-project.html#section-2-of-3-analysis-with-visuals-45-points",
    "href": "groupproject/final-project.html#section-2-of-3-analysis-with-visuals-45-points",
    "title": "Final project",
    "section": "Section 2 of 3: Analysis with Visuals (45 points)",
    "text": "Section 2 of 3: Analysis with Visuals (45 points)\nWith your dataset(s) cleaned and ready, explore the data. Focusing on 1-2 aspects of the data that you find elucidate the outcome of interest, explore and analyze the relationship between your outcome and the explanatory variables. For example, if you’re interested in state-level educational spending and county-level test scores, you want to go beyond the simple relationship between the two: does the same relationship hold for wealthy counties vs. poor counties? Does it hold for all ages of students? Is it non-linear in some respect?\nYou will present this analysis primarily through visuals, as in Project 1. Your memo will guide the reader through the visuals, but should let the visuals do the “talking”. As in Project 1, do not narrarate the data exploration process (“we tried X, but it didn’t show anything, so we tried Y”). The memo should communicate a clear flow of logic that starts with the question and ends with a clear pattern in the data not easily expressed in a closed form.\nYou should not need more than 3 visualizations for this (4 max) – you should not substitute many shallow explorations for one deep exploration. Follow the thread through the data and take the reader along for the ride.\nI will be irrationally militant about the fine details in your plots. Your plots should be of publication quality, have perfect labeling and capitalization, have color-blind friendly colors, and should make a striking impact on first glance.\n\nScoring Rubric Section 2\n\nContent (20 points): Does your analysis find an interesting pattern in the data not easily expressed in a closed form? Does the reader learn something not obvious about the question at hand? Does the analysis “follow a thread” and build in the progression through your 3-4 visualizations? Did we learn something?\nTechnicals (15 points): Are the visualizations perfect in spelling, capitalization, labeling, and colors?\nMemo (10 points): Is the writing succinct, clear, and written as a polished memo without extraneous commentary or narraration?",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Final project"
    ]
  },
  {
    "objectID": "groupproject/final-project.html#section-3-of-3-predictive-model-40-points",
    "href": "groupproject/final-project.html#section-3-of-3-predictive-model-40-points",
    "title": "Final project",
    "section": "Section 3 of 3: Predictive Model (40 points)",
    "text": "Section 3 of 3: Predictive Model (40 points)\nYour final section requires estimating a predictive model on your data using a method of your choice. Similar to Project 2, you will need to setup your dataset such that it is appropriate for use in your chosen predictive method. You must use LASSO, K-Nearest Neighbors, or a Regression Tree (or random forest, if you wish to be adventureous). You will predict your outcome of interest, basing the model construction (to the extent there is an underlying model structure) on your knowledge of the data gleaned from Section 2.\nStart with a very complex specification in your formula. Unless you have thousands of predictors (explanatory variables), then starting with a simple multivariate regression with no interactions won’t get you very far. Remember that your model tuning parameters (e.g. lambda) can decrease complexity, but cannot increase beyond what you start out with. If you have panel data (e.g. multiple observations of the same unit) consider adding interactions between the unit of observation and the explanatory variables. Look back at our linear regression unit to understand the role of interactions.\nSet aside 10% of your sample as an evaluation sample as in Project 2. In this project, you may use built-in cross-validation functions (e.g. cv.glmnet() for LASSO/ridge/elastic net), or may choose to cross-validate “manually” as we did in Project 2. If choosing the former, you must be able to extract and show the train and test RMSEs across all the values of the tuning parameters (lambda, K, minsplit, cp, etc.). Estimate the model, extract the relevant RMSE’s, identify your optimal tuning parameters, and calculate the evaluation sample’s final RMSE (a single number). Show a plot of both test and train RMSE over the range of values of the tuning parameter, ensuring that you show a reasonable range of the tuning parameter, that the test-RMSE-minimizing point is clear on the plot, and that the x-axis is increasing in complexity.\nFinally, using your evaluation sample, calculate your model’s Evaluation RMSE (a single number) and report it with interpretation.\nIn this section, the memo writing will be quite brief. In one paragraph, discuss the method you employ and briefly describe the underlying model specification (what variables are included, what isn’t, etc). Show the train-test RMSEs in a properly-formatted publication-ready plot, state the final test RMSE for the optimal tuning parameter values, and discuss and interpret that RMSE of your evaluation sample.\n\nScoring Rubric Section 3\n\nEstimation (20 points): Did you properly estimate and tune your predictive model using cross-validation? Did you use a model specification (formula) appropriate for your method? Did you identify the test-RMSE minimizing values of the tuning parameter? Did you use an appropriate range of values for the tuning parameter?\nPlot (10 points): Did you show the train and test RMSE on an appropriately-formatted plot? Does the plot cover the relevant range of the tuning parameter such that the canonical form of the test RMSE is clear? Does the plot show a line or point at the optimal value of the tuning parameter?\nMemo (10 points): Does the memo clearly state the model chosen, the formula used, the methods used, and the results? Does it discuss the evaluation sample RMSE and interpret it appropriately?",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Final project"
    ]
  },
  {
    "objectID": "groupproject/final-project.html#additional-instruction-10-points",
    "href": "groupproject/final-project.html#additional-instruction-10-points",
    "title": "Final project",
    "section": "Additional Instruction: (10 points)",
    "text": "Additional Instruction: (10 points)\nOne person in the group will turn in the assignment to D2L no later than the time listed on the syllabus. I allow you the maximum amount of time to complete the project. Grades are due 48 hours after the due date, and thus I cannot extend anyone’s project for any reason.\nYou must turn in TWO PDF’s, properly rendered as per our usual course requirements. ONE copy will have all code output turned off (echo=F). The easiest way to do this is the set the global knitr option in the first code chunk (and set that first code chunk’s echo to the appropriate setting), knitting to PDF, then turning the global echo setting on and knitting it again to a differently-named PDF file. You will turn in both PDF’s, and I will use them both in grading.\n\nScoring Rubric\n\nTurning in (8 points): Did you turn in two copies, one with zero code showing and one with full code? Are all group member names shown on the assignment?\nIntro (2 points): Did you start with a proper intro paragraph as specified above?",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Final project"
    ]
  },
  {
    "objectID": "groupproject/final-project.html#some-help-to-get-you-started",
    "href": "groupproject/final-project.html#some-help-to-get-you-started",
    "title": "Final project",
    "section": "Some help to get you started",
    "text": "Some help to get you started\nStill having trouble finding data, especially with \\(N\\) large enough for a predictive model?\nOne fun source is the Census Public Use Microdata Survey, which is the individual level survey response to ACS questions. “Individual-level” means we see the individual answer of every person in the ACS. PUMS is publicly available and can be downloaded from https://data.census.gov/mdat. Exploring the data, you can see that it contains the respondent’s answers to questions like “Age” and “Number of cars” and all sorts of stuff that then gets tallied into block/tract/county-level ACS estimates.\nOf course, there is a catch: it only reports the person’s state, census regions, and “PUMA” or “Public Use Microdata Area”. PUMAs are much larger than census tracts, but can be pretty useful. To give you an idea of scale, there are about 3 PUMA’s in the greater Lansing area.\nYou can download the data through the link above. When doing so, at the “Download” tab, make sure you select “extract raw data (.csv)” option, otherwise it will try to summarize the data to a single observation per PUMA, and you (likely) want the individual responses.\nOnce downloaded tigris::pumas() will download the PUMA polygons just like it did counties and tracts. Use ?pumas to learn more.\nUsing tidycensus: Another way to access PUMS data is through the tidycensus package. This is a new feature for tidycensus, so you’ll have to lear more here https://walker-data.com/tidycensus/articles/pums-data.html. You’ll probably still want to use the census website above to make your list of variables of interest.\nOne last note: each individual is located within a household of size 1 or more. The household is identified by SERIALNO and the individual people are numbered starting with 1 in SPORDER, so for household measures you group on SERIALNO and use summarize to calculate the family variables.",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Final project"
    ]
  },
  {
    "objectID": "groupproject/final-project.html#footnotes",
    "href": "groupproject/final-project.html#footnotes",
    "title": "Final project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that existing is taken to mean that you are not permitted to collect data by interacting with other people. That is not to say that you cannot gather data that previously has not been gathered into a single place—this sort of exercise is necessary. But you cannot stand with a clipboard outside a store and count visitors (for instance). That would require involving the Institutional Review Board (IRB) and nobody wants to do that.↩︎",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Final project"
    ]
  },
  {
    "objectID": "content/Week_14/14b.html",
    "href": "content/Week_14/14b.html",
    "title": "Geospatial with R",
    "section": "",
    "text": "Our last class!",
    "crumbs": [
      "Course Content",
      "Week 14",
      "Geospatial with R"
    ]
  },
  {
    "objectID": "content/Week_14/14b.html#tigris-for-polygons",
    "href": "content/Week_14/14b.html#tigris-for-polygons",
    "title": "Geospatial with R",
    "section": "Tigris for polygons",
    "text": "Tigris for polygons\nMake sure you load tigris (for spatial data) and tidycensus for census data to attach to the spatial data:\n\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(tidyverse)\n\nWe already saw that we could extract counties using the tigris::counties function. Some “census geographies” change from year to year. Counties don’t (usually), but census tracts, block groups, and blocks do. These last three are all nested units of observation - tracts hold multiple block groups, each block group holds multiple blocks. Blocks are pretty small, and a lot of census data isn’t reported at that level for confidentiality. Block groups don’t report all data, either. Tracts are usually the most reliable balance between “census data availability” and “small geographic area”. We’ll use the 2010 census boundaries by specifying year = 2010.\nLet’s look at MI counties, then look at Ingham County census tracts:\n\nMI.counties = tigris::counties(state='MI', year = 2010, progress_bar = FALSE)\nggplot(MI.counties, aes(fill = NAME10) ) + \n  geom_sf() + \n  theme_minimal() + theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nAs you can see, we get some counties that extend into the Great Lakes. That’s OK - we can use st_intersect with a map of the Great Lakes to clean that up, but I won’t do that here.\nNow, the census tracts for Ingham County. We can use str_detect to find the County FIPS for Ingham:\n\nIngham_FIPS = MI.counties %&gt;%\n  dplyr::filter(str_detect(NAME10, 'Ingham')) %&gt;%\n  dplyr::select(COUNTYFP, STATEFP) %&gt;%\n  pull(COUNTYFP)\n\nIngham.tracts = tigris::tracts(state='MI', county = Ingham_FIPS, year = 2010, progress_bar = FALSE)\n\nprint(Ingham.tracts %&gt;% dplyr::select(TRACTCE10, NAME10, GEOID10))\n\nSimple feature collection with 81 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.60314 ymin: 42.42194 xmax: -84.1406 ymax: 42.77664\nGeodetic CRS:  NAD83\nFirst 10 features:\n     TRACTCE10 NAME10     GEOID10                       geometry\n2011    006600     66 26065006600 MULTIPOLYGON (((-84.54269 4...\n2012    003103  31.03 26065003103 MULTIPOLYGON (((-84.52985 4...\n2013    006800     68 26065006800 MULTIPOLYGON (((-84.56177 4...\n2014    006700     67 26065006700 MULTIPOLYGON (((-84.5616 42...\n2015    006001  60.01 26065006001 MULTIPOLYGON (((-84.2875 42...\n2016    006002  60.02 26065006002 MULTIPOLYGON (((-84.21456 4...\n2017    002101  21.01 26065002101 MULTIPOLYGON (((-84.53305 4...\n2018    007000     70 26065007000 MULTIPOLYGON (((-84.58218 4...\n2023    001703  17.03 26065001703 MULTIPOLYGON (((-84.58662 4...\n2024    004494  44.94 26065004494 MULTIPOLYGON (((-84.46658 4...\n\n\nNotice the GEOID10 starts with the MI FIPS (26), the Ingham FIPS (065), then the 6 digits that make up the TRACTCE10 value. This is because tracts are nested in states and counties.\nFinally, let’s show all the Ingham County tracts. I’ve changed the color mapping for the col aesthetic so that the tract that holds MSU will be outlined in green:\n\nggplot(Ingham.tracts, aes(fill = TRACTCE10, col = (NAME10=='9800'))) + \n  geom_sf() + theme_minimal() + theme(legend.position = 'none') + scale_fill_viridis_d() +\n  scale_color_manual(values = c('TRUE' = 'green','FALSE' = 'gray50'))",
    "crumbs": [
      "Course Content",
      "Week 14",
      "Geospatial with R"
    ]
  },
  {
    "objectID": "content/Week_14/14b.html#tidycensus",
    "href": "content/Week_14/14b.html#tidycensus",
    "title": "Geospatial with R",
    "section": "Tidycensus",
    "text": "Tidycensus\nTidycensus takes some getting used to because census data is very complicated, has many geographies, and many subsets (e.g. you could be asking about the age of a specific combination of race and income). Here, we’re going to learn two things: how to find some basic census data by tract, and how to extract it with tidycensus.\nFirst, we can use the tidyverse author’s “basic usage” page as a guide: https://walker-data.com/tidycensus/articles/basic-usage.html. There are important directions for getting a Census API key, which must be added to your system. tidycensus makes it easy - once you get your key (free from the US census, see the link for instructions), you just add it to your system using the census_api_key function. You only have to do it once, it records the API key in your system files, and will find it automatically on subsequent projects.\n\nFinding variables\nThis is actually pretty tricky. The census asks many questions about demographics and other topics (house characteristics, earnings, health, etc.) and it can be hard to find exactly what you’re looking for. We’re going to seek some simple data: the average income, and the average “tenure” (how long someone has lived in their current residence) in each tract in Ingham County.\ntidycensus has some useful functions for finding variables, especially when combined with str_detect, which helps us find certain words or phrases (our RegExp skills come in handy!) in the census variable names. Variables are identified by an ID, not by name. “P013001”, for instance, is the median age in a given geography. All variables are not available at all geographic units: “P013001” is available for all state, county, tract, and block group, but not block. The link above shows all of the geographies available.\nTo make things even more complicated, there is more than the decennial census. The American Community Survey (ACS) samples 1-5% of the population each year, and reports yearly but on a limited number of variables. We’ll use the ACS five-year (acs5) as it has more easily available data, even if it asks fewer questions. We’ll use the 2019 data, the most recently available.\nLet’s start by finding all variables that contain “income”. Variable descriptions have the variable name as well as the “concept”. We want to use “concept” to search for our term. Each concept has multiple variables with it:\n\nallvars = load_variables(2019, 'acs5')\nincvars = allvars %&gt;%\n  dplyr::filter(str_detect(concept, 'MEDIAN INCOME'))\n\nincvars[1:5,]\n\n# A tibble: 5 × 4\n  name         label                                           concept geography\n  &lt;chr&gt;        &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n1 B06011PR_001 Estimate!!Median income in the past 12 months … MEDIAN… &lt;NA&gt;     \n2 B06011PR_002 Estimate!!Median income in the past 12 months … MEDIAN… &lt;NA&gt;     \n3 B06011PR_003 Estimate!!Median income in the past 12 months … MEDIAN… &lt;NA&gt;     \n4 B06011PR_004 Estimate!!Median income in the past 12 months … MEDIAN… &lt;NA&gt;     \n5 B06011PR_005 Estimate!!Median income in the past 12 months … MEDIAN… &lt;NA&gt;     \n\n\nWe got lucky - the first concept is “MEDIAN INCOME IN THE PAST 12 MONTHS (IN 2019 INFLATION-ADJUSTED DOLLARS) BY PLACE OF BIRTH”. Now, we didn’t want the “place of birth” part (though…sounds interesting), but look at the first label. It is the !!Total:, meaning it is the median income when you combine all places of birth. The second label is !!Total:!!Born in state of residence, which is a subset of the total. Same with the next. We do not need to use the subsets in order to use the !!Total value. Note the name is B06011_001.\nLots and lots of variables will have the same total - the next variable B07011_001 (after the Puerto Rico version) is the median income broken down by whether or not the household lives in the same house, instead of by place of birth. It’s long, but you can see that it is “MEDIAN INCOME IN THE PAST 12 MONTHS (IN 2019 INFLATION-ADJUSTED DOLLARS) BY GEOGRAPHICAL MOBILITY IN THE PAST YEAR FOR CURRENT RESIDENCE IN THE UNITED STATES”\n\nincvars[6:15,]\n\n# A tibble: 10 × 4\n   name         label                                          concept geography\n   &lt;chr&gt;        &lt;chr&gt;                                          &lt;chr&gt;   &lt;chr&gt;    \n 1 B06011_001   Estimate!!Median income in the past 12 months… MEDIAN… tract    \n 2 B06011_002   Estimate!!Median income in the past 12 months… MEDIAN… tract    \n 3 B06011_003   Estimate!!Median income in the past 12 months… MEDIAN… tract    \n 4 B06011_004   Estimate!!Median income in the past 12 months… MEDIAN… tract    \n 5 B06011_005   Estimate!!Median income in the past 12 months… MEDIAN… tract    \n 6 B07011PR_001 Estimate!!Median income in the past 12 months… MEDIAN… &lt;NA&gt;     \n 7 B07011PR_002 Estimate!!Median income in the past 12 months… MEDIAN… &lt;NA&gt;     \n 8 B07011PR_003 Estimate!!Median income in the past 12 months… MEDIAN… &lt;NA&gt;     \n 9 B07011PR_004 Estimate!!Median income in the past 12 months… MEDIAN… &lt;NA&gt;     \n10 B07011PR_005 Estimate!!Median income in the past 12 months… MEDIAN… &lt;NA&gt;     \n\n\nWhile the values of the subsets will be different, the !!Total: variable will be the same. So, we can really choose any !!Total: variable for any set of variables in a concept that fits our search.\nLet’s call up the median income using B06011_001. Note that we are naming our variables in the third line, and specifying that we only want county = '065' for Ingham:\n\ning.medincome = get_acs(geography = 'tract',\n                        county = '065', state = 'MI',\n                        variables = c(medincome = 'B06011_001'),\n                        year = 2019)\nhead(ing.medincome)\n\n# A tibble: 6 × 5\n  GEOID       NAME                                     variable  estimate   moe\n  &lt;chr&gt;       &lt;chr&gt;                                    &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 26065000100 Census Tract 1, Ingham County, Michigan  medincome    22994  6149\n2 26065000400 Census Tract 4, Ingham County, Michigan  medincome    29059  4043\n3 26065000600 Census Tract 6, Ingham County, Michigan  medincome    18549  4830\n4 26065000700 Census Tract 7, Ingham County, Michigan  medincome    17196  8455\n5 26065000800 Census Tract 8, Ingham County, Michigan  medincome    23142  5845\n6 26065001000 Census Tract 10, Ingham County, Michigan medincome    30704  1982\n\n\nSince the ACS is a sample, the variable is returned as the estimate along with a margin of error, which we’ll ignore for now. Note that the data is “tidy” for now - each row is one observation of one variable. If we have multiple variables, we’ll have to keep it tidy. We’ll compare the B06011_001 with B007011_001:\n\ning.medincome = get_acs(geography = 'tract',\n                        county = '065', state = 'MI',\n                        variables = c(medincome = 'B06011_001',\n                                      medincome2 = 'B07011_001'), \n                        year = 2019) %&gt;% arrange(GEOID)\nhead(ing.medincome)\n\n# A tibble: 6 × 5\n  GEOID       NAME                                    variable   estimate   moe\n  &lt;chr&gt;       &lt;chr&gt;                                   &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n1 26065000100 Census Tract 1, Ingham County, Michigan medincome     22994  6149\n2 26065000100 Census Tract 1, Ingham County, Michigan medincome2    22994  6149\n3 26065000400 Census Tract 4, Ingham County, Michigan medincome     29059  4043\n4 26065000400 Census Tract 4, Ingham County, Michigan medincome2    29059  4043\n5 26065000600 Census Tract 6, Ingham County, Michigan medincome     18549  4830\n6 26065000600 Census Tract 6, Ingham County, Michigan medincome2    18549  4830\n\n\nWe got to name the variables (useful!) as you see in the variable column. However, there is more than one row per observation now: the results are not “tidy”. But clearly the two variable’s totals are the same, so we have convinced ourselves that !!Total: works for any of the different concepts that cover median income.\nOften, the data we wish to pull is a total, but it is broken down (by race, income groups, etc.). This means we want one observation per row (census tract), but multiple columns representing the total and the breakdown of a variable. Imagine “race” by tract with counts of households by race. We’ll have to use output = 'wide' in our get_acs call:\n\ning.medincome.wide = get_acs(geography = 'tract',\n                        county = '065', state = 'MI',\n                        variables = c(medincome = 'B06011_001',\n                                      medincome2 = 'B07011_001'), \n                        year = 2019,\n                        output = 'wide') %&gt;% arrange(GEOID)\nhead(ing.medincome.wide)\n\n# A tibble: 6 × 6\n  GEOID       NAME                 medincomeE medincomeM medincome2E medincome2M\n  &lt;chr&gt;       &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 26065000100 Census Tract 1, Ing…      22994       6149       22994        6149\n2 26065000400 Census Tract 4, Ing…      29059       4043       29059        4043\n3 26065000600 Census Tract 6, Ing…      18549       4830       18549        4830\n4 26065000700 Census Tract 7, Ing…      17196       8455       17196        8455\n5 26065000800 Census Tract 8, Ing…      23142       5845       23142        5845\n6 26065001000 Census Tract 10, In…      30704       1982       30704        1982\n\n\nWe still get to name the variables (so name wisely!). We get an Estimate and w Margin of error for each variable. We’ll mostly stick with the estimate and ignore the MoE.\n\n\nTidycensus and geographies\nNow, let’s find the other variable we’re interested in - housing tenure. We’ll keep it simple and calculate the percentage of people in a tract who lived in the same house last year.\n\ntenvars = allvars %&gt;%\n  dplyr::filter(str_detect(concept, 'TENURE'))\n\ntenvars[1:5,]\n\n# A tibble: 5 × 4\n  name         label                                           concept geography\n  &lt;chr&gt;        &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n1 B07013PR_001 Estimate!!Total:                                GEOGRA… &lt;NA&gt;     \n2 B07013PR_002 Estimate!!Total:!!Householder lived in owner-o… GEOGRA… &lt;NA&gt;     \n3 B07013PR_003 Estimate!!Total:!!Householder lived in renter-… GEOGRA… &lt;NA&gt;     \n4 B07013PR_004 Estimate!!Total:!!Same house 1 year ago:        GEOGRA… &lt;NA&gt;     \n5 B07013PR_005 Estimate!!Total:!!Same house 1 year ago:!!Hous… GEOGRA… &lt;NA&gt;     \n\n\nHere, we get B07013_001, which is the total number of households, and B07014_004, which is the total number of households that lived in the same house a year ago. The ratio of these two tells us what percent didn’t move in the last year, which is what we want. We’re going to take the output in wide format so that we can calculate this ratio.\nNow, with variable numbers in hand, we can pretty easily get the attached geography of interest. We just add geometry = TRUE to the get_acs call. We’re also going to ask for wide format, which gives us on row per tract and a column for each variable.\n\ning.tenure = get_acs(geography = 'tract',\n                     county = '065', state = 'MI',\n                     variables = c(samehouse = 'B07013_004',\n                                   totalhouse = 'B07013_001'),\n                     geometry = TRUE, output = 'wide', progress_bar = FALSE,\n                     year = 2019)\ning.tenure\n\nSimple feature collection with 81 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.60314 ymin: 42.42195 xmax: -84.14062 ymax: 42.77664\nGeodetic CRS:  NAD83\nFirst 10 features:\n         GEOID                                        NAME samehouseE\n1  26065004302 Census Tract 43.02, Ingham County, Michigan        498\n2  26065002800    Census Tract 28, Ingham County, Michigan       2396\n3  26065005201 Census Tract 52.01, Ingham County, Michigan       5216\n4  26065001200    Census Tract 12, Ingham County, Michigan       1201\n5  26065006302 Census Tract 63.02, Ingham County, Michigan       3660\n6  26065002200    Census Tract 22, Ingham County, Michigan       1349\n7  26065000800     Census Tract 8, Ingham County, Michigan       2601\n8  26065004494 Census Tract 44.94, Ingham County, Michigan          0\n9  26065004700    Census Tract 47, Ingham County, Michigan       2346\n10 26065006301 Census Tract 63.01, Ingham County, Michigan       3580\n   samehouseM totalhouseE totalhouseM                       geometry\n1         165        2383         326 MULTIPOLYGON (((-84.47205 4...\n2         310        2656         296 MULTIPOLYGON (((-84.54808 4...\n3         539        5961         481 MULTIPOLYGON (((-84.5826 42...\n4         233        2137         239 MULTIPOLYGON (((-84.54113 4...\n5         242        4139         201 MULTIPOLYGON (((-84.48344 4...\n6         143        1577         110 MULTIPOLYGON (((-84.53812 4...\n7         400        2987         403 MULTIPOLYGON (((-84.54287 4...\n8          10           0          10 MULTIPOLYGON (((-84.46662 4...\n9         181        2908         186 MULTIPOLYGON (((-84.43269 4...\n10        257        4221         213 MULTIPOLYGON (((-84.48351 4...\n\n\nWe get an “E” at the end of the variable which stands for “Estimate” (the “M” is “Margin of Error” since the ACS is a sample). We can now calculate the percent living in the same house they were in last year:\n\ning.tenure = ing.tenure %&gt;%\n  dplyr::mutate(percentSameHouse = samehouseE/totalhouseE) %&gt;%\n  dplyr::select(GEOID, NAME, percentSameHouse)\n\ning.tenure\n\nSimple feature collection with 81 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.60314 ymin: 42.42195 xmax: -84.14062 ymax: 42.77664\nGeodetic CRS:  NAD83\nFirst 10 features:\n         GEOID                                        NAME percentSameHouse\n1  26065004302 Census Tract 43.02, Ingham County, Michigan        0.2089803\n2  26065002800    Census Tract 28, Ingham County, Michigan        0.9021084\n3  26065005201 Census Tract 52.01, Ingham County, Michigan        0.8750210\n4  26065001200    Census Tract 12, Ingham County, Michigan        0.5620028\n5  26065006302 Census Tract 63.02, Ingham County, Michigan        0.8842716\n6  26065002200    Census Tract 22, Ingham County, Michigan        0.8554217\n7  26065000800     Census Tract 8, Ingham County, Michigan        0.8707734\n8  26065004494 Census Tract 44.94, Ingham County, Michigan              NaN\n9  26065004700    Census Tract 47, Ingham County, Michigan        0.8067400\n10 26065006301 Census Tract 63.01, Ingham County, Michigan        0.8481403\n                         geometry\n1  MULTIPOLYGON (((-84.47205 4...\n2  MULTIPOLYGON (((-84.54808 4...\n3  MULTIPOLYGON (((-84.5826 42...\n4  MULTIPOLYGON (((-84.54113 4...\n5  MULTIPOLYGON (((-84.48344 4...\n6  MULTIPOLYGON (((-84.53812 4...\n7  MULTIPOLYGON (((-84.54287 4...\n8  MULTIPOLYGON (((-84.46662 4...\n9  MULTIPOLYGON (((-84.43269 4...\n10 MULTIPOLYGON (((-84.48351 4...\n\n\nThe NA’s come from tracts with zero households in them (the haunted Seven Gables Nature Area, GM’s Lansing plant, industrial areas, etc.). Let’s plot this!\n\nggplot(ing.tenure, aes(fill = percentSameHouse)) + \n  geom_sf() + theme_minimal() + scale_fill_viridis_c() +\n  labs(fill = 'Percent Same House\\n1 yr ago')\n\n\n\n\n\n\n\n\n\nAn alternative source for finding Census variables\nWe can also find census variables by searching the data.census.gov website. We are looking for tables, so click on “explore tables”.\nFirst thing we want to do is set our level (e.g. block, block-group, tract, county, etc.) so that we are searching for data that exists at the level we want. No sense in searching for something you want at the county level to find out it’s only available at the state level! On the left side under “geography” click on the level you want (say, “census tract”) then choose a random state and tract – you’re only using this to find the variable’s code, not to actually retrieve data. I always use the first county in Alabama just because it’s small-ish so it doesn’t take long to load up.\nWith the tract(s) set on geography, go up to the search box and type in the topic you’re interested in. It’ll draw from decennial census, the ACS, and more. You can view the data (for the set of selected geographies) by clicking on the results to make sure the numbers look like you’d expect. That will also let you see the crosstabs – some data will have further breakdowns (by race, by tenure, etc.) and the crosstabs will show you what those breakdowns are, and if there is a “topline” number that serves your purpose.\nOnce you find the series you want, you’re getting close to being done. The variable code will be on the results (you’ll see “B….” or “H…” or “S….”). Note that and the source (ACS, decennial) and the year(s) you want. What’s missing, though? The _001 suffix! Frustrating, I know.\nTo find the exact variable you need, you’ll need what’s called a “table shell”. You can find them all here. Download the “table shells for all detailed data” for the data series and year you need (ACS 2021, decennial 2020, etc.) and search using your variable name. That’ll take you to the complete crosstabs available for that variable, including the _00X suffixes. Those are the variables (“table ID’s” in census terms) you’ll need to feed to tidycensus. You can name each of them as you see fit, take whichever ones you need (just the topline, or all of the crosstabs – it’s up to you).\n\n\n\nWrapping up\nFinding census variables via tidycensus can be frustrating, and other sources exist to help lookup census tables. One useful tip is that you can use the “filter” button in your Rstudio View() window to interactively filter the concept column from load_variables. Once you find what you want there, then use the get_acs function in your code. Fundamentally, it’s a lot of information with the ability to subset in many way, which makes it hard to wade through and find what you’re looking for.\n\n\n\n\n\n\nTry it! (This will be your Lab 15, due on Monday)\n\n\n\n\nLoad up all the necessary packages\nFind the FIPS for your home county (or a county you’re interested in)\nChoose a variable you’re interested in. Finding census variables can be tricky! If you want a challenge, find more than one variable that can construct something you’re interested in (like our “percent of housholds residing at the same address a year ago” measure).\nUsing load_variables, find the best representation of that variable\nMake a map of that variable at the census tract level",
    "crumbs": [
      "Course Content",
      "Week 14",
      "Geospatial with R"
    ]
  },
  {
    "objectID": "content/Week_13/13b.html",
    "href": "content/Week_13/13b.html",
    "title": "Final Project Review",
    "section": "",
    "text": "Final Projects\nToday we will dedicate in-class time to discussing and reviewing successful final projects in the course from past semesters.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Final Project Review"
    ]
  },
  {
    "objectID": "content/Week_12/12b.html",
    "href": "content/Week_12/12b.html",
    "title": "Illustrating Classification",
    "section": "",
    "text": "Note\n\n\n\nToday’s example will build on material from the Principles lecture (earlier this week).",
    "crumbs": [
      "Course Content",
      "Week 12",
      "Illustrating Classification"
    ]
  },
  {
    "objectID": "content/Week_12/12b.html#predicting-defaults",
    "href": "content/Week_12/12b.html#predicting-defaults",
    "title": "Illustrating Classification",
    "section": "Predicting Defaults",
    "text": "Predicting Defaults\nToday, we will continue to use the ISLR data on defaults:\n\nlibrary(ISLR)\nlibrary(tibble)\nDefault = ISLR::Default\n\n\n\n\n\n\n\nNote\n\n\n\nIn our first breakout:\n\nClean the data so that the default column is a binary indicator for default\nBuild a logistic model to predict default using any combination of variables and interactions in the data. For now, just use your best judgement for choosing the variables and interactions.\nUse a Bayes Classifier cutoff of .50 to generate your classifier output. Do you need to alter the cutoff?\n\n\n\nBack in class, let’s look at how we did. What variables were most useful in explaining default?\n\n\n\n\n\n\nNote\n\n\n\nIn our second breakout, we will create a ROC curve manually. To do this\n\nTake your model from the first breakout, and using a loop (or sapply), step through a large number of possible cutoffs for classification ranging from 0 to 1.\nFor each cutoff, generate a confusion matrix with accuracy, sensitivity and specificity.\nCombine the cutoff with the sensitivity and specificity results and make a ROC plot. Use ggplot for your plot and map the color aesthetic to the cutoff value.\nCalculate the AUC (the area under the curve). This is a little tricky but can be done with your data.",
    "crumbs": [
      "Course Content",
      "Week 12",
      "Illustrating Classification"
    ]
  },
  {
    "objectID": "content/Week_11/11b.html",
    "href": "content/Week_11/11b.html",
    "title": "Shrinkage with LASSO, Ridge, and Random Forests",
    "section": "",
    "text": "This page.\n\n\n\n\nWhat is shrinkage?\nWhat do we do with too may right-hand-side variables?\nWhat is LASSO?",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Shrinkage with LASSO, Ridge, and Random Forests"
    ]
  },
  {
    "objectID": "content/Week_11/11b.html#required-reading",
    "href": "content/Week_11/11b.html#required-reading",
    "title": "Shrinkage with LASSO, Ridge, and Random Forests",
    "section": "",
    "text": "This page.\n\n\n\n\nWhat is shrinkage?\nWhat do we do with too may right-hand-side variables?\nWhat is LASSO?",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Shrinkage with LASSO, Ridge, and Random Forests"
    ]
  },
  {
    "objectID": "content/Week_11/11b.html#the-data-and-packages",
    "href": "content/Week_11/11b.html#the-data-and-packages",
    "title": "Shrinkage with LASSO, Ridge, and Random Forests",
    "section": "The data and packages",
    "text": "The data and packages\nDr. Rubin uses the ISLR package’s credit dataset, which we can get from the ISLR package (which you may need to install). You’ll also want to install the wooldridge package for our later work:\n\nlibrary(ISLR)\ncredit = ISLR::Credit\nhead(credit)\n\n  ID  Income Limit Rating Cards Age Education Gender Student Married Ethnicity\n1  1  14.891  3606    283     2  34        11   Male      No     Yes Caucasian\n2  2 106.025  6645    483     3  82        15 Female     Yes     Yes     Asian\n3  3 104.593  7075    514     4  71        11   Male      No      No     Asian\n4  4 148.924  9504    681     3  36        11 Female      No      No     Asian\n5  5  55.882  4897    357     2  68        16   Male      No     Yes Caucasian\n6  6  80.180  8047    569     4  77        10   Male      No      No Caucasian\n  Balance\n1     333\n2     903\n3     580\n4     964\n5     331\n6    1151\n\n\nWe will also need to load the caret package (which you’re used before), as well as the glmnet package, which is new for us.",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Shrinkage with LASSO, Ridge, and Random Forests"
    ]
  },
  {
    "objectID": "content/Week_11/11b.html#terminology",
    "href": "content/Week_11/11b.html#terminology",
    "title": "Shrinkage with LASSO, Ridge, and Random Forests",
    "section": "Terminology",
    "text": "Terminology\nI used to have this lecture at the end of our semester, but I think the intuition behind LASSO and ridge regression helps understand our “overfitting” problem. There are two terms I want to cover before we dive into the slides:\n\nBias vs. Variance: We saw that super-overfit polynomial last week (where we took 20 observations and fit a 16th-degree polynomial). The model we fit was very flexible and bendy, but it did get most of the data points right. It had low bias as it was generally right, but had huge variance – it was all over the place, even within a small range of advertising mode spending. Bias vs. variance refers to the innate tradeoff between these two things. When we used the train and test samples to get the best out of sample fit, we were balancing bias and variance\nCross validation: This is the term for using two (or more) different subsets of sample data (e.g. test and train) to fit a model.\n\nNow, back to the lecture notes for today\n\n\n\n\n\n\nNote\n\n\n\nTry it! Use the wooldridge::wage2 dataset and LASSO to predict wage. Since these shrinkage methods (LASSO, ridge) work well with many right-hand-side variables, we can create some additional variables. A good candidate would be squared versions of all of the numeric variables. To do this, we’ll use mutate_if along with is.numeric. The function mutate_if, when used in a pipe %&gt;%, will check each column to see if the given function is true, then will add a new mutation of that column when true. We need this because we don’t want to try to add a squared term for things like factor variables. It will look something like this:\ndata %&gt;%\n  mutate_if(is.numeric, list(squared = function(x) x^2))\nWe pass a list with potentially many functions, but only one here in the example. For each passed function, mutate_if will create a copy of each existing column, square the data, and call name it by appending the name (here, ‘squared’) to the original column name.\nThis will results in some columns we don’t want to keep – wage_squared should be dropped since wage is the outcome we want to predict. We should also drop lwage and lwage_squared since those are transformations of our outcome. We want our data to only have the outcome and all the possible predictors so we can simply pass everything but wage for our X’s and wage for our y.\n\nDrop all of the copies of the outcome variable and transformations of the outcome variable. Also, drop any of the squared terms of binary variables – if a variable is \\(\\{0,1\\}\\), then the squared term is exactly the same. For instance, married and married_squared are exactly the same.\nHow many predictors (right-side variabes) do we have?\nUse glmnet from the glmnet package to run a LASSO (alpha=1) using all of the data you’ve assembled in the wage2 dataset. Select a wide range of values for lambda. You can use cv.glmnet to do the cross-validation for you (see the Rubin lecture notes), or you can do it manually.\nFind the lambda that minimizes RMSE in the test data. You can extract the optimal lambda from a cv.glmnet object by referring to object$lambda, and the RMSE using sqrt(object$cvm). These can be used to make the plots in the Rubin lecture notes, and to find the lambda that minimizes RMSE. object$lambda.min will also tell you the optimal lambda.\nUsing the optimal lambda, run a final LASSO model.\nUse coef to extract the coefficients from the optimal model. Coefficients that are zeroed out by the LASSO are shown as ‘.’\n\n\nWhich variables were “kept” in the model?\nWhich variables were eliminated from the model at the optimal lambda?\nIs the train RMSE lower than it would be if we just ran OLS with all of the variables?",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Shrinkage with LASSO, Ridge, and Random Forests"
    ]
  },
  {
    "objectID": "content/Week_11/11b.html#model-flexibility",
    "href": "content/Week_11/11b.html#model-flexibility",
    "title": "Shrinkage with LASSO, Ridge, and Random Forests",
    "section": "Model Flexibility",
    "text": "Model Flexibility\nLet’s return to the simulated dataset we used occaisionally in the linear regression content. Recall there was a single feature \\(x\\) with the following properties:\n\n# define regression function\ncubic_mean = function(x) {\n  1 - 2 * x - 3 * x ^ 2 + 5 * x ^ 3\n}\n\nWe then generated some data around this function with some added noise:\n\n# define full data generating process\ngen_slr_data = function(sample_size = 100, mu) {\n  x = runif(n = sample_size, min = -1, max = 1)\n  y = mu(x) + rnorm(n = sample_size)\n  tibble(x, y)\n}\n\nAfter defining the data generating process, we generate and split the data.\n\n# simulate entire dataset\nset.seed(3)\nsim_slr_data = gen_slr_data(sample_size = 100, mu = cubic_mean)\n\n# test-train split\nslr_trn_idx = sample(nrow(sim_slr_data), size = 0.8 * nrow(sim_slr_data))\nslr_trn = sim_slr_data[slr_trn_idx, ]\nslr_tst = sim_slr_data[-slr_trn_idx, ]\n\n# estimation-validation split\nslr_est_idx = sample(nrow(slr_trn), size = 0.8 * nrow(slr_trn))\nslr_est = slr_trn[slr_est_idx, ]\nslr_val = slr_trn[-slr_est_idx, ]\n\n# check data\nhead(slr_trn, n = 10)\n\n# A tibble: 10 × 2\n        x      y\n    &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.573 -1.18 \n 2  0.807  0.576\n 3  0.272 -0.973\n 4 -0.813 -1.78 \n 5 -0.161  0.833\n 6  0.736  1.07 \n 7 -0.242  2.97 \n 8  0.520 -1.64 \n 9 -0.664  0.269\n10 -0.777 -2.02 \n\n\nFor validating models, we will use RMSE.\n\n# helper function for calculating RMSE\ncalc_rmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\nLet’s check how linear, k-nearest neighbors, and decision tree models fit to this data make errors, while paying attention to their flexibility.\nThis picture is an idealized version of what we expect to see, but we’ll illustrate the sorts of validate “curves” that we might see in practice.\nNote that in the following three sub-sections, a significant portion of the code is suppressed for visual clarity. See the source document for full details.\n\nLinear Models\nFirst up, linear models. We will fit polynomial models with degree from one to nine, and then validate.\n\n# fit polynomial models\npoly_mod_est_list = list(\n  poly_mod_1_est = lm(y ~ poly(x, degree = 1), data = slr_est),\n  poly_mod_2_est = lm(y ~ poly(x, degree = 2), data = slr_est),\n  poly_mod_3_est = lm(y ~ poly(x, degree = 3), data = slr_est),\n  poly_mod_4_est = lm(y ~ poly(x, degree = 4), data = slr_est),\n  poly_mod_5_est = lm(y ~ poly(x, degree = 5), data = slr_est),\n  poly_mod_6_est = lm(y ~ poly(x, degree = 6), data = slr_est),\n  poly_mod_7_est = lm(y ~ poly(x, degree = 7), data = slr_est),\n  poly_mod_8_est = lm(y ~ poly(x, degree = 8), data = slr_est),\n  poly_mod_9_est = lm(y ~ poly(x, degree = 9), data = slr_est)\n)\n\nThe plot below visualizes the results.\n\n\n\n\n\n\n\n\n\nWhat do we see here? As the polynomial degree increases:\n\nThe training error decreases.\nThe validation error decreases, then increases.\n\nThis more of less matches the idealized version above, but the validation “curve” is much more jagged. This is something that we can expect in practice.\nWe have previously noted that training error isn’t particularly useful for validating models. That is still true. However, it can be useful for checking that everything is working as planned. In this case, since we known that training error decreases as model flexibility increases, we can verify our intuition that a higher degree polynomial is indeed more flexible.1\n\n\nk-Nearest Neighbors\nNext up, k-nearest neighbors. We will consider values for \\(k\\) that are odd and between \\(1\\) and \\(45\\) inclusive.\n\nlibrary(caret)\n# helper function for fitting knn models\nfit_knn_mod = function(neighbors) {\n  knnreg(y ~ x, data = slr_est, k = neighbors)\n}\n\n\n# define values of tuning parameter k to evaluate\nk_to_try = seq(from = 1, to = 45, by = 2)\n\n# fit knn models\nknn_mod_est_list = lapply(k_to_try, fit_knn_mod)\n\nThe plot below visualizes the results.\n\n\n\n\n\n\n\n\n\nHere we see the “opposite” of the usual plot. Why? Because with k-nearest neighbors, a small value of \\(k\\) generates a flexible model compared to larger values of \\(k\\). So visually, this plot is flipped. That is we see that as \\(k\\) increases:\n\nThe training error increases.\nThe validation error decreases, then increases.\n\nImportant to note here: the pattern above only holds “in general,” that is, there can be minor deviations in the validation pattern along the way. This is due to the random nature of selection the data for the validate set.\n\n\nDecision Trees\nLastly, we evaluate some decision tree models. We choose some arbitrary values of cp to evaluate, while holding minsplit constant at 5. There are arbitrary choices that produce a plot that is useful for discussion.\n\n# helper function for fitting decision tree models\nlibrary(rpart)\nlibrary(rpart.plot)\ntree_knn_mod = function(flex) {\n  rpart(y ~ x, data = slr_est, cp = flex, minsplit = 5)\n}\n\n\n# define values of tuning parameter cp to evaluate\ncp_to_try = c(0.5, 0.3, 0.1, 0.05, 0.01, 0.001, 0.0001)\n\n# fit decision tree models\ntree_mod_est_list = lapply(cp_to_try, tree_knn_mod)\n\nThe plot below visualizes the results.\n\n\n\n\n\n\n\n\n\nBased on this plot, how is cp related to model flexibility?2",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Shrinkage with LASSO, Ridge, and Random Forests"
    ]
  },
  {
    "objectID": "content/Week_11/11b.html#footnotes",
    "href": "content/Week_11/11b.html#footnotes",
    "title": "Shrinkage with LASSO, Ridge, and Random Forests",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn practice, if you already know how your model’s flexibility works, by checking that the training error goes down as you increase flexibility, you can check that you have done your coding and model training correctly.↩︎\nAs cp increases, model flexibility decreases.↩︎",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Shrinkage with LASSO, Ridge, and Random Forests"
    ]
  },
  {
    "objectID": "content/Week_10/10b.html",
    "href": "content/Week_10/10b.html",
    "title": "Nonparametric Regression",
    "section": "",
    "text": "We want to use the wooldridge::wage2 data on wages to generate and tune a non-parametric model of wages using a regression tree (or decision tree, same thing). Use ?wage2 (after you’ve loaded the wooldridge package) to see the data dictionary.\nWe’ve learned that our RMSE calculations have a hard time with NAs in the data. So let’s use the skim output to tell us which variables have too many NA (see n_missing) values and should be dropped. Let’s set a high bar here, and drop anything that isn’t 100% complete. Of course, there are other things we can do (impute the NAs, or make dummies for them), but for now, it’s easiest to drop them.\nRemember, skim() is for your own exploration, it’s not something that you should include in your output.\nOnce cleaned, we should be able to use rpart(wage ~ ., data = wage_clean) and not have any NAs in our prediction. That’s what we’ll do in our first Breakout.\n\n\n\n\n\n\nTRY IT\n\n\n\n\n\nI’m going to have you form groups of 2-3 to work on live-coding. One person in the group will need to have Rstudio up, be able to share screen, and have the correct packages loaded (caret, rpart, and rpart.plot, plus skimr). Copy the rmse and get_rmse code into a blank R script (you don’t have to use RMarkdown, just use a blank R script and run from there). You’ll also want to have these functions from last week loaded:\n\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\n\nget_rmse = function(model, data, response) {\n  rmse(actual = subset(data, select = response, drop = TRUE),\n       predicted = predict(model, data))\n}\n\nFor the first breakout, all I want you to do is the following. We are trying to predict wage for this exercise:\n\nEstimate a default regression tree on wage_clean using the default parameters (use the whole dataset for now, we’ll train-test on the problem set).\nUse rpart.plot to vizualize your regression tree, and talk through the interpretation with each other.\nCalculate the RMSE for your regression tree.\n\nWe’ll spend about 5 minutes on this. Remember, you can use ?wage2 to see the variable names. Make sure you know what variables are showing up in the plot and explaining wage in your model. You may find something odd at first and may need to drop more variables…\n5 minutes\n\n\n\nLet’s choose a group to share their plot and discuss the results. Use our sharing zoom link: bit.ly/EC242\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we did in the last two labs, we’ll use lapply to iterate over a wide range of “tuning parameters” to optimize our model (find the lowest test RMSE).\nOnce we have a list of things we want to iterate on, lapply lets us do something to each element of the list, and then returns a list of results (of the same length as the input). The arguments of lapply are (1) the list we want to iterate over, (2) the function we want to use, and, when needed, (3) other arguments to pass to the function (that are the same for every element in the list). The list we want to iterate over might be a list of models (as we do when we want to use get_rmse on each model), or it might be a list (or a vector that can be coerced to a list) of tuning parameters.\nLet’s say we wanted to repeat our TRY IT with the cp values of c(0, .01, .02, .03). We could make a list of those values, and use lapply to run rpart() and use each of the 4 values of cp. One way we could try doing this would be something like:\n\nmyList = lapply(c(0, .01, .02, 03), rpart, formula = wage ~ ., data = wage_clean, cp = ???)\n\nThe only problem here is that the first argument of rpart is not the cp parameter, so we can’t just pass it in additional arguments.\nInstead, we previously introduced the anonymous function in lapply as follows:\n\nmyList = lapply(c(0, .01, .02, 03), function(x){\n  rpart(wage ~ ., data = wage_clean, cp = x)\n})\n\nWell, it gets us the right answer, but whaaaaaat is going on? Curly brackets? x?\nThis is an “anonymous function”, or a function created on the fly. Here’s how it works in lapply:\n\nThe first argument is the list you want to do something to (same as before)\nThe second argument would usually be the function you want to apply, like get_rmse\nHere, we’re going to ask R to temporarily create a function that takes one argument, x.\nx is going to be each list element in myList.\nThink of it as a loop:\n\nTake the first element of myList and refer to it as x. Run the function’s code.\nThen it’ll take the second element of myList and refer to it as x and run the function’s code.\nRepeat until all elements of x have been used.\n\nOnce the anonymous function has been applied to x, the result is passed back and saved as the new element of the list output, always in the same position from where the x was taken.\n\nThe curly brackets are the temporary function you want to create.\nSo you can think of it like this:\nx = 0\nmyList[[1]] = rpart(wage ~ ., data = wage_clean, cp = x)\n\nx = 0.01\nmyList[[2]] = rpart(wage ~ ., data = wage_clean, cp = x)\n\nx = 0.02\nmyList[[3]] = rpart(wage ~ ., data = wage_clean, cp = x)\nAs long as every entry in myList is a valid value of cp, then you’ll get the results in a list. Then, unlist just coerces the list to a vector. This is also the same way we get the RMSE, but instead of the anonymous function, we write one out and use that.\nIf you had a list of rpart objects and wanted to, say, extract the cp tuning parameter from each, you could also use an anonymous function. Each rpart object has a list called control that includes all of the control (tuning) parameters used in its estimation:\n\ncpList = lapply(myList, function(y){\n  y$control$cp\n})\n\nunlist(cpList)\n\n\n\n\n\n\n\nTRY IT\n\n\n\n\n\nUsing the loop method or the lapply method, generate 10 regression trees to explain wage in wage_clean. You can iterate through values of cp, the complexity parameter, or minsplit, the minimum # of points that have to be in each split.\n10 minutes\n\n\n\nWe’ll ask someone to share (a few of) their group’s 10 regression trees using rpart.plot. Use bit.ly/KIRKPATRICK to volunteer to share.\n\n\n\n\n\n\n\n\n\nTRY IT\n\n\n\n\nUse lapply to get a list of your RMSE’s (one for each of your models). The anonymous function may come in handy here (though it’s not necessary). Note that we are not yet splitting into test and train (which you will need to do on your lab assignment).\nOnce you have your list, create the plot of RMSEs similar to the one we looked at in Content this week. Note: you can use unlist(myRMSE) to get a numeric vector of the RMSE’s (as long as all of the elements in myRMSE are numeric). Then, it’s a matter of plotting either with base plot or with ggplot (if you use ggplot you’ll have to tidy the RMSE by adding the index column or naming the x-axis).",
    "crumbs": [
      "Course Content",
      "Week 10",
      "Nonparametric Regression"
    ]
  },
  {
    "objectID": "content/Week_10/10b.html#our-data-and-goal",
    "href": "content/Week_10/10b.html#our-data-and-goal",
    "title": "Nonparametric Regression",
    "section": "",
    "text": "We want to use the wooldridge::wage2 data on wages to generate and tune a non-parametric model of wages using a regression tree (or decision tree, same thing). Use ?wage2 (after you’ve loaded the wooldridge package) to see the data dictionary.\nWe’ve learned that our RMSE calculations have a hard time with NAs in the data. So let’s use the skim output to tell us which variables have too many NA (see n_missing) values and should be dropped. Let’s set a high bar here, and drop anything that isn’t 100% complete. Of course, there are other things we can do (impute the NAs, or make dummies for them), but for now, it’s easiest to drop them.\nRemember, skim() is for your own exploration, it’s not something that you should include in your output.\nOnce cleaned, we should be able to use rpart(wage ~ ., data = wage_clean) and not have any NAs in our prediction. That’s what we’ll do in our first Breakout.\n\n\n\n\n\n\nTRY IT\n\n\n\n\n\nI’m going to have you form groups of 2-3 to work on live-coding. One person in the group will need to have Rstudio up, be able to share screen, and have the correct packages loaded (caret, rpart, and rpart.plot, plus skimr). Copy the rmse and get_rmse code into a blank R script (you don’t have to use RMarkdown, just use a blank R script and run from there). You’ll also want to have these functions from last week loaded:\n\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\n\nget_rmse = function(model, data, response) {\n  rmse(actual = subset(data, select = response, drop = TRUE),\n       predicted = predict(model, data))\n}\n\nFor the first breakout, all I want you to do is the following. We are trying to predict wage for this exercise:\n\nEstimate a default regression tree on wage_clean using the default parameters (use the whole dataset for now, we’ll train-test on the problem set).\nUse rpart.plot to vizualize your regression tree, and talk through the interpretation with each other.\nCalculate the RMSE for your regression tree.\n\nWe’ll spend about 5 minutes on this. Remember, you can use ?wage2 to see the variable names. Make sure you know what variables are showing up in the plot and explaining wage in your model. You may find something odd at first and may need to drop more variables…\n5 minutes\n\n\n\nLet’s choose a group to share their plot and discuss the results. Use our sharing zoom link: bit.ly/EC242\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we did in the last two labs, we’ll use lapply to iterate over a wide range of “tuning parameters” to optimize our model (find the lowest test RMSE).\nOnce we have a list of things we want to iterate on, lapply lets us do something to each element of the list, and then returns a list of results (of the same length as the input). The arguments of lapply are (1) the list we want to iterate over, (2) the function we want to use, and, when needed, (3) other arguments to pass to the function (that are the same for every element in the list). The list we want to iterate over might be a list of models (as we do when we want to use get_rmse on each model), or it might be a list (or a vector that can be coerced to a list) of tuning parameters.\nLet’s say we wanted to repeat our TRY IT with the cp values of c(0, .01, .02, .03). We could make a list of those values, and use lapply to run rpart() and use each of the 4 values of cp. One way we could try doing this would be something like:\n\nmyList = lapply(c(0, .01, .02, 03), rpart, formula = wage ~ ., data = wage_clean, cp = ???)\n\nThe only problem here is that the first argument of rpart is not the cp parameter, so we can’t just pass it in additional arguments.\nInstead, we previously introduced the anonymous function in lapply as follows:\n\nmyList = lapply(c(0, .01, .02, 03), function(x){\n  rpart(wage ~ ., data = wage_clean, cp = x)\n})\n\nWell, it gets us the right answer, but whaaaaaat is going on? Curly brackets? x?\nThis is an “anonymous function”, or a function created on the fly. Here’s how it works in lapply:\n\nThe first argument is the list you want to do something to (same as before)\nThe second argument would usually be the function you want to apply, like get_rmse\nHere, we’re going to ask R to temporarily create a function that takes one argument, x.\nx is going to be each list element in myList.\nThink of it as a loop:\n\nTake the first element of myList and refer to it as x. Run the function’s code.\nThen it’ll take the second element of myList and refer to it as x and run the function’s code.\nRepeat until all elements of x have been used.\n\nOnce the anonymous function has been applied to x, the result is passed back and saved as the new element of the list output, always in the same position from where the x was taken.\n\nThe curly brackets are the temporary function you want to create.\nSo you can think of it like this:\nx = 0\nmyList[[1]] = rpart(wage ~ ., data = wage_clean, cp = x)\n\nx = 0.01\nmyList[[2]] = rpart(wage ~ ., data = wage_clean, cp = x)\n\nx = 0.02\nmyList[[3]] = rpart(wage ~ ., data = wage_clean, cp = x)\nAs long as every entry in myList is a valid value of cp, then you’ll get the results in a list. Then, unlist just coerces the list to a vector. This is also the same way we get the RMSE, but instead of the anonymous function, we write one out and use that.\nIf you had a list of rpart objects and wanted to, say, extract the cp tuning parameter from each, you could also use an anonymous function. Each rpart object has a list called control that includes all of the control (tuning) parameters used in its estimation:\n\ncpList = lapply(myList, function(y){\n  y$control$cp\n})\n\nunlist(cpList)\n\n\n\n\n\n\n\nTRY IT\n\n\n\n\n\nUsing the loop method or the lapply method, generate 10 regression trees to explain wage in wage_clean. You can iterate through values of cp, the complexity parameter, or minsplit, the minimum # of points that have to be in each split.\n10 minutes\n\n\n\nWe’ll ask someone to share (a few of) their group’s 10 regression trees using rpart.plot. Use bit.ly/KIRKPATRICK to volunteer to share.\n\n\n\n\n\n\n\n\n\nTRY IT\n\n\n\n\nUse lapply to get a list of your RMSE’s (one for each of your models). The anonymous function may come in handy here (though it’s not necessary). Note that we are not yet splitting into test and train (which you will need to do on your lab assignment).\nOnce you have your list, create the plot of RMSEs similar to the one we looked at in Content this week. Note: you can use unlist(myRMSE) to get a numeric vector of the RMSE’s (as long as all of the elements in myRMSE are numeric). Then, it’s a matter of plotting either with base plot or with ggplot (if you use ggplot you’ll have to tidy the RMSE by adding the index column or naming the x-axis).",
    "crumbs": [
      "Course Content",
      "Week 10",
      "Nonparametric Regression"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html",
    "href": "content/Week_09/09b.html",
    "title": "Visualizing Uncertainty",
    "section": "",
    "text": "In data science, we often deal with data that is affected by chance in some way: the data comes from a random sample, the data is affected by measurement error, or the data measures some outcome that is random in nature. Being able to quantify the uncertainty introduced by randomness is one of the most important jobs of a data analyst. Statistical inference offers a framework, as well as several practical tools, for doing this. The first step is to learn how to mathematically describe random variables.\nIn this section, we introduce random variables and their properties starting with their application to games of chance. We then describe some of the events surrounding the financial crisis of 2007-20081 using probability theory. This financial crisis was in part caused by underestimating the risk of certain securities2 sold by financial institutions. Specifically, the risks of mortgage-backed securities (MBS) and collateralized debt obligations (CDO) were grossly underestimated. These assets were sold at prices that assumed most homeowners would make their monthly payments, and the probability of this not occurring was calculated as being low. A combination of factors resulted in many more defaults than were expected, which led to a price crash of these securities. As a consequence, banks lost so much money that they needed government bailouts to avoid closing down completely.\n\n\nRandom variables are numeric outcomes resulting from random processes. We can easily generate random variables using some of the simple examples we have shown. For example, define X to be 1 if a bead is blue and red otherwise:\n\nbeads &lt;- rep( c(\"red\", \"blue\"), times = c(2,3))\nX &lt;- ifelse(sample(beads, 1) == \"blue\", 1, 0)\n\nHere X is a random variable: every time we select a new bead the outcome changes randomly. See below:\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n\n[1] 1\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n\n[1] 0\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n\n[1] 0\n\n\nSometimes it’s 1 and sometimes it’s 0.\n\n\n\nMany data generation procedures, those that produce the data we study, can be modeled quite well as draws from an urn. For instance, we can model the process of polling likely voters as drawing 0s (Republicans) and 1s (Democrats) from an urn containing the 0 and 1 code for all likely voters. In epidemiological studies, we often assume that the subjects in our study are a random sample from the population of interest. The data related to a specific outcome can be modeled as a random sample from an urn containing the outcome for the entire population of interest. Similarly, in experimental research, we often assume that the individual organisms we are studying, for example worms, flies, or mice, are a random sample from a larger population. Randomized experiments can also be modeled by draws from an urn given the way individuals are assigned into groups: when getting assigned, you draw your group at random. Sampling models are therefore ubiquitous in data science. Casino games offer a plethora of examples of real-world situations in which sampling models are used to answer specific questions. We will therefore start with such examples.\nSuppose a very small casino hires you to consult on whether they should set up roulette wheels. To keep the example simple, we will assume that 1,000 people will play and that the only game you can play on the roulette wheel is to bet on red or black. The casino wants you to predict how much money they will make or lose. They want a range of values and, in particular, they want to know what’s the chance of losing money. If this probability is too high, they will pass on installing roulette wheels.\nWe are going to define a random variable \\(S\\) that will represent the casino’s total winnings. Let’s start by constructing the urn. A roulette wheel has 18 red pockets, 18 black pockets and 2 green ones. So playing a color in one game of roulette is equivalent to drawing from this urn:\n\ncolor &lt;- rep(c(\"Black\", \"Red\", \"Green\"), c(18, 18, 2))\n\nThe 1,000 outcomes from 1,000 people playing are independent draws from this urn. If red comes up, the gambler wins and the casino loses a dollar, so we draw a -1. Otherwise, the casino wins a dollar and we draw a 1. To construct our random variable \\(S\\), we can use this code:\n\nn &lt;- 1000\nX &lt;- sample(ifelse(color == \"Red\", -1, 1),  n, replace = TRUE)\nX[1:10]\n\n [1] -1  1  1 -1 -1 -1  1  1  1  1\n\n\nBecause we know the proportions of 1s and -1s, we can generate the draws with one line of code, without defining color:\n\nX &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\n\nWe call this a sampling model since we are modeling the random behavior of roulette with the sampling of draws from an urn. The total winnings \\(S\\) is simply the sum of these 1,000 independent draws:\n\nset.seed(100)\nX &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\nS &lt;- sum(X)\nS\n\n[1] 16\n\n\n\n\n\nIf you run the code above, you see that \\(S\\) changes every time. This is, of course, because \\(S\\) is a random variable. The probability distribution of a random variable tells us the probability of the observed value falling at any given interval. So, for example, if we want to know the probability that we lose money, we are asking the probability that \\(S\\) is in the interval \\(S&lt;0\\).\nNote that if we can define a cumulative distribution function \\(F(a) = \\mbox{Pr}(S\\leq a)\\), then we will be able to answer any question related to the probability of events defined by our random variable \\(S\\), including the event \\(S&lt;0\\). We call this \\(F\\) the random variable’s distribution function.\nWe can estimate the distribution function for the random variable \\(S\\) by using a Monte Carlo simulation to generate many realizations of the random variable. With this code, we run the experiment of having 1,000 people play roulette, over and over, specifically \\(B = 10,000\\) times:\n\nset.seed(200)\nn &lt;- 1000\nB &lt;- 10000\nroulette_winnings &lt;- function(n){\n  X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\n  sum(X)\n}\nS &lt;- replicate(B, roulette_winnings(n))\n\nNow we can ask the following: in our simulations, how often did we get sums less than or equal to a?\n\nmean(S &lt;= a)\n\nThis will be a very good approximation of \\(F(a)\\) and we can easily answer the casino’s question: how likely is it that we will lose money? We can see it is quite low:\n\nmean(S&lt;0)\n\n[1] 0.0474\n\n\nWe can visualize the distribution of \\(S\\) by creating a histogram showing the probability \\(F(b)-F(a)\\) for several intervals \\((a,b]\\):\n\n\n\n\n\n\n\n\n\nWe see that the distribution appears to be approximately normal. A qq-plot will confirm that the normal approximation is close to a perfect approximation for this distribution. If, in fact, the distribution is normal, then all we need to define the distribution is the average and the standard deviation. Because we have the original values from which the distribution is created, we can easily compute these with mean(S) and sd(S). The blue curve you see added to the histogram above is a normal density with this average and standard deviation.\nThis average and this standard deviation have special names. They are referred to as the expected value and standard error of the random variable \\(S\\). We will say more about these in the next section where we will discuss an incredibly useful approximation provided by mathematical theory that applies generally to sums and averages of draws from any urn: the Central Limit Theorem (CLT).\n\n\n\nBefore we continue, let’s make an important distinction and connection between the distribution of a list of numbers and a probability distribution. In the visualization lectures, we described how any list of numbers \\(x_1,\\dots,x_n\\) has a distribution. The definition is quite straightforward. We define \\(F(a)\\) as the function that tells us what proportion of the list is less than or equal to \\(a\\). Because they are useful summaries when the distribution is approximately normal, we define the average and standard deviation. These are defined with a straightforward operation of the vector containing the list of numbers x:\n\nm &lt;- sum(x)/length(x)\ns &lt;- sqrt(sum((x - m)^2) / length(x))\n\nA random variable \\(X\\) has a distribution function. To define this, we do not need a list of numbers. It is a theoretical concept. In this case, we define the distribution as the \\(F(a)\\) that answers the question: what is the probability that \\(X\\) is less than or equal to \\(a\\)? There is no list of numbers.\nHowever, if \\(X\\) is defined by drawing from an urn with numbers in it, then there is a list: the list of numbers inside the urn. In this case, the distribution of that list is the probability distribution of \\(X\\) and the average and standard deviation of that list are the expected value and standard error of the random variable.\nAnother way to think about it that does not involve an urn is to run a Monte Carlo simulation and generate a very large list of outcomes of \\(X\\). These outcomes are a list of numbers. The distribution of this list will be a very good approximation of the probability distribution of \\(X\\). The longer the list, the better the approximation. The average and standard deviation of this list will approximate the expected value and standard error of the random variable.\n\n\n\nIn statistical textbooks, upper case letters are used to denote random variables and we follow this convention here. Lower case letters are used for observed values. You will see some notation that includes both. For example, you will see events defined as \\(X \\leq x\\). Here \\(X\\) is a random variable, making it a random event, and \\(x\\) is an arbitrary value and not random. So, for example, \\(X\\) might represent the number on a die roll and \\(x\\) will represent an actual value we see 1, 2, 3, 4, 5, or 6. So in this case, the probability of \\(X=x\\) is 1/6 regardless of the observed value \\(x\\). This notation is a bit strange because, when we ask questions about probability, \\(X\\) is not an observed quantity. Instead, it’s a random quantity that we will see in the future. We can talk about what we expect it to be, what values are probable, but not what it is. But once we have data, we do see a realization of \\(X\\). So data scientists talk of what could have been after we see what actually happened.\n\n\n\nWe have described sampling models for draws. We will now go over the mathematical theory that lets us approximate the probability distributions for the sum of draws. Once we do this, we will be able to help the casino predict how much money they will make. The same approach we use for the sum of draws will be useful for describing the distribution of averages and proportion which we will need to understand how polls work.\nThe first important concept to learn is the expected value. In statistics books, it is common to use letter \\(\\mbox{E}\\) like this:\n\\[\\mbox{E}[X]\\]\nto denote the expected value of the random variable \\(X\\).\nA random variable will vary around its expected value in a way that if you take the average of many, many draws, the average of the draws will approximate the expected value, getting closer and closer the more draws you take.\nTheoretical statistics provides techniques that facilitate the calculation of expected values in different circumstances. For example, a useful formula tells us that the expected value of a random variable defined by one draw is the average of the numbers in the urn. In the urn used to model betting on red in roulette, we have 20 one dollars and 18 negative one dollars. The expected value is thus:\n\\[\n\\mbox{E}[X] = (20 + -18)/38\n\\]\nwhich is about 5 cents. It is a bit counterintuitive to say that \\(X\\) varies around 0.05, when the only values it takes is 1 and -1. One way to make sense of the expected value in this context is by realizing that if we play the game over and over, the casino wins, on average, 5 cents per game. A Monte Carlo simulation confirms this:\n\nB &lt;- 10^6\nx &lt;- sample(c(-1,1), B, replace = TRUE, prob=c(9/19, 10/19))\nmean(x)\n\n[1] 0.053422\n\n\nIn general, if the urn has two possible outcomes, say \\(a\\) and \\(b\\), with proportions \\(p\\) and \\(1-p\\) respectively, the average is:\n\\[\\mbox{E}[X] = pa + (1-p)b\\]\nTo see this, notice that if there are \\(n\\) beads in the urn, then we have \\(np\\) \\(a\\)s and \\(n(1-p)\\) \\(b\\)s and because the average is the sum, \\(n\\times a \\times p + n\\times b \\times (1-p)\\), divided by the total \\(n\\), we get that the average is \\(ap + b(1-p)\\).\nNow the reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distributions of sum, which then is useful for describing the distribution of averages and proportions. The first useful fact is that the expected value of the sum of the draws is:\n\\[\n\\mbox{}\\mbox{number of draws } \\times \\mbox{ average of the numbers in the urn}\n\\]\nSo if 1,000 people play roulette, the casino expects to win, on average, about 1,000 \\(\\times\\) $0.05 = $50. But this is an expected value. How different can one observation be from the expected value? The casino really needs to know this. What is the range of possibilities? If negative numbers are too likely, they will not install roulette wheels. Statistical theory once again answers this question. The standard error (SE) gives us an idea of the size of the variation around the expected value. In statistics books, it’s common to use:\n\\[\\mbox{SE}[X]\\]\nto denote the standard error of a random variable.\nIf our draws are independent, then the standard error of the sum is given by the equation:\n\\[\n\\sqrt{\\mbox{number of draws }} \\times \\mbox{ standard deviation of the numbers in the urn}\n\\]\nUsing the definition of standard deviation, we can derive, with a bit of math, that if an urn contains two values \\(a\\) and \\(b\\) with proportions \\(p\\) and \\((1-p)\\), respectively, the standard deviation is:\n\\[\\mid b - a \\mid \\sqrt{p(1-p)}.\\]\nSo in our roulette example, the standard deviation of the values inside the urn is: \\(\\mid 1 - (-1) \\mid \\sqrt{10/19 \\times 9/19}\\) or:\n\n2 * sqrt(90)/19\n\n[1] 0.998614\n\n\nThe standard error tells us the typical difference between a random variable and its expectation. Since one draw is obviously the sum of just one draw, we can use the formula above to calculate that the random variable defined by one draw has an expected value of 0.05 and a standard error of about 1. This makes sense since we either get 1 or -1, with 1 slightly favored over -1.\nUsing the formula above, the sum of 1,000 people playing has standard error of about $32:\n\nn &lt;- 1000\nsqrt(n) * 2 * sqrt(90)/19\n\n[1] 31.57895\n\n\nAs a result, when 1,000 people bet on red, the casino is expected to win $50 with a standard error of $32. It therefore seems like a safe bet. But we still haven’t answered the question: how likely is it to lose money? Here the CLT will help.\nAdvanced note: Before continuing we should point out that exact probability calculations for the casino winnings can be performed with the binomial distribution. However, here we focus on the CLT, which can be generally applied to sums of random variables in a way that the binomial distribution can’t.\n\n\nThe standard deviation of a list x (below we use heights as an example) is defined as the square root of the average of the squared differences:\n\nlibrary(dslabs)\nx &lt;- heights$height\nm &lt;- mean(x)\ns &lt;- sqrt(mean((x-m)^2))\n\nThe SD is the the square root of the sample variance, and the sample variance is the square of the sample SD. Using mathematical notation we write:\n\\[\n\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i \\\\\n\\] and \\[\n\\sigma =  \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2}\n\\]\nHowever, be aware that the sd function returns a slightly different result:\n\nidentical(s, sd(x))\n\n[1] FALSE\n\ns-sd(x)\n\n[1] -0.001942661\n\n\nThis is because the sd function R does not return the sd of the list, but rather uses a formula that estimates standard deviations of a population from a random sample \\(X_1, \\dots, X_N\\) which, for reasons not discussed here, divide the sum of squares by the \\(N-1\\).\n\\[\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i, \\,\\,\\,\\,\ns =  \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2}\n\\]\nYou can see that this is the case by typing:\n\nn &lt;- length(x)\ns-sd(x)*sqrt((n-1) / n)\n\n[1] 5.329071e-15\n\n\nFor all the theory discussed here, you need to compute the actual standard deviation as defined:\n\nsqrt(mean((x-m)^2))\n\nSo be careful when using the sd function in R. However, keep in mind that throughout the book we sometimes use the sd function when we really want the actual SD. This is because when the list size is big, these two are practically equivalent since \\(\\sqrt{(N-1)/N} \\approx 1\\).\n\n\n\n\nThe Central Limit Theorem (CLT) tells us that when the number of draws, also called the sample size, is large, the probability distribution of the sum of the independent draws is approximately normal. Because sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history.\nPreviously, we discussed that if we know that the distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and standard deviation. We also know that the same applies to probability distributions. If a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe the probability distribution are the average and standard deviation, referred to as the expected value and standard error.\nWe previously ran this Monte Carlo simulation:\n\nn &lt;- 1000\nB &lt;- 10000\nroulette_winnings &lt;- function(n){\n  X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\n  sum(X)\n}\nS &lt;- replicate(B, roulette_winnings(n))\n\nThe Central Limit Theorem (CLT) tells us that the sum \\(S\\) is approximated by a normal distribution. Using the formulas above, we know that the expected value and standard error are:\n\nn * (20-18)/38\n\n[1] 52.63158\n\nsqrt(n) * 2 * sqrt(90)/19\n\n[1] 31.57895\n\n\nThe theoretical values above match those obtained with the Monte Carlo simulation:\n\nmean(S)\n\n[1] 52.3038\n\nsd(S)\n\n[1] 31.20746\n\n\nUsing the CLT, we can skip the Monte Carlo simulation and instead compute the probability of the casino losing money using this approximation:\n\nmu &lt;- n * (20-18)/38\nse &lt;-  sqrt(n) * 2 * sqrt(90)/19\npnorm(0, mu, se)\n\n[1] 0.04779035\n\n\nwhich is also in very good agreement with our Monte Carlo result:\n\nmean(S &lt; 0)\n\n[1] 0.0446\n\n\nIt’s pretty powerful to have a function representing the outcome’s distribution, instead of having a (possibly huge) list of sampled outcomes.\n\n\nThe CLT works when the number of draws is large. But large is a relative term. In many circumstances as few as 30 draws is enough to make the CLT useful. In some specific instances, as few as 10 is enough. However, these should not be considered general rules. Note, for example, that when the probability of success is very small, we need much larger sample sizes.\nBy way of illustration, let’s consider the lottery. In the lottery, the chances of winning are less than 1 in a million. Thousands of people play so the number of draws is very large. Yet the number of winners, the sum of the draws, range between 0 and 4. This sum is certainly not well approximated by a normal distribution, so the CLT does not apply, even with the very large sample size. This is generally true when the probability of a success is very low. In these cases, the Poisson distribution is more appropriate.\nYou can examine the properties of the Poisson distribution using dpois and ppois. You can generate random variables following this distribution with rpois. However, we do not cover the theory here. You can learn about the Poisson distribution in any probability textbook and even Wikipedia3\n\n\n\n\nThere are several useful mathematical results that we used above and often employ when working with data. We list them below.\n1. The expected value of the sum of random variables is the sum of each random variable’s expected value. We can write it like this:\n\\[\n\\mbox{E}[X_1+X_2+\\dots+X_n] =  \\mbox{E}[X_1] + \\mbox{E}[X_2]+\\dots+\\mbox{E}[X_n]\n\\]\nIf the \\(X\\) are independent draws from the urn, then they all have the same expected value. Let’s call it \\(\\mu\\) and thus:\n\\[\n\\mbox{E}[X_1+X_2+\\dots+X_n]=  n\\mu\n\\]\nwhich is another way of writing the result we show above for the sum of draws.\n2. The expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable. This is easier to explain with symbols:\n\\[\n\\mbox{E}[aX] =  a\\times\\mbox{E}[X]\n\\]\nTo see why this is intuitive, consider change of units. If we change the units of a random variable, say from dollars to cents, the expectation should change in the same way. A consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, call it \\(\\mu\\) again:\n\\[\n\\mbox{E}[(X_1+X_2+\\dots+X_n) / n]=   \\mbox{E}[X_1+X_2+\\dots+X_n] / n = n\\mu/n = \\mu\n\\]\n3. The square of the standard error of the sum of independent random variables is the sum of the square of the standard error of each random variable. This one is easier to understand in math form:\n\\[\n\\mbox{SE}[X_1+X_2+\\dots+X_n] = \\sqrt{\\mbox{SE}[X_1]^2 + \\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2  }\n\\]\nThe square of the standard error is referred to as the variance in statistical textbooks. Note that this particular property is not as intuitive as the previous three and more in depth explanations can be found in statistics textbooks.\n4. The standard error of a non-random constant times a random variable is the non-random constant times the random variable’s standard error. As with the expectation: \\[\n\\mbox{SE}[aX] =  a \\times \\mbox{SE}[X]\n\\]\nTo see why this is intuitive, again think of units.\nA consequence of 3 and 4 is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of \\(n\\) (the number of draws), call it \\(\\sigma\\):\n\\[\n\\begin{aligned}\n\\mbox{SE}[(X_1+X_2+\\dots+X_n) / n] &=   \\mbox{SE}[X_1+X_2+\\dots+X_n]/n \\\\\n&= \\sqrt{\\mbox{SE}[X_1]^2+\\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2}/n \\\\\n&= \\sqrt{\\sigma^2+\\sigma^2+\\dots+\\sigma^2}/n\\\\\n&= \\sqrt{n\\sigma^2}/n\\\\\n&= \\sigma / \\sqrt{n}\n\\end{aligned}\n\\]\n5. If \\(X\\) is a normally distributed random variable, then if \\(a\\) and \\(b\\) are non-random constants, \\(aX + b\\) is also a normally distributed random variable. All we are doing is changing the units of the random variable by multiplying by \\(a\\), then shifting the center by \\(b\\).\nNote that statistical textbooks use the Greek letters \\(\\mu\\) and \\(\\sigma\\) to denote the expected value and standard error, respectively. This is because \\(\\mu\\) is the Greek letter for \\(m\\), the first letter of mean, which is another term used for expected value. Similarly, \\(\\sigma\\) is the Greek letter for \\(s\\), the first letter of standard error.\n\n\n\nAn important implication of the final result is that the standard error of the average becomes smaller and smaller as \\(n\\) grows larger. When \\(n\\) is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn. This is known in statistical textbooks as the law of large numbers or the law of averages.\n\n\nThe law of averages is sometimes misinterpreted. For example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50% heads and 50% tails. A similar argument would be to say that red “is due” on the roulette wheel after seeing black come up five times in a row. These events are independent so the chance of a coin landing heads is 50% regardless of the previous 5. This is also the case for the roulette outcome. The law of averages applies only when the number of draws is very large and not in small samples. After a million tosses, you will definitely see about 50% heads regardless of the outcome of the first five tosses.\nAnother funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row.\nProbabilistic thinking is central in the human experience. How we describe that thinking is mixed, but most of the time we use (rather imprecise) language. With only a few moments of searching, one can find thousands of articles that use probabilistic words to describe events. Here are some examples:\n\n“‘Highly unlikely’ State of the Union will happen amid shutdown” – The Hill\n\n\n“Tiger Woods makes Masters 15th and most improbable major” – Fox\n\n\n“Trump predicts ‘very good chance’ of China trade deal” – CNN\n\nYet people don’t have a good sense of what these things mean. Uncertainty is key to data analytics: if we were certain of things, A study in the 1960s explored the perception of probabilistic words like these among NATO officers. A more modern replication of this found the following basic pattern:\n\n\n\n\n\n\n\n\n\nA deep, basic fact about humans is that we struggle to understand probabilities. But visualizing things can help. The graphic above shows the uncertainty about uncertainty (very meta). We can convey all manner of uncertainty with clever graphics. Today’s practical example works through some of the myriad of ways to visualize variable data. We’ll cover some territory that we’ve already hit, in hopes of locking in some key concepts.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#definition-of-random-variables",
    "href": "content/Week_09/09b.html#definition-of-random-variables",
    "title": "Visualizing Uncertainty",
    "section": "",
    "text": "Random variables are numeric outcomes resulting from random processes. We can easily generate random variables using some of the simple examples we have shown. For example, define X to be 1 if a bead is blue and red otherwise:\n\nbeads &lt;- rep( c(\"red\", \"blue\"), times = c(2,3))\nX &lt;- ifelse(sample(beads, 1) == \"blue\", 1, 0)\n\nHere X is a random variable: every time we select a new bead the outcome changes randomly. See below:\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n\n[1] 1\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n\n[1] 0\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n\n[1] 0\n\n\nSometimes it’s 1 and sometimes it’s 0.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#sampling-models",
    "href": "content/Week_09/09b.html#sampling-models",
    "title": "Visualizing Uncertainty",
    "section": "",
    "text": "Many data generation procedures, those that produce the data we study, can be modeled quite well as draws from an urn. For instance, we can model the process of polling likely voters as drawing 0s (Republicans) and 1s (Democrats) from an urn containing the 0 and 1 code for all likely voters. In epidemiological studies, we often assume that the subjects in our study are a random sample from the population of interest. The data related to a specific outcome can be modeled as a random sample from an urn containing the outcome for the entire population of interest. Similarly, in experimental research, we often assume that the individual organisms we are studying, for example worms, flies, or mice, are a random sample from a larger population. Randomized experiments can also be modeled by draws from an urn given the way individuals are assigned into groups: when getting assigned, you draw your group at random. Sampling models are therefore ubiquitous in data science. Casino games offer a plethora of examples of real-world situations in which sampling models are used to answer specific questions. We will therefore start with such examples.\nSuppose a very small casino hires you to consult on whether they should set up roulette wheels. To keep the example simple, we will assume that 1,000 people will play and that the only game you can play on the roulette wheel is to bet on red or black. The casino wants you to predict how much money they will make or lose. They want a range of values and, in particular, they want to know what’s the chance of losing money. If this probability is too high, they will pass on installing roulette wheels.\nWe are going to define a random variable \\(S\\) that will represent the casino’s total winnings. Let’s start by constructing the urn. A roulette wheel has 18 red pockets, 18 black pockets and 2 green ones. So playing a color in one game of roulette is equivalent to drawing from this urn:\n\ncolor &lt;- rep(c(\"Black\", \"Red\", \"Green\"), c(18, 18, 2))\n\nThe 1,000 outcomes from 1,000 people playing are independent draws from this urn. If red comes up, the gambler wins and the casino loses a dollar, so we draw a -1. Otherwise, the casino wins a dollar and we draw a 1. To construct our random variable \\(S\\), we can use this code:\n\nn &lt;- 1000\nX &lt;- sample(ifelse(color == \"Red\", -1, 1),  n, replace = TRUE)\nX[1:10]\n\n [1] -1  1  1 -1 -1 -1  1  1  1  1\n\n\nBecause we know the proportions of 1s and -1s, we can generate the draws with one line of code, without defining color:\n\nX &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\n\nWe call this a sampling model since we are modeling the random behavior of roulette with the sampling of draws from an urn. The total winnings \\(S\\) is simply the sum of these 1,000 independent draws:\n\nset.seed(100)\nX &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\nS &lt;- sum(X)\nS\n\n[1] 16",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#the-probability-distribution-of-a-random-variable",
    "href": "content/Week_09/09b.html#the-probability-distribution-of-a-random-variable",
    "title": "Visualizing Uncertainty",
    "section": "",
    "text": "If you run the code above, you see that \\(S\\) changes every time. This is, of course, because \\(S\\) is a random variable. The probability distribution of a random variable tells us the probability of the observed value falling at any given interval. So, for example, if we want to know the probability that we lose money, we are asking the probability that \\(S\\) is in the interval \\(S&lt;0\\).\nNote that if we can define a cumulative distribution function \\(F(a) = \\mbox{Pr}(S\\leq a)\\), then we will be able to answer any question related to the probability of events defined by our random variable \\(S\\), including the event \\(S&lt;0\\). We call this \\(F\\) the random variable’s distribution function.\nWe can estimate the distribution function for the random variable \\(S\\) by using a Monte Carlo simulation to generate many realizations of the random variable. With this code, we run the experiment of having 1,000 people play roulette, over and over, specifically \\(B = 10,000\\) times:\n\nset.seed(200)\nn &lt;- 1000\nB &lt;- 10000\nroulette_winnings &lt;- function(n){\n  X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\n  sum(X)\n}\nS &lt;- replicate(B, roulette_winnings(n))\n\nNow we can ask the following: in our simulations, how often did we get sums less than or equal to a?\n\nmean(S &lt;= a)\n\nThis will be a very good approximation of \\(F(a)\\) and we can easily answer the casino’s question: how likely is it that we will lose money? We can see it is quite low:\n\nmean(S&lt;0)\n\n[1] 0.0474\n\n\nWe can visualize the distribution of \\(S\\) by creating a histogram showing the probability \\(F(b)-F(a)\\) for several intervals \\((a,b]\\):\n\n\n\n\n\n\n\n\n\nWe see that the distribution appears to be approximately normal. A qq-plot will confirm that the normal approximation is close to a perfect approximation for this distribution. If, in fact, the distribution is normal, then all we need to define the distribution is the average and the standard deviation. Because we have the original values from which the distribution is created, we can easily compute these with mean(S) and sd(S). The blue curve you see added to the histogram above is a normal density with this average and standard deviation.\nThis average and this standard deviation have special names. They are referred to as the expected value and standard error of the random variable \\(S\\). We will say more about these in the next section where we will discuss an incredibly useful approximation provided by mathematical theory that applies generally to sums and averages of draws from any urn: the Central Limit Theorem (CLT).",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#distributions-versus-probability-distributions",
    "href": "content/Week_09/09b.html#distributions-versus-probability-distributions",
    "title": "Visualizing Uncertainty",
    "section": "",
    "text": "Before we continue, let’s make an important distinction and connection between the distribution of a list of numbers and a probability distribution. In the visualization lectures, we described how any list of numbers \\(x_1,\\dots,x_n\\) has a distribution. The definition is quite straightforward. We define \\(F(a)\\) as the function that tells us what proportion of the list is less than or equal to \\(a\\). Because they are useful summaries when the distribution is approximately normal, we define the average and standard deviation. These are defined with a straightforward operation of the vector containing the list of numbers x:\n\nm &lt;- sum(x)/length(x)\ns &lt;- sqrt(sum((x - m)^2) / length(x))\n\nA random variable \\(X\\) has a distribution function. To define this, we do not need a list of numbers. It is a theoretical concept. In this case, we define the distribution as the \\(F(a)\\) that answers the question: what is the probability that \\(X\\) is less than or equal to \\(a\\)? There is no list of numbers.\nHowever, if \\(X\\) is defined by drawing from an urn with numbers in it, then there is a list: the list of numbers inside the urn. In this case, the distribution of that list is the probability distribution of \\(X\\) and the average and standard deviation of that list are the expected value and standard error of the random variable.\nAnother way to think about it that does not involve an urn is to run a Monte Carlo simulation and generate a very large list of outcomes of \\(X\\). These outcomes are a list of numbers. The distribution of this list will be a very good approximation of the probability distribution of \\(X\\). The longer the list, the better the approximation. The average and standard deviation of this list will approximate the expected value and standard error of the random variable.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#notation-for-random-variables",
    "href": "content/Week_09/09b.html#notation-for-random-variables",
    "title": "Visualizing Uncertainty",
    "section": "",
    "text": "In statistical textbooks, upper case letters are used to denote random variables and we follow this convention here. Lower case letters are used for observed values. You will see some notation that includes both. For example, you will see events defined as \\(X \\leq x\\). Here \\(X\\) is a random variable, making it a random event, and \\(x\\) is an arbitrary value and not random. So, for example, \\(X\\) might represent the number on a die roll and \\(x\\) will represent an actual value we see 1, 2, 3, 4, 5, or 6. So in this case, the probability of \\(X=x\\) is 1/6 regardless of the observed value \\(x\\). This notation is a bit strange because, when we ask questions about probability, \\(X\\) is not an observed quantity. Instead, it’s a random quantity that we will see in the future. We can talk about what we expect it to be, what values are probable, but not what it is. But once we have data, we do see a realization of \\(X\\). So data scientists talk of what could have been after we see what actually happened.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#the-expected-value-and-standard-error",
    "href": "content/Week_09/09b.html#the-expected-value-and-standard-error",
    "title": "Visualizing Uncertainty",
    "section": "",
    "text": "We have described sampling models for draws. We will now go over the mathematical theory that lets us approximate the probability distributions for the sum of draws. Once we do this, we will be able to help the casino predict how much money they will make. The same approach we use for the sum of draws will be useful for describing the distribution of averages and proportion which we will need to understand how polls work.\nThe first important concept to learn is the expected value. In statistics books, it is common to use letter \\(\\mbox{E}\\) like this:\n\\[\\mbox{E}[X]\\]\nto denote the expected value of the random variable \\(X\\).\nA random variable will vary around its expected value in a way that if you take the average of many, many draws, the average of the draws will approximate the expected value, getting closer and closer the more draws you take.\nTheoretical statistics provides techniques that facilitate the calculation of expected values in different circumstances. For example, a useful formula tells us that the expected value of a random variable defined by one draw is the average of the numbers in the urn. In the urn used to model betting on red in roulette, we have 20 one dollars and 18 negative one dollars. The expected value is thus:\n\\[\n\\mbox{E}[X] = (20 + -18)/38\n\\]\nwhich is about 5 cents. It is a bit counterintuitive to say that \\(X\\) varies around 0.05, when the only values it takes is 1 and -1. One way to make sense of the expected value in this context is by realizing that if we play the game over and over, the casino wins, on average, 5 cents per game. A Monte Carlo simulation confirms this:\n\nB &lt;- 10^6\nx &lt;- sample(c(-1,1), B, replace = TRUE, prob=c(9/19, 10/19))\nmean(x)\n\n[1] 0.053422\n\n\nIn general, if the urn has two possible outcomes, say \\(a\\) and \\(b\\), with proportions \\(p\\) and \\(1-p\\) respectively, the average is:\n\\[\\mbox{E}[X] = pa + (1-p)b\\]\nTo see this, notice that if there are \\(n\\) beads in the urn, then we have \\(np\\) \\(a\\)s and \\(n(1-p)\\) \\(b\\)s and because the average is the sum, \\(n\\times a \\times p + n\\times b \\times (1-p)\\), divided by the total \\(n\\), we get that the average is \\(ap + b(1-p)\\).\nNow the reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distributions of sum, which then is useful for describing the distribution of averages and proportions. The first useful fact is that the expected value of the sum of the draws is:\n\\[\n\\mbox{}\\mbox{number of draws } \\times \\mbox{ average of the numbers in the urn}\n\\]\nSo if 1,000 people play roulette, the casino expects to win, on average, about 1,000 \\(\\times\\) $0.05 = $50. But this is an expected value. How different can one observation be from the expected value? The casino really needs to know this. What is the range of possibilities? If negative numbers are too likely, they will not install roulette wheels. Statistical theory once again answers this question. The standard error (SE) gives us an idea of the size of the variation around the expected value. In statistics books, it’s common to use:\n\\[\\mbox{SE}[X]\\]\nto denote the standard error of a random variable.\nIf our draws are independent, then the standard error of the sum is given by the equation:\n\\[\n\\sqrt{\\mbox{number of draws }} \\times \\mbox{ standard deviation of the numbers in the urn}\n\\]\nUsing the definition of standard deviation, we can derive, with a bit of math, that if an urn contains two values \\(a\\) and \\(b\\) with proportions \\(p\\) and \\((1-p)\\), respectively, the standard deviation is:\n\\[\\mid b - a \\mid \\sqrt{p(1-p)}.\\]\nSo in our roulette example, the standard deviation of the values inside the urn is: \\(\\mid 1 - (-1) \\mid \\sqrt{10/19 \\times 9/19}\\) or:\n\n2 * sqrt(90)/19\n\n[1] 0.998614\n\n\nThe standard error tells us the typical difference between a random variable and its expectation. Since one draw is obviously the sum of just one draw, we can use the formula above to calculate that the random variable defined by one draw has an expected value of 0.05 and a standard error of about 1. This makes sense since we either get 1 or -1, with 1 slightly favored over -1.\nUsing the formula above, the sum of 1,000 people playing has standard error of about $32:\n\nn &lt;- 1000\nsqrt(n) * 2 * sqrt(90)/19\n\n[1] 31.57895\n\n\nAs a result, when 1,000 people bet on red, the casino is expected to win $50 with a standard error of $32. It therefore seems like a safe bet. But we still haven’t answered the question: how likely is it to lose money? Here the CLT will help.\nAdvanced note: Before continuing we should point out that exact probability calculations for the casino winnings can be performed with the binomial distribution. However, here we focus on the CLT, which can be generally applied to sums of random variables in a way that the binomial distribution can’t.\n\n\nThe standard deviation of a list x (below we use heights as an example) is defined as the square root of the average of the squared differences:\n\nlibrary(dslabs)\nx &lt;- heights$height\nm &lt;- mean(x)\ns &lt;- sqrt(mean((x-m)^2))\n\nThe SD is the the square root of the sample variance, and the sample variance is the square of the sample SD. Using mathematical notation we write:\n\\[\n\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i \\\\\n\\] and \\[\n\\sigma =  \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2}\n\\]\nHowever, be aware that the sd function returns a slightly different result:\n\nidentical(s, sd(x))\n\n[1] FALSE\n\ns-sd(x)\n\n[1] -0.001942661\n\n\nThis is because the sd function R does not return the sd of the list, but rather uses a formula that estimates standard deviations of a population from a random sample \\(X_1, \\dots, X_N\\) which, for reasons not discussed here, divide the sum of squares by the \\(N-1\\).\n\\[\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i, \\,\\,\\,\\,\ns =  \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2}\n\\]\nYou can see that this is the case by typing:\n\nn &lt;- length(x)\ns-sd(x)*sqrt((n-1) / n)\n\n[1] 5.329071e-15\n\n\nFor all the theory discussed here, you need to compute the actual standard deviation as defined:\n\nsqrt(mean((x-m)^2))\n\nSo be careful when using the sd function in R. However, keep in mind that throughout the book we sometimes use the sd function when we really want the actual SD. This is because when the list size is big, these two are practically equivalent since \\(\\sqrt{(N-1)/N} \\approx 1\\).",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#central-limit-theorem",
    "href": "content/Week_09/09b.html#central-limit-theorem",
    "title": "Visualizing Uncertainty",
    "section": "",
    "text": "The Central Limit Theorem (CLT) tells us that when the number of draws, also called the sample size, is large, the probability distribution of the sum of the independent draws is approximately normal. Because sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history.\nPreviously, we discussed that if we know that the distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and standard deviation. We also know that the same applies to probability distributions. If a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe the probability distribution are the average and standard deviation, referred to as the expected value and standard error.\nWe previously ran this Monte Carlo simulation:\n\nn &lt;- 1000\nB &lt;- 10000\nroulette_winnings &lt;- function(n){\n  X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\n  sum(X)\n}\nS &lt;- replicate(B, roulette_winnings(n))\n\nThe Central Limit Theorem (CLT) tells us that the sum \\(S\\) is approximated by a normal distribution. Using the formulas above, we know that the expected value and standard error are:\n\nn * (20-18)/38\n\n[1] 52.63158\n\nsqrt(n) * 2 * sqrt(90)/19\n\n[1] 31.57895\n\n\nThe theoretical values above match those obtained with the Monte Carlo simulation:\n\nmean(S)\n\n[1] 52.3038\n\nsd(S)\n\n[1] 31.20746\n\n\nUsing the CLT, we can skip the Monte Carlo simulation and instead compute the probability of the casino losing money using this approximation:\n\nmu &lt;- n * (20-18)/38\nse &lt;-  sqrt(n) * 2 * sqrt(90)/19\npnorm(0, mu, se)\n\n[1] 0.04779035\n\n\nwhich is also in very good agreement with our Monte Carlo result:\n\nmean(S &lt; 0)\n\n[1] 0.0446\n\n\nIt’s pretty powerful to have a function representing the outcome’s distribution, instead of having a (possibly huge) list of sampled outcomes.\n\n\nThe CLT works when the number of draws is large. But large is a relative term. In many circumstances as few as 30 draws is enough to make the CLT useful. In some specific instances, as few as 10 is enough. However, these should not be considered general rules. Note, for example, that when the probability of success is very small, we need much larger sample sizes.\nBy way of illustration, let’s consider the lottery. In the lottery, the chances of winning are less than 1 in a million. Thousands of people play so the number of draws is very large. Yet the number of winners, the sum of the draws, range between 0 and 4. This sum is certainly not well approximated by a normal distribution, so the CLT does not apply, even with the very large sample size. This is generally true when the probability of a success is very low. In these cases, the Poisson distribution is more appropriate.\nYou can examine the properties of the Poisson distribution using dpois and ppois. You can generate random variables following this distribution with rpois. However, we do not cover the theory here. You can learn about the Poisson distribution in any probability textbook and even Wikipedia3",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#statistical-properties-of-averages",
    "href": "content/Week_09/09b.html#statistical-properties-of-averages",
    "title": "Visualizing Uncertainty",
    "section": "",
    "text": "There are several useful mathematical results that we used above and often employ when working with data. We list them below.\n1. The expected value of the sum of random variables is the sum of each random variable’s expected value. We can write it like this:\n\\[\n\\mbox{E}[X_1+X_2+\\dots+X_n] =  \\mbox{E}[X_1] + \\mbox{E}[X_2]+\\dots+\\mbox{E}[X_n]\n\\]\nIf the \\(X\\) are independent draws from the urn, then they all have the same expected value. Let’s call it \\(\\mu\\) and thus:\n\\[\n\\mbox{E}[X_1+X_2+\\dots+X_n]=  n\\mu\n\\]\nwhich is another way of writing the result we show above for the sum of draws.\n2. The expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable. This is easier to explain with symbols:\n\\[\n\\mbox{E}[aX] =  a\\times\\mbox{E}[X]\n\\]\nTo see why this is intuitive, consider change of units. If we change the units of a random variable, say from dollars to cents, the expectation should change in the same way. A consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, call it \\(\\mu\\) again:\n\\[\n\\mbox{E}[(X_1+X_2+\\dots+X_n) / n]=   \\mbox{E}[X_1+X_2+\\dots+X_n] / n = n\\mu/n = \\mu\n\\]\n3. The square of the standard error of the sum of independent random variables is the sum of the square of the standard error of each random variable. This one is easier to understand in math form:\n\\[\n\\mbox{SE}[X_1+X_2+\\dots+X_n] = \\sqrt{\\mbox{SE}[X_1]^2 + \\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2  }\n\\]\nThe square of the standard error is referred to as the variance in statistical textbooks. Note that this particular property is not as intuitive as the previous three and more in depth explanations can be found in statistics textbooks.\n4. The standard error of a non-random constant times a random variable is the non-random constant times the random variable’s standard error. As with the expectation: \\[\n\\mbox{SE}[aX] =  a \\times \\mbox{SE}[X]\n\\]\nTo see why this is intuitive, again think of units.\nA consequence of 3 and 4 is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of \\(n\\) (the number of draws), call it \\(\\sigma\\):\n\\[\n\\begin{aligned}\n\\mbox{SE}[(X_1+X_2+\\dots+X_n) / n] &=   \\mbox{SE}[X_1+X_2+\\dots+X_n]/n \\\\\n&= \\sqrt{\\mbox{SE}[X_1]^2+\\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2}/n \\\\\n&= \\sqrt{\\sigma^2+\\sigma^2+\\dots+\\sigma^2}/n\\\\\n&= \\sqrt{n\\sigma^2}/n\\\\\n&= \\sigma / \\sqrt{n}\n\\end{aligned}\n\\]\n5. If \\(X\\) is a normally distributed random variable, then if \\(a\\) and \\(b\\) are non-random constants, \\(aX + b\\) is also a normally distributed random variable. All we are doing is changing the units of the random variable by multiplying by \\(a\\), then shifting the center by \\(b\\).\nNote that statistical textbooks use the Greek letters \\(\\mu\\) and \\(\\sigma\\) to denote the expected value and standard error, respectively. This is because \\(\\mu\\) is the Greek letter for \\(m\\), the first letter of mean, which is another term used for expected value. Similarly, \\(\\sigma\\) is the Greek letter for \\(s\\), the first letter of standard error.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#law-of-large-numbers",
    "href": "content/Week_09/09b.html#law-of-large-numbers",
    "title": "Visualizing Uncertainty",
    "section": "",
    "text": "An important implication of the final result is that the standard error of the average becomes smaller and smaller as \\(n\\) grows larger. When \\(n\\) is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn. This is known in statistical textbooks as the law of large numbers or the law of averages.\n\n\nThe law of averages is sometimes misinterpreted. For example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50% heads and 50% tails. A similar argument would be to say that red “is due” on the roulette wheel after seeing black come up five times in a row. These events are independent so the chance of a coin landing heads is 50% regardless of the previous 5. This is also the case for the roulette outcome. The law of averages applies only when the number of draws is very large and not in small samples. After a million tosses, you will definitely see about 50% heads regardless of the outcome of the first five tosses.\nAnother funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row.\nProbabilistic thinking is central in the human experience. How we describe that thinking is mixed, but most of the time we use (rather imprecise) language. With only a few moments of searching, one can find thousands of articles that use probabilistic words to describe events. Here are some examples:\n\n“‘Highly unlikely’ State of the Union will happen amid shutdown” – The Hill\n\n\n“Tiger Woods makes Masters 15th and most improbable major” – Fox\n\n\n“Trump predicts ‘very good chance’ of China trade deal” – CNN\n\nYet people don’t have a good sense of what these things mean. Uncertainty is key to data analytics: if we were certain of things, A study in the 1960s explored the perception of probabilistic words like these among NATO officers. A more modern replication of this found the following basic pattern:\n\n\n\n\n\n\n\n\n\nA deep, basic fact about humans is that we struggle to understand probabilities. But visualizing things can help. The graphic above shows the uncertainty about uncertainty (very meta). We can convey all manner of uncertainty with clever graphics. Today’s practical example works through some of the myriad of ways to visualize variable data. We’ll cover some territory that we’ve already hit, in hopes of locking in some key concepts.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#polls",
    "href": "content/Week_09/09b.html#polls",
    "title": "Visualizing Uncertainty",
    "section": "Polls",
    "text": "Polls\nOpinion polling has been conducted since the 19th century. The general goal is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as inference and it is the main topic of this chapter.\nPerhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know in which geographical locations to focus their “get out the vote” efforts.\nElections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results.\nAlthough typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at such data.\nReal Clear Politics4 is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the 2016 presidential election5:\n\nAlthough in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC and we will go into some detail on this later.\nLet’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different spread: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled MoE which stands for margin of error.\nIn this example, we will show how the probability concepts we learned in the previous content can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define estimates and margins of errors, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: confidence intervals and p-values. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modeling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election.\nWe start by connecting probability theory to the task of using polls to learn about a population.\n\nThe sampling model for polls\nTo help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this hypothetical urn.\nBefore making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you 10 cents per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay \\$25 to collect your \\$25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.\nThe dslabs package includes a function that shows a random draw from this urn:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ntake_poll(25)\n\n\n\n\n\n\n\n\n\n\nThink about how you would construct your interval based on the data shown above.\nWe have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors. That is, that there are just two parties: Republican and Democratic.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#populations-samples-parameters-and-estimates",
    "href": "content/Week_09/09b.html#populations-samples-parameters-and-estimates",
    "title": "Visualizing Uncertainty",
    "section": "Populations, samples, parameters, and estimates",
    "text": "Populations, samples, parameters, and estimates\nWe want to predict the proportion of blue beads in the urn. Let’s call this quantity \\(p\\), which then tells us the proportion of red beads \\(1-p\\), and the spread \\(p - (1-p)\\), which simplifies to \\(2p - 1\\).\nIn statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the population \\(p\\) is called a parameter. The 25 beads we see in the previous plot are called a sample. The task of statistical inference is to predict the parameter \\(p\\) using the observed data in the sample.\nCan we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that \\(p\\) &gt; .9 or \\(p\\) &lt; .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?\nWe want to construct an estimate of \\(p\\) using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample \\(0.48\\) must be at least related to the actual proportion \\(p\\). But do we simply predict \\(p\\) to be 0.48? First, remember that the sample proportion is a random variable. If we run the command take_poll(25) four times, we get a different answer each time, since the sample proportion is a random variable.\n\n\n\n\n\n\n\n\n\nNote that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better.\n\nThe sample average\nConducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter \\(p\\). Once we have this estimate, we can easily report an estimate for the spread \\(2p-1\\), but for simplicity we will illustrate the concepts for estimating \\(p\\). We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion \\(p\\).\nWe start by defining the random variable \\(X\\) as: \\(X=1\\) if we pick a blue bead at random and \\(X=0\\) if it is red. This implies that the population is a list of 0s and 1s. If we sample \\(N\\) beads, then the average of the draws \\(X_1, \\dots, X_N\\) is equivalent to the proportion of blue beads in our sample. This is because adding the \\(X\\)s is equivalent to counting the blue beads and dividing this count by the total \\(N\\) is equivalent to computing a proportion. We use the symbol \\(\\bar{X}\\) to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant \\(1/N\\):\n\\[\\bar{X} = 1/N \\times \\sum_{i=1}^N X_i\\]\nFor simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is \\(N\\) times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be \\(p\\), the proportion of blue beads.\nHere we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to estimate \\(p\\).\n\n\nParameters\nJust like we use variables to define unknowns in systems of equations, in statistical inference we define parameters to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters \\(p\\) to represent this quantity. \\(p\\) is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is \\(p\\), we are going to estimate this parameter.\nThe ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask, what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample.\n\n\nPolling versus forecasting\nBefore we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the \\(p\\) for that moment and not for election day. The \\(p\\) for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section.\n\n\nProperties of our estimate: expected value and standard error\nTo understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion \\(\\bar{X}\\). Remember that \\(\\bar{X}\\) is the sum of independent draws so the rules we covered in the probability chapter apply.\nUsing what we have learned, the expected value of the sum \\(N\\bar{X}\\) is \\(N \\times\\) the average of the urn, \\(p\\). So dividing by the non-random constant \\(N\\) gives us that the expected value of the average \\(\\bar{X}\\) is \\(p\\). We can write it using our mathematical notation:\n\\[\n\\mbox{E}(\\bar{X}) = p\n\\]\nWe can also use what we learned to figure out the standard error: the standard error of the sum is \\(\\sqrt{N} \\times\\) the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is \\((1-0) \\sqrt{p (1-p)}\\) = \\(\\sqrt{p (1-p)}\\). Because we are dividing the sum by \\(N\\), we arrive at the following formula for the standard error of the average:\n\\[\n\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\n\\]\nThis result reveals the power of polls. The expected value of the sample proportion \\(\\bar{X}\\) is the parameter of interest \\(p\\) and we can make the standard error as small as we want by increasing \\(N\\). The law of large numbers tells us that with a large enough poll, our estimate converges to \\(p\\).\nIf we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?\nOne problem is that we do not know \\(p\\), so we can’t compute the standard error. However, for illustrative purposes, let’s assume that \\(p=0.51\\) and make a plot of the standard error versus the sample size \\(N\\):\n\n\n\n\n\n\n\n\n\nFrom the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and \\(p=0.51\\), the standard error is:\n\nsqrt(p*(1-p))/sqrt(1000)\n\n[1] 0.01580823\n\n\nor 1.5 percentage points. So even with large polls, for close elections, \\(\\bar{X}\\) can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the \\(p\\).",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#clt",
    "href": "content/Week_09/09b.html#clt",
    "title": "Visualizing Uncertainty",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nIf we didn’t cover this earlier this week, let’s talk about it here:\nThe Central Limit Theorem (CLT) tells us that the distribution function for a sum of draws is approximately normal. You also may recall that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of \\(\\bar{X}\\) is approximately normal.\nIn summary, we have that \\(\\bar{X}\\) has an approximately normal distribution with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\nNow how does this help us? Suppose we want to know what is the probability that we are within 1% from \\(p\\). We are basically asking what is\n\\[\n\\mbox{Pr}(| \\bar{X} - p| \\leq .01)\n\\] which is the same as:\n\\[\n\\mbox{Pr}(\\bar{X}\\leq p + .01) - \\mbox{Pr}(\\bar{X} \\leq p - .01)\n\\]\nCan we answer this question? We can use the mathematical trick we learned in the previous lecture. Subtract the expected value and divide by the standard error to get a standard normal random variable, call it \\(Z\\), on the left. Since \\(p\\) is the expected value and \\(\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\\) is the standard error we get:\n\\[\n\\mbox{Pr}\\left(Z \\leq \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right) -\n\\mbox{Pr}\\left(Z \\leq - \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right)\n\\]\nOne problem we have is that since we don’t know \\(p\\), we don’t know \\(\\mbox{SE}(\\bar{X})\\). But it turns out that the CLT still works if we estimate the standard error by using \\(\\bar{X}\\) in place of \\(p\\). We say that we plug-in the estimate. Our estimate of the standard error is therefore:\n\\[\n\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N}\n\\] In statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and \\(N\\).\nNow we continue with our calculation, but dividing by \\(\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N})\\) instead. In our first sample we had 12 blue and 13 red so \\(\\bar{X} = 0.48\\) and our estimate of standard error is:\n\nx_hat &lt;- 0.48\nse &lt;- sqrt(x_hat*(1-x_hat)/25)\nse\n\n[1] 0.09991997\n\n\nAnd now we can answer the question of the probability of being close to \\(p\\). The answer is:\n\npnorm(0.01/se) - pnorm(-0.01/se)\n\n[1] 0.07971926\n\n\nTherefore, there is a small chance that we will be close. A poll of only \\(N=25\\) people is not really very useful, at least not for a close election.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#a-monte-carlo-simulation",
    "href": "content/Week_09/09b.html#a-monte-carlo-simulation",
    "title": "Visualizing Uncertainty",
    "section": "A Monte Carlo simulation",
    "text": "A Monte Carlo simulation\n(Optional) Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:\n\nB &lt;- 10000\nN &lt;- 1000\nx_hat &lt;- replicate(B, {\n  x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\n  mean(x)\n})\n\nThe problem is, of course, we don’t know p. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function take_poll(n=1000) instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.\nOne thing we therefore do to corroborate theoretical results is to pick one or several values of p and run the simulations. Let’s set p=0.45. We can then simulate a poll:\n\np &lt;- 0.45\nN &lt;- 1000\n\nx &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\nx_hat &lt;- mean(x)\n\nIn this particular sample, our estimate is x_hat. We can use that code to do a Monte Carlo simulation:\n\nB &lt;- 10000\nx_hat &lt;- replicate(B, {\n  x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\n  mean(x)\n})\n\nTo review, the theory tells us that \\(\\bar{X}\\) is approximately normally distributed, has expected value \\(p=\\) 0.45 and standard error \\(\\sqrt{p(1-p)/N}\\) = 0.0157321. The simulation confirms this:\n\nmean(x_hat)\n\n[1] 0.4500761\n\nsd(x_hat)\n\n[1] 0.01579523\n\n\nA histogram and qq-plot confirm that the normal approximation is accurate as well:\n\n\n\n\n\n\n\n\n\nOf course, in real life we would never be able to run such an experiment because we don’t know \\(p\\). But we could run it for various values of \\(p\\) and \\(N\\) and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing p and N.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#code",
    "href": "content/Week_09/09b.html#code",
    "title": "Visualizing Uncertainty",
    "section": "Code",
    "text": "Code\n\nLoad and clean data\nFirst, we load the libraries we’ll be using:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggridges)\nlibrary(gghalves)\n\nThen we load the data with read_csv(). Here we assume that the CSV file lives in a subfolder in my project named data. Naturally, you’ll need to point this to wherever you stashed the data.\n\nweather_atl_raw &lt;- read_csv(\"data/atl-weather-2019.csv\")\n\nWe’ll add a couple columns that we can use for faceting and filling using the month() and wday() functions from lubridate for extracting parts of the date:\n\nweather_atl &lt;- weather_atl_raw %&gt;%\n  mutate(Month = month(time, label = TRUE, abbr = FALSE),\n         Day = wday(time, label = TRUE, abbr = FALSE))\n\nNow we’re ready to go!\n\n\nHistograms\nWe can first make a histogram of wind speed. We’ll use a bin width of 1 and color the edges of the bars white:\n\nggplot(weather_atl, aes(x = windSpeed)) +\n  geom_histogram(binwidth = 1, color = \"white\")\n\n\n\n\n\n\n\n\nThis is fine enough, but we can improve it by forcing the buckets/bins to start at whole numbers instead of containing ranges like 2.5–3.5. We’ll use the boundary argument for that. We also add scale_x_continuous() to add our own x-axis breaks instead of having things like 2.5, 5, and 7.5:\n\nggplot(weather_atl, aes(x = windSpeed)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 1) +\n  scale_x_continuous(breaks = seq(0, 12, by = 1))\n\n\n\n\n\n\n\n\nWe can show the distribution of wind speed by month if we map the Month column we made onto the fill aesthetic:\n\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 1) +\n  scale_x_continuous(breaks = seq(0, 12, by = 1))\n\n\n\n\n\n\n\n\nThis is colorful, but it’s impossible to actually interpret. Instead of only filling, we’ll also facet by month to see separate graphs for each month. We can turn off the fill legend because it’s now redundant.\n\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 1) +\n  scale_x_continuous(breaks = seq(0, 12, by = 1)) +\n  guides(fill = FALSE) +\n  facet_wrap(vars(Month))\n\n\n\n\n\n\n\n\nNeat! January, March, and April appear to have the most variation in windy days, with a few wind-less days and a few very-windy days, while August was very wind-less.\n\n\nDensity plots beyond geom_density\nOr we can stack the density plots behind each other with ggridges. For that to work, we also need to map Month to the y-axis. We can reverse the y-axis so that January is at the top if we use the fct_rev() function:\n\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +\n  geom_density_ridges() +\n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\nWe can add some extra information to geom_density_ridges() with some other arguments like quantile_lines. We can use the quantiles argument to tell the plow how many parts to be cut into. Since we just want to show the median, we’ll set that to 2 so each density plot is divided in half:\n\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +\n  geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +\n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\nNow that we have good working code, we can easily substitute in other variables by changing the x mapping:\n\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = Month)) +\n  geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +\n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\nWe can get extra fancy if we fill by temperature instead of filling by month. To get this to work, we need to use geom_density_ridges_gradient(), and we need to change the fill mapping to the strange looking ..x.., which is a weird ggplot trick that tells it to use the variable we mapped to the x-axis. For whatever reason, fill = temperatureHigh doesn’t work:\n\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = ..x..)) +\n  geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(x = \"High temperature\", y = NULL, color = \"Temp\")\n\n\n\n\n\n\n\n\nAnd finally, we can get extra fancy and show the distributions for both the high and low temperatures each month. To make this work, we need to manipulate the data a little. Right now there are two columns for high and low temperature: temperatureLow and temperatureHigh. To be able to map temperature to the x-axis and high vs. low to another aesthetic (like linetype), we need a column with the temperature and a column with an indicator variable for whether it is high or low. This data needs to be tidied (since right now we have a variable (high/low) encoded in the column name). We can tidy this data using pivot_longer() from tidyr, which was already loaded with library(tidyverse). In the RStudio primers, you did this same thing with gather()—pivot_longer() is the newer version of gather():\n\nweather_atl_long &lt;- weather_atl %&gt;%\n  pivot_longer(cols = c(temperatureLow, temperatureHigh),\n               names_to = \"temp_type\",\n               values_to = \"temp\") %&gt;%\n  # Clean up the new temp_type column so that \"temperatureHigh\" becomes \"High\", etc.\n  mutate(temp_type = recode(temp_type,\n                            temperatureHigh = \"High\",\n                            temperatureLow = \"Low\")) %&gt;%\n  # This is optional—just select a handful of columns\n  select(time, temp_type, temp, Month)\n\n# Show the first few rows\nhead(weather_atl_long)\n\n# A tibble: 6 × 4\n  time                temp_type  temp Month  \n  &lt;dttm&gt;              &lt;chr&gt;     &lt;dbl&gt; &lt;ord&gt;  \n1 2019-01-01 05:00:00 Low        50.6 January\n2 2019-01-01 05:00:00 High       63.9 January\n3 2019-01-02 05:00:00 Low        49.0 January\n4 2019-01-02 05:00:00 High       57.4 January\n5 2019-01-03 05:00:00 Low        53.1 January\n6 2019-01-03 05:00:00 High       55.3 January\n\n\nNow we have a column for the temperature (temp) and a column indicating if it is high or low (temp_type). The dataset is also twice as long (730 rows) because each day has two rows (high and low). Let’s plot it and map high/low to the linetype aesthetic to show high/low in the border of the plots:\n\nggplot(weather_atl_long, aes(x = temp, y = fct_rev(Month),\n                             fill = ..x.., linetype = temp_type)) +\n  geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(x = \"High temperature\", y = NULL, color = \"Temp\")\n\n\n\n\n\n\n\n\nWe can see much wider temperature disparities during the summer, with large gaps between high and low, and relatively equal high/low temperatures during the winter.\n\n\nBox, violin, and rain cloud plots\nFinally, we can look at the distribution of variables with box plots, violin plots, and other similar graphs. First, we’ll make a box plot of windspeed, filled by the Day variable we made indicating weekday:\n\nggplot(weather_atl,\n       aes(y = windSpeed, fill = Day)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nWe can switch this to a violin plot by just changing the geom layer and mapping Day to the x-axis:\n\nggplot(weather_atl,\n       aes(y = windSpeed, x = Day, fill = Day)) +\n  geom_violin()\n\n\n\n\n\n\n\n\nWith violin plots it’s typically good to overlay other geoms. We can add some jittered points for a strip plot:\n\nggplot(weather_atl,\n       aes(y = windSpeed, x = Day, fill = Day)) +\n  geom_violin() +\n  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +\n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\nWe can also add larger points for the daily averages. We’ll use a special layer for this: stat_summary(). It has a slightly different syntax, since we’re not actually mapping a column from the dataset. Instead, we’re feeding a column from a dataset into a function (here \"mean\") and then plotting that result:\n\nggplot(weather_atl,\n       aes(y = windSpeed, x = Day, fill = Day)) +\n  geom_violin() +\n  stat_summary(geom = \"point\", fun = \"mean\", size = 5, color = \"white\") +\n  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +\n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\nWe can also show the mean and confidence interval at the same time by changing the summary function:\n\nggplot(weather_atl,\n       aes(y = windSpeed, x = Day, fill = Day)) +\n  geom_violin() +\n  stat_summary(geom = \"pointrange\", fun.data = \"mean_se\", size = 1, color = \"white\") +\n  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +\n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\nOverlaying the points directly on top of the violins shows extra information, but it’s also really crowded and hard to read. If we use the gghalves package, we can use special halved versions of some of these geoms like so:\n\nggplot(weather_atl,\n       aes(x = fct_rev(Day), y = temperatureHigh)) +\n  geom_half_point(aes(color = Day), side = \"l\", size = 0.5) +\n  geom_half_boxplot(aes(fill = Day), side = \"r\") +\n  guides(color = FALSE, fill = FALSE)\n\n\n\n\n\n\n\n\nNote the side argument for specifying which half of the column the geom goes. We can also use geom_half_violin():\n\nggplot(weather_atl,\n       aes(x = fct_rev(Day), y = temperatureHigh)) +\n  geom_half_point(aes(color = Day), side = \"l\", size = 0.5) +\n  geom_half_violin(aes(fill = Day), side = \"r\") +\n  guides(color = FALSE, fill = FALSE)\n\n\n\n\n\n\n\n\nIf we flip the plot, we can make a rain cloud plot:\n\nggplot(weather_atl,\n       aes(x = fct_rev(Day), y = temperatureHigh)) +\n  geom_half_boxplot(aes(fill = Day), side = \"l\", width = 0.5, nudge = 0.1) +\n  geom_half_point(aes(color = Day), side = \"l\", size = 0.5) +\n  geom_half_violin(aes(fill = Day), side = \"r\") +\n  guides(color = FALSE, fill = FALSE) +\n  coord_flip()",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_09/09b.html#footnotes",
    "href": "content/Week_09/09b.html#footnotes",
    "title": "Visualizing Uncertainty",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/w/index.php?title=Financial_crisis_of_2007%E2%80%932008↩︎\nhttps://en.wikipedia.org/w/index.php?title=Security_(finance)↩︎\nhttps://en.wikipedia.org/w/index.php?title=Poisson_distribution↩︎\nhttp://www.realclearpolitics.com↩︎\nhttp://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html↩︎",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Visualizing Uncertainty"
    ]
  },
  {
    "objectID": "content/Week_08/08b.html",
    "href": "content/Week_08/08b.html",
    "title": "Model Selection in Linear Regression",
    "section": "",
    "text": "Often when we are developing a linear regression model, part of our goal is to explain a relationship. Now, we will ignore much of what we have learned and instead simply use regression as a tool to predict. Instead of a model which explains relationships, we seek a model which minimizes errors.\n\n\n\n\n\n\n\n\n\nFirst, note that a linear model is one of many methods used in regression.\nTo discuss linear models in the context of prediction, we introduce the (very boring) Advertising data that is discussed in the ISL text (see supplemental readings). It can be found at https://www.statlearning.com/s/Advertising.csv.\n\nAdvertising &lt;- read_csv('https://www.statlearning.com/s/Advertising.csv')\nAdvertising = Advertising %&gt;% \n  dplyr::select(TV, radio, newspaper, sales)\nAdvertising\n\n# A tibble: 200 × 4\n      TV radio newspaper sales\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 230.   37.8      69.2  22.1\n 2  44.5  39.3      45.1  10.4\n 3  17.2  45.9      69.3   9.3\n 4 152.   41.3      58.5  18.5\n 5 181.   10.8      58.4  12.9\n 6   8.7  48.9      75     7.2\n 7  57.5  32.8      23.5  11.8\n 8 120.   19.6      11.6  13.2\n 9   8.6   2.1       1     4.8\n10 200.    2.6      21.2  10.6\n# ℹ 190 more rows\n\n\nWe can take a look at the relationship between sales and the three modes of advertising using the patchwork package (or any number of other ways)\n\nlibrary(patchwork)\ntv = ggplot(Advertising, aes(x = TV, y = sales)) + geom_point() + theme_bw()\nrad = ggplot(Advertising, aes(x = radio, y = sales)) + geom_point() + theme_bw()\nnews = ggplot(Advertising, aes(x = newspaper, y = sales)) + geom_point() + theme_bw()\ntv + rad + news\n\n\n\n\n\n\n\n\nHere we see a relationship between each of the advertising modes and sales, but we might want to know more. Specifically, we probably want to know something about the sales-maximizing combination of the three modes. For that, we’d need a good model of sales. But what does “good” mean?",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Model Selection in Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_08/08b.html#model-selection",
    "href": "content/Week_08/08b.html#model-selection",
    "title": "Model Selection in Linear Regression",
    "section": "",
    "text": "Often when we are developing a linear regression model, part of our goal is to explain a relationship. Now, we will ignore much of what we have learned and instead simply use regression as a tool to predict. Instead of a model which explains relationships, we seek a model which minimizes errors.\n\n\n\n\n\n\n\n\n\nFirst, note that a linear model is one of many methods used in regression.\nTo discuss linear models in the context of prediction, we introduce the (very boring) Advertising data that is discussed in the ISL text (see supplemental readings). It can be found at https://www.statlearning.com/s/Advertising.csv.\n\nAdvertising &lt;- read_csv('https://www.statlearning.com/s/Advertising.csv')\nAdvertising = Advertising %&gt;% \n  dplyr::select(TV, radio, newspaper, sales)\nAdvertising\n\n# A tibble: 200 × 4\n      TV radio newspaper sales\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 230.   37.8      69.2  22.1\n 2  44.5  39.3      45.1  10.4\n 3  17.2  45.9      69.3   9.3\n 4 152.   41.3      58.5  18.5\n 5 181.   10.8      58.4  12.9\n 6   8.7  48.9      75     7.2\n 7  57.5  32.8      23.5  11.8\n 8 120.   19.6      11.6  13.2\n 9   8.6   2.1       1     4.8\n10 200.    2.6      21.2  10.6\n# ℹ 190 more rows\n\n\nWe can take a look at the relationship between sales and the three modes of advertising using the patchwork package (or any number of other ways)\n\nlibrary(patchwork)\ntv = ggplot(Advertising, aes(x = TV, y = sales)) + geom_point() + theme_bw()\nrad = ggplot(Advertising, aes(x = radio, y = sales)) + geom_point() + theme_bw()\nnews = ggplot(Advertising, aes(x = newspaper, y = sales)) + geom_point() + theme_bw()\ntv + rad + news\n\n\n\n\n\n\n\n\nHere we see a relationship between each of the advertising modes and sales, but we might want to know more. Specifically, we probably want to know something about the sales-maximizing combination of the three modes. For that, we’d need a good model of sales. But what does “good” mean?",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Model Selection in Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_08/08b.html#assesing-model-accuracy",
    "href": "content/Week_08/08b.html#assesing-model-accuracy",
    "title": "Model Selection in Linear Regression",
    "section": "Assesing Model Accuracy",
    "text": "Assesing Model Accuracy\nThere are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.\n\\[\n\\text{RMSE}(\\hat{f}, \\text{Data}) = \\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i = 1}^{n}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\n\\]\nWhile for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA (summary(lm(...))) context, the denominator for MSE may not be \\(n\\).\nFor a linear model , the estimate of \\(f\\), \\(\\hat{f}\\), is given by the fitted regression line.\n\\[\n\\hat{y}({\\bf{x}_i}) = \\hat{f}({\\bf{x}_i})\n\\]\nWe can write an R function that will be useful for performing this calculation.\n\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Model Selection in Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_08/08b.html#model-complexity",
    "href": "content/Week_08/08b.html#model-complexity",
    "title": "Model Selection in Linear Regression",
    "section": "Model Complexity",
    "text": "Model Complexity\nAside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. What is a “nested model”? When you have “nested models”, then one model contains all of the same terms that the other model has. If we have TV, Radio, Newspaper, then we would have a “nested model” in this case:\nmodel1 = lm(sales ~ TV + radio, data = Advertising)\nmodel2 = lm(sales ~ TV + radio + newspaper, data = Advertising)\nHere, model1 is nested in model2. Here are non-nested models:\nmodel1 = lm(sales ~ TV + radio, data = Advertising)\nmodel2 = lm(sales ~ TV + newspaper, data = Advertising)\nWhen we add polynomial terms, we always add the lower-order terms as well. This will always make a nested model:\nmodel1 = lm(sales ~ TV + radio, data = Advertising)\nmodel2 = lm(sales ~ TV + TV^2 + radio + radio^2, data = Advertising)\nmodel3 = lm(sales ~ TV + TV^2 + radio + radio^2 + TV:radio, data = Advertising)\nThose are nested models – 1 and 2 use a subset of terms from 3, and 1 uses a subset of 2.\nWhen models are nested, then the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, \\(p\\).\nWe write a simple R function to extract this information from a model.\n\nget_complexity = function(model) {\n  length(coef(model)) - 1\n}",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Model Selection in Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_08/08b.html#test-train-split",
    "href": "content/Week_08/08b.html#test-train-split",
    "title": "Model Selection in Linear Regression",
    "section": "Test-Train Split",
    "text": "Test-Train Split\nThere is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts. It is essentially cheating! As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down, or in very specific cases, stay the same.\nThis would suggest that to predict well, we should use the largest possible model! However, in reality we have hard fit to a specific dataset, but as soon as we see new data, a large model may in fact predict poorly. This is called overfitting.\n\nOverfitting in action\nLet’s take a quick look at why overfitting may harm us, despite the notion that we want to minimize RMSE. I’m going to take a 20-row subset of Advertising and fit a 16th-degree polynomial. If you remember your mathematics training, an Nth degree polynomial has N-1 “inflection points”, which translates to fitting a curve with 15 inflections. That’s pretty flexible! R is smart and won’t let us fit a 20-degree polynomial, though. Here’s as close as we can get:\n\nsmallset = Advertising %&gt;% slice(1:20)\nflexible.lm = lm(sales ~ poly(TV, 16), smallset)\nsummary(flexible.lm)\n\n\nCall:\nlm(formula = sales ~ poly(TV, 16), data = smallset)\n\nResiduals:\n         1          2          3          4          5          6          7 \n 6.804e-03 -1.362e-01 -8.586e-02  1.154e+00 -7.661e-01  9.626e-01  5.371e-01 \n         8          9         10         11         12         13         14 \n 1.509e-01 -9.328e-01 -7.052e+00 -1.591e+00 -2.264e-01  8.249e-02 -1.121e-01 \n        15         16         17         18         19         20 \n 3.317e+00  4.554e+00  7.361e-02 -1.741e-06  1.143e+00 -1.078e+00 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)     13.4950     1.2288  10.982  0.00162 **\npoly(TV, 16)1   20.4353     5.4954   3.719  0.03384 * \npoly(TV, 16)2    1.6908     5.4954   0.308  0.77845   \npoly(TV, 16)3    2.4704     5.4954   0.450  0.68354   \npoly(TV, 16)4   -1.2439     5.4954  -0.226  0.83547   \npoly(TV, 16)5    0.9185     5.4954   0.167  0.87789   \npoly(TV, 16)6   -3.9055     5.4954  -0.711  0.52854   \npoly(TV, 16)7   -2.2700     5.4954  -0.413  0.70730   \npoly(TV, 16)8    0.2120     5.4954   0.039  0.97165   \npoly(TV, 16)9    1.4021     5.4954   0.255  0.81510   \npoly(TV, 16)10  -2.3341     5.4954  -0.425  0.69965   \npoly(TV, 16)11  -1.1539     5.4954  -0.210  0.84713   \npoly(TV, 16)12  -0.6833     5.4954  -0.124  0.90891   \npoly(TV, 16)13   1.2426     5.4954   0.226  0.83564   \npoly(TV, 16)14   1.5138     5.4954   0.275  0.80084   \npoly(TV, 16)15   2.0444     5.4954   0.372  0.73461   \npoly(TV, 16)16  -1.8923     5.4954  -0.344  0.75331   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.495 on 3 degrees of freedom\nMultiple R-squared:  0.8385,    Adjusted R-squared:  -0.02313 \nF-statistic: 0.9732 on 16 and 3 DF,  p-value: 0.5933\n\n\nLooks great - we are explaining a lot of the variation in sales! Let’s plot this 16-degree polynomial:\n\n# Plot a curve using predict over a very fine set of values\nplotseq = seq(from = min(smallset$TV), to = max(smallset$TV), length = 300)\npredseq = predict(flexible.lm, newdata = data.frame(TV = plotseq))\n\nplot(sales ~ TV, smallset, ylim = c(0, 100))\nlines(y = predseq, x = plotseq)\n\n\n\n\n\n\n\n\nand this model has complexity of:\n\nget_complexity(flexible.lm)\n\n[1] 16\n\n\nBut we don’t really believe that TV advertising of around 20 would result in almost 40 in sales. We certainly don’t trust that huge spike at 250, either! But how do we stop ourselves from overfitting?\n\n\n\n\n\n\nTRY IT\n\n\n\nWhen I was a teenager and had to drive in the snow for the first time, my mom took me out to the empty, frozen parking lot for my local roller rink. She had me drive fast, then slam on the brakes so that we skidded on the ice. The point of doing this was to get a feel for what skidding on ice is like. You can be told what it’s like to skid, but until you do it, it’s abstract. Same thing with overfitting.\nLet’s make overfitting concrete.\nRun the following code to download and subset some data. The code does a couple other things. Can you tell what?\n\nset.seed(242)\n\nRECS = read.csv('https://www.eia.gov/consumption/residential/data/2009/csv/recs2009_public.csv', stringsAsFactors = F) %&gt;%\n  as_tibble() %&gt;%  \n  slice(sample(1:n(), 30)) %&gt;%\n  dplyr::select(EnergyUsed = TOTALBTU, \n                REGIONC, DIVISION, Climate_Region_Pub, METROMICRO, Urban=UR, OWNRENT = KOWNRENT,YEARMADERANGE, \n                Occupants = OCCUPYYRANGE,ColdDays = HDD65, HotDayss = CDD65, SquareFeet = TOTHSQFT, CarsGarage = SIZEOFGARAGE, \n                AgeOfHome = YEARMADERANGE, TreeShade = TREESHAD, TVSize = TVSIZE1, HeaterAge = EQUIPAGE, HasAC = AIRCOND,\n                TempHome = TEMPHOME) %&gt;%\n  dplyr::filter(HeaterAge != -2 & TempHome !=-2)  %&gt;% # get rid of some NA's\n  dplyr::mutate(var1 = rnorm(n(), mean=0, sd = 1),\n                var2 = rnorm(n(), mean=10, sd = .5),\n                var3 = rpois(n(), 5),\n                var4 = runif(n(), -10, 0))\n\nWe have a lot of potential X-values here, so have at it. Use interactions, use polynomials, use polynomials of interactions – just make a lm() model that gets the RMSE down low. To use your rmse function, give it the prediction from your lm() model and the original date: rmse(RECS$EnergyUsed, predict(MyOLS).\nNow, my challenge to you is to estimate a linear-in-parameters model that minimizes RMSE (using your rmse function) for a prediction of EnergyUsed. Use as many variables as you want. Don’t use the poly function like we did before, since that’s too easy. Who can get the lowest RMSE?\n\n\n\n\nAvoiding overfitting\nFrequently we will take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data. Test data should never be used to train a model.\nNote that sometimes the terms evaluation set and test set are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.\nHere we use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to allow use to reproduce the same random split each time we perform this analysis.\n\nset.seed(90)\nnum_obs = nrow(Advertising)\n\ntrain_index = sample(num_obs, size = trunc(0.50 * num_obs))\ntrain_data = Advertising[train_index, ]\ntest_data = Advertising[-train_index, ]\n\nWe will look at two measures that assess how well a model is predicting, the train RMSE and the test RMSE.\n\\[\n\\text{RMSE}_{\\text{Train}} = \\text{RMSE}(\\hat{f}, \\text{Train Data}) = \\sqrt{\\frac{1}{n_{\\text{Tr}}}\\displaystyle\\sum_{i \\in \\text{Train}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\n\\]\nHere \\(n_{Tr}\\) is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.\n\\[\n\\text{RMSE}_{\\text{Test}} = \\text{RMSE}(\\hat{f}, \\text{Test Data}) = \\sqrt{\\frac{1}{n_{\\text{Te}}}\\displaystyle\\sum_{i \\in \\text{Test}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\n\\]\nHere \\(n_{Te}\\) is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict in general, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.\nWe will start with the simplest possible linear model, that is, a model with no predictors.\n\nfit_0 = lm(sales ~ 1, data = train_data)\nget_complexity(fit_0)\n\n[1] 0\n\n# train RMSE\nsqrt(mean((train_data$sales - predict(fit_0, train_data)) ^ 2))\n\n[1] 5.224106\n\n# test RMSE\nsqrt(mean((test_data$sales - predict(fit_0, test_data)) ^ 2))\n\n[1] 5.211881\n\n\nThe previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.\n\n# train RMSE\nrmse(actual = train_data$sales, predicted = predict(fit_0, train_data))\n\n[1] 5.224106\n\n# test RMSE\nrmse(actual = test_data$sales, predicted = predict(fit_0, test_data))\n\n[1] 5.211881\n\n\nThis function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.\n\nget_rmse = function(model, data, response) {\n  rmse(actual = subset(data, select = response, drop = TRUE),\n       predicted = predict(model, data))\n}\n\nBy using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.\n\nget_rmse(model = fit_0, data = train_data, response = \"sales\") # train RMSE\n\n[1] 5.224106\n\nget_rmse(model = fit_0, data = test_data, response = \"sales\") # test RMSE\n\n[1] 5.211881",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Model Selection in Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_08/08b.html#adding-flexibility-to-linear-models",
    "href": "content/Week_08/08b.html#adding-flexibility-to-linear-models",
    "title": "Model Selection in Linear Regression",
    "section": "Adding Flexibility to Linear Models",
    "text": "Adding Flexibility to Linear Models\nEach successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting.\n\nfit_1 = lm(sales ~ ., data = train_data)\nget_complexity(fit_1)\n\n[1] 3\n\nget_rmse(model = fit_1, data = train_data, response = \"sales\") # train RMSE\n\n[1] 1.649346\n\nget_rmse(model = fit_1, data = test_data, response = \"sales\") # test RMSE\n\n[1] 1.741642\n\n\n\nfit_2 = lm(sales ~ radio * newspaper * TV, data = train_data)\nget_complexity(fit_2)\n\n[1] 7\n\nget_rmse(model = fit_2, data = train_data, response = \"sales\") # train RMSE\n\n[1] 0.9875248\n\nget_rmse(model = fit_2, data = test_data, response = \"sales\") # test RMSE\n\n[1] 0.9014459\n\n\n\nfit_3 = lm(sales ~ radio * newspaper * TV + I(TV ^ 2), data = train_data)\nget_complexity(fit_3)\n\n[1] 8\n\nget_rmse(model = fit_3, data = train_data, response = \"sales\") # train RMSE\n\n[1] 0.6685005\n\nget_rmse(model = fit_3, data = test_data, response = \"sales\") # test RMSE\n\n[1] 0.6242491\n\n\n\nfit_4 = lm(sales ~ radio * newspaper * TV +\n           I(TV ^ 2) + I(radio ^ 2) + I(newspaper ^ 2), data = train_data)\nget_complexity(fit_4)\n\n[1] 10\n\nget_rmse(model = fit_4, data = train_data, response = \"sales\") # train RMSE\n\n[1] 0.6676294\n\nget_rmse(model = fit_4, data = test_data, response = \"sales\") # test RMSE\n\n[1] 0.6232819\n\n\n\nfit_5 = lm(sales ~ radio * newspaper * TV +\n           I(TV ^ 2) * I(radio ^ 2) * I(newspaper ^ 2), data = train_data)\nget_complexity(fit_5)\n\n[1] 14\n\nget_rmse(model = fit_5, data = train_data, response = \"sales\") # train RMSE\n\n[1] 0.6359041\n\nget_rmse(model = fit_5, data = test_data, response = \"sales\") # test RMSE\n\n[1] 0.7809674\n\n\n\nfit_6 = lm(sales ~ radio * newspaper * TV +\n           I(TV ^ 2) * I(radio ^ 2) * I(newspaper ^ 2) +\n             I(TV ^ 3) * I(radio ^ 3) * I(newspaper ^ 3), data = train_data)\nget_complexity(fit_6)\n\n[1] 21\n\nget_rmse(model = fit_6, data = train_data, response = \"sales\") # train RMSE\n\n[1] 0.4637712\n\nget_rmse(model = fit_6, data = test_data, response = \"sales\") # test RMSE\n\n[1] 1.579379",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Model Selection in Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_08/08b.html#choosing-a-model",
    "href": "content/Week_08/08b.html#choosing-a-model",
    "title": "Model Selection in Linear Regression",
    "section": "Choosing a Model",
    "text": "Choosing a Model\nTo better understand the relationship between train RMSE, test RMSE, and model complexity, we summarize our results, as the above is somewhat cluttered.\nFirst, we recap the models that we have fit.\n\nfit_0 = lm(sales ~ 1, data = train_data)\nfit_1 = lm(sales ~ ., data = train_data)\nfit_2 = lm(sales ~ radio * newspaper * TV, data = train_data)\nfit_3 = lm(sales ~ radio * newspaper * TV + I(TV ^ 2), data = train_data)\nfit_4 = lm(sales ~ radio * newspaper * TV +\n           I(TV ^ 2) + I(radio ^ 2) + I(newspaper ^ 2), data = train_data)\nfit_5 = lm(sales ~ radio * newspaper * TV +\n           I(TV ^ 2) * I(radio ^ 2) * I(newspaper ^ 2), data = train_data)\nfit_6 = lm(sales ~ radio * newspaper * TV +\n           I(TV ^ 2) * I(radio ^ 2) * I(newspaper ^ 2) +\n             I(TV ^ 3) * I(radio ^ 3) * I(newspaper ^ 3), data = train_data)\n\nNext, we create a list of the models fit.\n\nmodel_list = list(fit_0, fit_1, fit_2, fit_3, fit_4, fit_5, fit_6)\n\nWe then obtain train RMSE, test RMSE, and model complexity for each.\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\nWe then plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.\n\nplot(model_complexity, train_rmse, type = \"b\",\n     ylim = c(min(c(train_rmse, test_rmse)) - 0.02,\n              max(c(train_rmse, test_rmse)) + 0.02),\n     col = \"dodgerblue\",\n     xlab = \"Model Size (complexity)\",\n     ylab = \"RMSE\")\nlines(model_complexity, test_rmse, type = \"b\", col = \"darkorange\")\nlegend('topright', legend = c('train','test'), col = c('dodgerblue','darkorange'), lty=1)\n\n\n\n\n\n\n\n\nWe also summarize the results as a table. fit_1 is the least flexible, and fit_5 is the most flexible. We see the Train RMSE decrease as flexibility increases. We see that the Test RMSE is smallest for fit_3, thus is the model we believe will perform the best on future data not used to train the model. Note this may not be the best model, but it is the best model of the models we have seen in this example.\n\n\n\n\n\n\n\n\n\nModel\nTrain RMSE\nTest RMSE\nPredictors\n\n\n\n\nfit_0\n5.224106\n5.2118814\n0\n\n\nfit_1\n1.6493463\n1.7416416\n3\n\n\nfit_2\n0.9875248\n0.9014459\n7\n\n\nfit_3\n0.6685005\n0.6242491\n8\n\n\nfit_4\n0.6676294\n0.6232819\n10\n\n\nfit_5\n0.6359041\n0.7809674\n14\n\n\nfit_6\n0.4637712\n1.5793789\n21\n\n\n\nTo summarize:\n\nUnderfitting models: In general High Train RMSE, High Test RMSE. Seen in fit_1 and fit_2.\nOverfitting models: In general Low Train RMSE, High Test RMSE. Seen in fit_5 and fit_6.\n\nSpecifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE. Then a model is underfitting if there exists a more complex model with lower Test RMSE.\nA number of notes on these results:\n\nThe labels of under and overfitting are relative to the best model we see, fit_4. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.\nThe train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. Here we see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. There will always be a minimum point, though.\nOften we expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get lucky and this will not be true.\n\nA final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that predicted well, and paid no attention to a model for explaination. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)\n\nWhy does test RMSE increase?\nBased on our discussion above, the short answer is overfitting. To illustrate, here’s the overfit polynomial from earlier, but now I’m going to add to more randomly sampled points from Advertising. It’s in random order, so here I can just take the next 20 points. Usually we’d want to sample carefully\n\nsmallset_holdout = Advertising %&gt;% slice(21:40)\n\n# Plot a curve using predict over a very fine set of values\nplotseq = seq(from = min(smallset$TV), to = max(smallset$TV), length = 300)\npredseq = predict(flexible.lm, newdata = data.frame(TV = plotseq))\n\nplot(sales ~ TV, smallset, ylim = c(0, 100))\nlines(y = predseq, x = plotseq)\n\n# and add the next 20 points\npoints(y = smallset_holdout$sales, x = smallset_holdout$TV, col='red')\n\n\n\n\n\n\n\n\nThe RMSE is the square of the distance (in the vertical Y direction) between the point and the line. Where the black line meets the black points, the model is fitting well in training. But the red points to the black line have some much larger distances, which means much larger RMSE. As we wrangle the black line to the black points, we put a lot of wiggle in it that does not accommodate the red points well.",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Model Selection in Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_08/08b.html#coding-test-train",
    "href": "content/Week_08/08b.html#coding-test-train",
    "title": "Model Selection in Linear Regression",
    "section": "Coding for Test-Train Split",
    "text": "Coding for Test-Train Split\n\nMaking the split:\nAbove, when we created the split datasets, you’ll notice we used a sampled index to make our training and testing split. Let’s take a closer look using a “dataset” of 10:\n\ndataset = data.frame(index = 1:10)\n\ntrain_index = sample(NROW(dataset), size = trunc(.50*NROW(dataset)))\nprint(train_index)\n\n[1] 6 7 2 4 5\n\n\nIn the first line, we create a 10-row (and 1-column and very boring) dataset to use to illustrate the train/test index.\nIn the second line, we make a sample by drawing row numbers. The first argument to sample is “what do you want to draw from” and if we give it a single number, it assumes we meant 1 through that number. So giving it NROW(dataset) samples from the numbers 1:10, because we have 10 rows. The second argument is “how many should we draw”, and here, we ask for 50%. Since sometimes 50% will be fraction (if an odd number of rows), we use trunc to round it down. If we had 11 rows, this would give us a training sample of 5. Note that we don’t use replacement here – we don’t want to draw the same row index twice!\nNow we create the split data:\n\ntrain_dataset = dataset[train_index,]\ntest_dataset = dataset[-train_index,]\n\nWe want to make sure that all data is either in the training or the testing data, not both, and not left out. The first line should be intuitive – it keeps the row numbers corresponding to those drawn in train_index.\nThe second line, though, looks like it’s keeping the…negative rows? Huh? Let’s take a look:\n\nprint(train_index)\n\n[1] 6 7 2 4 5\n\nprint(-train_index)\n\n[1] -6 -7 -2 -4 -5\n\n\nSo what does it mean to ask R to give you the -6 row of dataset? R knows you can’t have negative rows, so the original authors thought it handy to use negatives to mean “omit”. So (1:5)[-2] would give you the numbers 1 to 5, but without the 2nd entry (gotta use the () or you’ll be asking for something different). 1, 3, 4, 5\nSo when we look at the test_dataset we see:\n\ndataset[-train_index,,drop=F]\n\n   index\n1      1\n3      3\n8      8\n9      9\n10    10\n\n# 'drop=F' keeps it from making the single column into a vector\n\nAnd that is the numbers 1:10, but without train_index: 6, 7, 2, 4, 5\nNow we know that every row is either in train or test. Super!\n\n\nlapply for model building\nWhile R has some great functionality for building and estimating multiple models, we’re going to learn how to do it at the most base level so that we can build from there. The best way is to use lapply since we’re fundamentally going to be using the same functions on different models. If we make a list of models, we can lapply our way to finding the best out-of-sample (test) fit!\nSo we start with a list of models. There are many ways to do this (see previous lecture), but here I’m going to write them out manually in a way that you should be able to do faster with code:\n\n# Create some random data:\nset.seed(242)\nN = 1000\ndat = data.frame(Y = rnorm(N),\n                 X1 = rnorm(N),\n                 X2 = rnorm(N),\n                 X3 = rpois(N, 5))\ntrain_index = sample(N, trunc(.5*N))\n\ndat.train = dat[train_index,]\ndat.test = dat[-train_index,]\n\nmodel_c = c(\n \"Y ~ 1\", # just intercept\n \"Y ~ X1\",\n \"Y ~ X1*X2\",\n \"Y ~ X1*X2*X3\",\n \"Y ~ X1*X2*X3*I(X1^2)\",\n \"Y ~ X1*X2*X3*I(X1^2)*I(X2^2)\",\n \"Y ~ X1*X2*X3*I(X1^2)*I(X2^2)*I(X3^2)\",\n \"Y ~ poly(X1, 3)*poly(X2,3)*poly(X3,3)\", \n \"Y ~ poly(X1, 4)*poly(X2,4)*poly(X3,4)\"\n)\n\nmodel_list = as.list(model_c)\n\nmodel_list_form = lapply(X = model_list, FUN = as.formula)\n\nprint(model_list_form)\n\n[[1]]\nY ~ 1\n&lt;environment: 0x0000024b779791a8&gt;\n\n[[2]]\nY ~ X1\n&lt;environment: 0x0000024b779791a8&gt;\n\n[[3]]\nY ~ X1 * X2\n&lt;environment: 0x0000024b779791a8&gt;\n\n[[4]]\nY ~ X1 * X2 * X3\n&lt;environment: 0x0000024b779791a8&gt;\n\n[[5]]\nY ~ X1 * X2 * X3 * I(X1^2)\n&lt;environment: 0x0000024b779791a8&gt;\n\n[[6]]\nY ~ X1 * X2 * X3 * I(X1^2) * I(X2^2)\n&lt;environment: 0x0000024b779791a8&gt;\n\n[[7]]\nY ~ X1 * X2 * X3 * I(X1^2) * I(X2^2) * I(X3^2)\n&lt;environment: 0x0000024b779791a8&gt;\n\n[[8]]\nY ~ poly(X1, 3) * poly(X2, 3) * poly(X3, 3)\n&lt;environment: 0x0000024b779791a8&gt;\n\n[[9]]\nY ~ poly(X1, 4) * poly(X2, 4) * poly(X3, 4)\n&lt;environment: 0x0000024b779791a8&gt;\n\n\nFirst we make some fake data to illustrate. Then, we get to the real business – this is the structure we’ll follow: we make a vector of model specifications, then we make it a list using as.list. Then, we use lapply (list-apply) to hit every entry with as.formula. Remember, for lapply, it takes each list element and uses it as the first argument in the given FUN, so it’ll run as.formula(model_list[[1]]) and then, in the output’s first list entry, place the result.\nNow, we lapply that model, returning the fit object (which we can use to predict results using new data). This requires an “anonymous function”. The anonymous function lets you, on the fly, write a function with one argument that lets you reference the list element being applied:\n\nmodel_ests = lapply(model_list_form, function(x) lm(x, data = dat.train ))\n\nAlright, no errors. Looks like we estimated seven models. You wouldn’t usually do this, but let’s look at the first 2 and the last one\n\nprint(lapply(model_ests[c(1:2, 9)], function(x) summary(x)))\n\n[[1]]\n\nCall:\nlm(formula = x, data = dat.train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5197 -0.6295  0.0288  0.5899  2.7199 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.02306    0.04289   0.538    0.591\n\nResidual standard error: 0.959 on 499 degrees of freedom\n\n\n[[2]]\n\nCall:\nlm(formula = x, data = dat.train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.52846 -0.62851  0.01821  0.57194  2.71830 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.02357    0.04291   0.549    0.583\nX1           0.03165    0.04400   0.719    0.472\n\nResidual standard error: 0.9594 on 498 degrees of freedom\nMultiple R-squared:  0.001038,  Adjusted R-squared:  -0.0009678 \nF-statistic: 0.5175 on 1 and 498 DF,  p-value: 0.4722\n\n\n[[3]]\n\nCall:\nlm(formula = x, data = dat.train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.61031 -0.47422  0.00208  0.49912  2.91785 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                             5.336e-01  5.435e-01   0.982    0.327\npoly(X1, 4)1                            2.988e+00  2.586e+01   0.116    0.908\npoly(X1, 4)2                            4.434e+01  4.541e+01   0.976    0.329\npoly(X1, 4)3                            5.822e+00  2.359e+01   0.247    0.805\npoly(X1, 4)4                            3.352e+01  3.547e+01   0.945    0.345\npoly(X2, 4)1                            6.035e+00  2.471e+01   0.244    0.807\npoly(X2, 4)2                            3.983e+01  5.033e+01   0.791    0.429\npoly(X2, 4)3                            1.200e+01  2.954e+01   0.406    0.685\npoly(X2, 4)4                            3.147e+01  3.462e+01   0.909    0.364\npoly(X3, 4)1                            1.680e+01  2.693e+01   0.624    0.533\npoly(X3, 4)2                            3.898e+01  3.782e+01   1.031    0.303\npoly(X3, 4)3                            1.108e+01  2.601e+01   0.426    0.670\npoly(X3, 4)4                            1.803e+01  1.766e+01   1.021    0.308\npoly(X1, 4)1:poly(X2, 4)1              -3.454e+02  9.044e+02  -0.382    0.703\npoly(X1, 4)2:poly(X2, 4)1              -9.672e+01  1.835e+03  -0.053    0.958\npoly(X1, 4)3:poly(X2, 4)1              -3.718e+02  8.203e+02  -0.453    0.651\npoly(X1, 4)4:poly(X2, 4)1              -3.746e+02  1.240e+03  -0.302    0.763\npoly(X1, 4)1:poly(X2, 4)2               2.363e+02  1.986e+03   0.119    0.905\npoly(X1, 4)2:poly(X2, 4)2               3.420e+03  4.136e+03   0.827    0.409\npoly(X1, 4)3:poly(X2, 4)2               2.352e+02  1.844e+03   0.127    0.899\npoly(X1, 4)4:poly(X2, 4)2               2.403e+03  3.164e+03   0.760    0.448\npoly(X1, 4)1:poly(X2, 4)3              -4.684e+02  1.018e+03  -0.460    0.646\npoly(X1, 4)2:poly(X2, 4)3               1.980e+02  2.174e+03   0.091    0.927\npoly(X1, 4)3:poly(X2, 4)3              -5.913e+02  9.579e+02  -0.617    0.537\npoly(X1, 4)4:poly(X2, 4)3               6.899e+00  1.433e+03   0.005    0.996\npoly(X1, 4)1:poly(X2, 4)4               2.933e+02  1.314e+03   0.223    0.823\npoly(X1, 4)2:poly(X2, 4)4               2.561e+03  2.868e+03   0.893    0.372\npoly(X1, 4)3:poly(X2, 4)4               2.453e+02  1.260e+03   0.195    0.846\npoly(X1, 4)4:poly(X2, 4)4               1.835e+03  2.202e+03   0.833    0.405\npoly(X1, 4)1:poly(X3, 4)1              -3.567e+02  9.755e+02  -0.366    0.715\npoly(X1, 4)2:poly(X3, 4)1               1.752e+03  2.150e+03   0.815    0.416\npoly(X1, 4)3:poly(X3, 4)1              -4.906e+01  8.957e+02  -0.055    0.956\npoly(X1, 4)4:poly(X3, 4)1               1.549e+03  1.706e+03   0.908    0.364\npoly(X1, 4)1:poly(X3, 4)2               2.176e+01  1.666e+03   0.013    0.990\npoly(X1, 4)2:poly(X3, 4)2               3.285e+03  3.157e+03   1.040    0.299\npoly(X1, 4)3:poly(X3, 4)2               1.420e+02  1.546e+03   0.092    0.927\npoly(X1, 4)4:poly(X3, 4)2               2.227e+03  2.607e+03   0.854    0.393\npoly(X1, 4)1:poly(X3, 4)3              -4.456e+02  8.990e+02  -0.496    0.620\npoly(X1, 4)2:poly(X3, 4)3               1.567e+03  2.000e+03   0.784    0.434\npoly(X1, 4)3:poly(X3, 4)3              -1.302e+02  8.559e+02  -0.152    0.879\npoly(X1, 4)4:poly(X3, 4)3               1.446e+03  1.594e+03   0.907    0.365\npoly(X1, 4)1:poly(X3, 4)4               4.225e+01  8.185e+02   0.052    0.959\npoly(X1, 4)2:poly(X3, 4)4               1.294e+03  1.503e+03   0.861    0.390\npoly(X1, 4)3:poly(X3, 4)4              -6.803e+00  7.924e+02  -0.009    0.993\npoly(X1, 4)4:poly(X3, 4)4               8.678e+02  1.249e+03   0.695    0.488\npoly(X2, 4)1:poly(X3, 4)1               6.100e+02  1.142e+03   0.534    0.593\npoly(X2, 4)2:poly(X3, 4)1               1.344e+03  2.537e+03   0.530    0.597\npoly(X2, 4)3:poly(X3, 4)1               8.483e+02  1.351e+03   0.628    0.530\npoly(X2, 4)4:poly(X3, 4)1               1.129e+03  1.816e+03   0.622    0.534\npoly(X2, 4)1:poly(X3, 4)2               5.422e+02  1.877e+03   0.289    0.773\npoly(X2, 4)2:poly(X3, 4)2               3.146e+03  3.468e+03   0.907    0.365\npoly(X2, 4)3:poly(X3, 4)2               8.970e+02  2.224e+03   0.403    0.687\npoly(X2, 4)4:poly(X3, 4)2               2.266e+03  2.418e+03   0.937    0.349\npoly(X2, 4)1:poly(X3, 4)3               6.174e+02  1.067e+03   0.579    0.563\npoly(X2, 4)2:poly(X3, 4)3               6.257e+02  2.456e+03   0.255    0.799\npoly(X2, 4)3:poly(X3, 4)3               9.380e+02  1.267e+03   0.740    0.460\npoly(X2, 4)4:poly(X3, 4)3               4.870e+02  1.777e+03   0.274    0.784\npoly(X2, 4)1:poly(X3, 4)4               2.291e+02  9.218e+02   0.248    0.804\npoly(X2, 4)2:poly(X3, 4)4               1.047e+03  1.549e+03   0.676    0.500\npoly(X2, 4)3:poly(X3, 4)4               5.522e+02  1.054e+03   0.524    0.601\npoly(X2, 4)4:poly(X3, 4)4               5.095e+02  1.105e+03   0.461    0.645\npoly(X1, 4)1:poly(X2, 4)1:poly(X3, 4)1 -1.028e+04  4.271e+04  -0.241    0.810\npoly(X1, 4)2:poly(X2, 4)1:poly(X3, 4)1  2.452e+04  8.704e+04   0.282    0.778\npoly(X1, 4)3:poly(X2, 4)1:poly(X3, 4)1 -1.324e+04  3.764e+04  -0.352    0.725\npoly(X1, 4)4:poly(X2, 4)1:poly(X3, 4)1  8.597e+03  6.093e+04   0.141    0.888\npoly(X1, 4)1:poly(X2, 4)2:poly(X3, 4)1 -3.084e+04  7.146e+04  -0.432    0.666\npoly(X1, 4)2:poly(X2, 4)2:poly(X3, 4)1  1.458e+05  1.990e+05   0.732    0.464\npoly(X1, 4)3:poly(X2, 4)2:poly(X3, 4)1 -2.507e+04  6.755e+04  -0.371    0.711\npoly(X1, 4)4:poly(X2, 4)2:poly(X3, 4)1  1.365e+05  1.523e+05   0.897    0.371\npoly(X1, 4)1:poly(X2, 4)3:poly(X3, 4)1 -1.007e+04  4.498e+04  -0.224    0.823\npoly(X1, 4)2:poly(X2, 4)3:poly(X3, 4)1  3.755e+04  1.009e+05   0.372    0.710\npoly(X1, 4)3:poly(X2, 4)3:poly(X3, 4)1 -1.758e+04  4.105e+04  -0.428    0.669\npoly(X1, 4)4:poly(X2, 4)3:poly(X3, 4)1  1.840e+04  6.872e+04   0.268    0.789\npoly(X1, 4)1:poly(X2, 4)4:poly(X3, 4)1 -1.843e+04  4.705e+04  -0.392    0.695\npoly(X1, 4)2:poly(X2, 4)4:poly(X3, 4)1  1.158e+05  1.425e+05   0.813    0.417\npoly(X1, 4)3:poly(X2, 4)4:poly(X3, 4)1 -1.754e+04  4.755e+04  -0.369    0.712\npoly(X1, 4)4:poly(X2, 4)4:poly(X3, 4)1  1.012e+05  1.074e+05   0.942    0.347\npoly(X1, 4)1:poly(X2, 4)1:poly(X3, 4)2 -3.281e+04  6.566e+04  -0.500    0.618\npoly(X1, 4)2:poly(X2, 4)1:poly(X3, 4)2 -3.988e+03  1.418e+05  -0.028    0.978\npoly(X1, 4)3:poly(X2, 4)1:poly(X3, 4)2 -4.125e+04  5.842e+04  -0.706    0.481\npoly(X1, 4)4:poly(X2, 4)1:poly(X3, 4)2 -1.954e+04  9.760e+04  -0.200    0.841\npoly(X1, 4)1:poly(X2, 4)2:poly(X3, 4)2  1.159e+04  1.230e+05   0.094    0.925\npoly(X1, 4)2:poly(X2, 4)2:poly(X3, 4)2  2.661e+05  2.818e+05   0.944    0.346\npoly(X1, 4)3:poly(X2, 4)2:poly(X3, 4)2  9.999e+02  1.181e+05   0.008    0.993\npoly(X1, 4)4:poly(X2, 4)2:poly(X3, 4)2  1.913e+05  2.275e+05   0.841    0.401\npoly(X1, 4)1:poly(X2, 4)3:poly(X3, 4)2 -4.594e+04  7.314e+04  -0.628    0.530\npoly(X1, 4)2:poly(X2, 4)3:poly(X3, 4)2  8.388e+03  1.670e+05   0.050    0.960\npoly(X1, 4)3:poly(X2, 4)3:poly(X3, 4)2 -6.232e+04  6.911e+04  -0.902    0.368\npoly(X1, 4)4:poly(X2, 4)3:poly(X3, 4)2 -5.225e+03  1.121e+05  -0.047    0.963\npoly(X1, 4)1:poly(X2, 4)4:poly(X3, 4)2  1.304e+04  8.228e+04   0.158    0.874\npoly(X1, 4)2:poly(X2, 4)4:poly(X3, 4)2  1.900e+05  1.946e+05   0.977    0.329\npoly(X1, 4)3:poly(X2, 4)4:poly(X3, 4)2  2.041e+03  8.242e+04   0.025    0.980\npoly(X1, 4)4:poly(X2, 4)4:poly(X3, 4)2  1.291e+05  1.559e+05   0.828    0.408\npoly(X1, 4)1:poly(X2, 4)1:poly(X3, 4)3 -6.263e+03  3.857e+04  -0.162    0.871\npoly(X1, 4)2:poly(X2, 4)1:poly(X3, 4)3  2.736e+04  8.086e+04   0.338    0.735\npoly(X1, 4)3:poly(X2, 4)1:poly(X3, 4)3 -5.336e+03  3.407e+04  -0.157    0.876\npoly(X1, 4)4:poly(X2, 4)1:poly(X3, 4)3  4.121e+03  5.720e+04   0.072    0.943\npoly(X1, 4)1:poly(X2, 4)2:poly(X3, 4)3 -4.828e+04  6.792e+04  -0.711    0.478\npoly(X1, 4)2:poly(X2, 4)2:poly(X3, 4)3  1.051e+05  1.854e+05   0.567    0.571\npoly(X1, 4)3:poly(X2, 4)2:poly(X3, 4)3 -3.908e+04  6.719e+04  -0.582    0.561\npoly(X1, 4)4:poly(X2, 4)2:poly(X3, 4)3  1.112e+05  1.416e+05   0.786    0.432\npoly(X1, 4)1:poly(X2, 4)3:poly(X3, 4)3 -8.359e+03  4.110e+04  -0.203    0.839\npoly(X1, 4)2:poly(X2, 4)3:poly(X3, 4)3  4.491e+04  9.429e+04   0.476    0.634\npoly(X1, 4)3:poly(X2, 4)3:poly(X3, 4)3 -1.304e+04  3.845e+04  -0.339    0.735\npoly(X1, 4)4:poly(X2, 4)3:poly(X3, 4)3  1.609e+04  6.491e+04   0.248    0.804\npoly(X1, 4)1:poly(X2, 4)4:poly(X3, 4)3 -3.632e+04  4.687e+04  -0.775    0.439\npoly(X1, 4)2:poly(X2, 4)4:poly(X3, 4)3  7.222e+04  1.329e+05   0.543    0.587\npoly(X1, 4)3:poly(X2, 4)4:poly(X3, 4)3 -3.442e+04  4.914e+04  -0.700    0.484\npoly(X1, 4)4:poly(X2, 4)4:poly(X3, 4)3  6.978e+04  9.955e+04   0.701    0.484\npoly(X1, 4)1:poly(X2, 4)1:poly(X3, 4)4 -2.251e+04  3.229e+04  -0.697    0.486\npoly(X1, 4)2:poly(X2, 4)1:poly(X3, 4)4 -3.537e+03  6.841e+04  -0.052    0.959\npoly(X1, 4)3:poly(X2, 4)1:poly(X3, 4)4 -2.899e+04  2.935e+04  -0.988    0.324\npoly(X1, 4)4:poly(X2, 4)1:poly(X3, 4)4 -5.152e+03  4.699e+04  -0.110    0.913\npoly(X1, 4)1:poly(X2, 4)2:poly(X3, 4)4  1.062e+04  6.631e+04   0.160    0.873\npoly(X1, 4)2:poly(X2, 4)2:poly(X3, 4)4  6.806e+04  1.271e+05   0.536    0.593\npoly(X1, 4)3:poly(X2, 4)2:poly(X3, 4)4 -8.527e+03  6.655e+04  -0.128    0.898\npoly(X1, 4)4:poly(X2, 4)2:poly(X3, 4)4  3.659e+04  1.031e+05   0.355    0.723\npoly(X1, 4)1:poly(X2, 4)3:poly(X3, 4)4 -3.521e+04  3.727e+04  -0.945    0.345\npoly(X1, 4)2:poly(X2, 4)3:poly(X3, 4)4  9.278e+03  7.880e+04   0.118    0.906\npoly(X1, 4)3:poly(X2, 4)3:poly(X3, 4)4 -4.595e+04  3.646e+04  -1.260    0.208\npoly(X1, 4)4:poly(X2, 4)3:poly(X3, 4)4  1.224e+04  5.329e+04   0.230    0.818\npoly(X1, 4)1:poly(X2, 4)4:poly(X3, 4)4  6.892e+03  4.890e+04   0.141    0.888\npoly(X1, 4)2:poly(X2, 4)4:poly(X3, 4)4  2.929e+04  8.802e+04   0.333    0.740\npoly(X1, 4)3:poly(X2, 4)4:poly(X3, 4)4 -6.078e+03  4.986e+04  -0.122    0.903\npoly(X1, 4)4:poly(X2, 4)4:poly(X3, 4)4  6.372e+03  7.030e+04   0.091    0.928\n\nResidual standard error: 0.9853 on 375 degrees of freedom\nMultiple R-squared:  0.2066,    Adjusted R-squared:  -0.05575 \nF-statistic: 0.7875 on 124 and 375 DF,  p-value: 0.9419\n\n\n\n\n\n\n\n\nWARNING\n\n\n\nDon’t print out the coefficient tables of your results for any of your models. We only want to predict, we don’t want to interpret.\n\n\nNow, calculate the rmse for train and test:\n\nmodel_rmse_train = lapply(model_ests, function(x) get_rmse(model = x, data = dat.train, response = 'Y'))\nprint(model_rmse_train)\n\n[[1]]\n[1] 0.957994\n\n[[2]]\n[1] 0.9574966\n\n[[3]]\n[1] 0.9551141\n\n[[4]]\n[1] 0.952224\n\n[[5]]\n[1] 0.9429713\n\n[[6]]\n[1] 0.9291898\n\n[[7]]\n[1] 0.9106093\n\n[[8]]\n[1] 0.9106093\n\n[[9]]\n[1] 0.8533138\n\n\n\nmodel_rmse_test = lapply(model_ests, function(x) get_rmse(model = x, data = dat.test, response = 'Y'))\nprint(model_rmse_test)\n\n[[1]]\n[1] 1.008688\n\n[[2]]\n[1] 1.012111\n\n[[3]]\n[1] 1.014515\n\n[[4]]\n[1] 1.018176\n\n[[5]]\n[1] 1.045088\n\n[[6]]\n[1] 1.134584\n\n[[7]]\n[1] 1.509911\n\n[[8]]\n[1] 1.509911\n\n[[9]]\n[1] 13.29548\n\n\nThen, we can make the data.frame of results. We’d usually have complexity in there too, but I’ll put the formulas in. In later work, we won’t really have “formulas” to put in here.\n\ndata.frame(formula = as.character(unlist(model_list_form)), # usually you'd have complexity here\n           train_rmse = unlist(model_rmse_train),\n           test_rmse = unlist(model_rmse_test))\n\n                                         formula train_rmse test_rmse\n1                                          Y ~ 1  0.9579940  1.008688\n2                                         Y ~ X1  0.9574966  1.012111\n3                                    Y ~ X1 * X2  0.9551141  1.014515\n4                               Y ~ X1 * X2 * X3  0.9522240  1.018176\n5                     Y ~ X1 * X2 * X3 * I(X1^2)  0.9429713  1.045088\n6           Y ~ X1 * X2 * X3 * I(X1^2) * I(X2^2)  0.9291898  1.134584\n7 Y ~ X1 * X2 * X3 * I(X1^2) * I(X2^2) * I(X3^2)  0.9106093  1.509911\n8    Y ~ poly(X1, 3) * poly(X2, 3) * poly(X3, 3)  0.9106093  1.509911\n9    Y ~ poly(X1, 4) * poly(X2, 4) * poly(X3, 4)  0.8533138 13.295477\n\n\n\n\n\n\n\n\nWhat’s our takeaway from this?\n\n\n\n\nWhich model does best?\nDoes this seem like an odd outcome? What did the data look like in the first place?",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Model Selection in Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_07/07b.html",
    "href": "content/Week_07/07b.html",
    "title": "Linear Regression: Interpreting Coefficients",
    "section": "",
    "text": "Tip\n\n\n\nToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n\n Ames.csv",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression: Interpreting Coefficients"
    ]
  },
  {
    "objectID": "content/Week_07/07b.html#preliminaries",
    "href": "content/Week_07/07b.html#preliminaries",
    "title": "Linear Regression: Interpreting Coefficients",
    "section": "Preliminaries",
    "text": "Preliminaries\nSo far in each of our analyses, we have only used numeric variables as predictors. We have also only used additive models, meaning the effect any predictor had on the response was not dependent on the other predictors. In this chapter, we will remove both of these restrictions. We will fit models with multiple predictors, categorical predictors, and use models that allow predictors to interact. Instead of a single variable, will use multiple linear regression. The mathematics of multiple regression will remain largely unchanging, however, we will pay close attention to interpretation, as well as some difference in R usage.",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression: Interpreting Coefficients"
    ]
  },
  {
    "objectID": "content/Week_07/07b.html#dummy-variables",
    "href": "content/Week_07/07b.html#dummy-variables",
    "title": "Linear Regression: Interpreting Coefficients",
    "section": "Dummy Variables",
    "text": "Dummy Variables\nFor this example and discussion, we will briefly use the built in dataset mtcars before introducing the autompg dataset. The reason to use these easy, straightforward datasets is that they make visualization of the entire dataset trivially easy. Accordingly, the mtcars dataset is small, so we’ll quickly take a look at the entire dataset.\n\nlibrary(tidyverse)\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nWe will be interested in three of the variables: mpg, hp, and am.\n\nmpg: fuel efficiency, in miles per gallon.\nhp: horsepower, in foot-pounds per second.\nam: transmission. Automatic or manual.\n\nAs we often do, we will start by plotting the data. We are interested in mpg as the response variable, and hp as a predictor.\n\nplot(mpg ~ hp, data = mtcars, cex = 2)\n\n\n\n\n\n\n\n\nSince we are also interested in the transmission type, we could also label the points accordingly.\n\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)\nlegend(\"topright\", c(\"Automatic\", \"Manual\"), col = c(1, 2), pch = c(1, 2))\n\n\n\n\n\n\n\n\nWe now fit the SLR model\n\\[Y = \\beta_0 + \\beta_1 x_1 + \\epsilon,\\]\nwhere \\(Y\\) is mpg and \\(x_1\\) is hp. For notational brevity, we drop the index \\(i\\) for observations.\n\nmpg_hp_slr = lm(mpg ~ hp, data = mtcars)\n\nWe then re-plot the data and add the fitted line to the plot.\n\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)\nabline(mpg_hp_slr, lwd = 3, col = \"grey\")\nlegend(\"topright\", c(\"Automatic\", \"Manual\"), col = c(1, 2), pch = c(1, 2))\n\n\n\n\n\n\n\n\nWe should notice a pattern here. The red, manual observations largely fall above the line, while the black, automatic observations are mostly below the line. This means our model underestimates the fuel efficiency of manual transmissions, and overestimates the fuel efficiency of automatic transmissions. To correct for this, we will add a predictor to our model, namely, am as \\(x_2\\).\nOur new model is\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\n\\]\nwhere \\(x_1\\) and \\(Y\\) remain the same, but now\n\\[\nx_2 =\n  \\begin{cases}\n   1 & \\text{manual transmission} \\\\\n   0       & \\text{automatic transmission}\n  \\end{cases}.\n\\]\nIn this case, we call \\(x_2\\) a dummy variable. A dummy variable (also called an “indicator” variable) is somewhat unfortunately named, as it is in no way “dumb”. In fact, it is actually somewhat clever. A dummy variable is a numerical variable that is used in a regression analysis to “code” for a binary categorical variable. Let’s see how this works.\nFirst, note that am is already a dummy variable, since it uses the values 0 and 1 to represent automatic and manual transmissions. Often, a variable like am would store the character values auto and man and we would either have to convert these to 0 and 1, or, as we will see later, R will take care of creating dummy variables for us.\nSo, to fit the above model, we do so like any other multiple regression model we have seen before.\n\nmpg_hp_add = lm(mpg ~ hp + am, data = mtcars)\n\nBriefly checking the output, we see that R has estimated the three \\(\\beta\\) parameters.\n\nmpg_hp_add\n\n\nCall:\nlm(formula = mpg ~ hp + am, data = mtcars)\n\nCoefficients:\n(Intercept)           hp           am  \n   26.58491     -0.05889      5.27709  \n\n\nSince \\(x_2\\) can only take values 0 and 1, we can effectively write two different models, one for manual and one for automatic transmissions.\nFor automatic transmissions, that is \\(x_2 = 0\\), we have,\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\epsilon.\n\\]\nThen for manual transmissions, that is \\(x_2 = 1\\), we have,\n\\[\nY = (\\beta_0 + \\beta_2) + \\beta_1 x_1 + \\epsilon.\n\\]\nNotice that these models share the same slope, \\(\\beta_1\\), but have different intercepts, differing by \\(\\beta_2\\). So the change in mpg is the same for both models, but on average mpg differs by \\(\\beta_2\\) between the two transmission types.\nWe’ll now calculate the estimated slope and intercept of these two models so that we can add them to a plot. Note that:\n\n\\(\\hat{\\beta}_0\\) = coef(mpg_hp_add)[1] = 26.5849137\n\\(\\hat{\\beta}_1\\) = coef(mpg_hp_add)[2] = -0.0588878\n\\(\\hat{\\beta}_2\\) = coef(mpg_hp_add)[3] = 5.2770853\n\nWe can then combine these to calculate the estimated slope and intercepts.\n\nint_auto = coef(mpg_hp_add)[1]\nint_manu = coef(mpg_hp_add)[1] + coef(mpg_hp_add)[3]\n\nslope_auto = coef(mpg_hp_add)[2]\nslope_manu = coef(mpg_hp_add)[2]\n\nRe-plotting the data, we use these slopes and intercepts to add the “two” fitted models to the plot.\n\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)\nabline(int_auto, slope_auto, col = 1, lty = 1, lwd = 2) # add line for auto\nabline(int_manu, slope_manu, col = 2, lty = 2, lwd = 2) # add line for manual\nlegend(\"topright\", c(\"Automatic\", \"Manual\"), col = c(1, 2), pch = c(1, 2))\n\n\n\n\n\n\n\n\nWe notice right away that the points are no longer systematically incorrect. The red, manual observations vary about the red line in no particular pattern without underestimating the observations as before. The black, automatic points vary about the black line, also without an obvious pattern.\nThey say a picture is worth a thousand words, but as a statistician, sometimes a picture is worth an entire analysis. The above picture makes it plainly obvious that \\(\\beta_2\\) is significant, but let’s verify mathematically. Essentially we would like to test:\n\\[\nH_0: \\beta_2 = 0 \\quad \\text{vs} \\quad H_1: \\beta_2 \\neq 0.\n\\]\nThis is nothing new. Again, the math is the same as the multiple regression analyses we have seen before. We could perform either a \\(t\\) or \\(F\\) test here. The only difference is a slight change in interpretation. We could think of this as testing a model with a single line (\\(H_0\\)) against a model that allows two lines (\\(H_1\\)).\nTo obtain the test statistic and p-value for the \\(t\\)-test, we would use\n\nsummary(mpg_hp_add)$coefficients[\"am\",]\n\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n5.277085e+00 1.079541e+00 4.888270e+00 3.460318e-05 \n\n\nTo do the same for the \\(F\\) test, we would use a call to anova. One way of comparing the fit of models (which accounts for the fact that one model is more “flexible” than another) is to use anova. The null hypothesis of the anova is that the two models explain the same amount of variation in the dependent variable (mpg). If we reject the F-test in the anova (if the p-value is small), then the more flexible model is actually explaining more. One caveat is that the models must be nested – one has to vary from the other only by omitting variables, not by omitting some and adding others. Here, mpg_hp_slr is nested in mpg_hp_add because it uses a subset of the variables.\n\nanova(mpg_hp_slr, mpg_hp_add)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ hp\nModel 2: mpg ~ hp + am\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)    \n1     30 447.67                                 \n2     29 245.44  1    202.24 23.895 3.46e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice that these are indeed testing the same thing, as the p-values are exactly equal. (And the \\(F\\) test statistic is the \\(t\\) test statistic squared.) This is only the case for different models that differ only by one additional variable.\nRecapping some interpretations:\n\n\\(\\hat{\\beta}_0 = 26.5849137\\) is the estimated average mpg for a car with an automatic transmission and 0 hp.\n\\(\\hat{\\beta}_0 + \\hat{\\beta}_2 = 31.8619991\\) is the estimated average mpg for a car with a manual transmission and 0 hp.\n\\(\\hat{\\beta}_2 = 5.2770853\\) is the estimated difference in average mpg for cars with manual transmissions as compared to those with automatic transmission, for any hp.\n\\(\\hat{\\beta}_1 = -0.0588878\\) is the estimated change in average mpg for an increase in one hp, for either transmission types.\n\nWe should take special notice of those last two. In the model,\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\n\\]\nwe see \\(\\beta_1\\) is the average change in \\(Y\\) for an increase in \\(x_1\\), no matter the value of \\(x_2\\). Also, \\(\\beta_2\\) is always the difference in the average of \\(Y\\) for any value of \\(x_1\\). These are two restrictions we won’t always want, so we need a way to specify a more flexible model.\nHere we restricted ourselves to a single numerical predictor \\(x_1\\) and one dummy variable \\(x_2\\). However, the concept of a dummy variable can be used with larger multiple regression models. We only use a single numerical predictor here for ease of visualization since we can think of the “two lines” interpretation. But in general, we can think of a dummy variable as creating “two models,” one for each category of a binary categorical variable.",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression: Interpreting Coefficients"
    ]
  },
  {
    "objectID": "content/Week_07/07b.html#interactions",
    "href": "content/Week_07/07b.html#interactions",
    "title": "Linear Regression: Interpreting Coefficients",
    "section": "Interactions",
    "text": "Interactions\nTo remove the “same slope” restriction, we will now discuss interactions. To illustrate this concept, we will use the autompg dataset with a few modifications.\n\n# read data frame from the web\nautompg = read.table(\n  \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\",\n  quote = \"\\\"\",\n  comment.char = \"\",\n  stringsAsFactors = FALSE)\n# give the dataframe headers\ncolnames(autompg) = c(\"mpg\", \"cyl\", \"disp\", \"hp\", \"wt\", \"acc\", \"year\", \"origin\", \"name\")\n\n\nautompg = autompg %&gt;%\n  filter(hp !='?' & name !='plymouth reliant') %&gt;% # the reliant causes some issues\n  mutate(hp = as.numeric(hp),\n         domestic = as.numeric(origin==1)) %&gt;%\n  filter(!cyl %in% c(5,3)) %&gt;%\n  mutate(cyl = as.factor(cyl))\n\n\nstr(autompg)\n\n'data.frame':   383 obs. of  10 variables:\n $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...\n $ cyl     : Factor w/ 3 levels \"4\",\"6\",\"8\": 3 3 3 3 3 3 3 3 3 3 ...\n $ disp    : num  307 350 318 304 302 429 454 440 455 390 ...\n $ hp      : num  130 165 150 150 140 198 220 215 225 190 ...\n $ wt      : num  3504 3693 3436 3433 3449 ...\n $ acc     : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n $ year    : int  70 70 70 70 70 70 70 70 70 70 ...\n $ origin  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ name    : chr  \"chevrolet chevelle malibu\" \"buick skylark 320\" \"plymouth satellite\" \"amc rebel sst\" ...\n $ domestic: num  1 1 1 1 1 1 1 1 1 1 ...\n\n\nWe’ve removed cars with 3 and 5 cylinders , as well as created a new variable domestic which indicates whether or not a car was built in the United States. Removing the 3 and 5 cylinders is simply for ease of demonstration later in the chapter and would not be done in practice. The new variable domestic takes the value 1 if the car was built in the United States, and 0 otherwise, which we will refer to as “foreign.” (We are arbitrarily using the United States as the reference point here.) We have also made cyl and origin into factor variables, which we will discuss later.\nWe’ll now be concerned with three variables: mpg, disp, and domestic. We will use mpg as the response. We can fit a model,\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\n\\]\nwhere\n\n\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\n\\(x_1\\) is disp, the displacement in cubic inches,\n\\(x_2\\) is domestic as described above, which is a dummy variable.\n\n\\[\nx_2 =\n  \\begin{cases}\n   1 & \\text{Domestic} \\\\\n   0 & \\text{Foreign}\n  \\end{cases}\n\\]\nWe will fit this model, extract the slope and intercept for the “two lines,” plot the data and add the lines.\n\nmpg_disp_add = lm(mpg ~ disp + as.factor(domestic), data = autompg)\n\nint_for = coef(mpg_disp_add)[1]\nint_dom = coef(mpg_disp_add)[1] + coef(mpg_disp_add)[3]\n\nslope_for = coef(mpg_disp_add)[2]\nslope_dom = coef(mpg_disp_add)[2] #--&gt; same slope!\n\nggplot(autompg, aes(x = disp, y = mpg, col = as.factor(domestic))) + \n  geom_point() +\n  geom_abline(intercept = int_dom, \n              slope = slope_dom, col = 'blue') +\n  geom_abline(intercept = int_for,\n              slope = slope_for, col = 'red') +\n  labs(color = 'Origin') +\n  scale_color_manual(labels = c('Foreign','Domestic'), values = c('red','blue')) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis is a model that allows for two parallel lines, meaning the mpg can be different on average between foreign and domestic cars of the same engine displacement, but the change in average mpg for an increase in displacement is the same for both. We can see this model isn’t doing very well here. The red line fits the red points fairly well, but the black line isn’t doing very well for the black points, it should clearly have a more negative slope. Essentially, we would like a model that allows for two different slopes.\nConsider the following model,\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\n\\]\nwhere \\(x_1\\), \\(x_2\\), and \\(Y\\) are the same as before, but we have added a new interaction term \\(x_1 x_2\\) which multiplies \\(x_1\\) and \\(x_2\\), so we also have an additional \\(\\beta\\) parameter \\(\\beta_3\\).\nThis model essentially creates two slopes and two intercepts, \\(\\beta_2\\) being the difference in intercepts and \\(\\beta_3\\) being the difference in slopes. To see this, we will break down the model into the two “sub-models” for foreign and domestic cars.\nFor foreign cars, that is \\(x_2 = 0\\), we have\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\epsilon.\n\\]\nFor domestic cars, that is \\(x_2 = 1\\), we have\n\\[\nY = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) x_1 + \\epsilon.\n\\]\nThese two models have both different slopes and intercepts.\n\n\\(\\beta_0\\) is the average mpg for a foreign car with 0 disp.\n\\(\\beta_1\\) is the change in average mpg for an increase of one disp, for foreign cars.\n\\(\\beta_0 + \\beta_2\\) is the average mpg for a domestic car with 0 disp.\n\\(\\beta_1 + \\beta_3\\) is the change in average mpg for an increase of one disp, for domestic cars.\n\nHow do we fit this model in R? There are a number of ways.\nOne method would be to simply create a new variable, then fit a model like any other.\n\nautompg$x3 = autompg$disp * autompg$domestic # THIS CODE NOT RUN!\ndo_not_do_this = lm(mpg ~ disp + domestic + x3, data = autompg) # THIS CODE NOT RUN!\n\nYou should only do this as a last resort. We greatly prefer not to have to modify our data simply to fit a model. Instead, we can tell R we would like to use the existing data with an interaction term, which it will create automatically when we use the : operator.\n\nmpg_disp_int = lm(mpg ~ disp + domestic + disp:domestic, data = autompg)\n\nAn alternative method, which will fit the exact same model as above would be to use the * operator. This method automatically creates the interaction term, as well as any “lower order terms,” which in this case are the first order terms for disp and domestic\n\nmpg_disp_int2 = lm(mpg ~ disp * domestic, data = autompg)\n\nWe can quickly verify that these are doing the same thing.\n\ncoef(mpg_disp_int)\n\n  (Intercept)          disp      domestic disp:domestic \n   46.0548423    -0.1569239   -12.5754714     0.1025184 \n\ncoef(mpg_disp_int2)\n\n  (Intercept)          disp      domestic disp:domestic \n   46.0548423    -0.1569239   -12.5754714     0.1025184 \n\n\nWe see that both the variables, and their coefficient estimates are indeed the same for both models.\n\nsummary(mpg_disp_int)\n\n\nCall:\nlm(formula = mpg ~ disp + domestic + disp:domestic, data = autompg)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8332  -2.8956  -0.8332   2.2828  18.7749 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    46.05484    1.80582  25.504  &lt; 2e-16 ***\ndisp           -0.15692    0.01668  -9.407  &lt; 2e-16 ***\ndomestic      -12.57547    1.95644  -6.428 3.90e-10 ***\ndisp:domestic   0.10252    0.01692   6.060 3.29e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.308 on 379 degrees of freedom\nMultiple R-squared:  0.7011,    Adjusted R-squared:  0.6987 \nF-statistic: 296.3 on 3 and 379 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that using summary() gives the usual output for a multiple regression model. We pay close attention to the row for disp:domestic which tests,\n\\[\nH_0: \\beta_3 = 0.\n\\]\nIn this case, testing for \\(\\beta_3 = 0\\) is testing for two lines with parallel slopes versus two lines with possibly different slopes. The disp:domestic line in the summary() output uses a \\(t\\)-test to perform the test.\n\nint_for = coef(mpg_disp_int)[1]\nint_dom = coef(mpg_disp_int)[1] + coef(mpg_disp_int)[3]\n\nslope_for = coef(mpg_disp_int)[2]\nslope_dom = coef(mpg_disp_int)[2] + coef(mpg_disp_int)[4]\n\nHere we again calculate the slope and intercepts for the two lines for use in plotting.\n\nggplot(autompg, aes(x = disp, y = mpg, col = as.factor(domestic))) + \n  geom_point() +\n  geom_abline(intercept = int_dom, \n              slope = slope_dom, col = 'blue') +\n  geom_abline(intercept = int_for,\n              slope = slope_for, col = 'red') +\n  labs(color = 'Origin') +\n  scale_color_manual(labels = c('Foreign','Domestic'), values = c('red','blue')) +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe see that these lines fit the data much better, which matches the result of our tests.\nSo far we have only seen interaction between a categorical variable (domestic) and a numerical variable (disp). While this is easy to visualize, since it allows for different slopes for two lines, it is not the only type of interaction we can use in a model. We can also consider interactions between two numerical variables.\nConsider the model,\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\n\\]\nwhere\n\n\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\n\\(x_1\\) is disp, the displacement in cubic inches,\n\\(x_2\\) is hp, the horsepower, in foot-pounds per second.\n\nHow does mpg change based on disp in this model? We can rearrange some terms to see how.\n\\[\nY = \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon\n\\]\nSo, for a one unit increase in \\(x_1\\) (disp), the mean of \\(Y\\) (mpg) increases \\(\\beta_1 + \\beta_3 x_2\\), which is a different value depending on the value of \\(x_2\\) (hp)!\nSince we’re now working in three dimensions, this model can’t be easily justified via visualizations like the previous example. Instead, we will have to rely on a test.\n\nmpg_disp_add_hp = lm(mpg ~ disp + hp, data = autompg)\nmpg_disp_int_hp = lm(mpg ~ disp * hp, data = autompg)\nsummary(mpg_disp_int_hp)\n\n\nCall:\nlm(formula = mpg ~ disp * hp, data = autompg)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7849  -2.3104  -0.5699   2.1453  17.9211 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.241e+01  1.523e+00   34.42   &lt;2e-16 ***\ndisp        -1.002e-01  6.638e-03  -15.09   &lt;2e-16 ***\nhp          -2.198e-01  1.987e-02  -11.06   &lt;2e-16 ***\ndisp:hp      5.658e-04  5.165e-05   10.96   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.896 on 379 degrees of freedom\nMultiple R-squared:  0.7554,    Adjusted R-squared:  0.7535 \nF-statistic: 390.2 on 3 and 379 DF,  p-value: &lt; 2.2e-16\n\n\nUsing summary() we focus on the row for disp:hp which tests,\n\\[\nH_0: \\beta_3 = 0.\n\\]\nAgain, we see a very low p-value so we reject the null (additive model) in favor of the interaction model. We can take a closer look at the coefficients of our fitted interaction model.\n\ncoef(mpg_disp_int_hp)\n\n  (Intercept)          disp            hp       disp:hp \n52.4081997848 -0.1001737655 -0.2198199720  0.0005658269 \n\n\n\n\\(\\hat{\\beta}_0 = 52.4081998\\) is the estimated average mpg for a car with 0 disp and 0 hp.\n\\(\\hat{\\beta}_1 = -0.1001738\\) is the estimated change in average mpg for an increase in 1 disp, for a car with 0 hp.\n\\(\\hat{\\beta}_2 = -0.21982\\) is the estimated change in average mpg for an increase in 1 hp, for a car with 0 disp.\n\\(\\hat{\\beta}_3 = 5.658269\\times 10^{-4}\\) is an estimate of the modification to the change in average mpg for an increase in disp, for a car of a certain hp (or vice versa).\n\nThat last coefficient needs further explanation. Recall the rearrangement we made earlier\n\\[\nY = \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon.\n\\]\nSo, our estimate for \\(\\beta_1 + \\beta_3 x_2\\), is \\(\\hat{\\beta}_1 + \\hat{\\beta}_3 x_2\\), which in this case is\n\\[\n-0.1001738 + 5.658269\\times 10^{-4} x_2.\n\\]\nThis says that, for an increase of one disp we see an estimated change in average mpg of \\(-0.1001738 + 5.658269\\times 10^{-4} x_2\\). So how disp and mpg are related, depends on the hp of the car.\nSo for a car with 50 hp, the estimated change in average mpg for an increase of one disp is\n\\[\n-0.1001738 + 5.658269\\times 10^{-4} \\cdot 50 = -0.0718824\n\\]\nAnd for a car with 350 hp, the estimated change in average mpg for an increase of one disp is\n\\[\n-0.1001738 + 5.658269\\times 10^{-4} \\cdot 350 = 0.0978657\n\\]\nNotice the sign changed!",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression: Interpreting Coefficients"
    ]
  },
  {
    "objectID": "content/Week_07/07b.html#factor-variables",
    "href": "content/Week_07/07b.html#factor-variables",
    "title": "Linear Regression: Interpreting Coefficients",
    "section": "Factor Variables",
    "text": "Factor Variables\nSo far in this chapter, we have limited our use of categorical variables to binary categorical variables. Specifically, we have limited ourselves to dummy variables which take a value of 0 or 1 and represent a categorical variable numerically.\nWe will now discuss factor variables, which is a special way that R deals with categorical variables. With factor variables, a human user can simply think about the categories of a variable, and R will take care of the necessary dummy variables without any 0/1 assignment being done by the user.\n\nis.factor(autompg$domestic)\n\n[1] FALSE\n\n\nEarlier when we used the domestic variable, it was not a factor variable. It was simply a numerical variable that only took two possible values, 1 for domestic, and 0 for foreign. Let’s create a new variable origin that stores the same information, but in a different way. First, we create an empty (all-NA) variable of type character. Then, we update it. Yes, we could also do this with ifelse or case_when:\n\nautompg$origin = as.character(NA)\nautompg$origin[autompg$domestic == 1] = \"domestic\"\nautompg$origin[autompg$domestic == 0] = \"foreign\"\nhead(autompg$origin)\n\n[1] \"domestic\" \"domestic\" \"domestic\" \"domestic\" \"domestic\" \"domestic\"\n\n\nNow the origin variable stores \"domestic\" for domestic cars and \"foreign\" for foreign cars.\n\nis.factor(autompg$origin)\n\n[1] FALSE\n\n\nHowever, this is simply a vector of character values. A vector of car models is a character variable in R. A vector of Vehicle Identification Numbers (VINs) is a character variable as well. But those don’t represent a short list of levels that might influence a response variable. We will want to coerce this origin variable to be something more: a factor variable.\n\nautompg$origin = as.factor(autompg$origin)\n\nNow when we check the structure of the autompg dataset, we see that origin is a factor variable.\n\nstr(autompg)\n\n'data.frame':   383 obs. of  10 variables:\n $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...\n $ cyl     : Factor w/ 3 levels \"4\",\"6\",\"8\": 3 3 3 3 3 3 3 3 3 3 ...\n $ disp    : num  307 350 318 304 302 429 454 440 455 390 ...\n $ hp      : num  130 165 150 150 140 198 220 215 225 190 ...\n $ wt      : num  3504 3693 3436 3433 3449 ...\n $ acc     : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n $ year    : int  70 70 70 70 70 70 70 70 70 70 ...\n $ origin  : Factor w/ 2 levels \"domestic\",\"foreign\": 1 1 1 1 1 1 1 1 1 1 ...\n $ name    : chr  \"chevrolet chevelle malibu\" \"buick skylark 320\" \"plymouth satellite\" \"amc rebel sst\" ...\n $ domestic: num  1 1 1 1 1 1 1 1 1 1 ...\n\n\nFactor variables have levels which are the possible values (categories) that the variable may take, in this case foreign or domestic.\n\nlevels(autompg$origin)\n\n[1] \"domestic\" \"foreign\" \n\n\nRecall that previously we have fit the model\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\n\\]\nwhere\n\n\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\n\\(x_1\\) is disp, the displacement in cubic inches,\n\\(x_2\\) is domestic a dummy variable where 1 indicates a domestic car.\n\n\n(mod_dummy = lm(mpg ~ disp * domestic, data = autompg))\n\n\nCall:\nlm(formula = mpg ~ disp * domestic, data = autompg)\n\nCoefficients:\n  (Intercept)           disp       domestic  disp:domestic  \n      46.0548        -0.1569       -12.5755         0.1025  \n\n\nSo here we see that\n\\[\n\\hat{\\beta}_0 + \\hat{\\beta}_2 = 46.0548423 + -12.5754714 = 33.4793709\n\\]\nis the estimated average mpg for a domestic car with 0 disp.\nNow let’s try to do the same, but using our new factor variable.\n\n(mod_factor = lm(mpg ~ disp * origin, data = autompg))\n\n\nCall:\nlm(formula = mpg ~ disp * origin, data = autompg)\n\nCoefficients:\n       (Intercept)                disp       originforeign  disp:originforeign  \n          33.47937            -0.05441            12.57547            -0.10252  \n\n\nIt seems that it doesn’t produce the same results. Right away we notice that the intercept is different, as is the the coefficient in front of disp. We also notice that the remaining two coefficients are of the same magnitude as their respective counterparts using the domestic variable, but with a different sign. Why is this happening?\nIt turns out, that by using a factor variable, R is automatically creating a dummy variable for us. However, it is not the dummy variable that we had originally used ourselves.\nR is fitting the model\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\n\\]\nwhere\n\n\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\n\\(x_1\\) is disp, the displacement in cubic inches,\n\\(x_2\\) is a dummy variable created by R. It uses 1 to represent a foreign car.\n\nYou may recall that we saw this in action in Assignment 05 when we used model.matrix on GarageType.\nSo now,\n\\[\n\\hat{\\beta}_0 = 33.4793709\n\\]\nis the estimated average mpg for a domestic car with 0 disp, which is indeed the same as before.\nWhen R created \\(x_2\\), the dummy variable, it used domestic cars as the reference level, that is the default value of the factor variable. So when the dummy variable is 0, the model represents this reference level, which is domestic. (R makes this choice because domestic comes before foreign alphabetically.)\nSo the two models have different estimated coefficients, but due to the different model representations, they are actually the same model.\n\nFactors with More Than Two Levels\nLet’s now consider a factor variable with more than two levels. In this dataset, cyl is an example.\n\nis.factor(autompg$cyl)\n\n[1] TRUE\n\nlevels(autompg$cyl)\n\n[1] \"4\" \"6\" \"8\"\n\n\nHere the cyl variable has three possible levels: 4, 6, and 8. You may wonder, why not simply use cyl as a numerical variable? You certainly could.\nHowever, that would force the difference in average mpg between 4 and 6 cylinders to be the same as the difference in average mpg between 6 and 8 cylinders. That usually make senses for a continuous variable, but not for a discrete variable with so few possible values. In the case of this variable, there is no such thing as a 7-cylinder engine or a 6.23-cylinder engine in personal vehicles. For these reasons, we will simply consider cyl to be categorical. This is a decision that will commonly need to be made with ordinal variables. Often, with a large number of categories, the decision to treat them as numerical variables is appropriate because, otherwise, a large number of dummy variables are then needed to represent these variables.\nLet’s define three dummy variables related to the cyl factor variable.\n\\[\nv_1 =\n  \\begin{cases}\n   1 & \\text{4 cylinder} \\\\\n   0       & \\text{not 4 cylinder}\n  \\end{cases}\n\\]\n\\[\nv_2 =\n  \\begin{cases}\n   1 & \\text{6 cylinder} \\\\\n   0       & \\text{not 6 cylinder}\n  \\end{cases}\n\\]\n\\[\nv_3 =\n  \\begin{cases}\n   1 & \\text{8 cylinder} \\\\\n   0       & \\text{not 8 cylinder}\n  \\end{cases}\n\\]\nNow, let’s fit an additive model in R, using mpg as the response, and disp and cyl as predictors. This should be a model that uses “three regression lines” to model mpg, one for each of the possible cyl levels. They will all have the same slope (since it is an additive model), but each will have its own intercept.\n\n(mpg_disp_add_cyl = lm(mpg ~ disp + cyl, data = autompg))\n\n\nCall:\nlm(formula = mpg ~ disp + cyl, data = autompg)\n\nCoefficients:\n(Intercept)         disp         cyl6         cyl8  \n   34.99929     -0.05217     -3.63325     -2.03603  \n\n\nThe question is, what is the model that R has fit here? It has chosen to use the model\n\\[\nY = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon,\n\\]\nwhere\n\n\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\n\\(x\\) is disp, the displacement in cubic inches,\n\\(v_2\\) and \\(v_3\\) are the dummy variables define above.\n\nWhy doesn’t R use \\(v_1\\)? Essentially because it doesn’t need to. To create three lines, it only needs two dummy variables since it is using a reference level, which in this case is a 4 cylinder car. The three “sub models” are then:\n\n4 Cylinder: \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\)\n6 Cylinder: \\(Y = (\\beta_0 + \\beta_2) + \\beta_1 x + \\epsilon\\)\n8 Cylinder: \\(Y = (\\beta_0 + \\beta_3) + \\beta_1 x + \\epsilon\\)\n\nNotice that they all have the same slope. However, using the two dummy variables, we achieve the three intercepts.\n\n\\(\\beta_0\\) is the average mpg for a 4 cylinder car with 0 disp.\n\\(\\beta_0 + \\beta_2\\) is the average mpg for a 6 cylinder car with 0 disp.\n\\(\\beta_0 + \\beta_3\\) is the average mpg for a 8 cylinder car with 0 disp.\n\nSo because 4 cylinder is the reference level, \\(\\beta_0\\) is specific to 4 cylinders, but \\(\\beta_2\\) and \\(\\beta_3\\) are used to represent quantities relative to 4 cylinders.\nAs we have done before, we can extract these intercepts and slopes for the three lines, and plot them accordingly.\n\nint_4cyl = coef(mpg_disp_add_cyl)[1]\nint_6cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[3]\nint_8cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[4]\n\nslope_all_cyl = coef(mpg_disp_add_cyl)[2]\n\nplot_colors = c(\"Darkorange\", \"Darkgrey\", \"Dodgerblue\")\nplot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))\nabline(int_4cyl, slope_all_cyl, col = plot_colors[1], lty = 1, lwd = 2)\nabline(int_6cyl, slope_all_cyl, col = plot_colors[2], lty = 2, lwd = 2)\nabline(int_8cyl, slope_all_cyl, col = plot_colors[3], lty = 3, lwd = 2)\nlegend(\"topright\", c(\"4 Cylinder\", \"6 Cylinder\", \"8 Cylinder\"),\n       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))\n\n\n\n\n\n\n\n\nOn this plot, we have\n\n4 Cylinder: orange dots, solid orange line.\n6 Cylinder: grey dots, dashed grey line.\n8 Cylinder: blue dots, dotted blue line.\n\nThe odd result here is that we’re estimating that 8 cylinder cars have better fuel efficiency than 6 cylinder cars at any displacement! The dotted blue line is always above the dashed grey line. That doesn’t seem right. Maybe for very large displacement engines that could be true, but that seems wrong for medium to low displacement.\nTo attempt to fix this, we will try using an interaction model, that is, instead of simply three intercepts and one slope, we will allow for three slopes. Again, we’ll let R take the wheel, (no pun intended) then figure out what model it has applied.\n\n(mpg_disp_int_cyl = lm(mpg ~ disp * cyl, data = autompg))\n\n\nCall:\nlm(formula = mpg ~ disp * cyl, data = autompg)\n\nCoefficients:\n(Intercept)         disp         cyl6         cyl8    disp:cyl6    disp:cyl8  \n   43.59052     -0.13069    -13.20026    -20.85706      0.08299      0.10817  \n\n# could also use mpg ~ disp + cyl + disp:cyl\n\nR has again chosen to use 4 cylinder cars as the reference level, but this also now has an effect on the interaction terms. R has fit the model.\n\\[\nY = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\gamma_2 x v_2 + \\gamma_3 x v_3 + \\epsilon\n\\]\nWe’re using \\(\\gamma\\) like a \\(\\beta\\) parameter for simplicity, so that, for example \\(\\beta_2\\) and \\(\\gamma_2\\) are both associated with \\(v_2\\).\nNow, the three “sub models” are:\n\n4 Cylinder: \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\).\n6 Cylinder: \\(Y = (\\beta_0 + \\beta_2) + (\\beta_1 + \\gamma_2) x + \\epsilon\\).\n8 Cylinder: \\(Y = (\\beta_0 + \\beta_3) + (\\beta_1 + \\gamma_3) x + \\epsilon\\).\n\nInterpreting some parameters and coefficients then:\n\n\\((\\beta_0 + \\beta_2)\\) is the average mpg of a 6 cylinder car with 0 disp\n\\((\\hat{\\beta}_1 + \\hat{\\gamma}_3) = -0.1306935 + 0.1081714 = -0.0225221\\) is the estimated change in average mpg for an increase of one disp, for an 8 cylinder car.\n\nSo, as we have seen before \\(\\beta_2\\) and \\(\\beta_3\\) change the intercepts for 6 and 8 cylinder cars relative to the reference level of \\(\\beta_0\\) for 4 cylinder cars.\nNow, similarly \\(\\gamma_2\\) and \\(\\gamma_3\\) change the slopes for 6 and 8 cylinder cars relative to the reference level of \\(\\beta_1\\) for 4 cylinder cars.\nOnce again, we extract the coefficients and plot the results.\n\nint_4cyl = coef(mpg_disp_int_cyl)[1]\nint_6cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[3]\nint_8cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[4]\n\nslope_4cyl = coef(mpg_disp_int_cyl)[2]\nslope_6cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[5]\nslope_8cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[6]\n\nplot_colors = c(\"Darkorange\", \"Darkgrey\", \"Dodgerblue\")\nplot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))\nabline(int_4cyl, slope_4cyl, col = plot_colors[1], lty = 1, lwd = 2)\nabline(int_6cyl, slope_6cyl, col = plot_colors[2], lty = 2, lwd = 2)\nabline(int_8cyl, slope_8cyl, col = plot_colors[3], lty = 3, lwd = 2)\nlegend(\"topright\", c(\"4 Cylinder\", \"6 Cylinder\", \"8 Cylinder\"),\n       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))\n\n\n\n\n\n\n\n\nThis looks much better! We can see that for medium displacement cars, 6 cylinder cars now perform better than 8 cylinder cars, which seems much more reasonable than before.\nTo completely justify the interaction model (i.e., a unique slope for each cyl level) compared to the additive model (single slope), we can perform an \\(F\\)-test. Notice first, that there is no \\(t\\)-test that will be able to do this since the difference between the two models is not a single parameter.\nWe will test,\n\\[\nH_0: \\gamma_2 = \\gamma_3 = 0\n\\]\nwhich represents the parallel regression lines we saw before,\n\\[\nY = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon.\n\\]\nAgain, this is a difference of two parameters, thus no \\(t\\)-test will be useful.\n\nanova(mpg_disp_add_cyl, mpg_disp_int_cyl)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + cyl\nModel 2: mpg ~ disp * cyl\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    379 7299.5                                  \n2    377 6551.7  2    747.79 21.515 1.419e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs expected, we see a very low p-value, and thus reject the null. We prefer the interaction model over the additive model.\nRecapping a bit:\n\nNull Model: \\(Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon\\)\n\nNumber of parameters: \\(q = 4\\)\n\nFull Model: \\(Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\gamma_2 x v_2 + \\gamma_3 x v_3 + \\epsilon\\)\n\nNumber of parameters: \\(p = 6\\)\n\n\n\nlength(coef(mpg_disp_int_cyl)) - length(coef(mpg_disp_add_cyl))\n\n[1] 2\n\n\nWe see there is a difference of two parameters, which is also displayed in the resulting ANOVA table from R. Notice that the following two values also appear on the ANOVA table.\n\nnrow(autompg) - length(coef(mpg_disp_int_cyl))\n\n[1] 377\n\nnrow(autompg) - length(coef(mpg_disp_add_cyl))\n\n[1] 379",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression: Interpreting Coefficients"
    ]
  },
  {
    "objectID": "content/Week_07/07b.html#parameterization",
    "href": "content/Week_07/07b.html#parameterization",
    "title": "Linear Regression: Interpreting Coefficients",
    "section": "Parameterization",
    "text": "Parameterization\nSo far we have been simply letting R decide how to create the dummy variables, and thus R has been deciding the parameterization of the models. To illustrate the ability to use alternative parameterizations, we will recreate the data, but directly creating the dummy variables ourselves.\n\nnew_param_data = data.frame(\n  y = autompg$mpg,\n  x = autompg$disp,\n  v1 = 1 * as.numeric(autompg$cyl == 4),\n  v2 = 1 * as.numeric(autompg$cyl == 6),\n  v3 = 1 * as.numeric(autompg$cyl == 8))\n\nhead(new_param_data, 20)\n\n    y   x v1 v2 v3\n1  18 307  0  0  1\n2  15 350  0  0  1\n3  18 318  0  0  1\n4  16 304  0  0  1\n5  17 302  0  0  1\n6  15 429  0  0  1\n7  14 454  0  0  1\n8  14 440  0  0  1\n9  14 455  0  0  1\n10 15 390  0  0  1\n11 15 383  0  0  1\n12 14 340  0  0  1\n13 15 400  0  0  1\n14 14 455  0  0  1\n15 24 113  1  0  0\n16 22 198  0  1  0\n17 18 199  0  1  0\n18 21 200  0  1  0\n19 27  97  1  0  0\n20 26  97  1  0  0\n\n\nNow,\n\ny is mpg\nx is disp, the displacement in cubic inches,\nv1, v2, and v3 are dummy variables as defined above.\n\nFirst let’s try to fit an additive model using x as well as the three dummy variables.\n\nlm(y ~ x + v1 + v2 + v3, data = new_param_data)\n\n\nCall:\nlm(formula = y ~ x + v1 + v2 + v3, data = new_param_data)\n\nCoefficients:\n(Intercept)            x           v1           v2           v3  \n   32.96326     -0.05217      2.03603     -1.59722           NA  \n\n\nWhat is happening here? Notice that R is essentially ignoring v3, but why? Well, because R uses an intercept, it cannot also use v3. This is because\n\\[\n\\boldsymbol{1} = v_1 + v_2 + v_3\n\\]\nwhich means that \\(\\boldsymbol{1}\\), \\(v_1\\), \\(v_2\\), and \\(v_3\\) are linearly dependent. This would make the \\(X^\\top X\\) matrix singular, but we need to be able to invert it to solve the normal equations and obtain \\(\\hat{\\beta}.\\) With the intercept, v1, and v2, R can make the necessary “three intercepts”. So, in this case v3 is the reference level.\nIf we remove the intercept, then we can directly obtain all “three intercepts” without a reference level.\n\nlm(y ~ 0 + x + v1 + v2 + v3, data = new_param_data)\n\n\nCall:\nlm(formula = y ~ 0 + x + v1 + v2 + v3, data = new_param_data)\n\nCoefficients:\n       x        v1        v2        v3  \n-0.05217  34.99929  31.36604  32.96326  \n\n\nHere, we are fitting the model\n\\[\nY = \\mu_1 v_1 + \\mu_2 v_2 + \\mu_3 v_3 + \\beta x +\\epsilon.\n\\]\nThus we have:\n\n4 Cylinder: \\(Y = \\mu_1 + \\beta x + \\epsilon\\)\n6 Cylinder: \\(Y = \\mu_2 + \\beta x + \\epsilon\\)\n8 Cylinder: \\(Y = \\mu_3 + \\beta x + \\epsilon\\)\n\nWe could also do something similar with the interaction model, and give each line an intercept and slope, without the need for a reference level.\n\nlm(y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)\n\n\nCall:\nlm(formula = y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)\n\nCoefficients:\n      v1        v2        v3      v1:x      v2:x      v3:x  \n43.59052  30.39026  22.73346  -0.13069  -0.04770  -0.02252  \n\n\n\\[\nY = \\mu_1 v_1 + \\mu_2 v_2 + \\mu_3 v_3 + \\beta_1 x v_1 + \\beta_2 x v_2 + \\beta_3 x v_3 +\\epsilon\n\\]\n\n4 Cylinder: \\(Y = \\mu_1 + \\beta_1 x + \\epsilon\\)\n6 Cylinder: \\(Y = \\mu_2 + \\beta_2 x + \\epsilon\\)\n8 Cylinder: \\(Y = \\mu_3 + \\beta_3 x + \\epsilon\\)\n\nUsing the original data, we have (at least) three equivalent ways to specify the interaction model with R.\n\nlm(mpg ~ disp * cyl, data = autompg)\n\n\nCall:\nlm(formula = mpg ~ disp * cyl, data = autompg)\n\nCoefficients:\n(Intercept)         disp         cyl6         cyl8    disp:cyl6    disp:cyl8  \n   43.59052     -0.13069    -13.20026    -20.85706      0.08299      0.10817  \n\nlm(mpg ~ 0 + cyl + disp : cyl, data = autompg)\n\n\nCall:\nlm(formula = mpg ~ 0 + cyl + disp:cyl, data = autompg)\n\nCoefficients:\n     cyl4       cyl6       cyl8  cyl4:disp  cyl6:disp  cyl8:disp  \n 43.59052   30.39026   22.73346   -0.13069   -0.04770   -0.02252  \n\nlm(mpg ~ 0 + disp + cyl + disp : cyl, data = autompg)\n\n\nCall:\nlm(formula = mpg ~ 0 + disp + cyl + disp:cyl, data = autompg)\n\nCoefficients:\n     disp       cyl4       cyl6       cyl8  disp:cyl6  disp:cyl8  \n -0.13069   43.59052   30.39026   22.73346    0.08299    0.10817  \n\n\nThey all fit the same model, importantly each using six parameters, but the coefficients mean slightly different things in each. However, once they are interpreted as slopes and intercepts for the “three lines” they will have the same result.\nUse ?all.equal to learn about the all.equal() function, and think about how the following code verifies that the residuals of the two models are the same.\n\nall.equal(fitted(lm(mpg ~ disp * cyl, data = autompg)),\n          fitted(lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)))\n\n[1] TRUE\n\n\n\nTRY IT\nLet’s try some interaction terms using our Ames data from before (linked at the top of this page). For now, we’ll stick to interacting square footage (GrLivArea) with dummy or factor variables. As usual, we’re trying to predict SalePrice.\nWhat factor variables should we use? Here are some options:\n\nCentralAir (binary, but stored as “Y”/“N”)\nNeighborhood (25 different neighborhoods, stored as chr)\nYearBuilt (not a factor, but case_when can be used to make a few bins based on year)\nOverallCond (subjective 1-9 scale, but stored as an integer – use as.factor!)\n??? Anything else interesting you see?\n\nI’d like you to choose one of the above and run two models: one with GrLivArea and your chosen factor variable, and one with GrLivArea, your chosen factor variable, and the interaction of the two. Once you have run these:\n\nBe prepared to interpret the coefficients on the “baseline” coefficient on GrLivArea, as well as one of the coefficients on the interaction.\nTest the two models using anova(mod1, mod2) and tell us if your more flexible model is statistically better than the less flexible model.",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression: Interpreting Coefficients"
    ]
  },
  {
    "objectID": "content/Week_07/07b.html#building-larger-models",
    "href": "content/Week_07/07b.html#building-larger-models",
    "title": "Linear Regression: Interpreting Coefficients",
    "section": "Building Larger Models",
    "text": "Building Larger Models\nNow that we have seen how to incorporate categorical predictors as well as interaction terms, we can start to build much larger, much more flexible models which can potentially fit data better.\nLet’s define a “big” model,\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\beta_7 x_1 x_2 x_3 + \\epsilon.\n\\]\nHere,\n\n\\(Y\\) is mpg.\n\\(x_1\\) is disp.\n\\(x_2\\) is hp.\n\\(x_3\\) is domestic, which is a dummy variable we defined, where 1 is a domestic vehicle.\n\nFirst thing to note here, we have included a new term \\(x_1 x_2 x_3\\) which is a three-way interaction. Interaction terms can be larger and larger, up to the number of predictors in the model.\nSince we are using the three-way interaction term, we also use all possible two-way interactions, as well as each of the first order (main effect) terms. This is the concept of a hierarchy. Any time a “higher-order” term is in a model, the related “lower-order” terms should also be included. Mathematically their inclusion or exclusion is sometimes irrelevant, but from an interpretation standpoint, it is best to follow the hierarchy rules.\nLet’s do some rearrangement to obtain a “coefficient” in front of \\(x_1\\).\n\\[\nY = \\beta_0 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_6 x_2 x_3 + (\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3)x_1 + \\epsilon.\n\\]\nSpecifically, the “coefficient” in front of \\(x_1\\) is\n\\[\n(\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3).\n\\]\nLet’s discuss this “coefficient” to help us understand the idea of the flexibility of a model. Recall that,\n\n\\(\\beta_1\\) is the coefficient for a first order term,\n\\(\\beta_4\\) and \\(\\beta_5\\) are coefficients for two-way interactions,\n\\(\\beta_7\\) is the coefficient for the three-way interaction.\n\nIf the two and three way interactions were not in the model, the whole “coefficient” would simply be\n\\[\n\\beta_1.\n\\]\nThus, no matter the values of \\(x_2\\) and \\(x_3\\), \\(\\beta_1\\) would determine the relationship between \\(x_1\\) (disp) and \\(Y\\) (mpg).\nWith the addition of the two-way interactions, now the “coefficient” would be\n\\[\n(\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3).\n\\]\nNow, changing \\(x_1\\) (disp) has a different effect on \\(Y\\) (mpg), depending on the values of \\(x_2\\) and \\(x_3\\).\nLastly, adding the three-way interaction gives the whole “coefficient”\n\\[\n(\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3)\n\\]\nwhich is even more flexible. Now changing \\(x_1\\) (disp) has a different effect on \\(Y\\) (mpg), depending on the values of \\(x_2\\) and \\(x_3\\), but in a more flexible way which we can see with some more rearrangement. Now the “coefficient” in front of \\(x_3\\) in this “coefficient” is dependent on \\(x_2\\).\n\\[\n(\\beta_1 + \\beta_4 x_2 + (\\beta_5 + \\beta_7 x_2) x_3)\n\\]\nIt is so flexible, it is becoming hard to interpret!\nLet’s fit this three-way interaction model in R.\n\nbig_model = lm(mpg ~ disp * hp * domestic, data = autompg)\nsummary(big_model)\n\n\nCall:\nlm(formula = mpg ~ disp * hp * domestic, data = autompg)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.9410  -2.2147  -0.4008   1.9430  18.4094 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       6.065e+01  6.600e+00   9.189  &lt; 2e-16 ***\ndisp             -1.416e-01  6.344e-02  -2.232   0.0262 *  \nhp               -3.545e-01  8.123e-02  -4.364 1.65e-05 ***\ndomestic         -1.257e+01  7.064e+00  -1.780   0.0759 .  \ndisp:hp           1.369e-03  6.727e-04   2.035   0.0426 *  \ndisp:domestic     4.933e-02  6.400e-02   0.771   0.4414    \nhp:domestic       1.852e-01  8.709e-02   2.126   0.0342 *  \ndisp:hp:domestic -9.163e-04  6.768e-04  -1.354   0.1766    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.88 on 375 degrees of freedom\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7556 \nF-statistic: 169.7 on 7 and 375 DF,  p-value: &lt; 2.2e-16\n\n\nDo we actually need this large of a model? Let’s first test for the necessity of the three-way interaction term. That is,\n\\[\nH_0: \\beta_7 = 0.\n\\]\nSo,\n\nFull Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\beta_7 x_1 x_2 x_3 + \\epsilon\\)\nNull Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon\\)\n\nWe fit the null model in R as two_way_int_mod. Again, we check to see if the big model is any more explanatory:\n\ntwo_way_int_mod = lm(mpg ~ disp * hp + disp * domestic + hp * domestic, data = autompg)\n#two_way_int_mod = lm(mpg ~ (disp + hp + domestic) ^ 2, data = autompg)\nanova(two_way_int_mod, big_model)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp * hp + disp * domestic + hp * domestic\nModel 2: mpg ~ disp * hp * domestic\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    376 5673.2                           \n2    375 5645.6  1    27.599 1.8332 0.1766\n\n\nWe see the p-value is somewhat large, so we would fail to reject. We prefer the smaller, less flexible, null model, without the three-way interaction.\nA quick note here: the full model does still “fit better.” Notice that it has a smaller RMSE than the null model, which means the full model makes smaller (squared) errors on average.\n\nmean(resid(big_model) ^ 2)\n\n[1] 14.74053\n\nmean(resid(two_way_int_mod) ^ 2)\n\n[1] 14.81259\n\n\nHowever, it is not much smaller. We could even say that, the difference is insignificant. This is an idea we will return to later in greater detail.\nNow that we have chosen the model without the three-way interaction, can we go further? Do we need the two-way interactions? Let’s test\n\\[\nH_0: \\beta_4 = \\beta_5 = \\beta_6 = 0.\n\\]\nRemember we already chose \\(\\beta_7 = 0\\), so,\n\nFull Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon\\)\nNull Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\\)\n\nWe fit the null model in R as additive_mod, then use anova() to perform an \\(F\\)-test as usual.\n\nadditive_mod = lm(mpg ~ disp + hp + domestic, data = autompg)\nanova(additive_mod, two_way_int_mod)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + hp + domestic\nModel 2: mpg ~ disp * hp + disp * domestic + hp * domestic\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    379 7369.7                                  \n2    376 5673.2  3    1696.5 37.478 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere the p-value is small, so we reject the null, and we prefer the full (alternative) model. Of the models we have considered, our final preference is for\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon.\n\\]",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression: Interpreting Coefficients"
    ]
  },
  {
    "objectID": "content/Week_06/06b.html",
    "href": "content/Week_06/06b.html",
    "title": "Introduction to Regression",
    "section": "",
    "text": "For this example, we’re going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…” and place it in a data folder next to your working R script):\n\n atl-weather-2019.csv",
    "crumbs": [
      "Course Content",
      "Week 06",
      "Introduction to Regression"
    ]
  },
  {
    "objectID": "content/Week_06/06b.html#preliminaries",
    "href": "content/Week_06/06b.html#preliminaries",
    "title": "Introduction to Regression",
    "section": "",
    "text": "For this example, we’re going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…” and place it in a data folder next to your working R script):\n\n atl-weather-2019.csv",
    "crumbs": [
      "Course Content",
      "Week 06",
      "Introduction to Regression"
    ]
  },
  {
    "objectID": "content/Week_06/06b.html#code",
    "href": "content/Week_06/06b.html#code",
    "title": "Introduction to Regression",
    "section": "Code",
    "text": "Code\n\nLoad the data\nFirst, we load the libraries we’ll be using:\n\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(patchwork)  # For combining ggplot plots\nlibrary(GGally)     # New one, for scatterplot matrices\nlibrary(broom)      # For converting model objects to data frames\n\nThen we load the data with read_csv(). This is tidyverse’s CSV reader (base R is read.csv). Here we assume that the CSV file was downloaded and lives in a subfolder named data:\n\nweather_atl &lt;- read_csv(\"data/atl-weather-2019.csv\")\n\nOf course, you can also load it directly with the URL from above (as we showed before).\n\n\nScatterplot matrices\nThe foundation of linear regression is corrleation. We can visualize the correlations between pairs of variables with the ggpairs() function in the GGally package. For instance, how correlated are high and low temperatures, humidity, wind speed, and the chance of precipitation? We first make a smaller dataset with just those columns, and then we feed that dataset into ggpairs() to see all the correlation information:\n\nlibrary(GGally)\n\nweather_correlations &lt;- weather_atl %&gt;%\n  select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability)\n\nggpairs(weather_correlations)\n\n\n\n\n\n\n\n\nIt looks like high and low temperatures are extremely highly positively correlated (r = 0.92). Wind speed and temperature are moderately negatively correlated, with low temperatures having a stronger negative correlation (r = -0.45). There’s no correlation whatsoever between low temperatures and the precipitation probability (r = -0.03) or humidity and high temperatures (r = -0.03).\nEven though ggpairs() doesn’t use the standard ggplot(...) + geom_whatever() syntax we’re familiar with, it does behind the scenes, so you can add regular ggplot layers to it:\n\nggpairs(weather_correlations) + \n  labs(title = \"Correlations!\") +\n  theme_dark()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTRY IT\n\n\n\nHere’s some code to load data from the Residential Energy Consumption Survey (RECS), a once-a-decade survey of household energy consumption done by the Energy Information Administration.. Use the code here to load the data. I’ve selected a few variables and renamed them to something intuitive. Each observation is a household surveyed by the EIA. The variable named EnergyUsed is the total BTU’s of energy consumed by that household.\nMost of the variables are self-explanatory except the following:\n\nAgeOfHome is on a 1-9 scale where 1 is the oldest and 9 is the newest. This is “discretized” age.\nHeaterAge is the age of the home’s heater from 1 to 6 where 6 is the oldest and 1 is the newest.\nTVSize is on a scale of 1 (smallest) to 4 (largest)\nTempHome tells us the home’s thermostat setting when home (in degrees farenheit)\n\nYour task is to make a ggpairs plot for EnergyUsed and five of the variables in the RECS data. What variables do you think are correlated with total energy use EnergyUsed? And what can we learn about energy consumption from these correlations?\n\nRECS = read.csv('https://www.eia.gov/consumption/residential/data/2009/csv/recs2009_public.csv', stringsAsFactors = F) %&gt;%\n  as_tibble() %&gt;%  \n  dplyr::select(EnergyUsed = TOTALBTU,\n                ColdDays = HDD65, HotDayss = CDD65, SquareFeet = TOTHSQFT, CarsGarage = SIZEOFGARAGE, \n                AgeOfHome = YEARMADERANGE, TreeShade = TREESHAD, TVSize = TVSIZE1, HeaterAge = EQUIPAGE, HasAC = AIRCOND,\n                TempHome = TEMPHOME) %&gt;%\n  dplyr::filter(HeaterAge != -2 & TempHome !=-2)  # these are NAs\n\n\n\n\n\nCorrelograms\nScatterplot matrices typically include way too much information to be used in actual publications. I use them when doing my own analysis just to see how different variables are related, but I rarely polish them up for public consumption. In some interesting supplemental material located here, Claus Wilke showed a type of plot called a correlogram which is more appropriate for publication.\nThese are essentially heatmaps of the different correlation coefficients. To make these with ggplot, we need to do a little bit of extra data processing, mostly to reshape data into a long, tidy format that we can plot. Here’s how.\nFirst we need to build a correlation matrix of the main variables we care about. Ordinarily the cor() function in R takes two arguments—x and y—and it will return a single correlation value. If you feed a data frame into cor() though, it’ll calculate the correlation between each pair of columns. But be careful - don’t feed in hundreds of variables by accident!\n\n# Create a correlation matrix\nthings_to_correlate &lt;- weather_atl %&gt;%\n  select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) %&gt;%\n  cor()\n\nthings_to_correlate\n\n                  temperatureHigh temperatureLow humidity windSpeed precipProbability\ntemperatureHigh              1.00          0.920   -0.030    -0.377            -0.124\ntemperatureLow               0.92          1.000    0.112    -0.450            -0.026\nhumidity                    -0.03          0.112    1.000     0.011             0.722\nwindSpeed                   -0.38         -0.450    0.011     1.000             0.196\nprecipProbability           -0.12         -0.026    0.722     0.196             1.000\n\n\nThe two halves of this matrix (split along the diagonal line) are identical, so we can remove the lower triangle with this code (which will set all the cells in the lower triangle to NA):\n\n# Get rid of the lower triangle\nthings_to_correlate[lower.tri(things_to_correlate)] &lt;- NA\nthings_to_correlate\n\n                  temperatureHigh temperatureLow humidity windSpeed precipProbability\ntemperatureHigh                 1           0.92    -0.03    -0.377            -0.124\ntemperatureLow                 NA           1.00     0.11    -0.450            -0.026\nhumidity                       NA             NA     1.00     0.011             0.722\nwindSpeed                      NA             NA       NA     1.000             0.196\nprecipProbability              NA             NA       NA        NA             1.000\n\n\nFinally, in order to plot this, the data needs to be in tidy (or long) format. Here we convert the things_to_correlate matrix into a data frame, add a column for the row names, take all the columns and put them into a single column named measure1, and take all the correlation numbers and put them in a column named cor In the end, we make sure the measure variables are ordered by their order of appearance (otherwise they plot alphabetically and don’t make a triangle)\n\nthings_to_correlate_long &lt;- things_to_correlate %&gt;%\n  # Convert from a matrix to a data frame\n  as.data.frame() %&gt;%\n  # Matrixes have column names that don't get converted to columns when using\n  # as.data.frame(), so this adds those names as a column\n  rownames_to_column(\"measure2\") %&gt;%\n  # Make this long. Take all the columns except measure2 and put their names in\n  # a column named measure1 and their values in a column named cor\n  pivot_longer(cols = -measure2,\n               names_to = \"measure1\",\n               values_to = \"cor\") %&gt;%\n  # Make a new column with the rounded version of the correlation value\n  mutate(nice_cor = round(cor, 2)) %&gt;%\n  # Remove rows where the two measures are the same (like the correlation\n  # between humidity and humidity)\n  filter(measure2 != measure1) %&gt;%\n  # Get rid of the empty triangle\n  filter(!is.na(cor)) %&gt;%\n  # Put these categories in order\n  mutate(measure1 = fct_inorder(measure1),\n         measure2 = fct_inorder(measure2))\n\nthings_to_correlate_long\n\n# A tibble: 10 × 4\n   measure2        measure1              cor nice_cor\n   &lt;fct&gt;           &lt;fct&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n 1 temperatureHigh temperatureLow     0.920      0.92\n 2 temperatureHigh humidity          -0.0301    -0.03\n 3 temperatureHigh windSpeed         -0.377     -0.38\n 4 temperatureHigh precipProbability -0.124     -0.12\n 5 temperatureLow  humidity           0.112      0.11\n 6 temperatureLow  windSpeed         -0.450     -0.45\n 7 temperatureLow  precipProbability -0.0255    -0.03\n 8 humidity        windSpeed          0.0108     0.01\n 9 humidity        precipProbability  0.722      0.72\n10 windSpeed       precipProbability  0.196      0.2 \n\n\nPhew. With the data all tidied like that, we can make a correlogram with a heatmap. We have manipulated the fill scale a little so that it’s diverging with three colors: a high value, a midpoint value, and a low value.\n\nggplot(things_to_correlate_long,\n       aes(x = measure2, y = measure1, fill = cor)) +\n  geom_tile() +\n  geom_text(aes(label = nice_cor)) +\n  scale_fill_gradient2(low = \"#E16462\", mid = \"white\", high = \"#0D0887\",\n                       limits = c(-1, 1)) +\n  labs(x = NULL, y = NULL, fill = 'Corr.') +\n  coord_equal() +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nInstead of using a heatmap, we can also use points, which encode the correlation information both as color and as size. To do that, we just need to switch geom_tile() to geom_point() and set the size = cor mapping (note the use of the absolute value function):\n\nggplot(things_to_correlate_long,\n       aes(x = measure2, y = measure1, color = cor)) +\n  # Size by the absolute value so that -0.7 and 0.7 are the same size\n  geom_point(aes(size = abs(cor))) +\n  scale_color_gradient2(low = \"#E16462\", mid = \"white\", high = \"#0D0887\",\n                        limits = c(-1, 1)) +\n  scale_size_area(max_size = 15, limits = c(-1, 1), guide = 'none') +\n  labs(x = NULL, y = NULL, color = 'Corr.') +\n  coord_equal() +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank())",
    "crumbs": [
      "Course Content",
      "Week 06",
      "Introduction to Regression"
    ]
  },
  {
    "objectID": "content/Week_06/06b.html#simple-regression",
    "href": "content/Week_06/06b.html#simple-regression",
    "title": "Introduction to Regression",
    "section": "Simple regression",
    "text": "Simple regression\nWe finally get to this week’s material. Although correlation is nice, we eventually may want to visualize regression. The lecture has shown us some very intuitive, straightforward ways to think about regression (aka, a line). Simple regression is easy to visualize, since you’re only working with an \\(X\\) and a \\(Y\\). For instance, what’s the relationship between humidity and high temperatures during the summer?\nFirst, let’s filter the Atlanta weather data to only look at the summer. We also add a column to scale up the humidity value—right now it’s on a scale of 0-1 (for percentages), but when interpreting regression we talk about increases in whole units, so we’d talk about moving from 0% humidity to 100% humidity, which isn’t helpful, so instead we multiply everything by 100 so we can talk about moving from 50% humidity to 51% humidity. We also scale up a couple other variables that we’ll use in the larger model later.\n\nweather_atl_summer &lt;- weather_atl %&gt;%\n  filter(time &gt;= \"2019-05-01\", time &lt;= \"2019-09-30\") %&gt;%\n  mutate(humidity_scaled = humidity * 100,\n         moonPhase_scaled = moonPhase * 100,\n         precipProbability_scaled = precipProbability * 100,\n         cloudCover_scaled = cloudCover * 100)\n\nThen we can build a simple, single-variable regression model for the high temperature “regressed on” humidity. We would write this regression as:\n\\[HighTempScaled_i = \\beta_0 + \\beta_1 HumidityScaled_i + \\varepsilon_i\\]\nEach of our 153 observations is indexed by \\(i\\). Since it will almost never be the case that \\(\\beta_0 + \\beta_1 HumidityScaled_i = HighTempScaled\\), we include the error term \\(\\varepsilon_i\\) to account for things that aren’t in our regression.\nOur estimated parameters \\(\\beta_0, \\beta_1\\) define the line of best fit, or the “regression line”. That regression line is our conditional expectation function:\n\\[E[HighTempScaled|HumidityScaled] = \\beta_0 + \\beta_1 HumidityScaled\\]\nNote that we do not have an error term here – the \\(E[]\\) doesn’t have an error term in it, only the regression function does.\nTo do this in R, we’ll first use the formula from Principles 06:\n\nweather_atl_results = weather_atl_summer %&gt;%\n  dplyr::summarize(mu_x = mean(humidity_scaled),\n                   mu_y = mean(temperatureHigh),\n                   sd_x = sd(humidity_scaled),\n                   sd_y = sd(temperatureHigh),\n                   rho = sum(((temperatureHigh-mu_y)/sd_y)*((humidity_scaled-mu_x)/sd_x))/(n()-1) ) %&gt;%\n                  # Note in rho we had to use that same \"N-1\" correction. cor() does this for us automatically\n  dplyr::mutate(beta_1 = rho*(sd_y/sd_x),\n                beta_0 = mu_y - beta_1*mu_x)\n\nprint(weather_atl_results)\n\n# A tibble: 1 × 7\n   mu_x  mu_y  sd_x  sd_y    rho beta_1 beta_0\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1  64.8  88.5  10.6  5.33 -0.481 -0.241   104.\n\n\nNote that the formulas from Principles 06 have a direct translation into the code. Also note that we used summarize on the data so that we get only one row of data (no more 1,000+ rows, we’ve summarized them using mean and sd).\n\nInterpretation of coefficients\nOur estimates of the \\(\\beta_0, \\beta_1\\) are our “coefficients”. It’s important we interpret them correctly. We can interpret these coefficients like so:\n\nbeta_0 (or \\(\\beta_0\\)) is the intercept. By definition of an intercept, it shows that the average temperature when there’s 0% humidity is 104°. There are no days with 0% humidity though, so we can ignore the intercept—it’s really just here so that we can draw the line.\nbeta_1 (or \\(\\beta_1\\)) is the coefficient for HumidityScaled. It is the slope of the regression line, and thus the change in E[HighTempScaled] per 1-unit increase in \\(HumidityScaled\\). It shows that a one percent increase in humidity is associated with a 0.241° decrease in temperature, on average.\n\nThis (and \\(\\beta_0\\)) are random variables – if we re-draw the sample, we’ll see a slightly different relationship. We’ll talk about the standard error of this (and how to see it) in a little bit.\n\n\nVisualizing this model is simple, since there are only two variables. We visualized the regression line in the Galton Heights data during Content 05, so let’s just use that code.\n\nggplot(weather_atl_summer, aes(x = humidity_scaled, y = temperatureHigh)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = weather_atl_results$beta_0, slope = weather_atl_results$beta_1)\n\n\n\n\n\n\n\n\nEvery point along the regression line is the \\(E[HighTempScaled|HumidityScaled]\\). Put in whatever value of humidity you want, and the line tells you what the expectation of the high temperature will be.\nAnother way we can add a regression line in a 2-D plot is using geom_smooth. geom_smooth is a ggplot geometry that adds a line of best fit. This can be flexible, or this can be based on a linear relationship – the regression line. We’ll see how to use built-in functions in R to do regression later, but for now, we’ll just ask ggplot to make a linear model (“lm”) for a line of best fit. The slope of that line is going to be exactly our estimate of \\(\\beta_1\\).\n\nggplot(weather_atl_summer,\n       aes(x = humidity_scaled, y = temperatureHigh)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nAnd indeed, we see the same line as we made “manually” before.\n\n\n\n\n\n\nTRY IT\n\n\n\nLet’s use that RECS data to regress EnergyUsed on one of the variables from your scatterplot matrix. Note that we say the left-hand-side variable (the “Y”) is regressed on the right-hand-side (the “X”) variable.\nUsing the formulas and codes for regression, choose the variable you want to use in the regression, run it, and visualize the result.\nWe’ll have you share your work and interpret the results when done. https://bit.ly/EC242",
    "crumbs": [
      "Course Content",
      "Week 06",
      "Introduction to Regression"
    ]
  },
  {
    "objectID": "content/Week_05/05b.html",
    "href": "content/Week_05/05b.html",
    "title": "Wrangle some Data",
    "section": "",
    "text": "Data to be wrangled\nYou work for a travel booking website as a data analyst. A hotel has asked your company for data on corporate bookings at the hotel via your site. Specifically, they have five corporations that are frequent customers of the hotel, and they want to know who spends the most with them. They’ve asked you to help out. Most of the corporate spending is in the form of room reservations, but there are also parking fees that the hotel wants included in the analysis. Your goal: total up spending by corporation and report the biggest and smallest spenders inclusive of rooms and parking.\nUnfortunately, you only have the following data:\n\n booking.csv - Contains the corporation name, the room type, and the dates someone from the corporation stayed at the hoted.\n roomrates.csv - Contains the price of each room on each day\n parking.csv - Contains the corporations who negotiated free parking for employees\nParking at the hotel is $60 per night if you don’t have free parking. This hotel is in California, so everyone drives and parks when they stay.\n\n\n\n\n\n\n\nTip\n\n\n\n\nRight-click on each of the links, copy the address, and read the URL in using read.csv to read .csv’s\nYou’ll find you need to use most of the tools we covered on Tuesday including gather, separate and more.\nYou’ll need lubridate and tidyverse loaded up.\n\n\n\nYour lab wil be based on similar data (with more wrinkles to fix) so share your code with your group when you’re done.",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Wrangle some Data"
    ]
  },
  {
    "objectID": "content/Week_04/04b.html",
    "href": "content/Week_04/04b.html",
    "title": "Visualizations",
    "section": "",
    "text": "This class has three group projects - Project 1, Project 2, and the Final Project - that comprise a significant portion of your grade.\nThis week, we will form groups for these projects. You will work with the same group for each of the projects. The details will be posted soon, but the general idea is that you are given some data and asked to use your visualization and analytical skills to illustrate important patterns in the data. The final project will be on a topic of your group’s choice, while the first two will use data given as part of the assignment.\nGroups may be 2 or 3 people, no more and no less. You are free to form your own group and anyone who does not form one will be assigned to one. With that in mind, let’s form our groups.\nIf you have a group of 2-3 individuals willing to work together for these three projects, do the following before this coming Sunday night at 11:59 pm:\n\nThink of a group name\nHave one person in the group email our TA, Zach, at smithsi6@msu.edu with the following:\n\n\nThe subject should be [EC242 Group Formation]\nThe email should CC all of the group members (so you know when you’ve been added to a group)\nThe email should list the names (as they appear in D2L) of each of the group members\nThe email should include the group name\n\nIf you would prefer to be assigned to a group, you don’t have to do anything. Once our TA has the voluntary groups set, they will assign groups to the rest of the class and email each group with instructions.",
    "crumbs": [
      "Course Content",
      "Week 04",
      "Visualizations"
    ]
  },
  {
    "objectID": "content/Week_04/04b.html#case-study-new-insights-on-poverty",
    "href": "content/Week_04/04b.html#case-study-new-insights-on-poverty",
    "title": "Visualizations",
    "section": "Case study: new insights on poverty",
    "text": "Case study: new insights on poverty\nHans Rosling1 was the co-founder of the Gapminder Foundation2, an organization dedicated to educating the public by using data to dispel common myths about the so-called developing world. The organization uses data to show how actual trends in health and economics contradict the narratives that emanate from sensationalist media coverage of catastrophes, tragedies, and other unfortunate events. As stated in the Gapminder Foundation’s website:\n\nJournalists and lobbyists tell dramatic stories. That’s their job. They tell stories about extraordinary events and unusual people. The piles of dramatic stories pile up in peoples’ minds into an over-dramatic worldview and strong negative stress feelings: “The world is getting worse!”, “It’s we vs. them!”, “Other people are strange!”, “The population just keeps growing!” and “Nobody cares!”\n\nHans Rosling conveyed actual data-based trends in a dramatic way of his own, using effective data visualization. This section is based on two talks that exemplify this approach to education: [New Insights on Poverty]3 and The Best Stats You’ve Ever Seen4. Specifically, in this section, we use data to attempt to answer the following two questions:\n\nIs it a fair characterization of today’s world to say it is divided into western rich nations and the developing world in Africa, Asia, and Latin America?\nHas income inequality across countries worsened during the last 40 years?\n\nTo answer these questions, we will be using the gapminder dataset provided in dslabs. This dataset was created using a number of spreadsheets available from the Gapminder Foundation. You can access the table like this:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(ggrepel)\nlibrary(ggthemes)\ngapminder = dslabs::gapminder %&gt;% as_tibble()\n\n\nExploring the Data\nTaking an exercise from the New Insights on Poverty video, we start by testing our knowledge regarding differences in child mortality across different countries. For each of the six pairs of countries below, which country do you think had the highest child mortality rates in 2015? Which pairs do you think are most similar?\n\nSri Lanka or Turkey\nPoland or South Korea\nMalaysia or Russia\nPakistan or Vietnam\nThailand or South Africa\n\nWhen answering these questions without data, the non-European countries are typically picked as having higher child mortality rates: Sri Lanka over Turkey, South Korea over Poland, and Malaysia over Russia. It is also common to assume that countries considered to be part of the developing world: Pakistan, Vietnam, Thailand, and South Africa, have similarly high mortality rates.\nTo answer these questions with data, we can use tidyverse functions. For example, for the first comparison we see that:\n\ndslabs::gapminder %&gt;%\n  filter(year == 2015 & country %in% c(\"Sri Lanka\",\"Turkey\")) %&gt;%\n  select(country, infant_mortality)\n\n    country infant_mortality\n1 Sri Lanka              8.4\n2    Turkey             11.6\n\n\nTurkey has the higher infant mortality rate.\nWe can use this code on all comparisons and find the following:\n\n\n\n\n\ncountry\ninfant mortality\ncountry\ninfant mortality\n\n\n\n\nSri Lanka\n8.4\nTurkey\n11.6\n\n\nPoland\n4.5\nSouth Korea\n2.9\n\n\nMalaysia\n6.0\nRussia\n8.2\n\n\nPakistan\n65.8\nVietnam\n17.3\n\n\nThailand\n10.5\nSouth Africa\n33.6\n\n\n\n\n\n\n\nWe see that the European countries on this list have higher child mortality rates: Poland has a higher rate than South Korea, and Russia has a higher rate than Malaysia. We also see that Pakistan has a much higher rate than Vietnam, and South Africa has a much higher rate than Thailand. It turns out that when Hans Rosling gave this quiz to educated groups of people, the average score was less than 2.5 out of 5, worse than what they would have obtained had they guessed randomly. This implies that more than ignorant, we are misinformed. In this chapter we see how data visualization helps inform us by presenting patterns in the data that might not be obvious on first glance.\n\n\nSlope charts\nThe slopechart is informative when you are comparing variables of the same type, but at different time points and for a relatively small number of comparisons. For example, comparing life expectancy between 2010 and 2015. In this case, we might recommend a slope chart.\nThere is no geometry for slope charts in ggplot2, but we can construct one using geom_line. We need to do some tinkering to add labels. We’ll paste together a character stright with the country name and the starting life expectancy, then do the same with just the later life expectancy for the right side. Below is an example comparing 2010 to 2015 for large western countries:\n\nwest &lt;- c(\"Western Europe\",\"Northern Europe\",\"Southern Europe\",\n          \"Northern America\",\"Australia and New Zealand\")\n\ndat &lt;- gapminder %&gt;%\n  filter(year%in% c(2010, 2015) & region %in% west &\n           !is.na(life_expectancy) & population &gt; 10^7) %&gt;%\n    mutate(label_first = ifelse(year == 2010, paste0(country, \": \", round(life_expectancy, 1), ' years'), NA),\n           label_last = ifelse(year == 2015,  paste0(round(life_expectancy, 1),' years'), NA))\n\ndat %&gt;%\n  mutate(location = ifelse(year == 2010, 1, 2),\n         location = ifelse(year == 2015 &\n                             country %in% c(\"United Kingdom\", \"Portugal\"),\n                           location+0.22, location),\n         hjust = ifelse(year == 2010, 1, 0)) %&gt;%\n  mutate(year = as.factor(year)) %&gt;%\n  ggplot(aes(year, life_expectancy, group = country)) +\n  geom_line(aes(color = country), show.legend = FALSE) +\n  geom_text_repel(aes(label = label_first, color = country), direction = 'y', nudge_x = -1, seed = 1234, show.legend = FALSE) +\n  geom_text_repel(aes(label = label_last, color = country), direction = 'y', nudge_x =  1, seed = 1234, show.legend = FALSE) +\n  xlab(\"\") + ylab(\"Life Expectancy\")\n\n\n\n\n\n\n\n\nAn advantage of the slope chart is that it permits us to quickly get an idea of changes based on the slope of the lines. Although we are using angle as the visual cue, we also have position to determine the exact values. Comparing the improvements is a bit harder with a scatterplot:\n\n\n\n\n\n\n\n\n\nIn the scatterplot, we have followed the principle use common axes since we are comparing these before and after. However, if we have many points, slope charts stop being useful as it becomes hard to see all the lines.\n\n\nBland-Altman plot\nSince we are primarily interested in the difference, it makes sense to dedicate one of our axes to it. The Bland-Altman plot, also known as the Tukey mean-difference plot and the MA-plot, shows the difference versus the average:\n\ndat %&gt;%\n  group_by(country) %&gt;%\n  filter(year %in% c(2010, 2015)) %&gt;%\n  dplyr::summarize(average = mean(life_expectancy),\n                   difference = life_expectancy[year==2015]-life_expectancy[year==2010]) %&gt;%\n  ggplot(aes(average, difference, label = country)) +\n  geom_point() +\n  geom_text_repel() +\n  geom_abline(lty = 2) +\n  xlab(\"Average of 2010 and 2015\") +\n  ylab(\"Difference between 2015 and 2010\")\n\n\n\n\n\n\n\n\nHere, by simply looking at the y-axis, we quickly see which countries have shown the most improvement. We also get an idea of the overall value from the x-axis. You already made a similar Altman plot in an earlier problem set, so we’ll move on.\n\n\nBump charts\nFinally, we can make a bump chart that shows changes in rankings over time. We’ll look at fertility in South Asia. First we need to calculate a new variable that shows the rank of each country within each year. We can do this if we group by year and then use the rank() function to rank countries by the fertility column.\n\nsa_fe &lt;- gapminder %&gt;%\n  filter(region == \"Southern Asia\") %&gt;%\n  filter(year &gt;= 2004, year &lt; 2015) %&gt;%\n  group_by(year) %&gt;%\n  mutate(rank = rank(fertility))\n\nWe then plot this with points and lines, reversing the y-axis so 1 is at the top:\n\nggplot(sa_fe, aes(x = year, y = rank, color = country)) +\n  geom_line() +\n  geom_point() +\n  scale_y_reverse(breaks = 1:8) +\n  labs(x = \"Year\", y = \"Rank\", color = \"Country\")\n\n\n\n\n\n\n\n\nIran holds the number 1 spot, while Sri Lanka dropped from 2 to 6, and Bangladesh increased from 4 to 2.\nAs with the slopegraph, there are 8 different colors in the legend and it’s hard to line them all up with the different lines, so we can plot the text directly instead. We’ll use geom_text() again. We don’t need to repel anything, since the text should fit in each row just fine. We need to change the data argument in geom_text() though and filter the data to only include one year, otherwise we’ll get labels on every point, which is excessive. We can also adjust the theme and colors to make it cleaner.\n\nbumpplot &lt;- ggplot(sa_fe, aes(x = year, y = rank, color = country)) +\n  geom_line(size = 2) +\n  geom_point(size = 4) +\n  geom_text(data = sa_fe %&gt;% dplyr::filter(year==2004) %&gt;% arrange(rank),\n            aes(label = country, x = 2003), fontface = \"bold\", angle = 45) +\n geom_text(data = sa_fe %&gt;% dplyr::filter(year==2014) %&gt;% arrange(rank),\n            aes(label = country, x = 2015), fontface = \"bold\", angle = 45) + \n  guides(color = 'none') + # another way of turning off legend\n  scale_y_reverse(breaks = 1:8) +\n  scale_x_continuous(breaks = 2004:2014) +\n  scale_color_viridis_d(option = \"C\", begin = 0.2, end = 0.9) +\n  labs(x = NULL, y = \"Rank\") +\n  theme(plot.margin = unit(c(0.5, 0.5, 0.5, 0.5),  \"inches\")) \n\nbumpplot\n\n\n\n\n\n\n\n\n\n\nThemes\nWe can go a little further towards a clean, easy-to-read data visualization by using themes in our plots. Themes allow us to set a particular range of plot settings in one command, and let us further tweak things like fonts, background colors, and much more. We’ve used them in passing a few times without highlighting them, but we’ll discuss them here.\nA pre-constructed set of instructions for making a visual theme can be had by using a theme’s ggplot function. Let’s look at two of my favorites.\ntheme_bw() uses the black-and-white theme, which is helpful in making a nice, clean plot:\n\nbumpplot + theme_bw()\n\n\n\n\n\n\n\n\nThe background shading is gone, which gives the plot a nice, crisp feel. It adds the black outline around the plot, but doesn’t mess with the colors in the plot.\nHere’s theme_minimal()\n\nbumpplot + theme_minimal()\n\n\n\n\n\n\n\n\nThemes can alter things in the plot as well. If we really want to strip it down and remove the Y-axis (which is rarely a good idea, but in a bump chart, it makes sense):\n\nbumpplot + theme_void()\n\n\n\n\n\n\n\n\nNow that’s clean!\nIn our opening unit, we had a plot that was styled after the plots in the magazine, The Economist. That’s a theme (in the ggthemes package that we loaded at the top)!\n\nbumpplot + theme_economist()\n\n\n\n\n\n\n\n\nThemes affect some of the plot elements that we haven’t gotten much into (like length of axis ticks and the color of the panel grid behind the plot). We can use a theme, then make further changes to the theme. We won’t go into a lot of detail, but here’s an example. Use the ?theme to learn more about what you can change. Half the challenge is finding the right term for the thing you want to tweak! Theme changes occur in code order, so you can update a pre-set theme with your own details:\n\nbumpplot +   theme_bw() + theme(strip.text = element_text(face = \"bold\"),\n                   plot.title = element_text(face = \"bold\"),\n                   axis.text.x = element_text(angle = 45, hjust = 1),\n                   panel.grid.major.y = element_blank(), # turn off all of the Y grid\n                   panel.grid.minor.y = element_blank(),\n                   panel.grid.minor.x = element_blank()) # turn off small x grid\n\n\n\n\n\n\n\n\n\n\nTheme elements\nThe above code looks a little odd in that asking it to leave out the minor and major Y grid, and the major X grid, required element_blank(), a function! Since a “blank” part (or a solid color part) might entail a lot of particular things, the ggplot function is necessary here to take care of all the particular details.\nRelatedly, if we wanted to change the background color of some panel (the plotted area), then element_rect() would be used because the panel is a rectangle. The theme argument would be panel.background =element_rect(fill=\"red\") if we wanted to make it red (and hideous)\n\nbumpplot +   theme_bw() + theme(strip.text = element_text(face = \"bold\"),\n                                panel.background  = element_rect(fill='red'),\n                   plot.title = element_text(face = \"bold\"),\n                   axis.text.x = element_text(angle = 45, hjust = 1),\n                   panel.grid.major.y = element_blank(), # turn off all of the Y grid\n                   panel.grid.minor.y = element_blank(),\n                   panel.grid.minor.x = element_blank()) # turn off small x grid\n\n\n\n\n\n\n\n\nWe can also set the strip.background for the “strip” rectangles that label our facet_wrap and facet_gridsections,and plot.background for the area behind the plot panel.\nIn the above example, we also set the element_text function for the axis text and specified a rotation (see the years are at 45 degrees) and a horizontal-adjustment so that they center over the line. element_text is a theme element that controls how the text looks (including font, face, color, etc). This is particularly useful when you have appropriate axis labels that might be long, like the scales::label_comma from earlier this week. We can avoid scrunching our labels by using element_text to set a new angle (45 or 90).\n\n\nSmall multiples\nFirst we can make some small multiples plots and show life expectancy over time for a handful of countries. We’ll make a list of some countries chosen at random while I scrolled through the data, and then filter our data to include only those rows. We then plot life expectancy, faceting by country.\n\nlife_expectancy_small &lt;- gapminder %&gt;%\n  filter(country %in% c(\"Argentina\", \"Bolivia\", \"Brazil\",\n                        \"Belize\", \"Canada\", \"Chile\"))\nggplot(data = life_expectancy_small,\n       mapping = aes(x = year, y = life_expectancy)) +\n  geom_line(size = 1) +\n  facet_wrap(vars(country))\n\n\n\n\n\n\n\n\nSmall multiples! That’s all we need to do.\nWe can do some fancier things, though. We can make this plot hyper minimalist with a theme:\n\nggplot(data = life_expectancy_small,\n       mapping = aes(x = year, y = life_expectancy)) +\n  geom_line(size = 1) +\n  facet_wrap(vars(country), scales = \"free_y\") +\n  theme_void() +\n  theme(strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\nWe can do a whole part of a continent (poor Syria 😿)\n\nlife_expectancy_mena &lt;- gapminder %&gt;%\n  filter(region == \"Northern Africa\" | region == \"Western Asia\")\n\nggplot(data = life_expectancy_mena,\n       mapping = aes(x = year, y = life_expectancy)) +\n  geom_line(size = 1) +\n  facet_wrap(vars(country), scales = \"free_y\", nrow = 3) +\n  theme_void() +\n  theme(strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\nWe can use the geofacet package to arrange these facets by geography:\n\nlibrary(geofacet)\n\nlife_expectancy_eu &lt;- gapminder %&gt;%\n  filter(region== 'Western Europe' | region=='Northern Europe' | region=='Southern Europe')\n\nggplot(life_expectancy_eu, aes(x = year, y = life_expectancy)) +\n  geom_line(size = 1) +\n  facet_geo(vars(country), grid = \"europe_countries_grid1\", scales = \"free_y\") +\n  labs(x = NULL, y = NULL, title = \"Life expectancy from 1960–2015\",\n       caption = \"Source: Gapminder\") +\n  theme_minimal() +\n  theme(strip.text = element_text(face = \"bold\"),\n        plot.title = element_text(face = \"bold\"),\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nNeat!\nAnybody see any problems here?",
    "crumbs": [
      "Course Content",
      "Week 04",
      "Visualizations"
    ]
  },
  {
    "objectID": "content/Week_04/04b.html#the-ecological-fallacy-and-importance-of-showing-the-data",
    "href": "content/Week_04/04b.html#the-ecological-fallacy-and-importance-of-showing-the-data",
    "title": "Visualizations",
    "section": "The ecological fallacy and importance of showing the data",
    "text": "The ecological fallacy and importance of showing the data\nThroughout this section, we have been comparing regions of the world. We have seen that, on average, some regions do better than others. In this section, we focus on describing the importance of variability within the groups when examining the relationship between a country’s infant mortality rates and average income.\nWe define a few more regions and compare the averages across regions:\n\n\n\n\n\n\n\n\n\nThe relationship between these two variables is almost perfectly linear and the graph shows a dramatic difference. While in the West less than 0.5% of infants die, in Sub-Saharan Africa the rate is higher than 6%!\nNote that the plot uses a new transformation, the logit transformation.\n\nLogit transformation\nThe logit transformation for a proportion or rate \\(p\\) is defined as:\n\\[f(p) = \\log \\left( \\frac{p}{1-p} \\right)\\]\nWhen \\(p\\) is a proportion or probability, the quantity that is being logged, \\(p/(1-p)\\), is called the odds. In this case \\(p\\) is the proportion of infants that survived. The odds tell us how many more infants are expected to survive than to die. The log transformation makes this symmetric. If the rates are the same, then the log odds is 0. Fold increases or decreases turn into positive and negative increments, respectively.\nThis scale is useful when we want to highlight differences near 0 or 1. For survival rates this is important because a survival rate of 90% is unacceptable, while a survival of 99% is relatively good. We would much prefer a survival rate closer to 99.9%. We want our scale to highlight these difference and the logit does this. Note that 99.9/0.1 is about 10 times bigger than 99/1 which is about 10 times larger than 90/10. By using the log, these fold changes turn into constant increases.\n\n\nShow the data\nNow, back to our plot. Based on the plot above, do we conclude that a country with a low income is destined to have low survival rate? Do we conclude that survival rates in Sub-Saharan Africa are all lower than in Southern Asia, which in turn are lower than in the Pacific Islands, and so on?\nJumping to this conclusion based on a plot showing averages is referred to as the ecological fallacy. The almost perfect relationship between survival rates and income is only observed for the averages at the region level. Once we show all the data, we see a somewhat more complicated story:\n\n\n\n\n\n\n\n\n\nSpecifically, we see that there is a large amount of variability. We see that countries from the same regions can be quite different and that countries with the same income can have different survival rates. For example, while on average Sub-Saharan Africa had the worse health and economic outcomes, there is wide variability within that group. Mauritius and Botswana are doing better than Angola and Sierra Leone, with Mauritius comparable to Western countries.",
    "crumbs": [
      "Course Content",
      "Week 04",
      "Visualizations"
    ]
  },
  {
    "objectID": "content/Week_04/04b.html#vaccines",
    "href": "content/Week_04/04b.html#vaccines",
    "title": "Visualizations",
    "section": "Case study: vaccines and infectious diseases",
    "text": "Case study: vaccines and infectious diseases\nVaccines have helped save millions of lives. In the 19th century, before herd immunization was achieved through vaccination programs, deaths from infectious diseases, such as smallpox and polio, were common. However, today vaccination programs have become somewhat controversial despite all the scientific evidence for their importance.\nThe controversy started with a paper5 published in 1988 and led by Andrew Wakefield claiming there was a link between the administration of the measles, mumps, and rubella (MMR) vaccine and the appearance of autism and bowel disease. Despite much scientific evidence contradicting this finding, sensationalist media reports and fear-mongering from conspiracy theorists led parts of the public into believing that vaccines were harmful. As a result, many parents ceased to vaccinate their children. This dangerous practice can be potentially disastrous given that the Centers for Disease Control (CDC) estimates that vaccinations will prevent more than 21 million hospitalizations and 732,000 deaths among children born in the last 20 years (see Benefits from Immunization during the Vaccines for Children Program Era — United States, 1994-2013, MMWR6). The 1988 paper has since been retracted and Andrew Wakefield was eventually “struck off the UK medical register, with a statement identifying deliberate falsification in the research published in The Lancet, and was thereby barred from practicing medicine in the UK.” (source: Wikipedia7). Yet misconceptions persist, in part due to self-proclaimed activists who continue to disseminate misinformation about vaccines.\nEffective communication of data is a strong antidote to misinformation and fear-mongering. Earlier we used an example provided by a Wall Street Journal article8 showing data related to the impact of vaccines on battling infectious diseases. Here we reconstruct that example.\nThe data used for these plots were collected, organized, and distributed by the Tycho Project9. They include weekly reported counts for seven diseases from 1928 to 2011, from all fifty states. The yearly totals are helpfully included in the dslabs package:\n\nlibrary(RColorBrewer)\ndata(us_contagious_diseases)\nnames(us_contagious_diseases)\n\n[1] \"disease\"         \"state\"           \"year\"            \"weeks_reporting\"\n[5] \"count\"           \"population\"     \n\n\nWe create a temporary object dat that stores only the measles data, includes a per 100,000 rate, orders states by average value of disease and removes Alaska and Hawaii since they only became states in the late 1950s. Note that there is a weeks_reporting column that tells us for how many weeks of the year data was reported. We have to adjust for that value when computing the rate.\n\nthe_disease &lt;- \"Measles\"\ndat &lt;- us_contagious_diseases %&gt;%\n  filter(!state%in%c(\"Hawaii\",\"Alaska\") & disease == the_disease) %&gt;%\n  mutate(rate = count / population * 10000 * 52 / weeks_reporting) %&gt;%\n  mutate(state = reorder(state, rate))\n\nWe can now easily plot disease rates per year. Here are the measles data from California:\n\ndat %&gt;% filter(state == \"California\" & !is.na(rate)) %&gt;%\n  ggplot(aes(year, rate)) +\n  geom_line() +\n  ylab(\"Cases per 10,000\")  +\n  geom_vline(xintercept=1963, col = \"blue\")\n\n\n\n\n\n\n\n\nWe add a vertical line at 1963 since this is when the vaccine was introduced [Control, Centers for Disease; Prevention (2014). CDC health information for international travel 2014 (the yellow book). p. 250. ISBN 9780199948505].\nNow can we show data for all states in one plot? We have three variables to show: year, state, and rate. In the WSJ figure, they use the x-axis for year, the y-axis for state, and color hue to represent rates. However, the color scale they use, which goes from yellow to blue to green to orange to red, can be improved.\nIn our example, we want to use a sequential palette since there is no meaningful center, just low and high rates.\nWe use the geometry geom_tile to tile the region with colors representing disease rates. We use a square root transformation to avoid having the really high counts dominate the plot. Notice that missing values are shown in grey. Note that once a disease was pretty much eradicated, some states stopped reporting cases all together. This is why we see so much grey after 1980.\n\ndat %&gt;% \n  mutate(state = factor(state, levels = rev(levels(state)[order(levels(state))]))) %&gt;%\n  ggplot(aes(year, state, fill = rate)) +\n  geom_tile(color = \"grey50\") +\n  scale_x_continuous(expand=c(0,0)) +\n  scale_fill_gradientn(colors = brewer.pal(9, \"Reds\"), trans = \"sqrt\") +\n  geom_vline(xintercept=1963, col = \"blue\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        legend.position=\"bottom\",\n        text = element_text(size = 8)) +\n  ggtitle(the_disease) +\n  ylab(\"\") + xlab(\"\")\n\n\n\n\n\n\n\n\nThis plot makes a very striking argument for the contribution of vaccines. However, one limitation of this plot is that it uses color to represent quantity, which we earlier explained makes it harder to know exactly how high values are going. Position and lengths are better cues. If we are willing to lose state information, we can make a version of the plot that shows the values with position. We can also show the average for the US, which we compute like this:\n\navg &lt;- us_contagious_diseases %&gt;%\n  filter(disease==the_disease) %&gt;% group_by(year) %&gt;%\n  summarize(us_rate = sum(count, na.rm = TRUE) /\n              sum(population, na.rm = TRUE) * 10000)\n\nNow to make the plot we simply use the geom_line geometry:\n\ndat %&gt;%\n  filter(!is.na(rate)) %&gt;%\n    ggplot() +\n  geom_line(aes(year, rate, group = state),  color = \"grey50\",\n            show.legend = FALSE, alpha = 0.2, size = 1) +\n  geom_line(mapping = aes(year, us_rate),  data = avg, size = 1) +\n  scale_y_continuous(trans = \"sqrt\", breaks = c(5, 25, 125, 300)) +\n  ggtitle(\"Cases per 10,000 by state\") +\n  xlab(\"\") + ylab(\"\") +\n  geom_text(data = data.frame(x = 1955, y = 50),\n            mapping = aes(x, y, label=\"US average\"),\n            color=\"black\") +\n  geom_vline(xintercept=1963, col = \"blue\")\n\n\n\n\n\n\n\n\nIn theory, we could use color to represent the categorical value state, but it is hard to pick 50 distinct colors.\n\nSaving your plot\nIn RMarkdown, we display the plot as part of the chunk output. For your group project (or for future use) you’ll likely need to save your plot as well. The function ggsave is a good way of outputting your plot.\nggsave(myPlot, file = 'myPlot.png', width = 7, height = 5, dpi=300)\nFile type will be discerned from the suffix on the file, and the height and width are in inches. The dpi argument sets the pixes-per-inch. Generally, screens display at around 150 (though this is rapidly increasing, it used to be 72 dpi), and print is usually 300 or better. A width of 5 inches at 300 dpi will render an image 1,500 pixels across.\n\n\n\n\n\n\nTRY IT\n\n\n\n\nReproduce the heatmap plot we previously made but for smallpox. For this plot, do not include years in which cases were not reported in 10 or more weeks.\nNow reproduce the time series plot we previously made, but this time following the instructions of the previous question for smallpox.\nFor the state of California, make a time series plot showing rates for all diseases. Include only years with 10 or more weeks reporting. Use a different color for each disease.\nNow do the same for the rates for the US. Hint: compute the US rate by using summarize: the total divided by total population.",
    "crumbs": [
      "Course Content",
      "Week 04",
      "Visualizations"
    ]
  },
  {
    "objectID": "content/Week_04/04b.html#footnotes",
    "href": "content/Week_04/04b.html#footnotes",
    "title": "Visualizations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Hans_Rosling↩︎\nhttp://www.gapminder.org/↩︎\nhttps://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en↩︎\nhttps://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen↩︎\nhttp://www.thelancet.com/journals/lancet/article/PIIS0140-6736(97)11096-0/abstract↩︎\nhttps://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm↩︎\nhttps://en.wikipedia.org/wiki/Andrew_Wakefield↩︎\nhttp://graphics.wsj.com/infectious-diseases-and-vaccines/↩︎\nhttp://www.tycho.pitt.edu/↩︎",
    "crumbs": [
      "Course Content",
      "Week 04",
      "Visualizations"
    ]
  },
  {
    "objectID": "content/Week_03/03b.html",
    "href": "content/Week_03/03b.html",
    "title": "ggplot2: Everything you ever wanted to know",
    "section": "",
    "text": "On Tuesday, I let us go early as I thought a part of our material was missing. It was not. Let’s jump to the end of Tuesday and talk about setting color palettes briefly.",
    "crumbs": [
      "Course Content",
      "Week 03",
      "ggplot2: Everything you ever wanted to know"
    ]
  },
  {
    "objectID": "content/Week_03/03b.html#variable-types",
    "href": "content/Week_03/03b.html#variable-types",
    "title": "ggplot2: Everything you ever wanted to know",
    "section": "Variable types",
    "text": "Variable types\nWe will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.\nWhen each entry in a vector comes from one of a small number of groups, we refer to the data as categorical data. Two simple examples are sex (male or female) and regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers per se, such as spiciness (mild, medium, hot). In statistics textbooks, ordered categorical data are referred to as ordinal data. In psychology, a number of different terms are used for this same idea.\nExamples of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches, respectively. Counts, such as population sizes, are discrete because they have to be integers—that’s how we count.1",
    "crumbs": [
      "Course Content",
      "Week 03",
      "ggplot2: Everything you ever wanted to know"
    ]
  },
  {
    "objectID": "content/Week_03/03b.html#case-study-describing-student-heights",
    "href": "content/Week_03/03b.html#case-study-describing-student-heights",
    "title": "ggplot2: Everything you ever wanted to know",
    "section": "Case study: describing student heights",
    "text": "Case study: describing student heights\nHere we consider an artificial problem to help us illustrate the underlying concepts.\nPretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the heights data frame:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(heights)\n\nOne way to convey the heights to ET is to simply send him this list of 1050 heights. But there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights.\n\nDistribution function\nIt turns out that, in some cases, the average and the standard deviation are pretty much all we need to understand the data. We will learn data visualization techniques that will help us determine when this two number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.\nThe most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for readers of this book. For example, with categorical data, the distribution simply describes the proportion of each unique category. The sex represented in the heights dataset is:\n\n\n\n   Female      Male \n0.2266667 0.7733333 \n\n\nThis two-category frequency table is the simplest form of a distribution. We don’t really need to visualize it since one number describes everything we need to know: 23% are females and the rest are males. When there are more categories, then a simple barplot describes the distribution. Here is an example with US state regions:\n\n\n\n\n\n\n\n\n\nThis particular plot simply shows us four numbers, one for each category. We usually use barplots to display a few numbers. Although this particular plot does not provide much more insight than a frequency table itself, it is a first example of how we convert a vector into a plot that succinctly summarizes all the information in the vector. When the data is numerical, the task of displaying distributions is more challenging.\n\n\nCumulative distribution functions\nNumerical data that are not categorical also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of 68.503937007874 inches and only one student reported a height 68.8976377952756 inches. We assume that they converted from 174 and 175 centimeters, respectively.\nStatistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below \\(a\\) for all possible values of \\(a\\). This function is called the cumulative distribution function (CDF). In statistics, the following notation is used:\n\\[ F(a) = \\mbox{Pr}(x \\leq a) \\]\nHere is a plot of \\(F\\) for the male height data:\n\n\n\n\n\n\n\n\n\nSimilar to what the frequency table does for categorical data, the CDF defines the distribution for numerical data. From the plot, we can see that 16% of the values are below 65, since \\(F(66)=\\) 0.1637931, or that 84% of the values are below 72, since \\(F(72)=\\) 0.841133, and so on. In fact, we can report the proportion of values between any two heights, say \\(a\\) and \\(b\\), by computing \\(F(b) - F(a)\\). This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousand words”, in this case, a picture is as informative as 812 numbers.\nA final note: because CDFs can be defined mathematically—and absent any data—the word empirical is added to make the distinction when data is used. We therefore use the term empirical CDF (eCDF).",
    "crumbs": [
      "Course Content",
      "Week 03",
      "ggplot2: Everything you ever wanted to know"
    ]
  },
  {
    "objectID": "content/Week_03/03b.html#geometries-for-describing-distributions",
    "href": "content/Week_03/03b.html#geometries-for-describing-distributions",
    "title": "ggplot2: Everything you ever wanted to know",
    "section": "Geometries for describing distributions",
    "text": "Geometries for describing distributions\nNow, we’ll introduce ggplot geometries useful for describing distributions (or for many other things).\n\nHistograms\nAlthough the CDF concept is widely discussed in statistics textbooks, the plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? I doubt you can figure these out from glancing at the plot above. Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce plots that are much easier to interpret.\nThe simplest way to make a histogram is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: \\((49.5, 50.5],(50.5, 51.5],(51.5,52.5],(52.5,53.5],...,(82.5,83.5]\\)\n\nheights %&gt;%\n  filter(sex==\"Male\") %&gt;%\n  ggplot(aes(x = height)) +\n  geom_histogram(binwidth = 1, color = \"black\")\n\n\n\n\n\n\n\n\nIf we send this histogram plot to some uninformed reader, she will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, this reader could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 heights with about 30 bin counts.\nWhat information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers.\nThe geom_histogram layer only requires one aesthetic mapping - the x-axis. This is because the y-axis is computed from counts of the x-axis. Giving an aesthetic mapping to an additional variable for y will result in an error. Using an aesthetic mapping like fill will work - it’ll give you two histograms on top of each other. Try it! Try setting the alpha aesthetic to .5 (not an aesthetic mapping) so you can see both layers when they overlap.\n\n\nSmoothed density\nSmooth density plots are aesthetically more appealing than histograms. geom_density is the geometry that gives a smoothed density. Here is what a smooth density plot looks like for our heights data:\n\nheights %&gt;%\n  filter(sex==\"Male\") %&gt;%\n  ggplot(aes(height)) +\n  geom_density(alpha = .2, fill= \"#00BFC4\", color = 'gray50')  \n\n\n\n\n\n\n\n\nIn this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to density. That is, the area under the curve will add up to 1, so we can read it like a probability density.\nTo understand the smooth densities, we have to understand estimates, a topic we don’t cover until later. However, we provide a heuristic explanation to help you understand the basics so you can use this useful data visualization tool.\nThe main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of 812 male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let’s say there are 1,000,000 of these measurements. This list of values has a distribution, like any list of values, and this larger distribution is really what we want to report to ET since it is much more general. Unfortunately, we don’t get to see it.\nHowever, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps in the heights of consecutive bins. Below we have a hypothetical histogram with bins of size 1:\n\n\n\n\n\n\n\n\n\nThe smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5, and 0.1:\n\n\n\n\n\n\n\n\n\nThe smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts. We do this by using the double-dot object ..density... Objects surrounded by .. are objects that are calculated by ggplot. If we look at ?geom_histogram, and go down to “Computed variables”, we see that we could use ..count.. to get “number of points in a bin”; ..ncount.. for the count scaled to a max of 1; or ..ndensity.. which scales the density to a max of 1 (which is a strange one). We can manually set the y aesthetic mapping, which defaults to ..count.., to ..density..:\n\nx %&gt;% ggplot(aes(x = height)) +\n  geom_histogram(aes(y=..density..), binwidth = 0.1, color = \"black\") \n\n\n\n\n\n\n\n\nNow, back to reality. We don’t have millions of measurements. In this concrete example, we have 812 and we can’t make a histogram with very small bins.\nWe therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars. The following plots (loosely) demonstrate the steps that the computer goes through to ultimately create a smooth density:\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the y-axis\nNote that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:\n\n\n\n\n\n\n\n\n\nThe proportion of this area is about 0.3, meaning that about 30% of male heights are between 65 and 68 inches.\nBy understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:\n\nheights %&gt;%\n  filter(sex==\"Male\") %&gt;%\n  ggplot(aes(x = height)) +\n  geom_density(alpha=.2, fill= \"#00BFC4\", color = 'black') \n\n\n\n\n\n\n\n\nNote that the only aesthetic mapping is x = height, while the fill and color are set as un-mapped aesthetics.\n\n\nDensities permit stratification\nAs a final note, we point out that an advantage of smooth densities over histograms for visualization purposes is that densities make it easier to compare two distributions. This is in large part because the jagged edges of the histogram add clutter. Here is an example comparing male and female heights:\n\nheights %&gt;%\n  ggplot(aes(height, fill=sex)) +\n  geom_density(alpha = 0.2, color = 'black')\n\n\n\n\n\n\n\n\nWith the right argument, ggplot automatically shades the intersecting region with a different color.",
    "crumbs": [
      "Course Content",
      "Week 03",
      "ggplot2: Everything you ever wanted to know"
    ]
  },
  {
    "objectID": "content/Week_03/03b.html#normal-distribution",
    "href": "content/Week_03/03b.html#normal-distribution",
    "title": "ggplot2: Everything you ever wanted to know",
    "section": "The normal distribution",
    "text": "The normal distribution\nHistograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.\nThe normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these later. Here we focus on how the normal distribution helps us summarize data.\nRather than using data, the normal distribution is defined with a mathematical formula. For any interval \\((a,b)\\), the proportion of values in that interval can be computed using this formula:\n\\[\\mbox{Pr}(a &lt; x &lt; b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}s} e^{-\\frac{1}{2}\\left( \\frac{x-m}{s} \\right)^2} \\, dx\\]\nYou don’t need to memorize or understand the details of the formula. But note that it is completely defined by just two parameters: \\(m\\) and \\(s\\). The rest of the symbols in the formula represent the interval ends that we determine, \\(a\\) and \\(b\\), and known mathematical constants \\(\\pi\\) and \\(e\\). These two parameters, \\(m\\) and \\(s\\), are referred to as the average (also called the mean) and the standard deviation (SD) of the distribution, respectively.\nThe distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average. Here is what the normal distribution looks like when the average is 0 and the SD is 1:\n\n\n\n\n\n\n\n\n\nThe fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the average and the standard deviation. We now define these values for an arbitrary list of numbers.\nFor a list of numbers contained in a vector x, the average is defined as:\n\nm &lt;- sum(x) / length(x)\n\nand the SD is defined as:\n\ns &lt;- sqrt(sum((x-mu)^2) / length(x))\n\nwhich can be interpreted as the average distance between values and their average.\nLet’s compute the values for the height for males which we will store in the object \\(x\\):\n\nindex &lt;- heights$sex == \"Male\"\nx &lt;- heights$height[index]\n\nThe pre-built functions mean and sd (note that for reasons explained later, sd divides by length(x)-1 rather than length(x)) can be used here:\n\nm &lt;- mean(x)\ns &lt;- sd(x)\nc(average = m, sd = s)\n\n  average        sd \n69.314755  3.611024 \n\n\nHere is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:\n\n\n\n\n\n\n\n\n\nNow, we can ask the question “is our height data approximately normally distributed?”. The normal distribution does appear to be quite a good approximation here. We now will see how well this approximation works at predicting the proportion of values within intervals.\n\nStandard units\nFor data that is approximately normally distributed, it is convenient to think in terms of standard units. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value x from a vector X, we define the value of x in standard units as z = (x - m)/s with m and s the average and standard deviation of X, respectively. Why is this convenient?\nFirst look back at the formula for the normal distribution and note that what is being exponentiated is \\(-z^2/2\\) with \\(z\\) equivalent to \\(x\\) in standard units. Because the maximum of \\(e^{-z^2/2}\\) is when \\(z=0\\), this explains why the maximum of the distribution occurs at the average. It also explains the symmetry since \\(- z^2/2\\) is symmetric around 0. Second, note that if we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average (\\(z=0\\)), one of the largest (\\(z \\approx 2\\)), one of the smallest (\\(z \\approx -2\\)), or an extremely rare occurrence (\\(z &gt; 3\\) or \\(z &lt; -3\\)). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.\nIn R, we can obtain standard units using the function scale:\n\nz &lt;- scale(x)\n\nNow to see how many men are within 2 SDs from the average, we simply type:\n\nmean(abs(z) &lt; 2)\n\n[1] 0.9495074\n\n\nThe proportion is about 95%, which is what the normal distribution predicts! To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots.\n\n\nQuantile-quantile plots\nA systematic way to assess how well the normal distribution fits the data is to check if the observed and predicted proportions match. In general, this is the approach of the quantile-quantile plot (QQ-plot). If our heights distribution is really normal, then the 10th percentile of our heights data should be the same as the 10th percentile of a theoretical normal, as should the 20th, 30th, 33rd, 37.5th, etc. percentiles.\nFirst let’s define the theoretical quantiles (percentiles) for the normal distribution. In statistics books we use the symbol \\(\\Phi(x)\\) to define the function that gives us the probability of a standard normal distribution being smaller than \\(x\\). So, for example, \\(\\Phi(-1.96) = 0.025\\) and \\(\\Phi(1.96) = 0.975\\). In R, we can evaluate \\(\\Phi\\) using the pnorm function:\n\npnorm(-1.96)\n\n[1] 0.0249979\n\n\nThe inverse function \\(\\Phi^{-1}(x)\\) gives us the theoretical quantiles for the normal distribution. So, for example, \\(\\Phi^{-1}(0.975) = 1.96\\). In R, we can evaluate the inverse of \\(\\Phi\\) using the qnorm function.\n\nqnorm(0.975)\n\n[1] 1.959964\n\n\nNote that these calculations are for the standard normal distribution by default (mean = 0, standard deviation = 1), but we can also define these for any normal distribution. We can do this using the mean and sd arguments in the pnorm and qnorm function. For example, we can use qnorm to determine quantiles of a distribution with a specific average and standard deviation\n\nqnorm(0.975, mean = 5, sd = 2)\n\n[1] 8.919928\n\n\nFor the normal distribution, all the calculations related to quantiles are done without data, thus the name theoretical quantiles. But quantiles can be defined for any distribution, including an empirical one. So if we have data in a vector \\(x\\), we can define the quantile associated with any proportion \\(p\\) as the \\(q\\) for which the proportion of values below \\(q\\) is \\(p\\). Using R code, we can define q as the value for which mean(x &lt;= q) = p. Notice that not all \\(p\\) have a \\(q\\) for which the proportion is exactly \\(p\\). There are several ways of defining the best \\(q\\) as discussed in the help for the quantile function.\nTo give a quick example, for the male heights data, we have that:\n\nmean(x &lt;= 69.5)\n\n[1] 0.5147783\n\n\nSo about 50% are shorter or equal to 69 inches. This implies that if \\(p=0.50\\) then \\(q=69.5\\).\nThe idea of a QQ-plot is that if your data is well approximated by normal distribution then the quantiles of your data should be similar to the quantiles of a normal distribution. To construct a QQ-plot, we do the following:\n\nDefine a vector of \\(m\\) proportions \\(p_1, p_2, \\dots, p_m\\).\nDefine a vector of quantiles \\(q_1, \\dots, q_m\\) for your data for the proportions \\(p_1, \\dots, p_m\\). We refer to these as the sample quantiles.\nDefine a vector of theoretical quantiles for the proportions \\(p_1, \\dots, p_m\\) for a normal distribution with the same average and standard deviation as the data.\nPlot the sample quantiles versus the theoretical quantiles.\n\nLet’s construct a QQ-plot using R code. Start by defining the vector of proportions.\n\np &lt;- seq(0.005, 0.995, 0.01)\n\nTo obtain the quantiles from the data, we can use the quantile function like this:\n\nsample_quantiles &lt;- quantile(x, p)\n\nTo obtain the theoretical normal distribution quantiles with the corresponding average and SD, we use the qnorm function:\n\ntheoretical_quantiles &lt;- qnorm(p, mean = mean(x), sd = sd(x))\n\ndf = data.frame(sample_quantiles, theoretical_quantiles)\n\nTo see if they match or not, we plot them against each other and draw the identity line:\n\nggplot(data = df, aes(x = theoretical_quantiles, y = sample_quantiles)) + \n  geom_point() + \n  geom_abline() # a 45-degree line \n\n\n\n\n\n\n\n\nNotice that this code becomes much cleaner if we use standard units:\n\nsample_quantiles &lt;- quantile(z, p)\ntheoretical_quantiles &lt;- qnorm(p)\ndf2 =  data.frame(sample_quantiles, theoretical_quantiles)\n\nggplot(data = df2, aes(x = theoretical_quantiles, y = sample_quantiles)) + \n  geom_point() + \n  geom_abline()\n\n\n\n\n\n\n\n\nThe above code is included to help describe QQ-plots. However, in practice it is easier to use the ggplot geometry geom_qq:\n\nheights %&gt;% filter(sex == \"Male\") %&gt;%\n  ggplot(aes(sample = scale(height))) +\n  geom_qq() +\n  geom_abline()\n\n\n\n\n\n\n\n\nWhile for the illustration above we used 100 quantiles, the default from the geom_qq function is to use as many quantiles as data points.\n\n\nPercentiles\nBefore we move on, let’s define some terms that are commonly used in exploratory data analysis.\nPercentiles are special cases of quantiles that are commonly used. The percentiles are the quantiles you obtain when setting the \\(p\\) at \\(0.01, 0.02, ..., 0.99\\). We call, for example, the case of \\(p=0.25\\) the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the median.\nFor the normal distribution the median and average are the same, but this is generally not the case.\nAnother special case that receives a name are the quartiles, which are obtained when setting \\(p=0.25,0.50\\), and \\(0.75\\).",
    "crumbs": [
      "Course Content",
      "Week 03",
      "ggplot2: Everything you ever wanted to know"
    ]
  },
  {
    "objectID": "content/Week_03/03b.html#other-geometries",
    "href": "content/Week_03/03b.html#other-geometries",
    "title": "ggplot2: Everything you ever wanted to know",
    "section": "ggplot2 geometries",
    "text": "ggplot2 geometries\nWe now will briefly discuss some of the geometries involved in the plots above. We will discuss ggplot2 in (excruciating) detail into next week. For now, we will briefly demonstrate how to generate plots related to distributions.\n\nBarplots\nTo generate a barplot we can use the geom_bar geometry. The default is to count the number of each category and draw a bar. Here is the plot for the regions of the US.\n\nmurders %&gt;% ggplot(aes(region)) + geom_bar()\n\n\n\n\n\n\n\n\nWe often already have a table with a distribution that we want to present as a barplot. Here is an example of such a table:\n\ndata(murders)\ntab &lt;- murders %&gt;%\n  count(region) %&gt;%\n  mutate(proportion = n/sum(n))\ntab\n\n         region  n proportion\n1     Northeast  9  0.1764706\n2         South 17  0.3333333\n3 North Central 12  0.2352941\n4          West 13  0.2549020\n\n\nWe no longer want geom_bar to count, but rather just plot a bar to the height provided by the proportion variable. For this we need to provide x (the categories) and y (the values) and use the stat=\"identity\" option. This tells R to just use the actual value in proportion for the y aesthetic. This is only necessary when you’re telling R that you have your own field (proportion) that you want to use instead of just the count.\n\ntab %&gt;% ggplot(aes(x = region, y = proportion)) + geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\n\nHistograms\nTo generate histograms we use geom_histogram. By looking at the help file for this function, we learn that the only required argument is x, the variable for which we will construct a histogram. We dropped the x because we know it is the first argument. The code looks like this:\n\nheights %&gt;%\n  filter(sex == \"Female\") %&gt;%\n  ggplot(aes(height)) +\n  geom_histogram()\n\nIf we run the code above, it gives us a message:\n\nstat_bin() using bins = 30. Pick better value with binwidth.\n\nWe previously used a bin size of 1 inch (of observed height), so the code looks like this:\n\nheights %&gt;%\n  filter(sex == \"Female\") %&gt;%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1)\n\nFinally, if for aesthetic reasons we want to add color, we use the arguments described in the help file. We also add labels and a title:\n\nheights %&gt;%\n  filter(sex == \"Female\") %&gt;%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", col = \"black\") +\n  labs(x = \"Male heights in inches\", title = \"Histogram\")\n\n\n\n\n\n\n\n\n\n\nDensity plots\nTo create a smooth density, we use the geom_density. To make a smooth density plot with the data previously shown as a histogram we can use this code:\n\nheights %&gt;%\n  filter(sex == \"Female\") %&gt;%\n  ggplot(aes(x = height)) +\n  geom_density()\n\nTo fill in with color, we can use the fill argument.\n\nheights %&gt;%\n  filter(sex == \"Female\") %&gt;%\n  ggplot(aes(x = height)) +\n  geom_density(fill=\"blue\")\n\n\n\n\n\n\n\n\nTo change the smoothness of the density, we use the adjust argument to multiply the default value by that adjust. For example, if we want the bandwidth to be twice as big we use:\n\nheights %&gt;%\n  filter(sex == \"Female\") %&gt;%\n  ggplot(aes(x = height)) + \n  geom_density(fill=\"blue\", adjust = 2)\n\n\n\nBoxplots\nThe geometry for boxplot is geom_boxplot. As discussed, boxplots are useful for comparing distributions. For example, below are the previously shown heights for women, but compared to men. For this geometry, we need arguments x as the categories, and y as the values.\n\n\n\n\n\n\n\n\n\nNote that our x-axis is a categorical variable. The order is determined by either the factor variable levels in heights or, if no levels are set, in the order in which the sex variable first encounters them. Later on, we’ll learn how to change the ordering.\nWe can do much more with boxplots when we have more data. Right now, our heights data has only two variables - sex and height. Let’s say we took the measurements over two different years - 2010 and 2020. That’s not in our data, so purely for exposition, we’ll add it by randomly drawing a year for each observation. We’ll do this with sample\n\nheights = heights %&gt;%\n  dplyr::mutate(year = sample(x = c(2010, 2020), size = n(), replace = TRUE, prob = c(.5, .5)))\n\nhead(heights)\n\n     sex height year\n1   Male     75 2020\n2   Male     70 2010\n3   Male     68 2020\n4   Male     74 2010\n5   Male     61 2020\n6 Female     65 2010\n\n\nNow, let’s look at the boxplot of heights by sex, but broken out by year. We can do this by adding year as an aesthetic mapping. Because our year variable is an integer, R will start by thinking it’s a continuous numeric, but we want to treat it as a discrete variable. So, we wrap it in as.factor() to force R to recognize it as a discrete variable.\n\nheights %&gt;% ggplot(aes(x = sex, y = height, fill = as.factor(year))) +\n  geom_boxplot() +\n  labs(fill = 'Year')\n\n\n\n\n\n\n\n\nNow we have each sex broken out by year! Since we randomly assigned year to our data (and didn’t actually take samples in two different decades), the distribution between years and within sex is nearly identical.\nWhat if we wanted to have year on the x-axis, but then put the sex boxplots next to each other. This would let us compare the difference in heights by sex over the two sample years.\n\nheights %&gt;% ggplot(aes(x = year, y = height, fill = sex)) + \n  geom_boxplot() +\n  labs(fill = 'Sex')\n\n\n\n\n\n\n\n\nWoah. Wait. What? Remember, in our data, class(heights$year) is numeric, so when we ask R to put year on the x-axis, it thinks it’s plotting a number. It gives us a nonsense x-axis. How do we fix this? We force as.factor(year) to tell R that yes, year is a categorical variable. Note that we didn’t have to use as.factor(sex) - that’s because sex is already a categorical variable.\n\nheights %&gt;% ggplot(aes(x = as.factor(year), y = height, fill = sex)) + \n  geom_boxplot() +\n  labs(fill = 'Sex')\n\n\n\n\n\n\n\n\nNow we can see the height difference by sex, by year.\nWe will explore more with boxplots and colors in our next lecture.\n\n\n\n\n\n\n\n\nTry it!\n\n\n\nStart by loading the dplyr and ggplot2 library as well as the murders and heights data.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(dslabs)\ndata(heights)\ndata(murders)\n\n\nFirst, create a new variable in murders that has murders_per_capita.\n\n\nmurders = murders %&gt;%\n  mutate(........)\n\n\nMake a histogram of murders per capita. Use the default values for color and fill, but make sure you label the x-axis with a meaningful label.\nMake the same histogram, but set the fill aesthetic to MSU Green and the color to black.\nDo the same, but make it a smooth density plot\nFinally, plot the smooth density but use a fill aesthetic mapping so that each region’s density is shown. Set a meaningful title on the legend, and make sure you make the density transparent so we can see all of the region’s densities (see alpha aesthetic).\nNow, try making a boxplot to show the same data - the distribution across states of murders per capita by region. What is the average Northeastern state’s murder rate? What about the average Southern state?",
    "crumbs": [
      "Course Content",
      "Week 03",
      "ggplot2: Everything you ever wanted to know"
    ]
  },
  {
    "objectID": "content/Week_03/03b.html#footnotes",
    "href": "content/Week_03/03b.html#footnotes",
    "title": "ggplot2: Everything you ever wanted to know",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKeep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable. But, indeed, there are examples that can be considered both numerical and ordinal when it comes to visualizing data.↩︎",
    "crumbs": [
      "Course Content",
      "Week 03",
      "ggplot2: Everything you ever wanted to know"
    ]
  },
  {
    "objectID": "content/Week_02/02b.html",
    "href": "content/Week_02/02b.html",
    "title": "Introduction to Visualization",
    "section": "",
    "text": "TRY IT\n\n\n\n\nThe mpg dataset is contained in ggplot2 (part of tidyverse which we installed earlier), so mpg = ggplot2::mpg will create the mpg dataset. Do this first.\nUsing conditionals we learned on Tuesday (case_when or maybe between or any other way you choose) and using the %&gt;% pipe, do the following:\n\n\nKeep only the year 2008\nCreate a column called engine and set it equal to “small” if it has fewer than 8 cylinders (cyl) and “large” if it’s 8 or more\nSummarize the data so that it reports the average highway fuel economy (hwy) and the average city fuel economy (cty) for each engine group\n\nUse the zoom screenshare when you’re done: bit.ly/EC242",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to Visualization"
    ]
  },
  {
    "objectID": "content/Week_02/02b.html#the-components-of-a-graph",
    "href": "content/Week_02/02b.html#the-components-of-a-graph",
    "title": "Introduction to Visualization",
    "section": "The components of a graph",
    "text": "The components of a graph\nWe will eventually construct a graph that summarizes the US murders dataset that looks like this:\n\n\n\n\n\n\n\n\n\nWe can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.\nThis data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.\nThe first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main five components to note are:\n\nData: The US murders data table is being summarized. We refer to this as the data component.\nGeometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, boxplot, pie (ew!), and many, many more. We will learn about these later.\nAesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using.\nAnnotations: These are things like axis labels, axis ticks (the lines along the axis at regular intervals or specific points of interest), axis scales (e.g. log-scale), titles, legends, etc.\nStyle: An overall appearance of the graph determined by fonts, color palattes, layout, blank spaces, and more.\n\nWe also note that:\n\nThe points are labeled with the state abbreviations.\nThe range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.\nThere are labels, a title, a legend, and we use the style of The Economist magazine.\n\nAll of the flexibility and visualization power of ggplot is contained in these four elements (plus your data)",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to Visualization"
    ]
  },
  {
    "objectID": "content/Week_02/02b.html#ggplot-objects",
    "href": "content/Week_02/02b.html#ggplot-objects",
    "title": "Introduction to Visualization",
    "section": "ggplot objects",
    "text": "ggplot objects\nWe will now construct the plot piece by piece.\nWe start by loading the dataset:\n\nlibrary(dslabs)\ndata(murders)\n\nThe first step in creating a ggplot2 graph is to define a ggplot object. We do this with the function ggplot, which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object:\n\nggplot(data = murders)\n\nWe can also pipe the data in as the first argument. So this line of code is equivalent to the one above:\n\nmurders %&gt;% ggplot()\n\n\n\n\n\n\n\n\nIt renders a plot, in this case a blank slate since no geometry has been defined. The only style choice we see is a grey background.\nWhat has happened above is that the object was created and, because it was not assigned, it was automatically evaluated. But we can assign our plot to an object, for example like this:\n\np &lt;- ggplot(data = murders)\nclass(p)\n\n[1] \"gg\"     \"ggplot\"\n\n\nTo render the plot associated with this object, we simply print the object p. The following two lines of code each produce the same plot we see above:\n\nprint(p)\np",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to Visualization"
    ]
  },
  {
    "objectID": "content/Week_02/02b.html#geometries-briefly",
    "href": "content/Week_02/02b.html#geometries-briefly",
    "title": "Introduction to Visualization",
    "section": "Geometries (briefly)",
    "text": "Geometries (briefly)\nIn ggplot2 we create graphs by adding geometry layers. Layers can define geometries, compute summary statistics, define what scales to use, create annotations, or even change styles. To add layers, we use the symbol +. In general, a line of code will look like this:\n\nDATA %&gt;% ggplot() + LAYER 1 + LAYER 2 + ... + LAYER N\n\nUsually, the first added layer after ggplot() + defines the geometry. After that, we may add additional geometries, we may rescale an axis, we may add annotations and labels, or we may change the style. For now, we want to make a scatterplot like the one you all created in your first lab. What geometry do we use?\n\n\n\n\n\n\n\n\n\nTaking a quick look at the cheat sheet, we see that the ggplot2 function used to create plots with this geometry is geom_point.\nSee Here\nGeometry function names follow the pattern: geom_X where X is the name of some specific geometry. Some examples include geom_point, geom_bar, and geom_histogram. You’ve already seen a few of these. We will start with a scatterplot created using geom_point() for now, then circle back to more geometries after we cover aesthetic mappings, layers, and annotations.\nFor geom_point to run properly we need to provide data and an aesthetic mapping. The simplest mapping for a scatter plot is to say we want one variable on the X-axis, and a different one on the Y-axis, so each point is an {X,Y} pair. That is an aesthetic mapping because X and Y are aesthetics in a geom_point scatterplot.\nWe have already connected the object p with the murders data table, and if we add the layer geom_point it defaults to using this data. To find out what mappings are expected, we read the Aesthetics section of the help file ?geom_point help file:\n&gt; Aesthetics\n&gt;\n&gt; geom_point understands the following aesthetics (required aesthetics are in bold):\n&gt;\n&gt; **x**\n&gt;\n&gt; **y**\n&gt;\n&gt; alpha\n&gt;\n&gt; colour\n&gt;\n&gt; fill\n&gt;\n&gt; group\n&gt;\n&gt; shape\n&gt;\n&gt; size\n&gt;\n&gt; stroke\nand—although it does not show in bold above—we see that at least two arguments are required: x and y. You can’t have a geom_point scatterplot unless you state what you want on the X and Y axes.",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to Visualization"
    ]
  },
  {
    "objectID": "content/Week_02/02b.html#aesthetic-mappings",
    "href": "content/Week_02/02b.html#aesthetic-mappings",
    "title": "Introduction to Visualization",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\nAesthetic mappings describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The aes function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the aes function is often used as the argument of a geometry function. This example produces a scatterplot of population in millions (x-axis) versus total murders (y-axis):\n\nmurders %&gt;% ggplot() +\n  geom_point(aes(x = population/10^6, y = total))\n\nInstead of defining our plot from scratch, we can also add a layer to the p object that was defined above as p &lt;- ggplot(data = murders):\n\np + geom_point(aes(x = population/10^6, y = total))\n\n\n\n\n\n\n\n\nThe scales and annotations like axis labels are defined by default when adding this layer (note the x-axis label is exactly what we wrote in the function call). Like dplyr functions, aes also uses the variable names from the object component: we can use population and total without having to call them as murders$population and murders$total. The behavior of recognizing the variables from the data component is quite specific to aes. With most functions, if you try to access the values of population or total outside of aes you receive an error.\nNote that we did some rescaling within the aes() call - we can do simple things like multiplication or division on the variable names in the ggplot call. The axis labels reflect this. We will change the axis labels later.\nThe aesthetic mappings are very powerful - changing the variable in x= or y= changes the meaning of the plot entirely. We’ll come back to additional aesthetic mappings once we talk about aesthetics in general.\n\nAesthetics in general\nEven without mappings, a plots aesthetics can be useful. Things like color, fill, alpha, and size are aesthetics that can be changed.\nLet’s say we want larger points in our scatterplot. The size aesthetic can be used to set the size. The scale of size is “multiples of the defaults” (so size = 1 is the default)\n\np + geom_point(aes(x = population/10^6, y = total), size = 3)\n\n\n\n\n\n\n\n\nsize is an aesthetic, but here it is not a mapping so it is not in the aes() part: whereas mappings use data from specific observations and need to be inside aes(), operations we want to affect all the points the same way do not need to be included inside aes. We’ll see what happens if size is inside aes(size = xxx) in a second.\nWe can change the shape aesthetic to one of the many different base-R options found here:\n\np + geom_point(aes(x = population/10^6, y = total), size = 3, shape = 17)\n\n\n\n\n\n\n\n\nWe can also change the fill and the color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 4, shape = 23, fill = '#18453B')\n\n\n\n\n\n\n\n\nfill can take a common name like 'green', or can take a hex color like '#18453B', which is MSU Green according to MSU’s branding site. You can also find UM Maize and OSU Scarlet on respective branding pages, or google “XXX color hex.” We’ll learn how to build a color palatte later on.\ncolor (or colour, same thing because ggplot creators allow both spellings) is a little tricky with points - it changes the outline of the geometry rather than the fill color, but in geom_point() most shapes are only the outline, including the default. This is more useful with, say, a barplot where the outline and the fill might be different colors. Still, shapes 21-25 have both fill and color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 5, shape = 23, fill = '#18453B', color = 'white')\n\n\n\n\n\n\n\n\nThe color = 'white' makes the outline of the shape white, which you can see if you look closely in the areas where the shapes overlap. This only works with shapes 21-25, or any other geometry that has both an outline and a fill.\n\n\nNow, back to aesthetic mappings\nNow that we’ve seen a few aesthetics (and know we can find more by looking at which aesthetics work with our geometry in the help file), let’s return to the power of aesthetic mappings.\nAn aesthetic mapping means we can vary an aesthetic (like fill or shape or size) according to some variable in our data. This opens up a world of possibilities! Let’s try adding to our x and y aesthetics with a color aesthetic (since points respond to color better than fill) that varies by region, which is a column in our data:\n\np + geom_point(aes(x = population/10^6, y = total, color = region), size = 3)",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to Visualization"
    ]
  },
  {
    "objectID": "content/Week_02/02b.html#aesthetics-vs.-aesthetic-mappings",
    "href": "content/Week_02/02b.html#aesthetics-vs.-aesthetic-mappings",
    "title": "Introduction to Visualization",
    "section": "Aesthetics vs. Aesthetic mappings",
    "text": "Aesthetics vs. Aesthetic mappings\nLots of features are aesthetics (color, size, etc.) but if we want to have them change according to the data, we have to use an aesthetic mapping.",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to Visualization"
    ]
  },
  {
    "objectID": "content/Week_02/02b.html#annotation-layers",
    "href": "content/Week_02/02b.html#annotation-layers",
    "title": "Introduction to Visualization",
    "section": "Annotation Layers",
    "text": "Annotation Layers\nA second layer in the plot we wish to make involves adding a label to each point to identify the state. The geom_label and geom_text functions permit us to add text to the plot with and without a rectangle behind the text, respectively.\nBecause each point (each state in this case) has a label, we need an aesthetic mapping to make the connection between points and labels. By reading the help file ?geom_text, we learn that we supply the mapping between point and label through the label argument of aes. That is, label is an aesthetic that we can map. So the code looks like this:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\n\n\n\n\n\n\n\nWe have successfully added a second layer to the plot.\nAs an example of the unique behavior of aes mentioned above, note that this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total, label = abb))\n\nis fine, whereas this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total), label = abb)\n\nwill give you an error since abb is not found because it is outside of the aes function. The layer geom_text does not know where to find abb since it is a column name and not a global variable, and ggplot does not look for column names for non-mapped aesthetics. For a trivial example:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(population/10^6, total), label = 'abb')\n\n\n\n\n\n\n\n\n\nGlobal versus local aesthetic mappings\nIn the previous line of code, we define the mapping aes(population/10^6, total) twice, once in each geometry. We can avoid this by using a global aesthetic mapping. We can do this when we define the blank slate ggplot object. Remember that the function ggplot contains an argument that permits us to define aesthetic mappings:\n\nargs(ggplot)\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \nNULL\n\n\nIf we define a mapping in ggplot, all the geometries that are added as layers will default to this mapping. We redefine p:\n\np &lt;- murders %&gt;% ggplot(aes(x = population/10^6, y = total, label = abb))\n\nand then we can simply write the following code to produce the previous plot:\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 1.5) # offsets the label\n\nWe keep the size and nudge_x arguments in geom_point and geom_text, respectively, because we want to only increase the size of points and only nudge the labels. If we put those arguments in aes then they would apply to both plots. Also note that the geom_point function does not need a label argument and therefore ignores that aesthetic.\nIf necessary, we can override the global mapping by defining a new mapping within each layer. These local definitions override the global. Here is an example:\n\np + geom_point(size = 3) +\n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))\n\n\n\n\n\n\n\n\nClearly, the second call to geom_text does not use x = population and y = total.\n\n\n\n\n\n\nTry it!\n\n\n\nSelect one of the following five sets of tasks. If your ID number ends in 1-5, choose the corresponding task. If your ID ends in 6-9, then do 6–&gt;1, 7–&gt;2, …, 9–&gt;4, and 0–&gt;5.\nEach set of tasks ask you to learn about an aesthetic and put it into action with the murder data. We’ll leave about 5 minutes to do the task, then have you come back and share your results with the class.\nFor each task, we’ll start with the following code:\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\nThe alpha aesthetic mapping.\n\nThe alpha aesthetic can only take a number between 0 and 1. So first, in murders, create a murders_per_capita column by dividing total by population. Second, find the max(murders$murders_per_capita) and then create another new column called murders_per_capita_rescaled which divides murders_per_capita by the max value. murders_per_capita_rescaled will be between 0 and 1, with the value of 1 for the state with the max murder rate. This is a little hard to do on the fly in ggplot.\nSet the alpha aesthetic mapping to murders_per_capita_rescaled for geom_point.\nTurn off the legend using show.legend=FALSE\nInclude the geom_text labels, but make sure the aesthetic mapping does not apply to the labels.\nUse nudge_x = 1.5 as before to offset the labels.\nBe able to explain the plot.\n\nDoes the alpha aesthetic help present the data here? It’s OK if it doesn’t!\n\n\nThe stroke aesthetic mapping.\n\nThe stroke aesthetic works a bit like the size aesthetic. It must be used with a plot that has both a border and a fill, like shapes 21-25, so use one of those.\nUse the stroke aesthetic mapping (meaning the stroke will change according to a value in the data) to set a different stroke size based on murders per capita. You can create a murders per capita variable on the fly, or add it to your murders data.\n\nInclude the text labels as before and use nudge_x = 1.5.\nMake sure you’re only setting the aesthetic for the points on the scatterplot!\n\n\nThe angle aesthetic\n\nUsing the ?geom_text help, note that geom_text takes an aesthetic of angle.\nUse the angle aesthetic (not aesthetic mapping) in the appropriate place (e.g. on geom_text and not on other geometries) to adjust the labels on our plot.\nNow, try using the angle aesthetic mapping by using the total field as both the y value and the angle value in the geom_text layer.\nDoes using angle as an aesthetic help? What about as an aesthetic mapping?\n\nThe color aesthetic mapping\n\nSet the color aesthetic mapping in geom_text to total/population.\n\nUse the nudge_x = 1.5 aesthetic in geom_text still\n\nTry it with and without the legend using show.legend.\nBe able to explain the plot.\n\nDoes the color aesthetic mapping help present the data here?\n\n\ngeom_label and the fill aesthetic\n\nLooking at ?geom_label (which is the same help as geom_text), we note that “The fill aesthetic controls the backgreound colour of the label”.\nSet the fill aesthetic mapping to total/population in geom_label (replacing geom_text but still using nudge_x=1.5)\nSet the fill aesthetic (not mapping) to the color of your choice.\nBe able to explain the plots.\n\n\nDoes the fill aesthetic mapping help present the data here?\nWhat color did you choose for the non-mapped fill aesthetic?",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to Visualization"
    ]
  },
  {
    "objectID": "content/Week_02/02b.html#footnotes",
    "href": "content/Week_02/02b.html#footnotes",
    "title": "Introduction to Visualization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttp://graphics.wsj.com/infectious-diseases-and-vaccines/?mc_cid=711ddeb86e↩︎\nhttp://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif↩︎\nhttps://www.nytimes.com/2011/02/19/nyregion/19schools.html↩︎\nhttps://en.wikipedia.org/wiki/John_Tukey↩︎\nhttps://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en↩︎\nhttps://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen↩︎\nhttps://ggplot2.tidyverse.org/↩︎\nhttp://www.springer.com/us/book/9780387245447↩︎",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to Visualization"
    ]
  },
  {
    "objectID": "content/Week_01/01b.html",
    "href": "content/Week_01/01b.html",
    "title": "Working with R and RStudio",
    "section": "",
    "text": "I like to use this spot to publish course announcements. Not so much for y’all, but more so I remember. If you see any announcements that don’t say “Spring 2025” there’s a good chance it’s leftover from earlier course offerings.\n\n\nA careful read of our syllabus under “class participation” will show that I do give extra credit for answering questions and (mainly) sharing completed R coding tasks. That is, we’ll walk through some examples, and when we hit a box that looks like this:\n\n\n\n\n\n\nTry it\n\n\n\nDo some stuff in R\n\n\nI’ll ask you to give it a go. Then, after a few minutes, I’ll ask if anyone wants to share their answer. You get one point of participation extra credit. Five points is worth 1% of a grade boost, so these aren’t negligible points. Max boost over the semester is 5%.\n\n\n\nThe Assignments page has all of our weekly lab assignments (including Week 1, due on Monday at 11:59pm). The assignments often have a preamble and some code that has to be used to set you up for the questions. The questions to be completed and turned in are under “Exercises” at the very end.",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Working with R and RStudio"
    ]
  },
  {
    "objectID": "content/Week_01/01b.html#the-very-basics-of-r",
    "href": "content/Week_01/01b.html#the-very-basics-of-r",
    "title": "Working with R and RStudio",
    "section": "The (very) basics of R",
    "text": "The (very) basics of R\nBefore we get started with the motivating dataset, we need to cover the very basics of R.\n\nConsole and Script\nYour Rstudio has two main areas in which code is written. The console appears at the bottom of your screen. You can interact directly with R through the console. Your script editor is at the top of your screen. This is where code that you want to save is written, usually in an order such that an entire set of commands can be written top-to-bottom and will run with the desired result. In the script editor, you can highlight lines of code and use command+enter (mac) or ctrl+enter (windows) to run just the bit of code.\nIn class, you’ll likely want to copy bits of code into a blank document and, at the end, save the document as notes, just for your reference. Remember, if you put something directly into the console, you won’t have a record of it. Putting it into a script will keep a version of it.\n\n\n\nScreenshot of RStudio\n\n\n\n\nRmarkdown\nRmarkdown lets us combine the processing and output from R code with text and headers written in plain english, which lets us do something in code and then show it and discuss it in one place.\nAn .Rmd (Rmarkdown document) like your lab and weekly writing templates has three parts: first, a YAML header up at the top that establishes some variables for use in rendering to PDF. Second, “code chunks” that are processed by R. And third, markdown text that is processed as normal text (via markdown langugage). You do work in code chunks, the output is included in the document, and you discuss the results in-line. Make sure you read using Rmarkdown and using markdown before you do your first weekly reading.\n\n\n\nA code chunk in a .Rmd\n\n\nYour R code goes here in these “chunks”. In the upper right, you’ll see a green down-pointing triangle and a green right-pointing triangle. The first one (down-pointing) runs all of the previous code chunks up to this one while the second (right-pointing) runs this code chunk.\nThis is very useful when you are iterating through steps to develop your code. Running a code chunk will show you the output from that code chunk, which is what will drop into your .Rmd file when you knit it. Note that you can also highlight code and use CTRL+ENTER (or CMD+ENTER for macs) to run code.\nThe header on the code chunk tells Rstudio what language to use to run the chunk (r), and can take some settings for displaying output. The one you want to know now is echo=T. When this is TRUE (the document’s default), then knitting will include a copy of your code along with the output. Don’t change this to FALSE or I can’t see your work when grading.\n\n\ninstall.packages()\nYou should never, ever have install.packages() in your recorded code (in your “code” file if using an R script, or in your code chunks in a .Rmd). If it’s in your .Rmd file, when you “knit”, it’ll try to install the packages and will get very confused. I saw a lot of install.packages() in code, so make sure to take them out.\nTo use a package, you include (in your .Rmd) library(packageName). That goes in your code, usually first thing.\nNow, on to the use of R\n\n\nObjects\nSuppose a relatively math unsavvy student asks us for help solving several quadratic equations of the form \\(ax^2+bx+c = 0\\). You—a savvy student—recall that the quadratic formula gives us the solutions:\n\\[\n  \\frac{-b + \\sqrt{b^2 - 4ac}}{2a}\\,\\, \\mbox{ and } \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\n\\]\nwhich of course depend on the values of \\(a\\), \\(b\\), and \\(c\\). That is, the quadratic equation represents a function with three arguments.\nOne advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve \\(x^2 + x -1 = 0\\), then we define:\n\na &lt;- 1\nb &lt;- 1\nc &lt;- -1\n\nwhich stores the values for later use. We use &lt;- to assign values to the variables.\nWe can also assign values using = instead of &lt;-, but some recommend against using = to avoid confusion.2\n\n\n\n\n\n\nTRY IT\n\n\n\nCopy and paste the code above into your console (or use the “copy code” button in the box) to define the three variables. Note that R does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message. Throughout these written notes, you’ll have the most success if you continue to copy code into a blank R script or into your own console.\n\n\nTo see the value stored in a variable, we simply ask R to evaluate a and it shows the stored value:\n\na\n\n[1] 1\n\n\nA more explicit way to ask R to show us the value stored in a is using print like this:\n\nprint(a)\n\n[1] 1\n\n\nBy default, just running a in the console results in the assuption that you wanted to print out a.\nWe use the term object to describe stuff that is stored in R. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.\n\n\nThe workspace\nAs we define objects in the console, we are actually changing the workspace. You can see all the variables saved in your workspace by typing:\n\nls()\n\n[1] \"a\"      \"b\"      \"c\"      \"filter\"\n\n\nIn RStudio Posit, the Environment tab shows the values:\n\n\n\n\n\n\n\n\n\nWe should see a, b, and c. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type x you will receive the following message: Error: object 'x' not found.\nNow since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:\n\n(-b + sqrt(b^2 - 4*a*c) ) / ( 2*a )\n\n[1] 0.618034\n\n(-b - sqrt(b^2 - 4*a*c) ) / ( 2*a )\n\n[1] -1.618034\n\n\n\n\n\n\n\n\nTRY IT\n\n\n\nCopy and paste the code above into your console (or use the “copy code” button in the box) to define the three variables, and the code used to implement the quadratic formula. Note that R does not print anything when we make the code assignment for a, b, and c. This means the objects were defined successfully. Had you made a mistake, you would have received an error message. Put them all together and run them to see the solutions. Note that the two lines that define the solution do not assign the result to a new object; thus, they get printed in the output.\nThroughout these written notes, you’ll have the most success if you continue to copy code into a blank R script or into your own console.\n\n\n\n\nFunctions\nOnce you define variables, the data analysis process can usually be described as a series of functions applied to the data. R includes several zillion predefined functions and most of the analysis pipelines we construct make extensive use of the built-in functions. But R’s power comes from its scalability. We have access to (nearly) infinite functions via install.packages and library. As we go through the course, we will carefully note new functions we bring to each problem. For now, though, we will stick to the basics.\nNote that you’ve used a function already: you used the function sqrt to solve the quadratic equation above. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.\nIn general, we need to use parentheses to evaluate a function. If you type ls, the function is not evaluated and instead R shows you the code that defines the function. If you type ls() the function is evaluated and, as seen above, we see objects in the workspace.\nUnlike ls, most functions require one or more arguments. Below is an example of how we assign an object to the argument of the function log. Remember that we earlier defined a to be 1:\n\nlog(8)\n\n[1] 2.079442\n\nlog(a)\n\n[1] 0\n\n\nYou can find out what the function expects and what it does by reviewing the very useful manuals included in R. You can get help by using the help function like this:\n\nhelp(\"log\")\n\nFor most functions, we can also use this shorthand:\n\n?log\n\nThe help page will show you what arguments the function is expecting. For example, log needs x and base to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with =. Defining these is optional.3 For example, the base of the function log defaults to base = exp(1)—that is, log evaluates the natural log by default.\nIf you want a quick look at the arguments without opening the help system, you can type:\n\nargs(log)\n\nfunction (x, base = exp(1)) \nNULL\n\n\nYou can change the default values by simply assigning another object:\n\nlog(8, base = 2)\n\n[1] 3\n\n\nNote that we have not been specifying the argument x as such:\n\nlog(x = 8, base = 2)\n\n[1] 3\n\n\nThe above code works, but we can save ourselves some typing: if no argument name is used, R assumes you are entering arguments in the order shown in the help file or by args. So by not using the names, it assumes the arguments are x followed by base:\n\nlog(8,2)\n\n[1] 3\n\n\nIf using the arguments’ names, then we can include them in whatever order we want:\n\nlog(base = 2, x = 8)\n\n[1] 3\n\n\nTo specify arguments, we must use =, and cannot use &lt;-.\nThere are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:\n\n2 ^ 3\n\n[1] 8\n\n\nYou can see the arithmetic operators by typing:\n\nhelp(\"+\")\n\nor\n\n?\"+\"\n\nand the relational operators by typing:\n\nhelp(\"&gt;\")\n\nor\n\n?\"&gt;\"\n\n\n\n\n\n\n\nTip\n\n\n\nNever use ? in your code. The help operator, ?..., should only be used directly in the console. If you put it in your code, it’ll keep opening the help, and when you include it in an Rmarkdown document, it’ll behave strangely. Don’t do it!\n\n\n\n\nOther prebuilt objects\nThere are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:\n\ndata()\n\nThis shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:\n\nco2\n\nR will show you Mauna Loa atmospheric \\(CO^2\\) concentration data.\nOther prebuilt objects are mathematical quantities, such as the constant \\(\\pi\\) and \\(\\infty\\):\n\npi\n\n[1] 3.141593\n\nInf+1\n\n[1] Inf\n\n\n\n\nVariable names\nWe have used the letters a, b, and c as variable names, but variable names can be almost anything. Some basic rules in R are that variable names have to start with a letter, can’t contain spaces, and should not be variables that are predefined in R. For example, don’t name one of your variables install.packages by typing something like install.packages &lt;- 2. Usually, R is smart enough to prevent you from doing such nonsense, but it’s important to develop good habits.\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:\n\nsolution_1 &lt;- (-b + sqrt(b^2 - 4*a*c)) / (2*a)\nsolution_2 &lt;- (-b - sqrt(b^2 - 4*a*c)) / (2*a)\n\nFor more advice, we highly recommend studying (Hadley Wickham’s style guide)[http://adv-r.had.co.nz/Style.html].\n\n\nSaving your workspace\nValues remain in the workspace until you end your session or erase them with the function rm. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.\nWe actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function save or save.image. To load, use the function load. When saving a workspace, we recommend the suffix rda or RData. In RStudio, you can also do this by navigating to the Session tab and choosing Save Workspace as. You can later load it using the Load Workspace options in the same tab. You can read the help pages on save, save.image, and load to learn more.\n\n\nMotivating scripts\nTo solve another equation such as \\(3x^2 + 2x -1\\), we can copy and paste the code above and then redefine the variables and recompute the solution:\n\na &lt;- 3\nb &lt;- 2\nc &lt;- -1\n(-b + sqrt(b^2 - 4*a*c)) / (2*a)\n(-b - sqrt(b^2 - 4*a*c)) / (2*a)\n\nBy creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.\nThe answer you get from the 4th and 5th lines will depend on the values of a, b, and c. If you were to type new numbers directly into your console: c = 5.33 then re-run the last two lines, you will get a different answer. Your R “environment” is affected by what is run from a script and by what you type in the console. It is good (and necessary) practice to write all your code in a script (or in your Rmarkdown document), and run from the script. Always. Periodically running a script fresh from the start (clearing everything out of the environment first) is a good idea as well.\n\n\n\n\n\n\nTry it!\n\n\n\n\nWhat is the sum of the first 100 positive integers? The formula for the sum of integers \\(1\\) through \\(n\\) is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum?\nNow use the same formula to compute the sum of the integers from 1 through 1,000.\nLook at the result of typing the following code into R:\n\n\nn &lt;- 1000\nx &lt;- seq(1, n)\nsum(x)\n\nBased on the result, what do you think the functions seq and sum do? You can use help.\n\nsum creates a list of numbers and seq adds them up.\nseq creates a list of numbers and sum adds them up.\nseq creates a random list and sum computes the sum of 1 through 1,000.\nsum always returns the same number.\n\n\nIn math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.\nWhich of the following will always return the numeric value stored in x? You can try out examples and use the help system if you want.\n\n\nlog(10^x)\nlog10(x^10)\nlog(exp(x))\nexp(log(x, base = 2))\n\n\n\n\n\nCommenting your code\nIf a line of R code starts with the symbol #, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:\n\n## Code to compute solution to quadratic equation of the form ax^2 + bx + c\n## define the variables\na &lt;- 3\nb &lt;- 2\nc &lt;- -1\n\n## now compute the solution\n(-b + sqrt(b^2 - 4*a*c)) / (2*a)\n(-b - sqrt(b^2 - 4*a*c)) / (2*a)",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Working with R and RStudio"
    ]
  },
  {
    "objectID": "content/Week_01/01b.html#data-types",
    "href": "content/Week_01/01b.html#data-types",
    "title": "Working with R and RStudio",
    "section": "Data types",
    "text": "Data types\nVariables in R can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function class helps us determine what type of object we have:\n\na &lt;- 2\nclass(a)\n\n[1] \"numeric\"\n\n\nTo work efficiently in R, it is important to learn the different types of variables and what we can do with these.\n\nData frames\nUp to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in R is in a data frame. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.\nA large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the dslabs library and loading the murders dataset using the data function:\n\nlibrary(dslabs)\ndata(murders)\n\nTo see that this is in fact a data frame, we type:\n\nclass(murders)\n\n[1] \"data.frame\"\n\n\n\n\nInstalling Packages\nWoah, there – data(\"murder\") gave me an error! Well, the data, like many functions, are part of a package. Here, it’s the dslabs package. We need to install the package before we can use it’s functions or data.\nTo install a new package, we use install.packages('packageName') (where hopefully-obviously “packageName” is the name of the package you want to install). We type this once directly into the console, which will add the package to our computer permanently (unless you delete it). Then, to use it thereafter, we use only library(dslabs).\nNote that there are quotes when using install.packages but no quotes when using library.\n\n\nExamining an object\nThe function str is useful for finding out more about the structure of an object:\n\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n$ state : chr \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n$ abb : chr \"AL\" \"AK\" \"AZ\" \"AR\" ...\n$ region : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n$ population: num 4779736 710231 6392017 2915918 37253956 ...\n$ total : num 135 19 232 93 1257 ...\n\n\nThis tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function head:\n\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nIn this dataset, each state is considered an observation and five variables are reported for each state.\nBefore we go any further in answering our original question about different states, let’s learn more about the components of this object.\n\n\nThe accessor: $\nFor our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator $ in the following way:\n\nmurders$population\n\n [1]  4779736   710231  6392017  2915918 37253956  5029196  3574097   897934\n [9]   601723 19687653  9920000  1360301  1567582 12830632  6483802  3046355\n[17]  2853118  4339367  4533372  1328361  5773552  6547629  9883640  5303925\n[25]  2967297  5988927   989415  1826341  2700551  1316470  8791894  2059179\n[33] 19378102  9535483   672591 11536504  3751351  3831074 12702379  1052567\n[41]  4625364   814180  6346105 25145561  2763885   625741  8001024  6724540\n[49]  1852994  5686986   563626\n\n\nBut how did we know to use population? Previously, by applying the function str to the object murders, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:\n\nnames(murders)\n\n[1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"     \n\n\nIt is important to know that the order of the entries in murders$population preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.\nTip: R comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing murders$p then hitting the tab key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.\n\n\nVectors: numerics, characters, and logical\nThe object murders$population is not one number but several. We call these types of objects vectors. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function length tells you how many entries are in the vector:\n\npop &lt;- murders$population\nlength(pop)\n\n[1] 51\n\n\nThis particular vector is numeric since population sizes are numbers:\n\nclass(pop)\n\n[1] \"numeric\"\n\n\nIn a numeric vector, every entry must be a number.\nTo store character strings, vectors can also be of class character. For example, the state names are characters:\n\nclass(murders$state)\n\n[1] \"character\"\n\n\nAs with numeric vectors, all entries in a character vector need to be a character.\nAnother important type of vectors are logical vectors. These must be either TRUE or FALSE.\n\nz &lt;- 3 == 2\nz\n\n[1] FALSE\n\nclass(z)\n\n[1] \"logical\"\n\n\nHere the == is a relational operator asking if 3 is equal to 2. In R, if you just use one =, you actually assign a variable, but if you use two == you test for equality. Yet another reason to avoid assigning via =… it can get confusing and typos can really mess things up.\nYou can ask if multiple things in a vector are equal to one specific thing:\n\nmurders$total == 5\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE  TRUE\n\n\nThat gives you 51 answers to the question “is this value of murders$total equal to 5? The answer is a vector of logicals.\nYou can see the other relational operators by typing:\n\n?Comparison\n\nIn future sections, you will see how useful relational operators can be.\nWe discuss more important features of vectors after the next set of exercises.\nAdvanced: Mathematically, the values in pop are integers and there is an integer class in R. However, by default, numbers are assigned class numeric even when they are round integers. For example, class(1) returns numeric. You can turn them into class integer with the as.integer() function or by adding an L like this: 1L. Note the class by typing: class(1L)\n\n\nFactors\nIn the murders dataset, we might expect the region to also be a character vector. However, it is not:\n\nclass(murders$region)\n\n[1] \"factor\"\n\n\nIt is a factor. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the levels function:\n\nlevels(murders$region)\n\n[1] \"Northeast\"     \"South\"         \"North Central\" \"West\"         \n\n\nIn the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters. It is also useful for computational reasons we’ll explore later.\nNote that the levels have an order that is different from the order of appearance in the factor object. The default in R is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the levels argument when creating the factor with the factor function. For example, in the murders dataset regions are ordered from east to west. The function reorder lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.\nSuppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the reorder and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.\n\nregion &lt;- murders$region\nvalue &lt;- murders$total\nregion &lt;- reorder(region, value, FUN = sum)\nlevels(region)\n\n[1] \"Northeast\"     \"North Central\" \"West\"          \"South\"        \n\n\nThe new order is in agreement with the fact that the Northeast has the least murders and the South has the most.\nWarning: Factors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.\n\n\nLists\nData frames are a special case of lists. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. In a data.frame, all columns have to be vectors of the same length (equal to the number of rows in the data.frame). In a list, each item can be of any length and of any type. Below is an example of a list we created for you:\n\nrecord\n\n$name\n[1] \"John Doe\"\n\n$student_id\n[1] 1234\n\n$grades\n[1] 95 82 91 97 93\n\n$final_grade\n[1] \"A\"\n\nclass(record)\n\n[1] \"list\"\n\n\nAs with data frames, you can extract the components of a list with the accessor $. In fact, data frames are a type of list.\n\nrecord$student_id\n\n[1] 1234\n\n\nWe can also use double square brackets ([[) like this:\n\nrecord[[\"student_id\"]]\n\n[1] 1234\n\n\nYou should get used to the fact that in R there are often several ways to do the same thing. such as accessing entries.4\nYou might also encounter lists without variable names.\n\nrecord2\n\n[[1]]\n[1] \"John Doe\"\n\n[[2]]\n[1] 1234\n\n\nIf a list does not have names, you cannot extract the elements with $, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:\n\nrecord2[[1]]\n\n[1] \"John Doe\"\n\n\nWe won’t be using lists until later, but you might encounter one in your own exploration of R. For this reason, we show you some basics here.\n\n\nMatrices\nMatrices are another type of object that are common in R. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.\nYet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this class, but much of what happens in the background when you perform a data analysis involves matrices. We describe them briefly here since some of the functions we will learn return matrices.\nWe can define a matrix using the matrix function. We need to specify the number of rows and columns.\n\nmat &lt;- matrix(1:12, 4, 3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\nYou can access specific entries in a matrix using square brackets ([). If you want the second row, third column, you use:\n\nmat[2, 3]\n\n[1] 10\n\n\nIf you want the entire second row, you leave the column spot empty:\n\nmat[2, ]\n\n[1]  2  6 10\n\n\nNotice that this returns a vector, not a matrix.\nSimilarly, if you want the entire third column, you leave the row spot empty:\n\nmat[, 3]\n\n[1]  9 10 11 12\n\n\nThis is also a vector, not a matrix.\nYou can access more than one column or more than one row if you like. This will give you a new matrix.\n\nmat[, 2:3]\n\n     [,1] [,2]\n[1,]    5    9\n[2,]    6   10\n[3,]    7   11\n[4,]    8   12\n\n\nYou can subset both rows and columns:\n\nmat[1:2, 2:3]\n\n     [,1] [,2]\n[1,]    5    9\n[2,]    6   10\n\n\nWe can convert matrices into data frames using the function as.data.frame:\n\nas.data.frame(mat)\n\n  V1 V2 V3\n1  1  5  9\n2  2  6 10\n3  3  7 11\n4  4  8 12\n\n\nYou can also use single square brackets ([) to access rows and columns of a data frame:\n\ndata(\"murders\")\nmurders[25, 1]\n\n[1] \"Mississippi\"\n\nmurders[2:3, ]\n\n    state abb region population total\n2  Alaska  AK   West     710231    19\n3 Arizona  AZ   West    6392017   232\n\n\n\n\n\n\n\n\nTry it!\n\n\n\n\nInstall the dslabs package, load the package, and load the US murders dataset.\n\n\nlibrary(dslabs)\ndata(murders)\n\nUse the function str to examine the structure of the murders object. Which of the following best describes the variables represented in this data frame?\n\n\nThe 51 states\n\n\nThe murder rates for all 50 states and DC.\n\n\nThe state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010.\n\n\nstr shows no relevant information.\n\n\n\nWhat are the column names used by the data frame for these five variables?\nUse the accessor $ to extract the state abbreviations and assign them to the object a. What is the class of this object?\nNow use the square brackets to extract the state abbreviations and assign them to the object b. Use the identical function to determine if a and b are the same.\nWe saw that the region column stores a factor. You can corroborate this by typing:\n\n\nclass(murders$region)\n\nWith one line of code, use the function levels and length to determine the number of regions defined by this dataset.\n\nThe function table takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region.\n\n\n\n\n\nSequences\nAnother useful function for creating vectors generates sequences:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nThe first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:\n\nseq(1, 10, 2)\n\n[1] 1 3 5 7 9\n\n\nIf we want consecutive integers, we can use the following shorthand:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWhen we use these functions, R produces integers, not numerics, because they are typically used to index something:\n\nclass(1:10)\n\n[1] \"integer\"\n\n\nHowever, if we create a sequence including non-integers, the class changes:\n\nclass(seq(1, 10, 0.5))\n\n[1] \"numeric\"",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Working with R and RStudio"
    ]
  },
  {
    "objectID": "content/Week_01/01b.html#creating-vectors",
    "href": "content/Week_01/01b.html#creating-vectors",
    "title": "Working with R and RStudio",
    "section": "Creating Vectors",
    "text": "Creating Vectors\nIn R, the most basic objects available to store data are vectors. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.\nWe can create vectors using the function c, which stands for concatenate. We use c to concatenate entries in the following way:\n\ncodes &lt;- c(380, 124, 818)\ncodes\n\n[1] 380 124 818\n\n\nWe can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.\n\ncountry &lt;- c(\"italy\", \"canada\", \"egypt\")\n\nIn R you can also use single quotes:\n\ncountry &lt;- c('italy', 'canada', 'egypt')\n\nBut be careful not to confuse the single quote ’ with the back quote, which shares a keyboard key with ~.\nBy now you should know that if you type:\n\ncountry &lt;- c(italy, canada, egypt)\n\nyou receive an error because the variables italy, canada, and egypt are not defined. If we do not use the quotes, R looks for variables with those names and returns an error.\n\nNames\nSometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:\n\ncodes &lt;- c(italy = 380, canada = 124, egypt = 818)\ncodes\n\n italy canada  egypt \n   380    124    818 \n\n\nThe object codes continues to be a numeric vector:\n\nclass(codes)\n\n[1] \"numeric\"\n\n\nbut with names:\n\nnames(codes)\n\n[1] \"italy\"  \"canada\" \"egypt\" \n\n\nIf the use of strings without quotes looks confusing, know that you can use the quotes as well:\n\ncodes &lt;- c(\"italy\" = 380, \"canada\" = 124, \"egypt\" = 818)\ncodes\n\n italy canada  egypt \n   380    124    818 \n\n\nThere is no difference between this function call and the previous one. This is one of the many ways in which R is quirky compared to other languages.\n\n\nSubsetting\nWe use square brackets to access specific elements of a vector. For the vector codes we defined above, we can access the second element using:\n\ncodes[2]\n\ncanada \n   124 \n\n\nYou can get more than one entry by using a multi-entry vector as an index:\n\ncodes[c(1,3)]\n\nitaly egypt \n  380   818 \n\n\nThe sequences defined above are particularly useful if we want to access, say, the first two elements:\n\ncodes[1:2]\n\n italy canada \n   380    124 \n\n\nIf the elements have names, we can also access the entries using these names. Below are two examples.\n\ncodes[\"canada\"]\n\ncanada \n   124 \n\ncodes[c(\"egypt\",\"italy\")]\n\negypt italy \n  818   380 \n\n\n\nSubsetting rows and columns\nWhen we have 2 dimensions (rows and columns in a data.frame or matrix) we can subset on either or both. Since we have two dimensions, we have to have room for two subsets in the square brackets. So, we use a , and subset by [row,col]:\nFor the first five rows and the first two columns of murders:\n\nmurders[1:5,1:2]\n\n       state abb\n1    Alabama  AL\n2     Alaska  AK\n3    Arizona  AZ\n4   Arkansas  AR\n5 California  CA\n\n\nFor the 3rd row, 4th column:\n\nmurders[3,4]\n\n[1] 6392017\n\n\nAnd, we can refer to columns by the column names (if we’re working with a data.frame):\n\nmurders[1:3,'population']\n\n[1] 4779736  710231 6392017\n\n\nWe aren’t limited to a sequence like 1:3 either. We can c() multiple rows or columns. Here, I do both:\n\nmurders[c(1,3,51), c('state','abb','population')]\n\n     state abb population\n1  Alabama  AL    4779736\n3  Arizona  AZ    6392017\n51 Wyoming  WY     563626\n\n\nAnd if we want all the rows or all the columns, we leave the row or column index blank:\n\nmurders[10:13,]\n\n     state abb region population total\n10 Florida  FL  South   19687653   669\n11 Georgia  GA  South    9920000   376\n12  Hawaii  HI   West    1360301     7\n13   Idaho  ID   West    1567582    12",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Working with R and RStudio"
    ]
  },
  {
    "objectID": "content/Week_01/01b.html#coercion",
    "href": "content/Week_01/01b.html#coercion",
    "title": "Working with R and RStudio",
    "section": "Coercion",
    "text": "Coercion\nIn general, coercion is an attempt by R to be flexible with data types. When an entry does not match the expected, some of the prebuilt R functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand coercion can drive programmers crazy when attempting to code in R since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples.\nWe said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:\n\nx &lt;- c(1, \"canada\", 3)\n\nBut we don’t get one, not even a warning! What happened? Look at x and its class:\n\nx\n\n[1] \"1\"      \"canada\" \"3\"     \n\nclass(x)\n\n[1] \"character\"\n\n\nR coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings \"1\" and “3”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in R.\nR also offers functions to change from one type to another. For example, you can turn numbers into characters with:\n\nx &lt;- 1:5\ny &lt;- as.character(x)\ny\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\nYou can turn it back with as.numeric:\n\nas.numeric(y)\n\n[1] 1 2 3 4 5\n\n\nThis function is actually quite useful since datasets that include numbers as character strings are common.\n\nNot availables (NA)\nThis “topic” seems to be wholly unappreciated and it has been our experience that students often panic when encountering an NA. This often happens when a function tries to coerce one type to another and encounters an impossible case. In such circumstances, R usually gives us a warning and turns the entry into a special value called an NA (for “not available”). For example:\n\nx &lt;- c(\"1\", \"b\", \"3\")\nas.numeric(x)\n\n[1]  1 NA  3\n\n\nR does not have any guesses for what number you want when you type b, so it does not try.\nWhile coercion is a common case leading to NAs, you’ll see them in nearly every real-world dataset. Most often, you will encounter the NAs as a stand-in for missing data. Again, this a common problem in real-world datasets and you need to be aware that it will come up.",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Working with R and RStudio"
    ]
  },
  {
    "objectID": "content/Week_01/01b.html#sorting",
    "href": "content/Week_01/01b.html#sorting",
    "title": "Working with R and RStudio",
    "section": "Sorting",
    "text": "Sorting\nNow that we have mastered some basic R knowledge (ha!), let’s try to gain some insights into the safety of different states in the context of gun murders.\n\nsort\nSay we want to rank the states from least to most gun murders. The function sort sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:\n\nlibrary(dslabs)\ndata(murders)\nsort(murders$total)\n\n [1]    2    4    5    5    7    8   11   12   12   16   19   21   22   27   32\n[16]   36   38   53   63   65   67   84   93   93   97   97   99  111  116  118\n[31]  120  135  142  207  219  232  246  250  286  293  310  321  351  364  376\n[46]  413  457  517  669  805 1257\n\n\nHowever, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257.\n\n\norder\nThe function order is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it:\n\nx &lt;- c(31, 4, 15, 92, 65)\nsort(x)\n\n[1]  4 15 31 65 92\n\n\nRather than sort the input vector, the function order returns the index that sorts input vector:\n\nindex &lt;- order(x)\nx[index]\n\n[1]  4 15 31 65 92\n\n\nThis is the same output as that returned by sort(x). If we look at this index, we see why it works:\n\nx\n\n[1] 31  4 15 92 65\n\norder(x)\n\n[1] 2 3 1 5 4\n\n\nThe second entry of x is the smallest, so order(x) starts with 2. The next smallest is the third entry, so the second entry is 3 and so on.\nHow does this help us order the states by murders? First, remember that the entries of vectors you access with $ follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:\n\nmurders$state[1:6]\n\n[1] \"Alabama\"    \"Alaska\"     \"Arizona\"    \"Arkansas\"   \"California\"\n[6] \"Colorado\"  \n\nmurders$abb[1:6]\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\"\n\n\nThis means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:\n\nind &lt;- order(murders$total)\nmurders$abb[ind]\n\n [1] \"VT\" \"ND\" \"NH\" \"WY\" \"HI\" \"SD\" \"ME\" \"ID\" \"MT\" \"RI\" \"AK\" \"IA\" \"UT\" \"WV\" \"NE\"\n[16] \"OR\" \"DE\" \"MN\" \"KS\" \"CO\" \"NM\" \"NV\" \"AR\" \"WA\" \"CT\" \"WI\" \"DC\" \"OK\" \"KY\" \"MA\"\n[31] \"MS\" \"AL\" \"IN\" \"SC\" \"TN\" \"AZ\" \"NJ\" \"VA\" \"NC\" \"MD\" \"OH\" \"MO\" \"LA\" \"IL\" \"GA\"\n[46] \"MI\" \"PA\" \"NY\" \"FL\" \"TX\" \"CA\"\n\n\nAccording to the above, California had the most murders.\nIf we wanted to re-order the whole data.frame based on the murders$total index, and overwrite with the new order:\n\nmurders_ordered = murders[ind,]\n\nThis saves murders in a new data.frame called murders_ordered that is in the order defined by ind.\n\n\nmax and which.max\nIf we are only interested in the entry with the largest value, we can use max for the value:\n\nmax(murders$total)\n\n[1] 1257\n\n\nand which.max for the index of the largest value:\n\ni_max &lt;- which.max(murders$total)\nmurders$state[i_max]\n\n[1] \"California\"\n\n\nFor the minimum, we can use min and which.min in the same way.\nDoes this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: rank.\n\n\nrank\nAlthough not as frequently used as order and sort, the function rank is also related to order and can be useful. For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:\n\nx &lt;- c(31, 4, 15, 92, 65)\nrank(x)\n\n[1] 3 1 2 5 4\n\n\nTo summarize, let’s look at the results of the three functions we have introduced:\n\n\n\n\n\noriginal\nsort\norder\nrank\n\n\n\n\n31\n4\n2\n3\n\n\n4\n15\n3\n1\n\n\n15\n31\n1\n2\n\n\n92\n65\n5\n5\n\n\n65\n92\n4\n4\n\n\n\n\n\n\n\n\n\nBeware of recycling\nAnother common source of unnoticed errors in R is the use of recycling. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens:\n\nx &lt;- c(1,2,3)\ny &lt;- c(10, 20, 30, 40, 50, 60, 70)\nx+y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 11 22 33 41 52 63 71\n\n\nWe do get a warning, but no error. For the output, R has recycled the numbers in x. Notice the last digit of numbers in the output.\n\n\n\n\n\n\nNote\n\n\n\nTRY IT\nFor these exercises we will use the US murders dataset. Make sure you load it prior to starting.\n\nlibrary(dslabs)\ndata(\"murders\")\n\n\nUse the $ operator to access the population size data and store it as the object pop. Then use the sort function to redefine pop so that it is sorted. Finally, use the [ operator to report the smallest population size.\nNow instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use order instead of sort.\nWe can actually perform the same operation as in the previous exercise using the function which.min. Write one line of code that does this.\nNow we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable states to be the state names from the murders data frame. Report the name of the state with the smallest population.\nYou can create a data frame using the data.frame function. Here is a quick example:\n\n\ntemp &lt;- c(35, 88, 42, 84, 81, 30)\ncity &lt;- c(\"Beijing\", \"Lagos\", \"Paris\", \"Rio de Janeiro\",\n          \"San Juan\", \"Toronto\")\ncity_temps &lt;- data.frame(name = city, temperature = temp)\n\nUse the rank function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called ranks, then create a data frame with the state name and its rank. Call the data frame my_df.\n\nRepeat the previous exercise, but this time order my_df so that the states are ordered from least populous to most populous. Hint: create an object ind that stores the indexes needed to order the population values. Then use the bracket operator [ to re-order each column in the data frame.\nThe na_example vector represents a series of counts. You can quickly examine the object using:\n\n\ndata(\"na_example\")\nstr(na_example)\n\n int [1:1000] 2 1 3 2 1 3 1 4 3 2 ...\n\n\nHowever, when we compute the average with the function mean, we obtain an NA:\n\nmean(na_example)\n\n[1] NA\n\n\nThe is.na function returns a logical vector that tells us which entries are NA. Assign this logical vector to an object called ind and determine how many NAs does na_example have. Note that TRUE=1 and FALSE=0 when “coerced”.",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Working with R and RStudio"
    ]
  },
  {
    "objectID": "content/Week_01/01b.html#vector-arithmetics",
    "href": "content/Week_01/01b.html#vector-arithmetics",
    "title": "Working with R and RStudio",
    "section": "Vector arithmetics",
    "text": "Vector arithmetics\nCalifornia had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:\n\nlibrary(dslabs)\ndata(\"murders\")\nmurders$state[which.max(murders$population)]\n\n[1] \"California\"\n\n\nwith over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of R come in handy.\n\nRescaling a vector\nIn R, arithmetic operations on vectors occur element-wise. For a quick example, suppose we have height in inches:\n\ninches &lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\n\nand want to convert to centimeters. Notice what happens when we multiply inches by 2.54:\n\ninches * 2.54\n\n [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18 177.80\n\n\nIn the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:\n\ninches - 69\n\n [1]  0 -7 -3  1  1  4 -2  4 -2  1\n\n\n\n\nTwo vectors\nIf we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:\n\\[\n  \\begin{pmatrix}\na\\\\\nb\\\\\nc\\\\\nd\n\\end{pmatrix}\n+\n  \\begin{pmatrix}\ne\\\\\nf\\\\\ng\\\\\nh\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\na +e\\\\\nb + f\\\\\nc + g\\\\\nd + h\n\\end{pmatrix}\n\\]\nThe same holds for other mathematical operations, such as -, * and /.\nThis implies that to compute the murder rates we can simply type:\n\nmurder_rate &lt;- murders$total / murders$population * 100000\n\nOnce we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:\n\nmurders$abb[order(murder_rate)]\n\n [1] \"VT\" \"NH\" \"HI\" \"ND\" \"IA\" \"ID\" \"UT\" \"ME\" \"WY\" \"OR\" \"SD\" \"MN\" \"MT\" \"CO\" \"WA\"\n[16] \"WV\" \"RI\" \"WI\" \"NE\" \"MA\" \"IN\" \"KS\" \"NY\" \"KY\" \"AK\" \"OH\" \"CT\" \"NJ\" \"AL\" \"IL\"\n[31] \"OK\" \"NC\" \"NV\" \"VA\" \"AR\" \"TX\" \"NM\" \"CA\" \"FL\" \"TN\" \"PA\" \"AZ\" \"GA\" \"MS\" \"MI\"\n[46] \"DE\" \"SC\" \"MD\" \"MO\" \"LA\" \"DC\"\n\n\nRight now, the murder_rate object isn’t in the murders data.frame, but we know it’s the right length (why?). So we can add it:\n\nmurders$rate = murder_rate\n\nNote that now, we have two copies of the same vector of numbers – one called murder_rate floatin’ around in our environment, and another in our murders data.frame with the column name rate. If we re-order murder_rate, it won’t affect anything in murders$rate and vice versa.\n\n\n\n\n\n\nTry it!\n\n\n\n\nPreviously we created this data frame:\n\n\ntemp &lt;- c(35, 88, 42, 84, 81, 30)\ncity &lt;- c(\"Beijing\", \"Lagos\", \"Paris\", \"Rio de Janeiro\",\n          \"San Juan\", \"Toronto\")\ncity_temps &lt;- data.frame(name = city, temperature = temp)\n\nRemake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is \\(C = \\frac{5}{9} \\times (F - 32)\\).\n\nWrite code to compute the following sum \\(1+1/2^2 + 1/3^2 + \\dots 1/100^2\\)? Hint: thanks to Euler, we know it should be close to \\(\\pi^2/6\\).\nCompute the per 100,000 murder rate for each state and store it in a new column called murder_rate. Then compute the average murder rate for the US using the function mean. What is the average?",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Working with R and RStudio"
    ]
  },
  {
    "objectID": "content/Week_01/01b.html#indexing",
    "href": "content/Week_01/01b.html#indexing",
    "title": "Working with R and RStudio",
    "section": "Indexing",
    "text": "Indexing\nIndexing is a boring name for an important tool. R provides a powerful and convenient way of referencing specific elements of vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example from before.\n\nSubsetting with logicals\nImagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of R is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:\n\nind &lt;- murder_rate &lt; 0.71\n\nIf we instead want to know if a value is less or equal, we can use:\n\nind &lt;- murder_rate &lt;= 0.71\n\nNote that we get back a logical vector with TRUE for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.\n\nmurders$state[ind]\n\n[1] \"Hawaii\"        \"Iowa\"          \"New Hampshire\" \"North Dakota\" \n[5] \"Vermont\"      \n\n\nIn order to count how many are TRUE, the function sum returns the sum of the entries of a vector and logical vectors get coerced to numeric with TRUE coded as 1 and FALSE as 0. Thus we can count the states using:\n\nsum(ind)\n\n[1] 5\n\n\nSince ind has the same length as all of the columns in murders, it can be used as a row index. When used as a row index, it will return all the rows for which the condition was true. If we use this, leaving the column index blank (for all columns):\n\nmurders[ind,]\n\n           state abb        region population total      rate\n12        Hawaii  HI          West    1360301     7 0.5145920\n16          Iowa  IA North Central    3046355    21 0.6893484\n30 New Hampshire  NH     Northeast    1316470     5 0.3798036\n35  North Dakota  ND North Central     672591     4 0.5947151\n46       Vermont  VT     Northeast     625741     2 0.3196211\n\n\n\n\nLogical operators\nSuppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator and, which in R is represented with &. This operation results in TRUE only when both logicals are TRUE. To see this, consider this example:\n\nTRUE & TRUE\n\n[1] TRUE\n\nTRUE & FALSE\n\n[1] FALSE\n\nFALSE & FALSE\n\n[1] FALSE\n\n\nFor our example, we can form two logicals:\n\nwest &lt;- murders$region == \"West\"\nsafe &lt;- murder_rate &lt;= 1\n\nand we can use the & to get a vector of logicals that tells us which states satisfy both conditions:\n\nind &lt;- safe & west\nmurders$state[ind]\n\n[1] \"Hawaii\"  \"Idaho\"   \"Oregon\"  \"Utah\"    \"Wyoming\"\n\n\n\n\nwhich\nSuppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type:\n\nind &lt;- which(murders$state == \"California\")\nmurders$rate[ind]\n\n[1] 3.374138\n\n\n\n\n%in%\nIf rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let’s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:\n\nc(\"Boston\", \"Dakota\", \"Washington\") %in% murders$state\n\n[1] FALSE FALSE  TRUE\n\n\nNote that we will be using %in% often throughout the course\n\n\n\n\n\n\nTry it!\n\n\n\nStart by loading the library and data.\n\nlibrary(dslabs)\ndata(murders)\n\nNote that every time you run this, you replace murder in your environment. So if you had created the murders$rate column, it’s gone. But if you created the free-floating object murder_rate, that still exists (but can be overwritten).\n\nCompute the per 100,000 murder rate for each state and store it in an object called murder_rate. Then use logical operators to create a logical vector named low that tells us which entries of murder_rate are lower than 1.\nNow use the results from the previous exercise and the function which to determine the indices of murder_rate associated with values lower than 1.\nUse the results from the previous exercise to report the names of the states with murder rates lower than 1.\nNow extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector low and the logical operator &.\nIn a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?\nUse the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of murders$abb that match the three abbreviations, then use the [ operator to extract the states.\nUse the %in% operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\nExtend the code you used in exercise 7 to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index.",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Working with R and RStudio"
    ]
  },
  {
    "objectID": "content/Week_01/01b.html#further-help-with-r",
    "href": "content/Week_01/01b.html#further-help-with-r",
    "title": "Working with R and RStudio",
    "section": "Further help with R",
    "text": "Further help with R\nIf you are not comfortable with R, the earlier you seek out help, the better. Quietly letting the course pass by you because you don’t know how to fix an error will do nobody any good. Attend TA office hours or attend TA or Prof. Bushong’s office hours see Syllabus for times and Zoom links. Also, join the course Slack (see the front page of our course website for a link) and post questions there.\nFinally, there are also primers on Rstudio.cloud that can be useful. There are many ways we can help you get used to R, but only if you reach out.",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Working with R and RStudio"
    ]
  },
  {
    "objectID": "content/Week_01/01b.html#footnotes",
    "href": "content/Week_01/01b.html#footnotes",
    "title": "Working with R and RStudio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nComments from previous classes indicate that I am not, in fact, funny.↩︎\nThis is, without a doubt, my least favorite aspect of R. I’d even venture to call it stupid. The logic behind this pesky &lt;- is a total mystery to me, but there is logic to avoiding =. But, you do you.↩︎\nThis equals sign is the reasons we assign values with &lt;-; then when arguments of a function are assigned values, we don’t end up with multiple equals signs. But… who cares.↩︎\nWhether you view this as a feature or a bug is a good indicator whether you’ll enjoy working with R.↩︎",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Working with R and RStudio"
    ]
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Assignments",
    "section": "",
    "text": "This course is the capstone of the Data Analytics Minor in the College of Social Science. Accordingly, you should—fingers crossed—enjoy data analysis. You will get the most of out this class if you:\nAssignments consist of Weekly Writings and Lab Assignments. Each type of assignment in this class helps with one of these strategies. Weekly writings engage with the materials, and lab assignments engage directly with R. The assignments are described below.\nTo get started, download and save the following files (right-click to Save Link As…)",
    "crumbs": [
      "Syllabus",
      "Lab Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#weekly-writings",
    "href": "assignment/index.html#weekly-writings",
    "title": "Assignments",
    "section": "Weekly Writings",
    "text": "Weekly Writings\nTo encourage you to actively engage with the course content, you will write a ≈150 word memorandum about the reading or lecture each week. That’s fairly short: there are ≈250 words on a typical double-spaced page. You must complete eleven of these in the course. I will drop your one lowest weekly writing score. Your actual prompt will be assigned in class, so you must login each day to ensure you get these assignments. To keep you on your toes, we will vary whether these are assigned on Tuesdays or Thursdays. Each week’s weekly writing will be due on D2L by 11:59pm on Saturday\nYou can do a lot of different things with this memo: discuss something you learned from the course content, write about the best or worst data visualization you saw recently, connect the course content to your own work, etc. These reflections let you explore and answer some of the key questions of this course, including:\n\nWhen is a link correlational vs causal? How can we still make useful statements about non-causal things?\nWhy do we visualize data?\nWhat makes a great data analysis? What makes a bad analysis?\nHow do you choose which kind of analysis method to use?\nWhat is the role of the data structure in choosing an analysis? Can we be flexible?\n\nThe course content for each day will also include a set of questions specific to that topic. You do not have to answer all (or any) of these questions. That would be impossible. They exist to guide your thinking and to make complex reading more digestible. The specific topic for each week will be assigned in class. (We can’t emphasize this enough.)\nThe TA will grade these mini-exercises using a very simple system:\n\n✔+: (9.2 points (115%) in gradebook) Work shows phenomenal thought and engagement with the course content. We will not assign these often.\n✔: (8 points (100%) in gradebook) Work is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance.\n✔−: (4 points (50%) in gradebook) Work is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.\n\n(There is an implicit 0 above for work that is not turned in by Saturday at 11:59pm). Notice that this is essentially a pass/fail or completion-based system. We’re not grading your writing ability; we’re not counting the exact number of words you’re writing; and we’re not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. We are looking for thoughtful engagement. Read the material, engage with the work and you’ll get a ✓.\n\nWeekly Writing Template\nYou will turn these reflections in via D2L. You will write them using R Markdown and this template  Weekly writing template. You must knit your work to a PDF document (this will be what you turn in). D2L will have eleven weekly writing assignments available. Upload your first weekly writing assignment to number 1, your second (regardless of which week you are writing on) to number 2, etc.",
    "crumbs": [
      "Syllabus",
      "Lab Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#lab-assignments",
    "href": "assignment/index.html#lab-assignments",
    "title": "Assignments",
    "section": "Lab Assignments",
    "text": "Lab Assignments\nEach week of the course has examples of code that teach and demonstrate how to do specific tasks in R. However, without practicing these principles and making graphics on your own, you won’t remember what you learn.\nPlease do not do labs more than one week ahead of time. I am updating the assignments as the semester proceeds, and you may do an entire assignment that is completely changed.\nFor example, to practice working with ggplot2 and making data-based graphics, you will complete a brief set of exercises over a few class sessions. These exercises will have 1–3 short tasks that are directly related to the topic for the week. You need to show that you made a good faith effort to work each question. There will also be a final question which requires significantly more thought and work. This will be where you get to show some creativity and stretch your abilities. Overall, labs will be graded the same check system:\n\n✔+: (17.5 points (115%) in gradebook) Exercises are complete. Every task was attempted and answered, and most answers are correct. Knitted document is clean and easy to follow. Work on the final problem shows creativity or is otherwise exceptional. We will not assign these often.\n✔: (15 points (100%) in gradebook) Exercises are complete and most answers are correct. This is the expected level of performance.\n✔−: (7.5 points (50%) in gradebook) Exercises are less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. We will hopefully not assign these often.\n\nThere is an implicit 0 for any assignment not turned in on time. If you have only partial work, then turn that in for partial credit. As noted in the syllabus, we are not grading your coding ability. We are not checking each line of code to make sure it produces some exact final figure, and we do not expect perfection. Also note that a ✓ does not require 100% success. You will sometimes get stuck with weird errors that you can’t solve, or the demands of pandemic living might occasionally become overwhelming. We are looking for good faith effort. Try hard, engage with the task, and you’ll get a ✓.\nYou may work together on the labs, but you must turn in your own answers.\n\nLab Template\nYou will turn these labs in via D2L. You will write them using R Markdown and this  Lab assignment template. You must knit your work to a PDF document (this will be what you turn in). Your output must be rendered in latex. I do not accept rendering to HTML or Word and then converting to PDF and unrendered .rmd files are not allowed.",
    "crumbs": [
      "Syllabus",
      "Lab Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#projects",
    "href": "assignment/index.html#projects",
    "title": "Assignments",
    "section": "Projects",
    "text": "Projects\nTo give you practice with the data and design principles you’ll learn in this class, you will complete two projects en route to the overarching final project of the course. Both these mini projects and the final project must be completed in groups. I will assign groups after the drop deadline passes. Groups will be 2-3 people. You are allowed to form your own groups, but I will assign groups. More details will follow later.\nThe two (mini) projects are checkpoints to ensure you’re working on your project seriously. They will be graded using a check system:\n\n✔+: (55 points (≈115%) in gradebook) Project is phenomenally well-designed and uses advanced R techniques. The project uncovers an important story that is not readily apparent from just looking at the raw data. I will not assign these often.\n✔: (50 points (100%) in gradebook) Project is fine, follows most design principles, answers a question from the data, and uses R correctly. This is the expected level of performance.\n✔−: (25 points (50%) in gradebook) Project is missing large components, is poorly designed, does not answer a relevant question, and/or uses R incorrectly. This indicates that you need to improve next time. I will hopefully not assign these often.\n\nBecause these mini projects give you practice for the final project, we will provide you with substantial feedback on your design and code.",
    "crumbs": [
      "Syllabus",
      "Lab Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#final-project",
    "href": "assignment/index.html#final-project",
    "title": "Assignments",
    "section": "Final project",
    "text": "Final project\nAt the end of the course, you will demonstrate your skills by completing a final project. Complete details for the final project (including past examples of excellent projects) are here. In brief, the final project has the following elements:\n\nYou must find existing data to analyze.1 Aggregating data from multiple sources is encouraged, but is not required.\n\n\nYou must visualize (at least) three interesting features of that data. Visualizations should aid the reader in understanding something about the data that might not be readily aparent.2\n\n\nYou must come up with some analysis—using tools from the course—which relates your data to either a prediction or a policy conclusion. For example, if you collected data from Major League Baseball games, you could try to “predict” whether a left-hander was pitching based solely on the outcomes of the batsmen.3\n\n\nYou must write your analysis as if presenting to a C-suite executive. If you are not familiar with this terminology, the C-suite includes, e.g., the CEO, CFO, and COO of a given company. Generally speaking, such executives are not particularly analytically oriented, and therefore your explanations need to be clear, consise (their time is valuable) and contain actionable (or valuable) information.\n\nThere is no final exam. This project is your final exam.\nThe project will not be graded using a check system, and will be graded by me (the main instructor, not a TA). I will evaluate the following four elements of your project:\n\nTechnical skills: Was the project easy? Does it showcase mastery of data analysis?\nVisual design: Was the information smartly conveyed and usable? Was it beautiful?\nAnalytic design: Was the analysis appropriate? Was it sensible, given the dataset?\nStory: Did we learn something?\n\nIf you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project.",
    "crumbs": [
      "Syllabus",
      "Lab Assignments"
    ]
  },
  {
    "objectID": "assignment/index.html#footnotes",
    "href": "assignment/index.html#footnotes",
    "title": "Assignments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that existing is taken to mean that you are not permitted to collect data by interacting with other people. That is not to say that you cannot gather data that previously has not been gathered into a single place—this sort of exercise is encouraged.↩︎\nPie charts of any kind will result in a 25% grade deduction.↩︎\nThis is an extremely dumb idea for a number of reasons. Moreover, it’s worth mentioning that sports data, while rich, can be overwhelming due to its sheer magnitude and the variety of approaches that can be applied. Use with caution.↩︎",
    "crumbs": [
      "Syllabus",
      "Lab Assignments"
    ]
  },
  {
    "objectID": "assignment/13-assignment.html",
    "href": "assignment/13-assignment.html",
    "title": "13: Text as Data",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, April 21st\n\n\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.\nFor this lab, you will need to make sure you set echo = T in the knitr options that are set in your template’s first chunk. Before turning your assignment in, check to make sure your code is showing. Labs that do not show all code will earn a zero.\nThis lab is deceptively short. Regular expressions can take a while to learn, and can be very frustrating. Leave yourself ample time.\n\n\n\n\n\n\nExercise 1 of 1\n\n\n\n\nCreate the following Sales2020 object. We want to convert the sales into a numeric format, but have to wrestle with the extra characters. Using no more than two lines of code, convert these to numeric.\n\nSales2020 = c('$1,420,142',\n              '$438,125.82',\n              '120,223.50',\n              '42,140')\n\nWe have the following student names:\n\nStudents = c('Ali',' Meza','McAvoy', 'Mc Evoy', '.Donaldson','Kirkpatrick ')\nWe would like to clean these student names and place them in alphabetical order. However, we note that “Mc Evoy” is a different name from “McEvoy” (the Irish are particular about that), so we need no more than two lines of code that can print these names in order (use order to place the cleaned names in order).\n\nWe want to check if each of these are valid, complete (including two decimal places for cents) prices. Write a line of code that returns TRUE if the price is in US dollars and includes cents to two digits.\n\nPrices = c('$12.95',\n           '$\\beta$',\n           '$1944.55',\n           '3.14',\n           '$CAN',\n           '$12.',\n           '$109',\n           '4,05',\n           '$200.00')\n\nWe will use groups to extract two columns of information from the following text we scraped from a land sales website:\n\nlandSales = c('Sold 12 acres at $105 per acre',\n              '200 ac. at $58.90 each',\n              '.25 acre lot for $1,000.00 ea',\n              'Offered 50 acres for $5,000 per')\nWe want to calculate the total amount spent on each of these transactions (which are all in “per acre” prices). In no more than 5 lines of code (each pipe is a separate line), extract the acres offered, the price per acre, and then calculate the total transaction (acres x price per acre). You can print the data.frame / tibble, or use %&gt;% knitr::kable() to output the table. Your final output should have four rows and three columns. Hint: use str_match with groups.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "13: Text as Data"
    ]
  },
  {
    "objectID": "assignment/11-assignment.html",
    "href": "assignment/11-assignment.html",
    "title": "11: LASSO",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, April 7th\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "11: LASSO"
    ]
  },
  {
    "objectID": "assignment/11-assignment.html#oh-no.-really-ames-again",
    "href": "assignment/11-assignment.html#oh-no.-really-ames-again",
    "title": "11: LASSO",
    "section": "Oh no. Really? Ames again?",
    "text": "Oh no. Really? Ames again?\nYes, Ames again. Let’s predict some SalePrices!\n\nAmes &lt;- read.table('https://raw.githubusercontent.com/ajkirkpatrick/FS20/postS21_rev/classdata/ames.csv', \n                   header = TRUE,\n                   sep = ',') %&gt;%\n  dplyr::select(-Id)\n\n\nData cleaning\nRepeat the data cleaning exercise from last week’s lab. The point is to make sure that every observation is non-NA and all predictor variables have more than one value. Use skimr::skim on Ames to find predictors with only one value or are missing many values. Take them out, and use na.omit to ensure there are no NA values left. Check to make sure you still have at least 800 or so observations!\n\n\nPredicive model\nFor the assignment below, we’ll use glmnet::cv.glmnet to estimate a LASSO model. Note that you’re asked to state 16 predictors and 5 interactions. You can go beyond this. Unlike our linear model building, complexity in LASSO is not controlled by writing out a bunch of formulas with more terms. It’s in the lambda parameter. So we write one formula and let lambda vary.\n\n\n\n\n\n\nExercise 1 of 1\n\n\n\n\nClean your data as described above.\nChoose up to 16 predictor variables and clean your data so that no NA values are left\nChoose at least 5 interactions between your predictor variables and print out the formula you’ll use to predict SalePrice.\nIn your code, use set.seed(24224) so that your results will always be the same. Why do we need to set seed? When we (well, glmnet::cvglmnet) makes the Train and Test sample(s), it’ll select them randomly. If you don’t set seed, every time you run it, you’ll get slightly different answers!\nUsing glmnet::cv.glmnet to estimate a LASSO model (see lecture notes this week) that predicts SalePrice given the observed data and using your formula. Slide 33 shows cross-validation using both alpha and lambda – a LASSO model holds alpha fixed at alpha = 1. We’ll search using lambda as our tuning parameter. Call the resulting object net_cv.\n\n\nTo do this, you’ll have to make a matrix to give to cv.glmnet in the x argument because it doesn’t take a formula. You can use model.matrix() to create the matrix. Use that matrix as your x. It will not add the SalePrice variable to the x matrix – you just have to give it SalePrice as the y variable.\n\n\nThe resulting object will be a glmnet object. You can see the optimal lambda just by printing the object print(net_cv) and looking at the min value for Lambda. Make sure the optimal (RMSE-minimizing) value of lambda is not the largest or smallest value of lambda you gave it. If it is, then extend the range of lambdas until you get an interior solution. Following the instructions from our lecture note’s TRY IT, extract the lambdas and their respective RMSE values into a data.frame and make a plot similar to the RMSE plot from lecture.\nAnswer the following question: What is the optimal lambda based on the plot/data? Do you see a minimum point in the plot?\nExtracting the non-zero coefficients is a little tricky, but let’s do it. We’ll use the coef function to extract the coefficients. The coef function, when used on a glmnet object, takes the argument s which is the lambda value for which you’d like to extract coefficients. Our s value should be the best value of lambda, which we can extract from net_cv$lambda.min. Put those together: coef(net_cv, s = net_cv$lambda.min). This may be kinda long, that’s OK.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "11: LASSO"
    ]
  },
  {
    "objectID": "assignment/09-assignment.html",
    "href": "assignment/09-assignment.html",
    "title": "9: Statistical Models and Uncertainty",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, March 24th\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "9: Statistical Models and Uncertainty"
    ]
  },
  {
    "objectID": "assignment/09-assignment.html#poll-aggregators",
    "href": "assignment/09-assignment.html#poll-aggregators",
    "title": "9: Statistical Models and Uncertainty",
    "section": "Poll aggregators",
    "text": "Poll aggregators\nA few weeks before the 2012 election Nate Silver was giving Obama a 90% chance of winning. How was Mr. Silver so confident? We will use a Monte Carlo simulation to illustrate the insight Mr. Silver had and others missed. To do this, we generate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls and construct and report 95% confidence intervals for each of the 12 polls. We save the results from this simulation in a data frame and add a poll ID column.\n\nlibrary(tidyverse)\nlibrary(dslabs)\nd &lt;- 0.039 # true spread\nNs &lt;- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)\np &lt;- (d + 1) / 2 # true pr(clinton vote)\n\npolls &lt;- map_df(Ns, function(N) {\n  x &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))\n  x_hat &lt;- mean(x)\n  se_hat &lt;- sqrt(x_hat * (1 - x_hat) / N)\n  list(estimate = 2 * x_hat - 1,\n    low = 2*(x_hat - 1.96*se_hat) - 1,\n    high = 2*(x_hat + 1.96*se_hat) - 1,\n    sample_size = N)\n}) %&gt;% mutate(poll = seq_along(Ns))\n\nHere is a visualization showing the intervals the pollsters would have reported for the difference between Obama and Romney:\n\n\n\n\n\n\n\n\n\nNot surprisingly, all 12 polls report confidence intervals that include the election night result (dashed line). However, all 12 polls also include 0 (solid black line) as well. Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up. Below we describe a key insight they are missing.\nPoll aggregators, such as Nate Silver, realized that by combining the results of different polls you could greatly improve precision. By doing this, we are effectively conducting a poll with a huge sample size. We can therefore report a smaller 95% confidence interval and a more precise prediction.\nAlthough as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what we would have obtained had we made one large poll with:\n\nsum(polls$sample_size)\n\n[1] 11269\n\n\nparticipants. Basically, we construct an estimate of the spread, let’s call it \\(d\\), with a weighted average in the following way:\n\nd_hat &lt;- polls %&gt;%\n  summarize(avg = sum(estimate*sample_size) / sum(sample_size)) %&gt;%\n  pull(avg)\n\nOnce we have an estimate of \\(d\\), we can construct an estimate for the proportion voting for Obama, which we can then use to estimate the standard error. Once we do this, we see that our margin of error is 0.0184545.\nThus, we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result we eventually observed on election night, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote.\n\n\n\n\n\n\n\n\n\nOf course, this was just a simulation to illustrate the idea. The actual data science exercise of forecasting elections is much more complicated and it involves modeling. Below we explain how pollsters fit multilevel models to the data and use this to forecast election results. In the 2008 and 2012 US presidential elections, Nate Silver used this approach to make an almost perfect prediction and silence the pundits.\nSince the 2008 elections, other organizations have started their own election forecasting group that, like Nate Silver’s, aggregates polling data and uses statistical models to make predictions. In 2016, forecasters underestimated Trump’s chances of winning greatly. The day before the election the New York Times reported2 the following probabilities for Hillary Clinton winning the presidency:\n\n\n\n\n\n\nNYT\n538\nHuffPost\nPW\nPEC\nDK\nCook\nRoth\n\n\n\n\nWin Prob\n85%\n71%\n98%\n89%\n&gt;99%\n92%\nLean Dem\nLean Dem\n\n\n\n\n\n\n\n\nFor example, the Princeton Election Consortium (PEC) gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, higher than tossing two coins and getting two heads. In fact, four days before the election FiveThirtyEight published an article titled Trump Is Just A Normal Polling Error Behind Clinton3. By understanding statistical models and how these forecasters use them, we will start to understand how this happened.\nAlthough not nearly as interesting as predicting the electoral college, for illustrative purposes we will start by looking at predictions for the popular vote. FiveThirtyEight predicted a 3.6% advantage for Clinton4, included the actual result of 2.1% (48.2% to 46.1%) in their interval, and was much more confident about Clinton winning the election, giving her an 81.4% chance. Their prediction was summarized with a chart like this:\n\n\n\n\n\n\n\n\n\nThe colored areas represent values with an 80% chance of including the actual result, according to the FiveThirtyEight model. \nWe introduce actual data from the 2016 US presidential election to show how models are motivated and built to produce these predictions. To understand the “81.4% chance” statement we need to describe Bayesian statistics, which we don’t cover in this course.\n\nPoll data\nWe use public polling data organized by FiveThirtyEight for the 2016 presidential election. The data is included as part of the dslabs package:\n\ndata(polls_us_election_2016)\n\nThe table includes results for national polls, as well as state polls, taken during the year prior to the election. For this first example, we will filter the data to include national polls conducted during the week before the election. We also remove polls that FiveThirtyEight has determined not to be reliable and graded with a “B” or less. Some polls have not been graded and we include those:\n\npolls &lt;- polls_us_election_2016 %&gt;%\n  filter(state == \"U.S.\" & enddate &gt;= \"2016-10-31\" &\n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade)))\n\nWe add a spread estimate:\n\npolls &lt;- polls %&gt;%\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nFor this example, we will assume that there are only two parties and call \\(p\\) the proportion voting for Clinton and \\(1-p\\) the proportion voting for Trump. We are interested in the spread \\(2p-1\\). Let’s call the spread \\(d\\) (for difference).\nWe have 49 estimates of the spread. The theory we learned tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread \\(d\\) and the standard error is \\(2\\sqrt{p (1 - p) / N}\\). Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is:\n\nd_hat &lt;- polls %&gt;%\n  summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %&gt;%\n  pull(d_hat)\n\nand the standard error is:\n\np_hat &lt;- (d_hat+1)/2\nmoe &lt;- 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(polls$samplesize))\nmoe\n\n[1] 0.006623178\n\n\nSo we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?\nA histogram of the reported spreads shows a problem:\n\npolls %&gt;%\n  ggplot(aes(spread)) +\n  geom_histogram(color=\"black\", binwidth = .01)\n\n\n\n\n\n\n\n\nThe data does not appear to be normally distributed and the standard error appears to be larger than 0.0066232. The theory is not quite working here.\n\n\nPollster bias\nNotice that various pollsters are involved and some are taking several polls a week:\n\npolls %&gt;% group_by(pollster) %&gt;% summarize(n())\n\n# A tibble: 15 × 2\n   pollster                                                   `n()`\n   &lt;fct&gt;                                                      &lt;int&gt;\n 1 ABC News/Washington Post                                       7\n 2 Angus Reid Global                                              1\n 3 CBS News/New York Times                                        2\n 4 Fox News/Anderson Robbins Research/Shaw & Company Research     2\n 5 IBD/TIPP                                                       8\n 6 Insights West                                                  1\n 7 Ipsos                                                          6\n 8 Marist College                                                 1\n 9 Monmouth University                                            1\n10 Morning Consult                                                1\n11 NBC News/Wall Street Journal                                   1\n12 RKM Research and Communications, Inc.                          1\n13 Selzer & Company                                               1\n14 The Times-Picayune/Lucid                                       8\n15 USC Dornsife/LA Times                                          8\n\n\nLet’s visualize the data for the pollsters that are regularly polling:\n\n\n\n\n\n\n\n\n\nThis plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll:\n\npolls %&gt;% group_by(pollster) %&gt;%\n  filter(n() &gt;= 6) %&gt;%\n  summarize(se = 2 * sqrt(p_hat * (1-p_hat) / median(samplesize)))\n\n# A tibble: 5 × 2\n  pollster                     se\n  &lt;fct&gt;                     &lt;dbl&gt;\n1 ABC News/Washington Post 0.0265\n2 IBD/TIPP                 0.0333\n3 Ipsos                    0.0225\n4 The Times-Picayune/Lucid 0.0196\n5 USC Dornsife/LA Times    0.0183\n\n\nis between 0.018 and 0.033, which agrees with the within poll variation we see. However, there appears to be differences across the polls. Note, for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump, while Ipsos is predicting a win larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values. All the polls should have the same expected value. FiveThirtyEight refers to these differences as “house effects”. We also call them pollster bias.\nIn the following section, rather than use the urn model theory, we are instead going to develop a data-driven model.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "9: Statistical Models and Uncertainty"
    ]
  },
  {
    "objectID": "assignment/09-assignment.html#data-driven-model",
    "href": "assignment/09-assignment.html#data-driven-model",
    "title": "9: Statistical Models and Uncertainty",
    "section": "Data-driven models",
    "text": "Data-driven models\nFor each pollster, let’s collect their last reported result before the election:\n\none_poll_per_pollster &lt;- polls %&gt;% group_by(pollster) %&gt;%\n  filter(enddate == max(enddate)) %&gt;%\n  ungroup()\n\nHere is a histogram of the data for these 15 pollsters:\n\nggplot(one_poll_per_pollster, aes(x = spread)) + \n  geom_histogram(binwidth=.01)\n\n\n\n\n\n\n\n\nIn the previous section, we saw that using the urn model theory to combine these results might not be appropriate due to the pollster effect. Instead, we will model this spread data directly.\nThe new model can also be thought of as an urn model, although the connection is not as direct. Rather than 0s (Republicans) and 1s (Democrats), our urn now contains poll results from all possible pollsters. We assume that the expected value of our urn is the actual spread \\(d=2p-1\\).\nBecause instead of 0s and 1s, our urn contains continuous numbers between -1 and 1, the standard deviation of the urn is no longer \\(\\sqrt{p(1-p)}\\). Rather than voter sampling variability, the standard error now includes the pollster-to-pollster variability. Our new urn also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter. In statistics textbooks, the Greek symbol \\(\\sigma\\) is used to represent this parameter.\nIn summary, we have two unknown parameters: the expected value \\(d\\) and the standard deviation \\(\\sigma\\).\nOur task is to estimate \\(d\\). Because we model the observed values \\(X_1,\\dots X_N\\) as a random sample from the urn, the CLT might still work in this situation because it is an average of independent random variables. For a large enough sample size \\(N\\), the probability distribution of the sample average \\(\\bar{X}\\) is approximately normal with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\). If we are willing to consider \\(N=15\\) large enough, we can use this to construct confidence intervals.\nA problem is that we don’t know \\(\\sigma\\). But theory tells us that we can estimate the urn model \\(\\sigma\\) with the sample standard deviation defined as \\(s = \\sqrt{ \\sum_{i=1}^N (X_i - \\bar{X})^2 / (N-1)}\\).\nUnlike for the population standard deviation definition, we now divide by \\(N-1\\). This makes \\(s\\) a better estimate of \\(\\sigma\\). There is a mathematical explanation for this, which is explained in most statistics textbooks, but we don’t cover it here.\nThe sd function in R computes the sample standard deviation:\n\nsd(one_poll_per_pollster$spread)\n\n[1] 0.02419369\n\n\nWe are now ready to form a new confidence interval based on our new data-driven model:\n\nresults &lt;- one_poll_per_pollster %&gt;%\n  summarize(avg = mean(spread),\n            se = sd(spread) / sqrt(length(spread))) %&gt;%\n  mutate(start = avg - 1.96 * se,\n         end = avg + 1.96 * se)\nround(results * 100, 1)\n\n  avg  se start end\n1 2.9 0.6   1.7 4.1\n\n\nOur confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.\n\n\n\n\n\n\nEXERCISES\n\n\n\nNote that using dollar signs $ $ to enclose some text is how you make the fancy math you see below. If you installed tinytex or some other Latex distribution in order to render your PDFs, you should be equipped to insert mathematics directly into your .Rmd file. It only works in the text – inside the code chunks, the dollar sign is still the accessor.\nIn this section, we talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election.\n\ndata(polls_us_election_2016)\npolls &lt;- polls_us_election_2016 %&gt;%\n  filter(pollster %in% c(\"Rasmussen Reports/Pulse Opinion Research\",\n                         \"The Times-Picayune/Lucid\") &\n           enddate &gt;= \"2016-10-15\" &\n           state == \"U.S.\") %&gt;%\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nWe want to answer the question: is there a poll bias?. That is, does one of these pollsters have a “thumb on the scale” (or methodology that makes them consistently different) relative to the other.\n\nFirst, make a plot showing the spreads for each poll. Visualizing our data first helps guide us!\nThe data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance.\n\nThe urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call \\(d\\).\nWe will model the observed data \\(Y_{i,j}\\) in the following way:\n\\[\nY_{i,j} = d + b_i + \\varepsilon_{i,j}\n\\]\nwith \\(i=1,2\\) indexing the two pollsters, \\(b_i\\) the bias for pollster \\(i\\) and \\(\\varepsilon_{ij}\\) poll to poll chance variability. We assume the \\(\\varepsilon\\) are independent from each other, have expected value \\(0\\) and standard deviation \\(\\sigma_i\\) regardless of \\(j\\).\nWhich of the following best represents our question about (relative) poll bias?\n\nIs \\(\\varepsilon_{i,j}\\) = 0?\nHow close are the \\(Y_{i,j}\\) to \\(d\\)?\nIs \\(b_1 \\neq b_2\\)?\nAre \\(b_1 = 0\\) and \\(b_2 = 0\\) ?\n\n\nSuppose we define \\(\\bar{Y}_1\\) as the average of poll results from the first pollster, \\(Y_{1,1},\\dots,Y_{1,N_1}\\) with \\(N_1\\) the number of polls conducted by the first pollster:\n\n\npolls %&gt;%\n  filter(pollster==\"Rasmussen Reports/Pulse Opinion Research\") %&gt;%\n  summarize(N_1 = n())\n\nWhat is the expected value of \\(\\bar{Y}_1\\)? What is the sample mean \\(\\bar{y}_1\\)?\n\nWhat is the sample variance \\(s^2_1\\) of the sample \\(Y_1\\)? Using \\(s^2_1\\), what is your estimate of the standard error of the mean \\(\\bar{Y}_1\\)?\nSuppose we define \\(\\bar{Y}_2\\) as the average of poll results from the second pollster, \\(Y_{2,1},\\dots,Y_{2,N_2}\\) with \\(N_2\\) the number of polls conducted by the second pollster. What is the expected value \\(\\bar{Y}_2\\), the sample mean \\(\\bar{y}_2\\), the sample variance \\(s^2_2\\), and the standard error of the mean \\(\\bar{Y}_2\\)?\nWhat does the CLT tell us about the distribution of a new random variable that is defined as \\(\\bar{Y}_2 - \\bar{Y}_1\\)?\n\n\nNothing, because this is not the average of a sample.\nBecause the \\(Y_{ij}\\) are approximately normal, so are the averages.\nNoting that \\(\\bar{Y}_2\\) and \\(\\bar{Y}_1\\) are sample averages, if we assume \\(N_2\\) and \\(N_1\\) are large enough, each is approximately normal. The difference of normals is also normal.\nThe data are not 0 or 1, so CLT does not apply.\n\n\nThe new random variable defined as \\(\\bar{Y}_2 - \\bar{Y}_1\\) has an expected value of \\(b_2 - b_1\\). To see this, take the equation from problem 2 defined for each pollster, write out the difference, and take expectations. \\(b_2 = b_1\\) is the pollster bias difference we want to learn about statistically. If our model holds, then this random variable has an approximately normal distribution and we know its standard error – by the rules of random variable variance, the standard error is the square root of the variance of the new random variable, and the variance of the new variable is the sum of the variances minus twice the covariance (which is zero by our assumptions). The standard error of our new random variable depends on (and can be calculated using) the standard errors of the \\(Y\\) above, which we already estimated in 3-5.\n\nThe statistic formed by dividing our estimate of \\(b_2-b_1\\) by its estimated standard error:\n\\[\n\\frac{\\bar{y}_2 - \\bar{y}_1}{\\sqrt{\\underbrace{s_2^2/N_2}_{\\text{Std error of mean from 4, squared}} + \\underbrace{s_1^2/N_1}_{\\text{Std error of mean from 5, squared}}}}\n\\]\nis called the t-statistic. Now you should be able to do the calculations necessary to answer the question: is \\(b_2 - b_1\\) different from 0? You can use the t-statistic (via a t-test), or you can build a 95% confidence interval around your estimate and see if it includes 0.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "9: Statistical Models and Uncertainty"
    ]
  },
  {
    "objectID": "assignment/09-assignment.html#footnotes",
    "href": "assignment/09-assignment.html#footnotes",
    "title": "9: Statistical Models and Uncertainty",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.youtube.com/watch?v=TbKkjm-gheY↩︎\nhttps://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html↩︎\nhttps://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/↩︎\nhttps://projects.fivethirtyeight.com/2016-election-forecast/↩︎",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "9: Statistical Models and Uncertainty"
    ]
  },
  {
    "objectID": "assignment/07-assignment.html",
    "href": "assignment/07-assignment.html",
    "title": "7: Model Building",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, March 10th\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.\nThis week’s lab will extend last week’s lab. The introduction is a direct repeat.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "7: Model Building"
    ]
  },
  {
    "objectID": "assignment/07-assignment.html#backstory-and-set-up",
    "href": "assignment/07-assignment.html#backstory-and-set-up",
    "title": "7: Model Building",
    "section": "Backstory and Set Up",
    "text": "Backstory and Set Up\nYou have been recently hired to Zillow’s Zestimate product team as a junior analyst. As a part of their regular hazing, they have given you access to a small subset of their historic sales data. Your job is to present some basic predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.\nFirst, let’s load the data.\n\nameslist &lt;- read.table('https://ec242.netlify.app/data/ames.csv', \n                   header = TRUE,\n                   sep = ',')",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "7: Model Building"
    ]
  },
  {
    "objectID": "assignment/07-assignment.html#building-a-model",
    "href": "assignment/07-assignment.html#building-a-model",
    "title": "7: Model Building",
    "section": "Building a Model",
    "text": "Building a Model\nWe’re now ready to start playing with a model. We will start by using the lm() function to fit a simple linear regression model, with SalePrice as the response and GrLivArea as the predictor.\nRecall that the basic lm() syntax is lm(y∼x,data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept. Let’s quickly run this with two variables:\n\nlm.fit = lm(SalePrice ~ GrLivArea, data = ameslist)\n\nIf we type lm.fit, some basic information about the model is output. For more detailed information, we use summary(lm.fit). This gives us p-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the entire model.1\nUtilizing these functions hels us see some interesting results. Note that we built (nearly) the simplest possible model:\n\\[\\text{SalePrice} = \\beta_0 + \\beta_1*(\\text{GrLivArea}) + \\epsilon.\\]\nBut even on its own, this model is instructive. It suggest that an increase in overall living area of 1 ft \\(^2\\) is correlated with an expected increase in sales price of $107. (Note that we cannot make causal claims!)\nSaving the model as we did above is useful because we can explore other pieces of information it stores. Specifically, we can use the names() function in order to find out what else is stored in lm.fit. Although we can extract these quantities by name—e.g. lm.fit$coefficients—it is safer to use the extractor functions like coef(lm.fit) to access them. We can also use a handy tool like plot() applied directly to lm.fit to see some interesting data that is automatically stored by the model.\nTry it: Use plot() to explore the model above (it will make a sequence of plots; don’t put it in a code chunk, just use it for your own exploration). Do you suspect that some outliers have a large influence on the data? We will explore this point specifically in the future.\nWe can now go crazy adding variables to our model. It’s as simple as appending them to the previous code—though you should be careful executing this, as it will overwrite your previous output:\n\nlm.fit = lm(SalePrice ~ GrLivArea + LotArea, data = ameslist)\n\nTry it: Does controlling for LotArea change the qualitative conclusions from the previous regression? What about the quantitative results? Does the direction of the change in the quantitative results make sense to you?\n\n\n\n\n\n\nExercises\n\n\n\n\nUse the lm() function in a simple linear regression (e.g., with only one predictor) with SalePrice as the response to determine the value of a garage.\nUse the lm() function to perform a multiple linear regression with SalePrice as the response and all other variables from your Ames data as the predictors. You can do this easily with the formula SalePrice ~ . which tells lm to use all of the data’s columns (except SalePrice) on the right-hand-side. To do this, you’ll need to drop a few variables first, though. Use dplyr::select(-PoolQC, -MiscFeature, -Fence, -FireplaceQu, -LotFrontage, -Exterior2nd, -Electrical) to get rid of some variables that have a lot of NA values. Use the summary() or tidy function to print the results. Comment on the output. For instance:\n\nIs there a relationship between the predictors and the response?\nWhich predictors appear to have a statistically significant relationship to the response? (Hint: look for stars)\nWhat does the coefficient for the year variable suggest?\n\nThere are a few NAs in the output from the regression in Question 2. You can use tidy to save the output in a familiar data.frame style “tibble”, and then explore it to see what variables are coming up NA. Remember what R did when we tried to give it dummy variables representing all three possible values of a factor variable (see “Parameterization” in Example 06). Keeping that in mind, scroll to the first NA in your regression output and see if you can explain why it might be NA. Please remember that we can use functions like View() to explore data, but we never put View() in a code chunk.\nIt’s rarely a good idea to throw all the variables into a regression. We want to be smarter about building our model. We’ll use fewer variables, but include interactions. As we saw this week, the : symbol allows you to create an interction term between two variables. Use the : symbols to fit a linear regression model with one well-chosen interaction effect plus 3-4 of the other variables of your choice. Why did you select the variables you did, and what was the result?\nTry a few (e.g., two) different transformations of the variables, such as \\(ln(x)\\), \\(x^2\\), \\(\\sqrt x\\). Do any of these make sense to include in a model of SalePrice? Comment on your findings.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "7: Model Building"
    ]
  },
  {
    "objectID": "assignment/07-assignment.html#footnotes",
    "href": "assignment/07-assignment.html#footnotes",
    "title": "7: Model Building",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen we use the simple regression model with a single input, the \\(F\\)-stat includes the intercept term. Otherwise, it does not. See Lecture 5 for more detail.↩︎",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "7: Model Building"
    ]
  },
  {
    "objectID": "assignment/05-assignment.html",
    "href": "assignment/05-assignment.html",
    "title": "5: Data Wrangling",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, February 17th\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "5: Data Wrangling"
    ]
  },
  {
    "objectID": "assignment/05-assignment.html#data-wrangling-continued",
    "href": "assignment/05-assignment.html#data-wrangling-continued",
    "title": "5: Data Wrangling",
    "section": "Data Wrangling Continued…",
    "text": "Data Wrangling Continued…\nYou still work for a travel booking website as a data analyst. The hotel has once again asked your company for data on corporate bookings at the hotel via your site. Specifically, they have five corporations that are frequent customers of the hotel, and they want to know who spends the most with them. They’ve asked you to help out. Most of the corporate spending is in the form of room reservations, but there are also parking fees that the hotel wants included in the analysis. Your goal: total up spending by corporation and report the biggest and smallest spenders inclusive of rooms and parking.\nYou did this already in class, but your boss now has some different data. It’s similar, and your code from before will help, but it has some new wrinkles in it to tackle. Do not use the data from this week’s Example. Here’s your new data:\n\n booking.csv - Contains the corporation name, the room type, and the dates someone from the corporation stayed at the hotel. It was pulled by an intern who doesn’t understand date-time stamps.\n roomrates.csv - Contains the price of each room on each day. The Lab 11 version of this data is from our German affiliate, so pay attention to the date format, and be careful – it’s very wide, so using head will throw a lot of data at you.\n parking.csv - Contains the corporations who negotiated free parking for employees. It has been updated.\nParking at the hotel is $60 per night if you don’t have free parking. This hotel is in California, so everyone drives and parks when they stay.\n\n\n\n\n\n\n\nEXERCISE 1\n\n\n\n\nAs you did in class, but with your new set of data, total up spending by corporation and report the biggest and smallest spenders inclusive of rooms and parking\nVisualize (using ggplot) each corporation’s spending at the hotel over time and by roomtype. Make one plot with ggplot that shows this.\nVisualize (using ggplot) the room rates over time by room type. Can you pick out one factor that determines when room prices are higher than usual? Note that we know each corporation gets the same room rate as the others on the same day, so this is about room rates, not corporate spending. Make two total plots, the first showing the room rates over time by room type, and the second explaining some feature of one of the room rates (e.g. when is the double room rate high? When is it low?). Using the month(...), day(...) or wday(..., label = TRUE) functions from lubridate will help with figuring out the patterns. Try exploring just one of the room types to start. You don’t have to perfectly analyze the room rate, just find one facet of the rate that changes regularly over time.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "5: Data Wrangling"
    ]
  },
  {
    "objectID": "assignment/03-assignment.html",
    "href": "assignment/03-assignment.html",
    "title": "3: Applying ggplot2 to Real Data",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, February 3rd\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "3: Applying ggplot2 to Real Data"
    ]
  },
  {
    "objectID": "assignment/03-assignment.html#preliminaries",
    "href": "assignment/03-assignment.html#preliminaries",
    "title": "3: Applying ggplot2 to Real Data",
    "section": "Preliminaries",
    "text": "Preliminaries\nAs always, we will first have to load ggplot2. To do this, we will load the tidyverse by running this code:\nlibrary(tidyverse)",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "3: Applying ggplot2 to Real Data"
    ]
  },
  {
    "objectID": "assignment/03-assignment.html#background",
    "href": "assignment/03-assignment.html#background",
    "title": "3: Applying ggplot2 to Real Data",
    "section": "Background",
    "text": "Background\nThe New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order. They’ve provided an interactive map here where you can see the different projects. There’s also a link there to download the complete dataset.\nFor this exercise, you’re going to use this data to visualize the amounts or proportions of different types of essential projects in the five boroughs of New York City (Brooklyn, Manhattan, the Bronx, Queens, and Staten Island).\nAs you hopefully figured out by now, you’ll be doing all your R work in R Markdown. You can use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud), but this is optional. If you decide to do so, either create a new project for this exercise only, or make a project for all your work in this class.\nYou’ll need to download one CSV file and put it somewhere on your computer (or upload it to RStudio.cloud if you’ve gone that direction)—preferably in a folder named data in your project folder. You can download the data from the DOB’s map, or use this link to get it directly:\n\n EssentialConstruction.csv\n\n\nR Markdown\nWriting regular text with R Markdown follows the rules of Markdown. You can make lists; different-size headers, etc. This should be relatively straightfoward. We talked about a few Markdown features like bold and italics in class. See this resource for more formatting.\nYou’ll also need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n\n\n\n\n\n\n\n\n\n\n\nData Prep\nOnce you download the EssentialConstruction.csv file and save it in your project folder, you can open it and start cleaning. Loading in the basic data is straightforward:\n\nlibrary(tidyverse)\nessential = read_csv('pathTo/EssentialConstruction.csv')\n\nWhere the “pathTo” part is the path to your local folder. If you saved the data in the same folder as your template, then you can just use:\n\nessential = read_csv('EssentialConstruction.csv')\n\nOnce loaded, note that each row is an approved project (the JOB NUMBERS are approved projects, so each row is one approved project).\n\n\n\n\n\n\nExercise 1 of 1: Essential pandemic construction\n\n\n\nUh-oh! One of our columns has different capitalization. Use case_when (or any other method) to make sure you have consistent character strings for each borough.\nThen, assume that each row (observation) is an approved construction project.\nA. Show the count or proportion of approved projects by borough using a bar chart. Make sure all the elements of your plot (axes, legend, etc.) are labeled.\nB. Show the count or proportion of approved projects by category using a lollipop chart. Not sure of what a lollipop chart is? Google R ggplot lollipop. A huge portion of knowing how to code is knowing how to google, find examples, and figure out where to put your variables from your data! Make sure all the elements of your plot (axes, legend, etc.) are labeled.\nYou don’t need to make these super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing fill colors with scale_fill_manual() or with palettes.\nBonus\nOverlay the data from Part 1 above onto a map of NYC. Make sure all the elements of your plot (axes, legend, etc.) are labeled. This is hard!",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "3: Applying ggplot2 to Real Data"
    ]
  },
  {
    "objectID": "assignment/03-assignment.html#getting-help",
    "href": "assignment/03-assignment.html#getting-help",
    "title": "3: Applying ggplot2 to Real Data",
    "section": "Getting help",
    "text": "Getting help\nUse the EC242 Slack if you get stuck (click the Slack logo at the top right of this website header).",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "3: Applying ggplot2 to Real Data"
    ]
  },
  {
    "objectID": "assignment/03-assignment.html#turning-everything-in",
    "href": "assignment/03-assignment.html#turning-everything-in",
    "title": "3: Applying ggplot2 to Real Data",
    "section": "Turning everything in",
    "text": "Turning everything in\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a PDF. Upload the PDF file to D2L.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "3: Applying ggplot2 to Real Data"
    ]
  },
  {
    "objectID": "assignment/01-assignment.html",
    "href": "assignment/01-assignment.html",
    "title": "1: Programming Basics in R",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, January 20th.\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.\nIf you have not yet done so, you’ll need to install both R and RStudio. See the Installing page of our course resources for instructions.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "1: Programming Basics in R"
    ]
  },
  {
    "objectID": "assignment/01-assignment.html#if-you-totally-forgot-how-to-use-r",
    "href": "assignment/01-assignment.html#if-you-totally-forgot-how-to-use-r",
    "title": "1: Programming Basics in R",
    "section": "If you totally forgot how to use R",
    "text": "If you totally forgot how to use R\nProf. Kirkpatrick (who teaches this course in Fall semesters) has created a video walkthrough for the basics of using R for another course. You may find it useful here. You can see part A here (labeled “Part 2a”) here ] and part B here (labeled “Part 2b”) . You should already be at this level of familiarity with R, but if you need a review, this is a good place to start.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "1: Programming Basics in R"
    ]
  },
  {
    "objectID": "assignment/01-assignment.html#conditionals",
    "href": "assignment/01-assignment.html#conditionals",
    "title": "1: Programming Basics in R",
    "section": "Conditional expressions",
    "text": "Conditional expressions\nConditional expressions are one of the basic features of programming. They are used for what is called flow control. The most common conditional expression is the if-else statement. In R, we can actually perform quite a bit of data analysis without conditionals. However, they do come up occasionally, and you will need them once you start writing your own functions and packages.\nHere is a very simple example showing the general structure of an if-else statement. The basic idea is to print the reciprocal of a unless a is 0:\n\na &lt;- 0\n\nif(a!=0){\n  print(1/a)\n} else{\n  print(\"No reciprocal for 0.\")\n}\n\n[1] \"No reciprocal for 0.\"\n\n\nLet’s look at one more example using the US murders data frame:\n\nlibrary(dslabs)\ndata(murders)\nmurder_rate &lt;- murders$total / murders$population*100000\n\nHere is a very simple example that tells us which states, if any, have a murder rate lower than 0.5 per 100,000. The if statement protects us from the case in which no state satisfies the condition.\n\nind &lt;- which.min(murder_rate)\n\nif(murder_rate[ind] &lt; 0.5){\n  print(murders$state[ind])\n} else{\n  print(\"No state has murder rate that low\")\n}\n\n[1] \"Vermont\"\n\n\nIf we try it again with a rate of 0.25, we get a different answer:\n\nif(murder_rate[ind] &lt; 0.25){\n  print(murders$state[ind])\n} else{\n  print(\"No state has a murder rate that low.\")\n}\n\n[1] \"No state has a murder rate that low.\"\n\n\nA related function that is very useful is ifelse. This function takes three arguments: a logical and two possible answers. If the logical is TRUE, the value in the second argument is returned and if FALSE, the value in the third argument is returned. Here is an example:\n\na &lt;- 0\nifelse(a &gt; 0, 1/a, NA)\n\n[1] NA\n\n\nThe function is particularly useful because it works on vectors. It examines each entry of the logical vector and returns elements from the vector provided in the second argument, if the entry is TRUE, or elements from the vector provided in the third argument, if the entry is FALSE.\n\na &lt;- c(0, 1, 2, -4, 5)\nresult &lt;- ifelse(a &gt; 0, 1/a, NA)\n\nThis table helps us see what happened:\n\n\n\n\n\na\nis_a_positive\nanswer1\nanswer2\nresult\n\n\n\n\n0\nFALSE\nInf\nNA\nNA\n\n\n1\nTRUE\n1.00\nNA\n1.0\n\n\n2\nTRUE\n0.50\nNA\n0.5\n\n\n-4\nFALSE\n-0.25\nNA\nNA\n\n\n5\nTRUE\n0.20\nNA\n0.2\n\n\n\n\n\n\n\nHere is an example of how this function can be readily used to replace all the missing values in a vector with zeros:\n\ndata(na_example)\nno_nas &lt;- ifelse(is.na(na_example), 0, na_example)\nsum(is.na(no_nas))\n\n[1] 0\n\n\nTwo other useful functions are any and all. The any function takes a vector of logicals and returns TRUE if any of the entries is TRUE. The all function takes a vector of logicals and returns TRUE if all of the entries are TRUE. Here is an example:\n\nz &lt;- c(TRUE, TRUE, FALSE)\nany(z)\n\n[1] TRUE\n\nall(z)\n\n[1] FALSE",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "1: Programming Basics in R"
    ]
  },
  {
    "objectID": "assignment/01-assignment.html#defining-functions",
    "href": "assignment/01-assignment.html#defining-functions",
    "title": "1: Programming Basics in R",
    "section": "Defining functions",
    "text": "Defining functions\nAs you become more experienced, you will find yourself needing to perform the same operations over and over. A simple example is computing averages. We can compute the average of a vector x using the sum and length functions: sum(x)/length(x). Because we do this repeatedly, it is much more efficient to write a function that performs this operation. This particular operation is so common that someone already wrote the mean function and it is included in base R. However, you will encounter situations in which the function does not already exist, so R permits you to write your own. A simple version of a function that computes the average can be defined like this:\n\navg &lt;- function(x){\n  s &lt;- sum(x)\n  n &lt;- length(x)\n  s/n\n}\n\nNow avg is a function that computes the mean:\n\nx &lt;- 1:100\nidentical(mean(x), avg(x))\n\n[1] TRUE\n\n\nNotice that variables defined inside a function are not saved in the workspace. So while we use s and n when we call avg, the values are created and changed only during the call. Here is an illustrative example:\n\ns &lt;- 3\navg(1:10)\n\n[1] 5.5\n\ns\n\n[1] 3\n\n\nNote how s is still 3 after we call avg.\nIn general, functions are objects, so we assign them to variable names with &lt;-. The function function tells R you are about to define a function. The general form of a function definition looks like this:\n\nmy_function &lt;- function(VARIABLE_NAME){\n  perform operations on VARIABLE_NAME and calculate VALUE\n  VALUE\n}\n\nThe functions you define can have multiple arguments as well as default values. For example, we can define a function that computes either the arithmetic or geometric average depending on a user defined variable like this:\n\navg &lt;- function(x, arithmetic = TRUE){\n  n &lt;- length(x)\n  ifelse(arithmetic, sum(x)/n, prod(x)^(1/n))\n}\n\nWe will learn more about how to create functions through experience as we face more complex tasks.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "1: Programming Basics in R"
    ]
  },
  {
    "objectID": "assignment/01-assignment.html#namespaces",
    "href": "assignment/01-assignment.html#namespaces",
    "title": "1: Programming Basics in R",
    "section": "Namespaces",
    "text": "Namespaces\nOnce you start becoming more of an R expert user, you will likely need to load several add-on packages for some of your analysis. Once you start doing this, it is likely that two packages use the same name for two different functions. And often these functions do completely different things. In fact, you have already encountered this because both dplyr and the R-base stats package define a filter function. There are five other examples in dplyr. We know this because when we first load dplyr we see the following message:\nThe following objects are masked from ‘package:stats’:\n\n    filter, lag\n\nThe following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\nSo what does R do when we type filter? Does it use the dplyr function or the stats function? From our previous work we know it uses the dplyr one. But what if we want to use the stats version?\nThese functions live in different namespaces. R will follow a certain order when searching for a function in these namespaces. You can see the order by typing:\n\nsearch()\n\nThe first entry in this list is the global environment which includes all the objects you define.\nSo what if we want to use the stats filter instead of the dplyr filter but dplyr appears first in the search list? You can force the use of a specific namespace by using double colons (::) like this:\n\nstats::filter\n\nIf we want to be absolutely sure that we use the dplyr filter, we can use\n\ndplyr::filter\n\nAlso note that if we want to use a function in a package without loading the entire package, we can use the double colon as well.\nFor more on this more advanced topic we recommend the R packages book1.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "1: Programming Basics in R"
    ]
  },
  {
    "objectID": "assignment/01-assignment.html#for-loops",
    "href": "assignment/01-assignment.html#for-loops",
    "title": "1: Programming Basics in R",
    "section": "For-loops",
    "text": "For-loops\nIf we had to write this section in a single sentence, it would be: Don’t use for-loops. Looping is intuitive, but R is designed to provide more computationally efficient solutions. For-loops should be considered a quick-and-dirty way to get an answer. But, hey, you live your own life. Below we provide a brief overview to for-looping.\nThe formula for the sum of the series \\(1+2+\\dots+n\\) is \\(n(n+1)/2\\). What if we weren’t sure that was the right function? How could we check? Using what we learned about functions we can create one that computes the \\(S_n\\):\n\ncompute_s_n &lt;- function(n){\n  x &lt;- 1:n\n  sum(x)\n}\n\nHow can we compute \\(S_n\\) for various values of \\(n\\), say \\(n=1,\\dots,25\\)? Do we write 25 lines of code calling compute_s_n? No, that is what for-loops are for in programming. In this case, we are performing exactly the same task over and over, and the only thing that is changing is the value of \\(n\\). For-loops let us define the range that our variable takes (in our example \\(n=1,\\dots,10\\)), then change the value and evaluate expression as you loop.\nPerhaps the simplest example of a for-loop is this useless piece of code:\n\nfor(i in 1:5){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nHere is the for-loop we would write for our \\(S_n\\) example:\n\nm &lt;- 25\ns_n &lt;- vector(length = m) # create an empty vector\nfor(n in 1:m){\n  s_n[n] &lt;- compute_s_n(n)\n}\n\nIn each iteration \\(n=1\\), \\(n=2\\), etc…, we compute \\(S_n\\) and store it in the \\(n\\)th entry of s_n.\nNow we can create a plot to search for a pattern:\n\nn &lt;- 1:m\nplot(n, s_n)\n\n\n\n\n\n\n\n\n\n\nIf you noticed that it appears to be a quadratic, you are on the right track because the formula is \\(n(n+1)/2\\).",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "1: Programming Basics in R"
    ]
  },
  {
    "objectID": "assignment/01-assignment.html#vectorization",
    "href": "assignment/01-assignment.html#vectorization",
    "title": "1: Programming Basics in R",
    "section": "Vectorization and functionals",
    "text": "Vectorization and functionals\nAlthough for-loops are an important concept to understand, in R we rarely use them. As you learn more R, you will realize that vectorization is preferred over for-loops since it results in shorter and clearer code. (It’s also vastly more efficient computationally, which can matter as your data grows.) A vectorized function is a function that will apply the same operation on each of the vectors.\n\nx &lt;- 1:10\nsqrt(x)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\ny &lt;- 1:10\nx*y\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\nTo make this calculation, there is no need for for-loops. However, not all functions work this way. For instance, the function we just wrote, compute_s_n, does not work element-wise since it is expecting a scalar. This piece of code does not run the function on each entry of n:\n\nn &lt;- 1:25\ncompute_s_n(n)\n\nFunctionals are functions that help us apply the same function to each entry in a vector, matrix, data frame, or list. Here we cover the functional that operates on numeric, logical, and character vectors: sapply.\nThe function sapply permits us to perform element-wise operations on any function. Here is how it works:\n\nx &lt;- 1:10\nsapply(x, sqrt)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\n\nEach element of x is passed on to the function sqrt and the result is returned. These results are concatenated. In this case, the result is a vector of the same length as the original x. This implies that the for-loop above can be written as follows:\n\nn &lt;- 1:25\ns_n &lt;- sapply(n, compute_s_n)\n\nOther functionals are apply, lapply, tapply, mapply, vapply, and replicate. We mostly use sapply, apply, and replicate in this book, but we recommend familiarizing yourselves with the others as they can be very useful.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "1: Programming Basics in R"
    ]
  },
  {
    "objectID": "assignment/01-assignment.html#footnotes",
    "href": "assignment/01-assignment.html#footnotes",
    "title": "1: Programming Basics in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttp://r-pkgs.had.co.nz/namespace.html↩︎",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "1: Programming Basics in R"
    ]
  },
  {
    "objectID": "assignment/02-assignment.html",
    "href": "assignment/02-assignment.html",
    "title": "2: Visualization with ggplot",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, January 27th.\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "2: Visualization with ggplot"
    ]
  },
  {
    "objectID": "assignment/02-assignment.html#a-note-on-exercises",
    "href": "assignment/02-assignment.html#a-note-on-exercises",
    "title": "2: Visualization with ggplot",
    "section": "A note on exercises",
    "text": "A note on exercises\nInstead of putting all of your exercises at the end in 1 section, each of the 3 exercises are contained in a callout box like this:\n\n\n\n\n\n\nExercise 1 of 3\n\n\n\nDo this stuff\n\n\nSo find those in the assignment below. There are a lot of questions clearly labeled “OPTIONAL” as well – it helps to at least take a look at them, but only those questions in the labeled boxes are required for this lab assignment.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "2: Visualization with ggplot"
    ]
  },
  {
    "objectID": "assignment/02-assignment.html#data-viz",
    "href": "assignment/02-assignment.html#data-viz",
    "title": "2: Visualization with ggplot",
    "section": "Data Viz",
    "text": "Data Viz\nOur primary tool for data visualization in the course will be ggplot. Technically, we’re using ggplot2; the o.g. version lacked some of the modern features of its big brother. ggplot2 implements the grammar of graphics, a coherent and relatively straightforward system for describing and building graphs. With ggplot2, you can do more faster by learning one system and applying it in many places. Other languages provide more specific tools, but require you to learn a different tool for each application. In this class, we’ll dig into a single package for our visuals.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "2: Visualization with ggplot"
    ]
  },
  {
    "objectID": "assignment/02-assignment.html#using-ggplot2",
    "href": "assignment/02-assignment.html#using-ggplot2",
    "title": "2: Visualization with ggplot",
    "section": "Using ggplot2",
    "text": "Using ggplot2\nIn order to get our hands dirty, we will first have to load ggplot2. To do this, and to access the datasets, help pages, and functions that we will use in this assignment, we will load the so-called tidyverse by running this code:\nlibrary(tidyverse)\nIf you run this code and get an error message “there is no package called ‘tidyverse’”, you’ll need to first install it, then run library() once again. To install packages in R, we utilize the simple function install.packages(). In this case, we would write:\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\nOnce we’re up and running, we’re ready to dive into some basic exercises. ggplot2 works by specifying the connections between the variables in the data and the colors, points, and shapes you see on the screen. These logical connections are called aesthetic mappings or simply aesthetics.\n\nHow to use ggplot2 – the too-fast and wholly unclear recipe\n\ndata =: Define what your data is. For instance, below we’ll use the mpg data frame found in ggplot2 (by using ggplot2::mpg). As a reminder, a data frame is a rectangular collection of variables (in the columns) and observations (in the rows). This structure of data is often called a “table” but we’ll try to use terms slightly more precisely. The mpg data frame contains observations collected by the US Environmental Protection Agency on 38 different models of car.\nmapping = aes(...): How to map the variables in the data to aesthetics\n\nAxes, size of points, intensities of colors, which colors, shape of points, lines/points\n\nThen say what type of plot you want:\n\nboxplot, scatterplot, histogram, …\nthese are called ‘geoms’ in ggplot’s grammar, such as geom_point() giving scatter plots\n\n\nlibrary(ggplot2)\n... + geom_point() # Produces scatterplots\n... + geom_bar() # Bar plots\n.... + geom_boxplot() # boxplots\n... #\nYou link these steps by literally adding them together with + as we’ll see.\nTry it: (OPTIONAL) What other types of plots are there? Try to find several more geom_ functions.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "2: Visualization with ggplot"
    ]
  },
  {
    "objectID": "assignment/02-assignment.html#mappings-link-data-to-things-you-see",
    "href": "assignment/02-assignment.html#mappings-link-data-to-things-you-see",
    "title": "2: Visualization with ggplot",
    "section": "Mappings Link Data to Things You See",
    "text": "Mappings Link Data to Things You See\n\nlibrary(gapminder)\nlibrary(ggplot2)\nhead(gapminder::gapminder)\n## # A tibble: 6 × 6\n##   country     continent  year lifeExp      pop gdpPercap\n##   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n## 1 Afghanistan Asia       1952    28.8  8425333      779.\n## 2 Afghanistan Asia       1957    30.3  9240934      821.\n## 3 Afghanistan Asia       1962    32.0 10267083      853.\n## 4 Afghanistan Asia       1967    34.0 11537966      836.\n## 5 Afghanistan Asia       1972    36.1 13079460      740.\n## 6 Afghanistan Asia       1977    38.4 14880372      786.\np &lt;- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap, y = lifeExp))\np + geom_point()\n\n\n\n\n\n\n\n\nNote that this gapminder dataset is from the gapminder package is not the same as the dslabs gapminder. If you’re in doubt, use gapminder = gapminder::gapminder to set the gapminder package’s gapminder datset as your gapminder object (whew!). Compare head(dslabs::gapminder) and head(gapminder::gapminder) to see the difference. The dslabs::... says to find the thing in the dslabs package called “gapminder”.\nAbove we’ve loaded a different dataset and have started to explore a particular relationship. Before putting in this code yourself, try to intuit what might be going on in this ggplot code. Any ideas?\nHere’s a breakdown of everything that happens after the p&lt;- ggplot() call:\n\ndata = gapminder tells ggplot to use gapminder dataset, so if variable names are mentioned, they should be looked up in gapminder\nmapping = aes(...) shows that the mapping is a function call. There is a deeper logic to this that I will disucss below, but it’s easiest to simply accept that this is how you write it. Put another way, the mapping = aes(...) argument links variables to things you will see on the plot.\naes(x = gdpPercap, y = lifeExp) maps the GDP data onto x, which is a known aesthetic (the x-coordinate) and life expectancy data onto y\n\nx and y are predefined names that are used by ggplot and friends\n\n\n\n\n\n\n\n\nExercise 1 of 3\n\n\n\nLet’s use some new data. Assuming you have ggplot2 loaded using library(ggplot2), you can load up a dataset called mpg using data(mpg).\nAmong the variables in mpg are:\n\ndispl, a car’s engine size, in litres. Bigger means more powerful.\nhwy, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance.\n\nGenerate a scatterplot between these two variables. Does it capture the intuitive relationship you expected? What happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n\n\n\nWhat do you see?\nIt turns out there’s a reason for doing all of this:\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.”” — John Tukey\n\nIn the plot you made above, one group of points seems to fall outside of the linear trend. These cars have a higher mileage than you might expect. How can you explain these cars?\nLet’s hypothesize that the cars are hybrids. One way to test this hypothesis is to look at the class value for each car. The class variable of the mpg dataset classifies cars into groups such as compact, midsize, and SUV. If the outlying points are hybrids, they should be classified as compact cars or, perhaps, subcompact cars (keep in mind that this data was collected before hybrid trucks and SUVs became popular).\nYou can add a third variable, like class, to a two dimensional scatterplot by mapping it to an aesthetic. An aesthetic is a visual property of the objects in your plot. Aesthetics include things like the size, the shape, or the color of your points. You can display a point (like the one below) in different ways by changing the values of its aesthetic properties. Since we already use the word “value” to describe data, let’s use the word “level” to describe aesthetic properties. Thus, we are interested in exploring class as a level.\nYou can convey information about your data by mapping the aesthetics in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car. To map an aesthetic to a variable, associate the name of the aesthetic to the name of the variable inside aes(). ggplot2 will automatically assign a unique level of the aesthetic (here a unique color) to each unique value of the variable, a process known as scaling. ggplot2 will also add a legend that explains which levels correspond to which values.\n\n\n\n\n\n\nExercise 2 of 3\n\n\n\nUsing your previous scatterplot of displ and hwy, map the colors of your points to the class variable to reveal the class of each car. What conclusions can we make?\n\n\n\n\nBack to gapminder::gapminder\nLet’s explore our previously saved p (using gapminder::gapminder) in greater detail. As with Exercise 1, we’ll add a layer. This says how some data gets turned into concrete visual aspects.\np + geom_point()\np + geom_smooth()\nNote: Both of the above geom’s use the same mapping, where the x-axis represents gdpPercap and the y-axis represents lifeExp. You can find this yourself with some ease. But the first one maps the data to individual points, the other one maps it to a smooth line with error ranges.\nWe get a message that tells us that geom_smooth() is using the method = ‘gam’, so presumably we can use other methods. Let’s see if we can figure out which other methods there are.\n?geom_smooth\np + geom_point() + geom_smooth() + geom_smooth(method = ...) + geom_smooth(method = ...)\np + geom_point() + geom_smooth() + geom_smooth(method = ...) + geom_smooth(method = ..., color = \"red\")\nYou may start to see why ggplot2’s way of breaking up tasks is quite powerful: the geometric objects can all reuse the same mapping of data to aesthetics, yet the results are quite different. And if we want later geoms to use different mappings, then we can override them – but it isn’t necessary.\nConsider the output we’ve explored thus far. One potential issue lurking in the data is that most of it is bunched to the left. If we instead used a logarithmic scale, we should be able to spread the data out better.\np + geom_point() + geom_smooth(method = \"lm\") + scale_x_log10()\nTry it: (OPTIONAL) Describe what the scale_x_log10() does. Why is it a more evenly distributed cloud of points now? (2-3 sentences.)\nNice. We’re starting to get somewhere. But, you might notice that the x-axis now has scientific notation. Let’s change that.\nlibrary(scales)\np + geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_x_log10(labels = scales::dollar)\np + geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_x_log10(labels = scales::...)\nTry it: (OPTIONAL) What does the dollar() call do? How can you find other ways of relabeling the scales when using scale_x_log10()?\n?dollar()",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "2: Visualization with ggplot"
    ]
  },
  {
    "objectID": "assignment/02-assignment.html#the-recipe",
    "href": "assignment/02-assignment.html#the-recipe",
    "title": "2: Visualization with ggplot",
    "section": "The Recipe",
    "text": "The Recipe\n\nTell the ggplot() function what our data is.\nTell ggplot() what relationships we want to see. For convenience we will put the results of the first two steps in an object called p.\nTell ggplot how we want to see the relationships in our data.\nLayer on geoms as needed, by adding them on the p object one at a time.\nUse some additional functions to adjust scales, labels, tickmarks, titles.\n\n\ne.g. scale_, labs(), and guides() functions\n\nAs you start to run more R code, you’re likely to run into problems. Don’t worry — it happens to everyone. I have been writing code in numerous languages for years, and every day I still write code that doesn’t work. Sadly, R is particularly persnickity, and its error messages are often opaque.\nStart by carefully comparing the code that you’re running to the code in these notes. R is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every ” is paired with another “. Sometimes you’ll run the code and nothing happens. Check the left-hand of your console: if it’s a +, it means that R doesn’t think you’ve typed a complete expression and it’s waiting for you to finish it. In this case, it’s usually easy to start from scratch again by pressing ESCAPE to abort processing the current command.\nOne common problem when creating ggplot2 graphics is to put the + in the wrong place: it has to come at the end of the line, not the start.\n\nMapping Aesthetics vs Setting them\np &lt;- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap, y = lifeExp, color = 'yellow'))\np + geom_point() + scale_x_log10()\nThis is interesting (or annoying): the points are not yellow. How can we tell ggplot to draw yellow points?\np &lt;- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap, y = lifeExp, ...))\np + geom_point(...) + scale_x_log10()\nTry it: (OPTIONAL) describe in your words what is going on. One way to avoid such mistakes is to read arguments inside aes(&lt;property&gt; = &lt;variable&gt;)as the property  in the graph is determined by the data in .\nTry it: (OPTIONAL) Write the above sentence for the original call aes(x = gdpPercap, y = lifeExp, color = 'yellow').\nAesthetics convey information about a variable in the dataset, whereas setting the color of all points to yellow conveys no information about the dataset - it changes the appearance of the plot in a way that is independent of the underlying data.\nRemember: color = 'yellow' and aes(color = 'yellow') are very different, and the second makes usually no sense, as 'yellow' is treated as data.\np &lt;- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap, y = lifeExp))\np + geom_point() + geom_smooth(color = \"orange\", se = FALSE, size = 8, method = \"lm\") + scale_x_log10()\nTry it: (OPTIONAL) Write down what all those arguments in geom_smooth(...) do.\np + geom_point(alpha = 0.3) +\n  geom_smooth(method = \"gam\") +\n  scale_x_log10(labels = scales::dollar) +\n  labs(x = \"GDP Per Capita\", y = \"Life Expectancy in Years\",\n       title = \"Economic Growth and Life Expectancy\",\n       subtitle = \"Data Points are country-years\",\n       caption = \"Source: Gapminder\")\nColoring by continent:\nlibrary(scales)\np &lt;- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap, y = lifeExp, color = continent, fill = continent))\np + geom_point()\np + geom_point() + scale_x_log10(labels = dollar)\np + geom_point() + scale_x_log10(labels = dollar) + geom_smooth()\nTry it: (OPTIONAL) What does fill = continent do? What do you think about the match of colors between lines and error bands?\np &lt;- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap, y = lifeExp))\np + geom_point(mapping = aes(color = continent)) + geom_smooth() + scale_x_log10()\nTry it: (OPTIONAL) Notice how the above code leads to a single smooth line, not one per continent. Why?\nTry it: (OPTIONAL) What is bad about the following example, assuming the graph is the one we want? Think about why you should set aesthetics at the top level rather than at the individual geometry level if that’s your intent.\np &lt;- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap, y = lifeExp))\np + geom_point(mapping = aes(color = continent)) +\n  geom_smooth(mapping = aes(color = continent, fill = continent)) +\n  scale_x_log10() +\n  geom_smooth(mapping = aes(color = continent), method = \"gam\")\n\n\n\n\n\n\nExercise 3 of 3\n\n\n\nGenerate two new plots with data = gapminder (note: you’ll need to install the package by the same name if you have not already). Label the axes and the header with clear, easy to understand language. In a few sentences, describe what you’ve visualized and why.\nNote that this is your first foray into ggplot2; accordingly, you should try to make sure that you do not bite off more than you can chew. We will improve and refine our abilities as we progress through the semester.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "2: Visualization with ggplot"
    ]
  },
  {
    "objectID": "assignment/04-assignment.html",
    "href": "assignment/04-assignment.html",
    "title": "4: Visualizing Large(ish) Data",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, February 10th\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "4: Visualizing Large(ish) Data"
    ]
  },
  {
    "objectID": "assignment/04-assignment.html#postscript-how-we-got-this-unemployment-data",
    "href": "assignment/04-assignment.html#postscript-how-we-got-this-unemployment-data",
    "title": "4: Visualizing Large(ish) Data",
    "section": "Postscript: how we got this unemployment data",
    "text": "Postscript: how we got this unemployment data\nFor the curious, here’s the code we used to download the unemployment data from the BLS.\nAnd to pull the curtain back and show how much googling is involved in data visualization (and data analysis and programming in general), here was my process for getting this data:\n\nWe thought “We want to have students show variation in something domestic over time” and then we googled “us data by state”. Nothing really came up (since it was an exceedingly vague search in the first place), but some results mentioned unemployment rates, so we figured that could be cool.\nWe googled “unemployment statistics by state over time” and found that the BLS keeps statistics on this. We clicked on the “Data Tools” link in their main navigation bar, clicked on “Unemployment”, and then clicked on the “Multi-screen data search” button for the Local Area Unemployment Statistics (LAUS).\nWe walked through the multiple screens and got excited that we’d be able to download all unemployment stats for all states for a ton of years, but then the final page had links to 51 individual Excel files, which was dumb.\nSo we went back to Google and searched for “download bls data r” and found a few different packages people have written to do this. The first one we clicked on was blscrapeR at GitHub, and it looked like it had been updated recently, so we went with it.\nWe followed the examples in the blscrapeR package and downloaded data for every state.\n\nAnother day in the life of doing modern data science. This is an example of something you will be able to do by the end of this class. we had no idea people had written R packages to access BLS data, but there are (at least) 3 packages out there. After a few minutes of tinkering, we got it working and it is relatively straightforward.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "4: Visualizing Large(ish) Data"
    ]
  },
  {
    "objectID": "assignment/06-assignment.html",
    "href": "assignment/06-assignment.html",
    "title": "6: Correlations and Simple Linear Models",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, February 24th\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "6: Correlations and Simple Linear Models"
    ]
  },
  {
    "objectID": "assignment/06-assignment.html#backstory-and-set-up",
    "href": "assignment/06-assignment.html#backstory-and-set-up",
    "title": "6: Correlations and Simple Linear Models",
    "section": "Backstory and Set Up",
    "text": "Backstory and Set Up\nYou have been recently hired to Zillow’s Zestimate product team as a junior analyst. As a part of their regular hazing, they have given you access to a small subset of their historic sales data. Your job is to present some basic predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.\nFirst, let’s load the data.\n\nameslist  &lt;- read.csv('https://ec242.netlify.app/data/ames.csv',\n                      stringsAsFactors = FALSE)\n\nBefore we proceed, let’s note one thing about the (simple) code above. We specify an argument to read.csv called stringsAsFactors. By default, read.csv (the base CSV reading function, which is different from read_csv, the tidyverse CSV function) turns anything that is a character vector into a factor variable. That’s great if you’re importing things like state abbreviations. That’s not helpful if you’re importing character strings that don’t have any repitition (e.g. names), or character strings that really should be numeric. We’ll handle our character strings our selves, thankyouverymuch.\n\nData Exploration and Processing\nWe are not going to tell you anything about this data. This is intended to replicate a real-world experience that you will all encounter in the (possibly near) future: someone hands you data and you’re expected to make sense of it. Fortunately for us, this data is (somewhat) self-contained. We’ll first check the variable names to try to divine some information. Recall, we have a handy little function for that:\n\nnames(ameslist)\n\nNote that, when doing data exploration, we will sometimes choose to not save our output. This is a judgement call; here we’ve chosen to merely inspect the variables rather than diving in.\nInspection yields some obvious truths. For example:\n\n\n\nVariable\nExplanation\nType\n\n\n\n\nID\nUnique identifier for each row\nint\n\n\nLotArea\nSize of lot (units unknown)\nint\n\n\nSalePrice\nSale price of house ($)\nint\n\n\n\n…but we face some not-so-obvious things as well. For example:\n\n\n\nVariable\nExplanation\nType\n\n\n\n\nLotShape\n? Something about the lot\nchr\n\n\nMSSubClass\n? No clue at all\nint\n\n\nCondition1\n? Seems like street info\nchr\n\n\n\nIt will be difficult to learn anything about the data that is of type int without outside documentation unless it refers to a count of something (e.g. bedrooms). However, we can learn something more about the chr-type variables. In order to understand these a little better, we need to review some of the values that each take on. We can use unique() to see the unique values it takes. Sometimes, it helps to see how often some value comes up if we’re trying to understand a variable’s meaning. One handy way of learning this is to use table() (which we’ve seen before) to get a count of the different values a variable can take. This dataset will have some pernicious NAs in it, so when you use table, add useNA = 'always' as an argument to ensure that we see counts of all values.\nTry it: Go through the variables in the dataset and make a note about your interpretation for each. Many will be obvious, but some require additional thought.\nAlthough there are some variables that would be difficult to clean, there are a few that we can address with relative ease. Consider, for instance, the variable GarageType. This might not be that important, but, remember, the weather in Ames, IA is pretty crummy—a detached garage might be a dealbreaker for some would-be homebuyers. Let’s inspect the values:\n\n&gt; unique(ameslist$GarageType)\n[1] Attchd  Detchd  BuiltIn CarPort &lt;NA&gt; Basment 2Types\n\nWith this, we could make an informed decision and create a new variable. Let’s create OutdoorGarage to indicate, say, homes that have any type of garage that requires the homeowner to walk outdoors after parking their car. (For those who aren’t familiar with different garage types, a car port is not insulated and is therefore considered outdoors. A detached garage presumably requires that the person walks outside after parking. The three other types are inside the main structure, and 2Types we can assume includes at least one attached garage of some sort).\n\n\n\n\n\n\nEXERCISE 1 of 5\n\n\n\n\nUse case_when to add a OutdoorGarage column to ameslist that takes the value of 1 when the house has an outdoor garage, and 0 otherwise. Make sure this is a numeric variable (type int). It’s up to you to take a stand on what to do with NA values. Are those outdoor? Indoors? Do we drop all NA values? You often have to decide what to do in cases like this using your knowledge of the context. Document your reasoning for how you handle NA values.\n\n\n\nGenerally speaking, this is a persistent issue, and you will spend an extraordinary amount of time dealing with missing data or data that does not encode a variable exactly as you want it. This is expecially true if you deal with real-world data: you will need to learn how to handle NAs. There are a number of fixes (as always, Google is your friend) and anything that works is good. But you should spend some time thinking about this and learning at least one approach.\nOur goal now is to learn something about correlates between home sale price and the rest of the data. Along the way, you may want to create more variables like OutdoorGarage – for now, make sure those variables are represented as 0 and 1. We’ll work with factor variables more later.\n\n\n\n\n\n\nEXERCISES 2-5\n\n\n\n\nPrune the data to 6-8 of the variables that are type = int about which you have some reasonable intuition for what they mean. Choose those that you believe are likely to be correlated with SalePrice. This must include the variable SalePrice and GrLivArea. Call this new dataset Ames. Produce documentation for this object in the form of a Markdown table or see further documentation here. This must describe each of the 6-8 preserved variables, the values it can take (e.g., can it be negative?) and your definition of the variable. Counting the variable name, this means your table should have three columns. Markdown tables are entered in the text body, not code chunks, of your .rmd, so your code creating Ames will be in a code chunk, and your table will be right after it.\nProduce a scatterplot matrix of the chosen variables1\nCompute a matrix of correlations between these variables using the function cor(). Do the correlations match your prior beliefs? Briefly discuss the correlation between the chosen variables and SalePrice and any correlations between these variables.\nProduce a scatterplot between SalePrice and GrLivArea. Run a linear model using lm() to explore the relationship. Finally, use the geom_abline() function to plot the relationship that you’ve found in the simple linear regression. You’ll need to extract the intercept and slope from your lm object. See coef(...) for information on this.2\n\nWhat is the largest outlier that is above the regression line? Produce the other information about this house.\n\n\n(Bonus) Create a visualization that shows the rise of air conditioning over time in homes in Ames.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "6: Correlations and Simple Linear Models"
    ]
  },
  {
    "objectID": "assignment/06-assignment.html#footnotes",
    "href": "assignment/06-assignment.html#footnotes",
    "title": "6: Correlations and Simple Linear Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you are not familiar with this type of visualization, consult the book (Introduction to Statistical Learning), Chapters 2 and 3.↩︎\nWe could also use geom_smooth(method = 'lm') to add the regression line, but it’s good practice to work with lm objects.↩︎",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "6: Correlations and Simple Linear Models"
    ]
  },
  {
    "objectID": "assignment/08-assignment.html",
    "href": "assignment/08-assignment.html",
    "title": "8: Advanced Linear Model Building",
    "section": "",
    "text": "This is a pretty long lab. Leave plenty of time to complete it.\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "8: Advanced Linear Model Building"
    ]
  },
  {
    "objectID": "assignment/08-assignment.html#backstory-and-set-up",
    "href": "assignment/08-assignment.html#backstory-and-set-up",
    "title": "8: Advanced Linear Model Building",
    "section": "Backstory and Set Up",
    "text": "Backstory and Set Up\nYou still work for Zillow as a junior analyst (sorry). But you’re hunting a promotion. Your job is to present some more advanced predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.\nAs always, let’s load the data.\n\nAmes &lt;- read.table('https://raw.githubusercontent.com/ajkirkpatrick/FS20/postS21_rev/classdata/ames.csv', \n                   header = TRUE,\n                   sep = ',')",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "8: Advanced Linear Model Building"
    ]
  },
  {
    "objectID": "assignment/08-assignment.html#data-cleaning",
    "href": "assignment/08-assignment.html#data-cleaning",
    "title": "8: Advanced Linear Model Building",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nDo not skip this section. This isn’t your kitchen junk drawer – you can’t get away with not cleaning your data.\nOh, the Ames data yet again. It’s given us lots of trouble. Many of you have found a few variables (columns) that should be avoided. The main problem is that some columns have only one value in them, or they have only NA and one value, so once lm(...) drops the NA rows, they are left with only one value. Linear regression by OLS does not like variables that don’t vary! So, let’s be systematic about figuring out which columns in our data are to be avoided.\nThe skimr package is very helpful for seeing what our data contains. Install it, and then use skim(Ames) directly in the console (we’re just looking at data at the moment – do not put skim output into your RMarkdown output - it will give you an error). Take a look at the “complete rate” column - this tells us the fraction of observations in that column that are NA. If it’s very small (see Alley), then that variable will be problematic. The “n_unique” column tells us if there are few or many different values - a “1” in “n_unique” is definitely going to be a problem and you must drop that variable.\nYou can make a note of those columns that have extremely low “complete rates” and drop them to start off. Of course, we could keep them, and drop all observations where any of those columns are NA, but once we do that, there probably won’t be many rows left! There are about 6-7 of them that will drop so many that it will cause an error if we include them in a regression. Let’s drop those. (Note: the list of columns to drop in Lab 06 is a good start and should suffice, but feel free to drop others that have low complete rates).\n\nMany models do not like NA values\npredict has some unusual behavior that can give unexpected results. Thus far, we have mostly used predict(myOLS), which gives the predicted values from a model using the same data that estimated the model. When we ask lm (or, later, other machine learning models) to estimate a model, it will drop any rows of our data that contain a NA value for any of the variables used in the estimation. If your regression is SalePrice ~ GrLivArea, then it will check SalePrice and GrLivArea for NA’s. If you add another variable, then you add another possible set of NA values that can be dropped, and R will estimate the model on a subset of the data.\nThis will mess with your measure of \\(RMSE\\) for model comparison - if you compare two models that use different sets of the data, then you aren’t really comparing the fit very well. Because of that, we need to take a moment to check our data.\nAmes has a lot of variables, and in this assignment, you’re going to be asked to construct 15 regressions of increasing complexity. So we’re going to choose 15 variables to be explanatory plus SalePrice, which will be our target variable. That makes 16 variables.\nSelect your variables by making a character vector of the variables you think will best predict your chosen variable. Then, make a new version of Ames that contains only those 16 variables (you can use dplyr::select or any other method, make sure you include SalePrice). Once you’ve done that, use na.omit to make a new, clean version of Ames that has (1) no NA’s in it, and (2) 16 variables of your choice (as long as one is SalePrice). Use the help for na.omit to see how it works. This way, we know every model we make will have the same number of rows in it as none will be dropped due to NA values.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "8: Advanced Linear Model Building"
    ]
  },
  {
    "objectID": "assignment/08-assignment.html#linear-models",
    "href": "assignment/08-assignment.html#linear-models",
    "title": "8: Advanced Linear Model Building",
    "section": "Linear Models",
    "text": "Linear Models\nWhen exploring linear models in other classes, we often emphasize asymptotic results under distributional assumptions. That is, we make assumptions about the model in order to derive properties of large samples. This general approach is useful for creating and performing hypothesis tests. Frequently, when developing a linear regression model, part of the goal is to explain a relationship. However, this isn’t always the case. And it’s often not a valid approach, as we discussed in this week’s content.\nSo, we will ignore much of what we have learned in other classes (sorry, EC420) and instead simply use regression as a tool to predict. Instead of a model which supposedly explains relationships, we seek a model which minimizes errors.\nTo discuss linear models in the context of prediction, we return to the Ames data.\n\nAssesing Model Accuracy\nThere are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.\n\\[\n\\text{RMSE}(\\hat{f}, \\text{Data}) = \\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i = 1}^{n}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\n\\]\nWhile for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA context, the denominator for MSE may not be \\(n\\).\nFor a linear model , the estimate of \\(f\\), \\(\\hat{f}\\), is given by the fitted regression line.\n\\[\n\\hat{y}({\\bf{x}_i}) = \\hat{f}({\\bf{x}_i})\n\\]\nWe can write an R function that will be useful for performing this calculation.\n\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\n\n\nModel Complexity\nAside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, \\(p\\).\nWe write a simple R function to extract this information from a model.\n\nget_complexity = function(model) {\n  length(coef(model)) - 1\n}\n\nWhen deciding how complex of a model to use, we can utilize two techniques: forward selection or backward selection. Forward selection means that we start with the simplest model (with a single predictor) and then add one at a time until we decide to stop. Backward selection means that we start with the most complex model (with every available predictor) and then remove one at a time until we decide to stop. There are many criteria for “when to stop”. Below, we’ll try to give you some intuition on the model-building process.\n\n\n\n\n\n\nEXERCISE 1 of 2\n\n\n\nYour task is to, once again, predict SalePrice using the data. In this exercise, we will build 15 increasingly flexible models to minimize RMSE. In the next exercise, we will look at (and address) overfitting. But for now, ignore it.\n\nChoose the 15 variables you want to use to predict SalePrice - they can be numeric or categorical, both will work. Then, as described in Data Cleaning above, use skimr::skim and na.omit to make a clean version of Ames where .{text-red}[every variable] is 100% complete and non-NA. Make sure you include SalePrice in your cleaning. In cleaning, you probably want to drop any variable with below 60% complete rate. 60% isn’t a magic number by any means, the “right” number is entirely dependent on your data. It is always standard practice to document the fields you have dropped from the data, so make sure you state which variables have been dropped. Also using skim, note the variables with values for “n_unique” equal to 1 and drop them. See Data Cleaning (above) for details\nUsing forward selection (that is, select one variable, then select another) create a series of models up to complexity length 15. While you can code each of the 15 regressions separately, if you really want to be efficient, try to use a loop along with a character vector of your 15 variables to “step” through the 15 regressions. There are multiple ways to specify a regression using a character vector of arguments (which were described in our lecture notes this week), but I’ll leave the solution to you. Use a list object to hold your results, and use lapply along with your RMSE function(s) to get a list of the RMSE’s, one for each model.\nMake a data.frame or tibble of the RMSE results and the model complexity (the function unlist is helpful when you have a list of identical types, as should be the case with your measures of model complexity and RMSE). Create a chart plotting the model complexity as the \\(x\\)-axis variable and RMSE as the \\(y\\)-axis variable. Describe any patterns you see. Do you think you should use the full-size model? Why or why not? What criterion are you using to make this statement?\n\n\n\n\n\n\nTest-Train Split\nWe will once again want to predict SalePrice. If you want to choose different predictors, feel free to do so. Make sure you repeat your data cleaning.\nThere is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts: it is essentially cheating. As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down—or, in very specific cases where a new predictor is completely uncorrelated with the target, stay the same. This might seem to suggest that in order to predict well, we should use the largest possible model. However, in reality we have fit to a specific dataset, but as soon as we see new data, a large model may (in fact) predict poorly. This is called overfitting.\nThe most common approach to overfitting is to take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data. Test data should never be used to train a model—its pupose is to evaluate the fitted model once you’ve settled on something.1\nHere we use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to allow use to reproduce the same random split each time we perform this analysis. Sometimes we don’t want to do this; if we want to run lots of independent splits, then we do not need to set the initial seed.\nIt should be obvious that the code here is just some example columns. You should have 15 predictors, and you {.text-red}[should make sure that the data is clean with no NAs for any of the values of any of the predictors].\nNow, we split the sample:\n\nset.seed(9)\nnum_obs = nrow(Amesclean)\n\ntrain_index = sample(num_obs, size = trunc(0.50 * num_obs))\ntrain_data = Amesclean[train_index, ]\ntest_data = Amesclean[-train_index, ]\n\nOf course, you’ll have different results here since you’ll have different columns in Amesclean and will thus have different numbers after using na.omit. We will look at two measures that assess how well a model is predicting: train RMSE and test RMSE.\n\\[\n\\text{RMSE}_\\text{Train} = \\text{RMSE}(\\hat{f}, \\text{Train Data}) = \\sqrt{\\frac{1}{n_{\\text{Tr}}}\\sum_{i \\in \\text{Train}}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\n\\]\nHere \\(n_{Tr}\\) is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.\n\\[\n\\text{RMSE}_{\\text{Test}} = \\text{RMSE}(\\hat{f}, \\text{Test Data}) = \\sqrt{\\frac{1}{n_{\\text{Te}}}\\sum_{i \\in \\text{Test}} \\left ( y_i - \\hat{f}(\\bf{x}_i) \\right ) ^2}\n\\]\nHere \\(n_{Te}\\) is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict in general, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.\nWe will start with the simplest possible linear model, that is, a model with no predictors.\n\nfit_0 = lm(SalePrice ~ 1, data = train_data)\nget_complexity(fit_0)\n\n[1] 0\n\n# train RMSE\nsqrt(mean((train_data$SalePrice - predict(fit_0, train_data)) ^ 2))\n\n[1] 80875.98\n\n# test RMSE\nsqrt(mean((test_data$SalePrice - predict(fit_0, test_data)) ^ 2))\n\n[1] 77928.62\n\n\nYour results will be different, depending on what variables you selected (and which rows contained NA for them). The previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.\n\n# train RMSE\nrmse(actual = train_data$SalePrice, predicted = predict(fit_0, train_data))\n\n[1] 80875.98\n\n# test RMSE\nrmse(actual = test_data$SalePrice, predicted = predict(fit_0, test_data))\n\n[1] 77928.62\n\n\nThis function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.\n\nget_rmse = function(model, data, response) {\n  rmse(actual = subset(data, select = response, drop = TRUE),\n       predicted = predict(model, data))\n}\n\nBy using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.\n\nget_rmse(model = fit_0, data = train_data, response = \"SalePrice\") # train RMSE\n\n[1] 80875.98\n\nget_rmse(model = fit_0, data = test_data, response = \"SalePrice\") # test RMSE\n\n[1] 77928.62\n\n\nTry it: Apply this basic function with different arguments. Do you understand how we’ve nested functions within functions?\n\n\nAdding Flexibility to Linear Models\nWe started with the simpliest model including only a constant (which gives us only the (Intercept) as a coefficient). This is identical to estimating \\(\\widehat{\\bar{y}}\\). But we want to let our model be more complex – we want to add variables, polynomial terms, and interactions. Let’s do this.\nTry it: (This is Exercise 2, Question 1, so probably a good idea to do this) Using lm(), predict SalePrice with no fewer than five nested models of increasing complexity (each new model must include all of the terms from the previous, plus something new in the predictor). Here, we probably want to use interactions and polynomial terms, but make sure your models are still nested! Call them fit_1 to fit_5. It may be easiest to write out the formulas in your code rather than automating as in Exercise 1.\nEach successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting. To better understand the relationship between train RMSE, test RMSE, and model complexity, we’ll explore further.\nHopefully, you tried the in-line excercise above. If so, we can create a list of the models fit.\n\nmodel_list = list(fit_0, fit_1, fit_2, fit_3, fit_4, fit_5)\n\nWe then obtain train RMSE, test RMSE, and model complexity for each using our old friend sapply().\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"SalePrice\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"SalePrice\")\nmodel_complexity = sapply(model_list, get_complexity)\n\nOnce you’ve done this, you’ll notice the following:\n\n# This is the same as the sapply command above\n\ntest_rmse = c(get_rmse(fit_0, test_data, \"SalePrice\"),\n              get_rmse(fit_1, test_data, \"SalePrice\"),\n              get_rmse(fit_2, test_data, \"SalePrice\"),\n              get_rmse(fit_3, test_data, \"SalePrice\"),\n              get_rmse(fit_4, test_data, \"SalePrice\"),\n              get_rmse(fit_5, test_data, \"SalePrice\"))\n\nWe can plot the results. If you execute the code below, you’ll see the train RMSE in blue, while the test RMSE is given in orange.2\n\nplot(model_complexity, train_rmse, type = \"b\",\n     ylim = c(min(c(train_rmse, test_rmse)) - 0.02,\n              max(c(train_rmse, test_rmse)) + 0.02),\n     col = \"dodgerblue\",\n     xlab = \"Model Size\",\n     ylab = \"RMSE\")\nlines(model_complexity, test_rmse, type = \"b\", col = \"darkorange\")\n\nWe could also summarize the results as a table. fit_1 is the least flexible, and fit_5 is the most flexible. If we were to do this (see the exercise below) we would see that Train RMSE decreases as flexibility increases forever. However, this may not be the case for the Test RMSE.\n\n\n\n\n\n\n\n\n\nModel\nTrain RMSE\nTest RMSE\nPredictors\n\n\n\n\nfit_1\nRMSE\\(_{\\text{train}}\\) for model 1\nRMSE\\(_{\\text{test}}\\) for model 1\nput predictors here\n\n\n…\n…\n….\n…\n\n\nfit_5\nRMSE\\(_{\\text{train}}\\) for model 5\nRMSE\\(_{\\text{train}}\\) for model 5\n\\(p\\) predictors\n\n\n\nTo summarize:\n\nUnderfitting models: In general High Train RMSE, High Test RMSE.\nOverfitting models: In general Low Train RMSE, High Test RMSE.\n\nSpecifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE.3 Then a model is underfitting if there exists a more complex model with lower Test RMSE.\n\n\n\n\n\n\nEXERCISE 2 of 2\n\n\n\n\nUsing lm() and any number of regressors, predict SalePrice with no fewer than eight models of increasing complexity (as in the try-it above). Complexity can be increased by adding variables or by adding interactions or polynomials of existing variables. You can use the models you made in Exercise 1 if you’d like. Put the models into a list.\nCalculate the Train and Test RMSE. The Test RMSE is what we’re really interested in.\nMake a table exactly like the table in Exercise 1, but for the 8 or more models you just fit, and include both the Train and the Test RMSE. The first column should have the name of the model (e.g. fit_1). Hint: you can get the names of the entries in a list using names(model_list) provided you named the list items when you added them.\nIn a short paragraph, describe the resulting model. Discuss how you arrived at this model, what interactions you’re using (if any) and how confident you are that your prediction will perform well, relative to other people in the class.\n\n\n\nA final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that predicted well, and paid no attention to a model for explaination. This is especially true if we interacted variables without any theory as to why they might need to be interacted – “exterior type interacted with number of half-baths” may help with RMSE, but it certainly isn’t grounded in a story that a real estate agent might tell. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "8: Advanced Linear Model Building"
    ]
  },
  {
    "objectID": "assignment/08-assignment.html#footnotes",
    "href": "assignment/08-assignment.html#footnotes",
    "title": "8: Advanced Linear Model Building",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that sometimes the terms evaluation set and test set are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.↩︎\nThe train RMSE is guaranteed to follow this non-increasing pattern as long as no data is being dropped when new variables are added (see Data Cleaning above). The same is not true of test RMSE. We often see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next week. We will discuss how we can help create this U-shape much later. Also, we might intuitively expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get (un)lucky and this will not be true.↩︎\nThe labels of under and overfitting are relative to the best model we see. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.↩︎",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "8: Advanced Linear Model Building"
    ]
  },
  {
    "objectID": "assignment/10-assignment.html",
    "href": "assignment/10-assignment.html",
    "title": "10: Nonparametric Models",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, March 31st\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "10: Nonparametric Models"
    ]
  },
  {
    "objectID": "assignment/10-assignment.html#backstory-and-set-up",
    "href": "assignment/10-assignment.html#backstory-and-set-up",
    "title": "10: Nonparametric Models",
    "section": "Backstory and Set Up",
    "text": "Backstory and Set Up\nYou work for a bank. This bank is trying to predict defaults on loans. These are costly to the bank and, while rare, avoiding them is how banks make money. They’ve given you a dataset on defaults (encoded as the variable default). You’re going to try to predict this (that is, default is your target variable).\nThis is some new data. The snippet below loads it.\n\nbank &lt;- read.csv(\"https://ec242.netlify.app/data/bank23.csv\",\n                 stringsAsFactors = FALSE)\n\nThere’s not going to be a whole lot of wind-up here. You should be well-versed in doing these sorts of things by now (if not, look back at the previous lab for sample code).\n\n\n\n\n\n\nEXERCISE 1\n\n\n\n\nCheck the data using skim and str to see what sort of data you have. kNN, as we’ve covered it so far, takes an average of the target variable for the \\(k\\) nearest neighbors. Do any data processing necessary to use kNN to predict default.\nkNN needs to make a numeric prediction. Since default is not numeric, make a new column for it that is numeric. Of course, you’ll need to encode the numbers in a meaningful way (as.numeric('no') will do you no good).\nSplit the data into an 80/20 train vs. test split. Make sure you explicitly set the seed for replicability.\nRun a series of KNN models with \\(k\\) ranging from 2 to 200. Use whatever variables you think will help predict defaults. Remember, \\(k\\) is our complexity parameter – we do not add or subtract any of the explanatory variables, we vary only \\(k\\). You must have at least 50 different values of \\(k\\). You can easily write a short function to do this using this week’s lessons and should avoid hand-coding 50 different models.\nCreate a chart plotting the model complexity as the \\(x\\)-axis variable and RMSE as the \\(y\\)-axis variable for both the training and test data. Pay attention to the values of \\(k\\) that are “higher” in complexity and “lower” in complexity, and make sure the \\(x\\)-axis is increasing in complexity as you go to the right.\nAnswer the following questions:\n\n\nWhat do you think is the optimal \\(k\\)?\nWhat are you using to decide the optimal \\(k\\)?\nIf we were to allow the model a little more complexity than the optimal, how will our training RMSE change? How will our test RMSE change?",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "10: Nonparametric Models"
    ]
  },
  {
    "objectID": "assignment/12-assignment.html",
    "href": "assignment/12-assignment.html",
    "title": "12: Applied Logistic Regression - Classification",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, April 14th\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "12: Applied Logistic Regression - Classification"
    ]
  },
  {
    "objectID": "assignment/12-assignment.html#backstory-and-set-up",
    "href": "assignment/12-assignment.html#backstory-and-set-up",
    "title": "12: Applied Logistic Regression - Classification",
    "section": "Backstory and Set Up",
    "text": "Backstory and Set Up\nYou work for a bank. This bank is trying to predict defaults on loans (a relatively uncommon occurence, but one that costs the bank a great deal of money when it does happen.) They’ve given you a dataset on defaults (encoded as the variable y, and not the column default). You’re going to try to predict this.\nThis is some new data. The snippet below loads it.\n\nbank &lt;- read.table(\"https://ec242.netlify.app/data/bank.csv\",\n                 header = TRUE,\n                 sep = \",\")\n\nThere’s not going to be a whole lot of wind-up here. You should be well-versed in doing these sorts of things by now (if not, look back at the previous lab for sample code).\n\n\n\n\n\n\nNote\n\n\n\nEXERCISE 1\n\nEncode the outcome we’re trying to predict (y, and not default) as a binary.\nSplit the data into an 80/20 train vs. test split. Make sure you explicitly set the seed for replicability.\nRun a series of logistic regressions with between 1 and 4 predictors of your choice (you can use interactions).\nCreate eight total confusion matrices: four by applying your models to the training data, and four by applying your models to the test data. Briefly discuss your findings. How does the error rate, sensitivity, and specificity change as the number of predictors increases?\n\nA few hints:\n\nIf you are not getting a 2x2 confusion matrix, you might need to adjust your cutoff probability.\nIt might be the case that your model perfectly predicts the outcome variable when the setup cutoff probability is too high.\nYou need to make sure your predictions take the same possible values as the actual data (which, remember, you had to convert to a binary 0/1 variable)",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "12: Applied Logistic Regression - Classification"
    ]
  },
  {
    "objectID": "assignment/14-assignment.html",
    "href": "assignment/14-assignment.html",
    "title": "14: Geospatial in R",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, April 28th\n\n\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.\nThis assignment builds from the Example for this week. Your assignment is to make a map of some interesting census variable for your home county. If your home county is outside the US, you can use any county you like.\n\n\n\n\n\n\nExercise 1 of 1\n\n\n\n\nLoad up all the necessary packages\nFind the FIPS for your home county\nChoose a variable you’re interested in. Finding census variables can be tricky!\nUsing load_variables, find the best representation of that variable\nMake a map of that variable at the census tract level",
    "crumbs": [
      "Assignments",
      "Lab Assignments",
      "Assignments",
      "14: Geospatial in R"
    ]
  },
  {
    "objectID": "content/Week_01/01a.html",
    "href": "content/Week_01/01a.html",
    "title": "Welcome To Data Analytics",
    "section": "",
    "text": "Your readings will be assigned each week in this area. For this initial week, please read the course content for this week. Read closely the following:\n\nThe syllabus, content, and lab assignment pages for this class.\nThis page. Yes, the whole thing.\n\n\n\nFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\nHow does this course work?\nDo you remember anything about R?\nDo you have a working R installation on your computer, along with Rstudio and tinytex?",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Welcome To Data Analytics"
    ]
  },
  {
    "objectID": "content/Week_01/01a.html#starting-point-for-this-course",
    "href": "content/Week_01/01a.html#starting-point-for-this-course",
    "title": "Welcome To Data Analytics",
    "section": "Starting point for this course",
    "text": "Starting point for this course\nBetter utilizing existing data can improve our predictive power whilst providing interpretable outputs for considering new policies.\nWARNING: Causation is tough and we will spend the entire course warning you to avoid making causal claims!\n\nNon-Social Science Approaches to Statistical Learning\nSuppose you want to find out how long it takes things to fall from a fixed height (and you don’t know the laws of physics). You might look for things falling and time them. Or you might climb to the top of a tower and drop a few things and time each of them. You might start to see a pattern in the data, and you might be able to fit a line or a curve to that data.\nThat’s learning. And since you might have measurement error, or you might not be keeping track of things like air resistance, you might have some deviations from that line or curve. That’s statistical learning.\nNow, suppose you are a researcher and you want to teach a computer to recognize images of a tree.\nNote: this is an “easy” problem. If you show pictures to a 3-year-old, that child will probably be able to tell you if there is a tree in the picture.\nComputer scientists spent about 20 years on this problem because they thought about the problem like nerds and tried to write down a series of rules.\nOften, rules are difficult to form, and simply writing rules misses the key insight: the data can tell you something.\n\n\nSocial Science Approaches to Statistical Learning\nSuppose you are a researcher and you want to know whether prisons reduce crime.\nfrom “A Call for a Moratorium on Prison Building” (1976)\n\nBetween 1955 and 1975, fifteen states increased the collective capacity of their adult prison systems by 56% (from, on average, 63,100 to 98,649).\nFifteen other states increased capacity by less than 4% (from 49,575 to 51,440).\nIn “heavy-construction” states the crime rate increased by 167%; in “low-construction” states the crime rate increased by 145%.\n\n\n\n\n\nPrison Capacity\nCrime Rate\n\n\n\n\nHigh construction\n\\(\\uparrow\\)~56%\n\\(\\uparrow\\)~167%\n\n\nLow construction\n\\(\\uparrow\\)~4%\n\\(\\uparrow\\)~145%\n\n\n\n\n\nThe Pros and Cons of Correlation\nThe product of analysis often takes the form of a documented correlation. Our prison example showed us a correlation. This has some pros and cons.\nPros:\n\nNature gives you correlations for free.\nIn principle, everyone can agree on the facts.\n\nCons:\n\nCorrelations are not very helpful.\nThey show what has happened, but not why.\nFor many things, we care about why. The social science perspective asks “why?”\n\n\nWhy a Correlation Exists Between X and Y\n\n\\(X \\rightarrow Y\\) X causes Y (causality)\n\\(X \\leftarrow Y\\) Y causes X (reverse causality)\n\\(Z \\rightarrow X\\); \\(Z \\rightarrow Y\\) Z causes X and Y (common cause)\n\\(X \\rightarrow Y\\); \\(Y \\rightarrow X\\) X causes Y and Y causes X (simultaneous equations)\n\nSo what does the “Social Science” in “Social Science Data Analytics” do? - People do not obey the laws of physics - People’s preferences and feelings do not obey a strict logic - You may prefer a burger for dinner today, but you might want spaghetti tomorrow - People interact with people to form outcomes we care about - Demand, supply, and price (economics) - Thus EC242 - But also, forming institutions (political science), social movements and norms (sociology), spatial patterns (geography), interactions with natural resources (agriculture, food, and resource economics), etc.\nIn a way, Social Science Data Analytics is a lot messier, complicated, and rewarding relative to just “data analytics”.\n\n\nUniting Social Science and Computer Science\nWe will start in this course by examining situations where we do not care about why something has happened, but instead we care about our ability to predict its occurrence from existing data.\n(But of course keep in back of mind that if you are making policy, you must care about why something happened).\nWe will also borrow a few other ideas from CS:\n\nAnything is data\n\nSatellite data\nUnstructured text or audio\nFacial expressions or vocal intonations\n\nSubtle improvements on existing techniques\nAn eye towards practical implementability over “cleanliness”\n\n\n\n\nA Paradigm to Motivate Prediction\nExample: a firm wishes to predict user behavior based on previous purchases or interactions.\nSmall margins \\(\\rightarrow\\) huge payoffs when scaled up.\n\\(.01\\% \\rightarrow\\) $10 million.\nNot obvious why this was true for Netflix; quite obvious why this is true in financial markets.\nFrom a computer science perspective, it only matters that you get that improvement ($$). From a social science perspective, we would want to use the predictions to learn more about why.\n\n\nMore Recent Examples of Prediction\n\nIdentify the risk factors for prostate cancer.\nClassify a tissue sample into one of several cancer classes, based on a gene expression profile.\nClassify a recorded phoneme based on a log-periodogram.\nPredict whether someone will have a heart attack on the basis of demographic, diet and clinical measurements.\nCustomize an email spam detection system.\nIdentify a hand-drawn object.\nDetermine which oscillations of stellar luminosity are likely due to exoplanets.\nIdentify food combinations that cause spikes in blood glucose level for an individual.\nEstablish the relationship between salary and demographic variables in population survey data.\n\n\n\nAn Aside: Nomenclature\nMachine learning arose as a subfield of Artificial Intelligence.\nStatistical learning arose as a subfield of Statistics.\nThere is much overlap; however, a few points of distinction:\n\nMachine learning has a greater emphasis on large scale applications and prediction accuracy.\nStatistical learning emphasizes models and their interpretability, and precision and uncertainty.\n\nBut the distinction has become more and more blurred, and there is a great deal of “cross-fertilization”.\n\n\nObviously true: machine learning has the upper hand in marketing.",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Welcome To Data Analytics"
    ]
  },
  {
    "objectID": "content/Week_01/01a.html#case-study-1-global-renewable-energy-production",
    "href": "content/Week_01/01a.html#case-study-1-global-renewable-energy-production",
    "title": "Welcome To Data Analytics",
    "section": "Case study 1: Global Renewable Energy Production",
    "text": "Case study 1: Global Renewable Energy Production\nImagine you are evaluating countries for a potential investment in renewable energy. Headlines like “Renewable Energy Capacity Growth Worldwide” have piqued your interest. Reports from various sources show diverse graphs and charts and you’re curious about the underlying data. You want to know which countries are leading the way in renewable energy production and which are lagging behind. You want to know which countries are growing their renewable energy production the fastest. In short: you want to know which countries are the best bets for investment. You might see something like the following:\n\n\n\n\n\n\n\n\n\nYou might want to look into the underlying data (in this case, fabricated) and think about what to do next. In this sense, you have learned from data. The data provides directions for exploration and clues as to potential explanations.",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Welcome To Data Analytics"
    ]
  },
  {
    "objectID": "content/Week_01/01a.html#case-study-2-us-homicides-by-firearm",
    "href": "content/Week_01/01a.html#case-study-2-us-homicides-by-firearm",
    "title": "Welcome To Data Analytics",
    "section": "Case study 2: US homicides by firearm",
    "text": "Case study 2: US homicides by firearm\nImagine you live in Europe (if only!) and are offered a job in a US company with many locations in every state. It is a great job, but headlines such as US Gun Homicide Rate Higher Than Other Developed Countries1 have you worried. Fox News runs a scary looking graphic, and charts like the one below only add to you anxiety:\n\n\n\n\n\n\n\n\n\n\nOr even worse, this version from everytown.org:\n\n\n\n\n\n\n\n\n\n\nBut then you remember that (1) this is a hypothetical exercise; (2) you’ll take literally any job at this point; and (3) Geographic diversity matters – the United States is a large and diverse country with 50 very different states (plus the District of Columbia and some lovely territories).2\n\n\n\n\n\n\n\n\n\nCalifornia, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial.\nThis is a relatively simple and straightforward problem in social science: you have options of where to live, and want to determine the safety of the various states. Your “research” is clearly policy-relevant: you will eventually have to live somewhere. In this course, we will begin to tackle the problem by examining data related to gun homicides in the US during 2010 using R as a motivating example along the way.\nBefore we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Ideally, this is a refresher. However, we are aware that your preparation in previously courses varies greatly from student to student. Moreover, we want you to be aware that the usefulness of some of these early building blocks may not be immediately obvious. Later in the class you will appreciate having these skills. Mastery will be rewarded both in this class and (of course) in life.\n\nThe Pre-Basics\nWe’ve now covered a short bit of material. The remainder of this first lecture will be covering setting up R and describing some common errors. For this, we’ll head over to resources to install R, RStudio, and tinytex.\nIf you leave class without a working installation of these pieces of software, then you are likely to run into trouble in completing your first assignments.",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Welcome To Data Analytics"
    ]
  },
  {
    "objectID": "content/Week_01/01a.html#footnotes",
    "href": "content/Week_01/01a.html#footnotes",
    "title": "Welcome To Data Analytics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttp://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/↩︎\nI’m especially partial to Puerto Rico.↩︎",
    "crumbs": [
      "Course Content",
      "Week 01",
      "Welcome To Data Analytics"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html",
    "href": "content/Week_02/02a.html",
    "title": "Introduction to the tidyverse",
    "section": "",
    "text": "This page.\nChapter 1 of Introduction to Statistical Learning, available here.\nOptional: The “Tidy Your Data” tutorial on Rstudio Cloud Primers",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#readings",
    "href": "content/Week_02/02a.html#readings",
    "title": "Introduction to the tidyverse",
    "section": "",
    "text": "This page.\nChapter 1 of Introduction to Statistical Learning, available here.\nOptional: The “Tidy Your Data” tutorial on Rstudio Cloud Primers",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#some-reminders",
    "href": "content/Week_02/02a.html#some-reminders",
    "title": "Introduction to the tidyverse",
    "section": "Some Reminders:",
    "text": "Some Reminders:\n\nEcon Club\nFor econ majors or people interested in quantitative social sciences in general (which is this whole class):\n\n\n\n\n\n\n\n\n\n\n\nYour labs\n\nStart labs early!\n\nThey are not trivial.\nThey are not short.\nThey are not easy.\nThey are not optional.\n\nYou install.packages(\"packageName\") once on your computer.\n\nAnd never ever ever in your code.\n\nYou load an already-installed package using library(packageName) in a code chunk\n\nNever in your console\nWhen RMarkdown knits, it starts a whole new, empty session that has no knowledge of what you typed into the console\n\nSlack\n\nUse it.\nAlways post in the class-visible channels. Others can learn from your issues.\n\nWe have a channel just for labs and R. Please use that one.\n\n\n\n\n\nGroup project\nIf you read ahead, you’d have seen a bit about our group project. I’d like to wait until after the add/drop deadline to do this as people come and go, ruining my group assignments. Know that we will have groups of 3, and you will be given the opportunity to form your own groups with default groups for those who do not form one. All groups must be exactly 3 people.\n\n\nShare your screen\nWe will often do some shared coding results – you’ll be asked to execute some code, and we’ll use Zoom to share screen so the class can see volunteer’s results. I’ll ask you a few easy questions about your code, and you’ll get participation points (or EC).\nI have set up a Zoom room with a short link: bit.ly/EC242.\nWe’ll use that link/zoom room all semester to share screen (since this room doesn’t have a screen sharing protocol).\nFor instance, last week we ran into the following try-it:\n\n\n\n\n\n\nTRY IT\n\n\n\n\nWhat is the sum of the first 100 positive integers? The formula for the sum of integers \\(1\\) through \\(n\\) is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum?\n\n\n\nI didn’t read my own notes closesly and thought we needed seq(...) or the shortcut 1:100 to do this, but we actually don’t because we are given the equation \\(n(n+1)/2\\). So, let’s try out our first TRY-IT and ask everyone to calculate the sum of the numbers 1 to 100 (\\(n=100\\)) using both the formula above, and the seq(...) function to sum up the numbers 1 to 100.",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#guiding-question",
    "href": "content/Week_02/02a.html#guiding-question",
    "title": "Introduction to the tidyverse",
    "section": "Guiding Question",
    "text": "Guiding Question\nFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\nWhy do we want tidy data?\nWhat are the challenges associated with shaping things into a tidy format?",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#tidy-data",
    "href": "content/Week_02/02a.html#tidy-data",
    "title": "Introduction to the tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\n\nWe say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\n\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nEach row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.\nTo see how the same information can be provided in different formats, consider the following example:\n\n\n      country year fertility\n1     Germany 1960      2.41\n2 South Korea 1960      6.16\n3     Germany 1961      2.44\n4 South Korea 1961      5.99\n5     Germany 1962      2.47\n6 South Korea 1962      5.79\n\n\nThis tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the dslabs package. Originally, the data was in the following format:\n\n\n      country 1960 1961 1962\n1     Germany 2.41 2.44 2.47\n2 South Korea 6.16 5.99 5.79\n\n\nThe same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables’ values, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into tidy format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.\nAlthough not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.\n\n\n\n\n\n\nTRY IT\n\n\n\n\nExamine the built-in dataset co2. Which of the following is true:\n\n\nco2 is tidy data: it has one year for each row.\nco2 is not tidy: we need at least one column with a character vector.\nco2 is not tidy: it is a matrix instead of a data frame.\nco2 is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.\n\n\nExamine the built-in dataset ChickWeight. Which of the following is true:\n\n\nChickWeight is not tidy: each chick has more than one row.\nChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.\nChickWeight is not tidy: we are missing the year column.\nChickWeight is tidy: it is stored in a data frame.\n\n\nExamine the built-in dataset BOD. Which of the following is true:\n\n\nBOD is not tidy: it only has six rows.\nBOD is not tidy: the first column is just an index.\nBOD is tidy: each row is an observation with two values (time and demand)\nBOD is tidy: all small datasets are tidy by definition.\n\n\nWhich of the following built-in datasets is tidy (you can pick more than one):\n\n\nBJsales\nEuStockMarkets\nDNase\nFormaldehyde\nOrange\nUCBAdmissions",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#manipulating-data-frames",
    "href": "content/Week_02/02a.html#manipulating-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Manipulating data frames",
    "text": "Manipulating data frames\nThe dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.\n\nAdding a column with mutate\nWe want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values. So, to add murder rates, we use:\n\nlibrary(dslabs)\ndata(\"murders\")\nmurders &lt;- mutate(murders, rate = total / population * 100000)\n\nNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error?\nThis is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable.\nWe can see that the new column is added:\n\nhead(murders)\n\n       state abb region population total     rate\n1    Alabama  AL  South    4779736   135 2.824424\n2     Alaska  AK   West     710231    19 2.675186\n3    Arizona  AZ   West    6392017   232 3.629527\n4   Arkansas  AR  South    2915918    93 3.189390\n5 California  CA   West   37253956  1257 3.374138\n6   Colorado  CO   West    5029196    65 1.292453\n\n\nNote: Although we have overwritten the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will overwrite our mutated version.\n\n\nSubsetting with filter\nNow suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the filter function, which takes the data table as the first argument and then the conditional statement as the second. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace.\n\nfilter(murders, rate &lt;= 0.71)\n\n          state abb        region population total      rate\n1        Hawaii  HI          West    1360301     7 0.5145920\n2          Iowa  IA North Central    3046355    21 0.6893484\n3 New Hampshire  NH     Northeast    1316470     5 0.3798036\n4  North Dakota  ND North Central     672591     4 0.5947151\n5       Vermont  VT     Northeast     625741     2 0.3196211\n\n\n\n\nSelecting columns with select\nAlthough our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object:\n\nnew_table &lt;- select(murders, state, region, rate)\nfilter(new_table, rate &lt;= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nIn the call to select, the first argument murders is an object, but state, region, and rate are variable names.\n\n\n\n\n\n\nTRY IT\n\n\n\n\nLoad the dplyr package and the murders dataset.\n\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\n\nYou can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted:\n\nmurders &lt;- mutate(murders, population_in_millions = population / 10^6)\n\nWe can write population rather than murders$population because mutate is part of dplyr. The function mutate knows we are grabbing columns from murders.\nUse the function mutate to add a murders column named rate with the per 100,000 murder rate as in the example code above. Make sure you redefine murders as done in the example code above ( murders &lt;- [your code]) so we can keep using this variable.\n\nIf rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\nWith dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes:\n\n\nselect(murders, state, population) %&gt;% head()\n\nUse select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\n\nThe dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this:\n\n\nfilter(murders, state == \"New York\")\n\nYou can use other logical vectors to filter rows.\nUse filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\n\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:\n\n\nno_florida &lt;- filter(murders, state != \"Florida\")\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n\nWe can also use %in% to filter with dplyr. You can therefore see the data from New York and Texas like this:\n\n\nfilter(murders, state %in% c(\"New York\", \"Texas\"))\n\nCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\n\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\n\n\nfilter(murders, population &lt; 5000000 & region == \"Northeast\")\n\nMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate, and the rank.",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#the-pipe",
    "href": "content/Week_02/02a.html#the-pipe",
    "title": "Introduction to the tidyverse",
    "section": "The pipe: %>%",
    "text": "The pipe: %&gt;%\nWith dplyr we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %&gt;%. Some details are included below.\nWe wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects:\n\\[ \\mbox{original data }\n\\rightarrow \\mbox{ select }\n\\rightarrow \\mbox{ filter } \\]\nFor such an operation, we can use the pipe %&gt;%. The code looks like this:\n\nmurders %&gt;% select(state, region, rate) %&gt;% filter(rate &lt;= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nThis line of code is equivalent to the two lines of code above. What is going on here?\nIn general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:\n\n16 %&gt;% sqrt()\n\n[1] 4\n\n\nWe can continue to pipe values along:\n\n16 %&gt;% sqrt() %&gt;% log2()\n\n[1] 2\n\n\nThe above statement is equivalent to log2(sqrt(16)).\nRemember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:\n\n16 %&gt;% sqrt() %&gt;% log(base = 2)\n\n[1] 2\n\n\nTherefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote:\n\nmurders %&gt;% select(state, region, rate) %&gt;% filter(rate &lt;= 0.71)\n\nmurders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function.\nNote that the pipe works well with functions where the first argument is the input data. Functions in tidyverse packages like dplyr have this format and can be used easily with the pipe. It’s worth noting that as of R 4.1, there is a base-R version of the pipe |&gt;, though this has its disadvantages. We’ll stick with %&gt;% for now.\n\n\n\n\n\n\nTRY IT\n\n\n\n\nThe pipe %&gt;% can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.\n\n\nmurders &lt;- mutate(murders, rate =  total / population * 100000,\n                  rank = rank(-rate))\n\nIn the solution to the previous exercise, we did the following:\n\nmy_states &lt;- filter(murders, region %in% c(\"Northeast\", \"West\") &\n                      rate &lt; 1)\n\nselect(my_states, state, rate, rank)\n\nThe pipe %&gt;% permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this:\n\nmutate(murders, rate =  total / population * 100000,\n       rank = rank(-rate)) %&gt;%\n  select(state, rate, rank)\n\nNotice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %&gt;%.\nRepeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %&gt;% to do this in just one line.\n\nReset murders to the original table by using data(murders). Use a pipe to create a new data frame called my_states that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three %&gt;%. The code should look something like this:\n\n\nmy_states &lt;- murders %&gt;%\n  mutate SOMETHING %&gt;%\n  filter SOMETHING %&gt;%\n  select SOMETHING",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#summarizing-data",
    "href": "content/Week_02/02a.html#summarizing-data",
    "title": "Introduction to the tidyverse",
    "section": "Summarizing data",
    "text": "Summarizing data\nAn important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using the pull function.\n\nsummarize\nThe summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The heights dataset includes heights and sex reported by students in an in-class survey.\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(heights)\nhead(heights)\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65\n\n\nThe following code computes the average and standard deviation for females:\n\ns &lt;- heights %&gt;%\n  filter(sex == \"Female\") %&gt;%\n  summarize(average = mean(height), standard_deviation = sd(height))\ns\n\n   average standard_deviation\n1 64.93942           3.760656\n\n\nThis takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same.\nBecause the resulting table stored in s is a data frame, we can access the components with the accessor $:\n\ns$average\n\n[1] 64.93942\n\ns$standard_deviation\n\n[1] 3.760656\n\n\nAs with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:\n\nheights %&gt;%\n  filter(sex == \"Female\") %&gt;%\n  summarize(median = median(height), minimum = min(height),\n            maximum = max(height))\n\n    median minimum maximum\n1 64.98031      51      79\n\n\nWe can obtain these three values with just one line using the quantile function: for example, quantile(x, c(0,0.5,1)) returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector x. However, if we attempt to use a function like this that returns two or more values inside summarize:\n\nheights %&gt;%\n  filter(sex == \"Female\") %&gt;%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nwe will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In later sections, we will learn how to deal with functions that return more than one value.\nFor another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column:\n\nmurders &lt;- murders %&gt;% mutate(rate = total/population*100000)\n\nRemember that the US murder rate is not the average of the state murder rates:\n\nsummarize(murders, mean(rate))\n\n  mean(rate)\n1   2.779125\n\n\nThis is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:\n\nus_murder_rate &lt;- murders %&gt;%\n  summarize(rate = sum(total) / sum(population) * 100000)\nus_murder_rate\n\n      rate\n1 3.034555\n\n\nThis computation counts larger states proportionally to their size which results in a larger value.\n\n\npull\nThe us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame:\n\nclass(us_murder_rate)\n\n[1] \"data.frame\"\n\n\nsince, as most dplyr functions, summarize always returns a data frame.\nThis might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. To understand what we mean take a look at this line of code:\n\nus_murder_rate %&gt;% pull(rate)\n\n[1] 3.034555\n\n\nThis returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate.\nTo get a number from the original data table with one line of code we can type:\n\nus_murder_rate &lt;- murders %&gt;%\n  summarize(rate = sum(total) / sum(population) * 100000) %&gt;%\n  pull(rate)\n\nus_murder_rate\n\n[1] 3.034555\n\n\nwhich is now a numeric:\n\nclass(us_murder_rate)\n\n[1] \"numeric\"\n\n\n\n\nGroup then summarize with group_by\nA common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this.\nIf we type this:\n\nheights %&gt;% group_by(sex)\n\n# A tibble: 1,050 × 2\n# Groups:   sex [2]\n   sex    height\n   &lt;fct&gt;   &lt;dbl&gt;\n 1 Male       75\n 2 Male       70\n 3 Male       68\n 4 Male       74\n 5 Male       61\n 6 Female     65\n 7 Female     66\n 8 Female     62\n 9 Female     66\n10 Male       67\n# ℹ 1,040 more rows\n\n\nThe result does not look very different from heights, except we see Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:\n\nheights %&gt;%\n  group_by(sex) %&gt;%\n  summarize(average = mean(height), standard_deviation = sd(height))\n\n# A tibble: 2 × 3\n  sex    average standard_deviation\n  &lt;fct&gt;    &lt;dbl&gt;              &lt;dbl&gt;\n1 Female    64.9               3.76\n2 Male      69.3               3.61\n\n\nThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median murder rate in the four regions of the country:\n\nmurders %&gt;%\n  group_by(region) %&gt;%\n  summarize(median_rate = median(rate))\n\n# A tibble: 4 × 2\n  region        median_rate\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Northeast            1.80\n2 South                3.40\n3 North Central        1.97\n4 West                 1.29",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#sorting-data-frames",
    "href": "content/Week_02/02a.html#sorting-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Sorting data frames",
    "text": "Sorting data frames\nWhen examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size:\n\nmurders %&gt;%\n  arrange(population) %&gt;%\n  head()\n\n                 state abb        region population total       rate\n1              Wyoming  WY          West     563626     5  0.8871131\n2 District of Columbia  DC         South     601723    99 16.4527532\n3              Vermont  VT     Northeast     625741     2  0.3196211\n4         North Dakota  ND North Central     672591     4  0.5947151\n5               Alaska  AK          West     710231    19  2.6751860\n6         South Dakota  SD North Central     814180     8  0.9825837\n\n\nWith arrange we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by rate instead:\n\nmurders %&gt;%\n  arrange(rate) %&gt;%\n  head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102\n\n\nNote that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type:\n\nmurders %&gt;%\n  arrange(desc(rate))\n\n\nNested sorting\nIf we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region, then within region we order by murder rate:\n\nmurders %&gt;%\n  arrange(region, rate) %&gt;%\n  head()\n\n          state abb    region population total      rate\n1       Vermont  VT Northeast     625741     2 0.3196211\n2 New Hampshire  NH Northeast    1316470     5 0.3798036\n3         Maine  ME Northeast    1328361    11 0.8280881\n4  Rhode Island  RI Northeast    1052567    16 1.5200933\n5 Massachusetts  MA Northeast    6547629   118 1.8021791\n6      New York  NY Northeast   19378102   517 2.6679599\n\n\n\n\nThe top \\(n\\)\nIn the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:\n\nmurders %&gt;% top_n(5, rate)\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Maryland  MD         South    5773552   293  5.074866\n4             Missouri  MO North Central    5988927   321  5.359892\n5       South Carolina  SC         South    4625364   207  4.475323\n\n\nNote that rows are not sorted by rate, only filtered. If we want to sort, we need to use arrange. Note that if the third argument is left blank, top_n filters by the last column.\n\n\n\n\n\n\nTRY IT\n\n\n\nFor these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\n\nlibrary(NHANES)\ndata(NHANES)\n\nThe NHANES data has many missing values. The mean and sd functions in R will return NA if any of the entries of the input vector is an NA. Here is an example:\n\nlibrary(dslabs)\ndata(na_example)\nmean(na_example)\n\n[1] NA\n\nsd(na_example)\n\n[1] NA\n\n\nTo ignore the NAs we can use the na.rm argument:\n\nmean(na_example, na.rm = TRUE)\n\n[1] 2.301754\n\nsd(na_example, na.rm = TRUE)\n\n[1] 1.22338\n\n\nLet’s now explore the NHANES data.\n\nWe will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like ” 20-29”, with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref.\n\nHint: Use filter and summarize and use the na.rm = TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter.\n\nUsing a pipe, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then pull.\nNow report the min and max values for the same group.\nCompute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age and gender, filter by Gender and then use group_by.\nRepeat exercise 4 for males.\nWe can actually combine both summaries for exercises 4 and 5 into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender).\nFor males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure.",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#tibbles",
    "href": "content/Week_02/02a.html#tibbles",
    "title": "Introduction to the tidyverse",
    "section": "Tibbles",
    "text": "Tibbles\nTidy data must be stored in data frames. We have been using the murders data frame throughout the unit. In an earlier section we introduced the group_by function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?\n\nmurders %&gt;% group_by(region)\n\n# A tibble: 51 × 6\n# Groups:   region [4]\n   state                abb   region    population total  rate\n   &lt;chr&gt;                &lt;chr&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Alabama              AL    South        4779736   135  2.82\n 2 Alaska               AK    West          710231    19  2.68\n 3 Arizona              AZ    West         6392017   232  3.63\n 4 Arkansas             AR    South        2915918    93  3.19\n 5 California           CA    West        37253956  1257  3.37\n 6 Colorado             CO    West         5029196    65  1.29\n 7 Connecticut          CT    Northeast    3574097    97  2.71\n 8 Delaware             DE    South         897934    38  4.23\n 9 District of Columbia DC    South         601723    99 16.5 \n10 Florida              FL    South       19687653   669  3.40\n# ℹ 41 more rows\n\n\nNotice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble followed by dimensions. We can learn the class of the returned object using:\n\nmurders %&gt;% group_by(region) %&gt;% class()\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe tbl, pronounced tibble, is a special kind of data frame. The functions group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. For consistency, the dplyr manipulation verbs (select, filter, mutate, and arrange) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble.\nTibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.\n\nTibbles display better\nThe print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing murders and the output of murders if we convert it to a tibble. We can do this using as_tibble(murders). If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.\n\n\nSubsets of tibbles are tibbles\nIf you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:\n\nclass(murders[,4])\n\n[1] \"numeric\"\n\n\nis not a data frame. With tibbles this does not happen:\n\nclass(as_tibble(murders)[,4])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThis is useful in the tidyverse since functions require data frames as input.\nWith tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $:\n\nclass(as_tibble(murders)$population)\n\n[1] \"numeric\"\n\n\nA related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write Population instead of population this:\n\nmurders$Population\n\nNULL\n\n\nreturns a NULL with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:\n\nas_tibble(murders)$Population\n\nNULL\n\n\n\n\n\n\n\n\n\nTibbles can be grouped\nThe function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information.\n\n\nCreate a tibble using tibble instead of data.frame\nIt is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the tibble function.\n\ngrades &lt;- tibble(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\n\nNote that base R (without packages loaded) has a function with a very similar name, data.frame, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default data.frame coerces characters into factors without providing a warning or message:\n\ngrades &lt;- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo avoid this, we use the rather cumbersome argument stringsAsFactors:\n\ngrades &lt;- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90),\n                     stringsAsFactors = FALSE)\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo convert a regular data frame to a tibble, you can use the as_tibble function.\n\nas_tibble(grades) %&gt;% class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#the-dot-operator",
    "href": "content/Week_02/02a.html#the-dot-operator",
    "title": "Introduction to the tidyverse",
    "section": "The dot operator",
    "text": "The dot operator\nOne of the advantages of using the pipe %&gt;% is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:\n\ntab_1 &lt;- filter(murders, region == \"South\")\ntab_2 &lt;- mutate(tab_1, rate = total / population * 10^5)\nrates &lt;- tab_2$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nWe can avoid defining any new intermediate objects by instead typing:\n\nfilter(murders, region == \"South\") %&gt;%\n  mutate(rate = total / population * 10^5) %&gt;%\n  summarize(median = median(rate)) %&gt;%\n  pull(median)\n\n[1] 3.398069\n\n\nWe can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the pull function was not available and we wanted to access tab_2$rate? What data frame name would we use? The answer is the dot operator.\nFor example to access the rate vector without the pull function we could use\n\nrates &lt;-   filter(murders, region == \"South\") %&gt;%\n  mutate(rate = total / population * 10^5) %&gt;%\n  .$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nIn the next section, we will see other instances in which using the . is useful.",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#do",
    "href": "content/Week_02/02a.html#do",
    "title": "Introduction to the tidyverse",
    "section": "do",
    "text": "do\nThe tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %&gt;%, tidyverse functions consistently take data frames and return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The quantile function is an example we described earlier. The do function serves as a bridge between R functions such as quantile and the tidyverse. The do function understands grouped tibbles and always returns a data frame.\nIn the summarize section (above), we noted that if we attempt to use quantile to obtain the min, median and max in one call, we will receive something unexpected. Prior to R 4.1, we would receive an error. After R 4.1, we actually get:\n\ndata(heights)\nheights %&gt;%\n  filter(sex == \"Female\") %&gt;%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nWe probably wanted three columns: min, median, and max. We can use the do function to fix this.\nFirst we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame. Note that it returns a single-row data frame.\n\nmy_summary &lt;- function(dat){\n  x &lt;- quantile(dat$height, c(0, 0.5, 1))\n  tibble(min = x[1], median = x[2], max = x[3])\n}\n\nWe can now apply the function to the heights dataset to obtain the summaries:\n\nheights %&gt;%\n  group_by(sex) %&gt;%\n  my_summary\n\n# A tibble: 1 × 3\n    min median   max\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    50   68.5  82.7\n\n\nBut this is not what we want. We want a summary for each sex and the code returned just one summary. This is because my_summary is not part of the tidyverse and does not know how to handled grouped tibbles. do makes this connection:\n\nheights %&gt;%\n  group_by(sex) %&gt;%\n  do(my_summary(.))\n\n# A tibble: 2 × 4\n# Groups:   sex [2]\n  sex      min median   max\n  &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Female    51   65.0  79  \n2 Male      50   69    82.7\n\n\nNote that here we need to use the dot operator. The tibble created by group_by is piped to do. Within the call to do, the name of this tibble is . and we want to send it to my_summary. If you do not use the dot, then my_summary has no argument and returns an error telling us that argument \"dat\" is missing. You can see the error by typing:\n\nheights %&gt;%\n  group_by(sex) %&gt;%\n  do(my_summary())\n\nIf you do not use the parenthesis, then the function is not executed and instead do tries to return the function. This gives an error because do must always return a data frame. You can see the error by typing:\n\nheights %&gt;%\n  group_by(sex) %&gt;%\n  do(my_summary)\n\nSo do serves as a bridge between non-tidyverse functions and the tidyverse.",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#the-purrr-package",
    "href": "content/Week_02/02a.html#the-purrr-package",
    "title": "Introduction to the tidyverse",
    "section": "The purrr package",
    "text": "The purrr package\nIn previous sections (and labs) we learned about the sapply function, which permitted us to apply the same function to each element of a vector. We constructed a function and used sapply to compute the sum of the first n integers for several values of n like this:\n\ncompute_s_n &lt;- function(n){\n  x &lt;- 1:n\n  sum(x)\n}\nn &lt;- 1:25\ns_n &lt;- sapply(n, compute_s_n)\ns_n\n\n [1]   1   3   6  10  15  21  28  36  45  55  66  78  91 105 120 136 153 171 190\n[20] 210 231 253 276 300 325\n\n\nThis type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, sapply can return several different object types; for example, we might expect a numeric result from a line of code, but sapply might convert our result to character under some circumstances. purrr functions will never do this: they will return objects of a specified type or return an error if this is not possible.\nThe first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list:\n\nlibrary(purrr) # or library(tidyverse)\nn &lt;- 1:25\ns_n &lt;- map(n, compute_s_n)\nclass(s_n)\n\n[1] \"list\"\n\n\nIf we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values.\n\ns_n &lt;- map_dbl(n, compute_s_n)\nclass(s_n)\n\n[1] \"numeric\"\n\n\nThis produces the same results as the sapply call shown above.\nA particularly useful purrr function for interacting with the rest of the tidyverse is map_df, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a Argument 1 must have names error:\n\ns_n &lt;- map_df(n, compute_s_n)\n\nWe need to change the function to make this work:\n\ncompute_s_n &lt;- function(n){\n  x &lt;- 1:n\n  tibble(sum = sum(x))\n}\ns_n &lt;- map_df(n, compute_s_n)\nhead(s_n)\n\n# A tibble: 6 × 1\n    sum\n  &lt;int&gt;\n1     1\n2     3\n3     6\n4    10\n5    15\n6    21\n\n\nBecause map_df returns a tibble, we can have more columns defined in our function and returned.\n\ncompute_s_n2 &lt;- function(n){\n  x &lt;- 1:n\n  tibble(sum = sum(x), sumSquared = sum(x^2))\n}\ns_n &lt;- map_df(n, compute_s_n2)\nhead(s_n)\n\n# A tibble: 6 × 2\n    sum sumSquared\n  &lt;int&gt;      &lt;dbl&gt;\n1     1          1\n2     3          5\n3     6         14\n4    10         30\n5    15         55\n6    21         91\n\n\nThe purrr package provides much more functionality not covered here. For more details you can consult this online resource.",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#tidyverse-conditionals",
    "href": "content/Week_02/02a.html#tidyverse-conditionals",
    "title": "Introduction to the tidyverse",
    "section": "Tidyverse conditionals",
    "text": "Tidyverse conditionals\nA typical data analysis will often involve one or more conditional operations. In the section on Conditionals, we described the ifelse function, which we will use extensively in this book. In this section we present two dplyr functions that provide further functionality for performing conditional operations.\n\ncase_when\nThe case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0:\n\nx &lt;- c(-2, -1, 0, 1, 2)\ncase_when(x &lt; 0 ~ \"Negative\",\n          x &gt; 0 ~ \"Positive\",\n          x == 0  ~ \"Zero\")\n\n[1] \"Negative\" \"Negative\" \"Zero\"     \"Positive\" \"Positive\"\n\n\nA common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this:\n\nmurders %&gt;%\n  mutate(group = case_when(\n    abb %in% c(\"ME\", \"NH\", \"VT\", \"MA\", \"RI\", \"CT\") ~ \"New England\",\n    abb %in% c(\"WA\", \"OR\", \"CA\") ~ \"West Coast\",\n    region == \"South\" ~ \"South\",\n    TRUE ~ \"Other\")) %&gt;%\n  group_by(group) %&gt;%\n  summarize(rate = sum(total) / sum(population) * 10^5)\n\n# A tibble: 4 × 2\n  group        rate\n  &lt;chr&gt;       &lt;dbl&gt;\n1 New England  1.72\n2 Other        2.71\n3 South        3.63\n4 West Coast   2.90\n\n\nThat TRUE on the fourth line of case_when serves as a catch-all. As case_when steps through the conditions, if none of them are true, it comes to the last line. Since TRUE is always true, the function will return “Other”. Leaving out the last line of case_when would result in NA values for any observation that fails the first three conditionals. This may or may not be what you want.\n\n\nbetween\nA common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type\n\nx &gt;= a & x &lt;= b\n\nHowever, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation.\n\nbetween(x, a, b)\n\n\n\n\n\n\n\nTRY IT\n\n\n\n\nLoad the murders dataset. Which of the following is true?\n\n\nmurders is in tidy format and is stored in a tibble.\nmurders is in tidy format and is stored in a data frame.\nmurders is not in tidy format and is stored in a tibble.\nmurders is not in tidy format and is stored in a data frame.\n\n\nUse as_tibble to convert the murders data table into a tibble and save it in an object called murders_tibble.\nUse the group_by function to convert murders into a tibble that is grouped by region.\nWrite tidyverse code that is equivalent to this code:\n\n\nexp(mean(log(murders$population)))\n\nWrite it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with murders %&gt;%.\n\nUse the map_df to create a data frame with three columns named n, s_n, and s_n_2. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through \\(n\\) with \\(n\\) the row number.",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_02/02a.html#footnotes",
    "href": "content/Week_02/02a.html#footnotes",
    "title": "Introduction to the tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you have not installed this package already, you must use install.packages(\"tidyverse\") prior to the library() call you see below.↩︎",
    "crumbs": [
      "Course Content",
      "Week 02",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "content/Week_03/03a.html",
    "href": "content/Week_03/03a.html",
    "title": "Effective Visualizations",
    "section": "",
    "text": "This page.",
    "crumbs": [
      "Course Content",
      "Week 03",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "content/Week_03/03a.html#a-starting-list-from-tufte",
    "href": "content/Week_03/03a.html#a-starting-list-from-tufte",
    "title": "Effective Visualizations",
    "section": "A Starting List (from Tufte)",
    "text": "A Starting List (from Tufte)\n\nShow data variation, not design variation.\nThe number of information carrying (variable) dimensions depicted should not exceed the number of dimensions in the data. Graphics must not quote data out of context.\nClear, detailed and thorough labeling should be used to defeat graphical distortion and ambiguity. Write out explanations of the data on the graph itself. Label important events in the data.\nViewers eyes will follow a predictable path. Visual cues serve to light up that path.\nThe representation of numbers, as physically measured on the surface of the graph itself, should be directly proportional to the numerical quantities represented.\nEmpty space is informative. Do not fill every inch, but let the absence of information guide the viewer.\nWhen possible (and in a visually suitable way), show the data.\n\n\nEncoding data using visual cues\nVisual cues are any element of design that gives the viewer clues as to how to use the object. For instance, we can look at door handles like these\n\n\n\n\n\n\n\n\n\nand know exactly what to do with them. We don’t even need the “PUSH” and “PULL” – approaching just the pull handle in the wild gives sufficient visual cues that you know the door is a “pull” door. Encountering a metal plate with no handle is going to imply “push”. If the door on the right were a “push” door, you’d be momentarily confused! It would be poor design. Your plots should use visual cues to help readers understand how to use them with no confusion.\nWe start by describing some principles for encoding data. There are several approaches at our disposal including position, aligned lengths, angles, area, brightness, and color hue.\nTo illustrate how some of these strategies compare, let’s suppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015. For each year, we are simply comparing five quantities – the five percentages. A widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart:\n\n\n\n\n\n\n\n\n\nLooking at the above graph(s), what are the visual cues here?\nHere we are representing quantities with both areas and angles, since both the angle and area of each pie slice are proportional to the quantity the slice represents. This turns out to be a sub-optimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when area is the only available visual cue. This plot fails to be a useful visual cue. The donut chart is an example of a plot that uses only area:\n\n\n\n\n\n\n\n\n\nTo see how hard it is to quantify angles and area, note that the rankings and all the percentages in the plots above changed from 2000 to 2015. Can you determine the actual percentages and rank the browsers’ popularity? Can you see how the percentages changed from 2000 to 2015? It is not easy to tell from the plot. In fact, the pie R function help file states that:\n\nPie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data.\n\nIn this case, simply showing the numbers is not only clearer, but would also save on printing costs if printing a paper copy:\n\n\n\n\n\nBrowser\n2000\n2015\n\n\n\n\nOpera\n3\n2\n\n\nSafari\n21\n22\n\n\nFirefox\n23\n21\n\n\nChrome\n26\n29\n\n\nIE\n28\n27\n\n\n\n\n\n\n\nThe preferred way to plot these quantities is to use length and position as visual cues, since humans are much better at judging linear measures. The barplot uses this approach by using bars of length proportional to the quantities of interest. By adding horizontal lines at strategically chosen values, in this case at every multiple of 10, we ease the visual burden of quantifying through the position of the top of the bars. Compare and contrast the information we can extract from the two figures.\n\n\n\n\n\n\n\n\n\nNotice how much easier it is to see the differences in the barplot. In fact, we can now determine the actual percentages by following a horizontal line to the x-axis.\nIf for some reason you need to make a pie chart, label each pie slice with its respective percentage so viewers do not have to infer them from the angles or area:\n\n\n\n\n\n\n\n\n\nIn general, when displaying quantities, position and length are preferred over angles and/or area as position and length provide better visual cues. Brightness and color are even harder to quantify than angles. But, as we will see later, they are sometimes useful when more than two dimensions must be displayed at once.\n\n\nAvoid pseudo-three-dimensional plots\nThe figure below, taken from the scientific literature5, shows three variables: dose, drug type and survival. Although your screen/book page is flat and two-dimensional, the plot tries to imitate three dimensions and assigned a dimension to each variable.\n\n\n\n\n\nImage courtesy of Karl Broman\n\n\n\n\nHumans are not good at seeing in three dimensions (which explains why it is hard to parallel park) and our limitation is even worse with regard to pseudo-three-dimensions. To see this, try to determine the values of the survival variable in the plot above. Can you tell when the purple ribbon intersects the red one? This is an example in which we can easily use color to represent the categorical variable instead of using a pseudo-3D:\n\n##First read data\nurl &lt;- \"https://github.com/kbroman/Talk_Graphs/raw/master/R/fig8dat.csv\"\ndat &lt;- read.csv(url)\n\n##Now make alternative plot\ndat %&gt;% gather(drug, survival, -log.dose) %&gt;%\n  mutate(drug = gsub(\"Drug.\",\"\",drug)) %&gt;%\n  ggplot(aes(log.dose, survival, color = drug)) +\n  geom_line()\n\n\n\n\n\n\n\n\nNotice how much easier it is to determine the survival values.\nPseudo-3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. Here are two examples:\n\n\n\n\n\nImages courtesy of Karl Broman\n\n\n\n\n\n\n\nImages courtesy of Karl Broman\n\n\n\n\nQuestion: Which of the Tufte design principles above are violated by these plots?\n\n\nAvoid too many significant digits\nBy default, statistical software like R returns many significant digits. The default behavior in R is to show 7 significant digits. That many digits often adds no information and the added visual clutter can make it hard for the viewer to understand the message. As an example, here are the per 10,000 disease rates, computed from totals and population in R, for California across the five decades:\n\n\n\n\n\nstate\nyear\nMeasles\nPertussis\nPolio\n\n\n\n\nCalifornia\n1940\n37.8826320\n18.3397861\n0.8266512\n\n\nCalifornia\n1950\n13.9124205\n4.7467350\n1.9742639\n\n\nCalifornia\n1960\n14.1386471\nNA\n0.2640419\n\n\nCalifornia\n1970\n0.9767889\nNA\nNA\n\n\nCalifornia\n1980\n0.3743467\n0.0515466\nNA\n\n\n\n\n\n\n\nWe are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates. In this case, two significant figures is more than enough and clearly makes the point that rates are decreasing:\n\n\n\n\n\nstate\nyear\nMeasles\nPertussis\nPolio\n\n\n\n\nCalifornia\n1940\n37.9\n18.3\n0.8\n\n\nCalifornia\n1950\n13.9\n4.7\n2.0\n\n\nCalifornia\n1960\n14.1\nNA\n0.3\n\n\nCalifornia\n1970\n1.0\nNA\nNA\n\n\nCalifornia\n1980\n0.4\n0.1\nNA\n\n\n\n\n\n\n\nUseful ways to change the number of significant digits or to round numbers are signif and round. You can define the number of significant digits globally by setting options like this: options(digits = 3).\nAnother principle related to displaying tables is to place values being compared on columns rather than rows. Note that our table above is easier to read than this one:\n\n\n\n\n\nstate\ndisease\n1940\n1950\n1960\n1970\n1980\n\n\n\n\nCalifornia\nMeasles\n37.9\n13.9\n14.1\n1\n0.4\n\n\nCalifornia\nPertussis\n18.3\n4.7\nNA\nNA\n0.1\n\n\nCalifornia\nPolio\n0.8\n2.0\n0.3\nNA\nNA\n\n\n\n\n\n\n\n\n\nKnow your audience\nGraphs can be used for 1) our own exploratory data analysis, 2) to convey a message to experts, or 3) to help tell a story to a general audience. Make sure that the intended audience understands each element of the plot.\nAs a simple example, consider that for your own exploration it may be more useful to log-transform data and then plot it. However, for a general audience that is unfamiliar with converting logged values back to the original measurements, using a log-scale for the axis instead of log-transformed values will be much easier to digest.\n\n\nKnow when to include 0\nWhen using barplots, it is misinformative not to start the bars at 0. This is because, by using a barplot, we are implying the length is proportional to the quantities being displayed – a natural visual cue. By avoiding 0, relatively small differences can be made to look much bigger than they actually are. This approach is often used by politicians or media organizations trying to exaggerate a difference. Below is an illustrative example used by Peter Aldhous in this lecture: http://paldhous.github.io/ucb/2016/dataviz/week2.html.\n\n\n\n\n\n(Source: Fox News, via Media Matters6\n\n\n\n\nFrom the plot above, it appears that apprehensions have almost tripled when, in fact, they have only increased by about 16%. Starting the graph at 0 illustrates this clearly:\n\n\n\n\n\n\n\n\n\nHere is another example, described in detail in a Flowing Data blog post:\n\n\n\n\n\n(Source: Fox News, via Flowing Data7.)\n\n\n\n\nThis plot makes a 13% increase look like a five fold change. Here is the appropriate plot:\n\n\n\n\n\n\n\n\n\nFinally, here is an extreme example that makes a very small difference of under 2% look like a 10-100 fold change:\n\n\n\n\n\n(Source: Venezolana de Televisión via Pakistan Today8 and Diego Mariano.)\n\n\n\n\n(note: this is a years-old graphic, yet timely yet again)\nHere is the appropriate plot:\n\n\n\n\n\n\n\n\n\nWhen using position rather than length, it is then not necessary to include 0. This is particularly the case when we want to compare differences between groups relative to the within-group variability. Here is an illustrative example showing country average life expectancy stratified across continents in 2012:\n\n\n\n\n\n\n\n\n\nNote that in the plot on the left, which includes 0, the space between 0 and 43 adds no information and makes it harder to compare the between and within group variability.\n\n\nDo not distort quantities\nDuring President Barack Obama’s 2011 State of the Union Address, the following chart was used to compare the US GDP to the GDP of four competing nations:\n\n\n\n\n\n(Source: The 2011 State of the Union Address9)\n\n\n\n\nJudging by the area of the circles, the US appears to have an economy over five times larger than China’s and over 30 times larger than France’s. However, if we look at the actual numbers, we see that this is not the case. The actual ratios are 2.6 and 5.8 times bigger than China and France, respectively. The reason for this distortion is that the radius, rather than the area, was made to be proportional to the quantity, which implies that the proportion between the areas is squared: 2.6 turns into 6.5 and 5.8 turns into 34.1. Here is a comparison of the circles we get if we make the value proportional to the radius and to the area:\n\ngdp &lt;- c(14.6, 5.7, 5.3, 3.3, 2.5)\ngdp_data &lt;- data.frame(Country = rep(c(\"United States\", \"China\", \"Japan\", \"Germany\", \"France\"),2),\n           y = factor(rep(c(\"Radius\",\"Area\"),each=5), levels = c(\"Radius\", \"Area\")),\n           GDP= c(gdp^2/min(gdp^2), gdp/min(gdp))) %&gt;%\n   mutate(Country = reorder(Country, GDP))\ngdp_data %&gt;%\n  ggplot(aes(Country, y, size = GDP)) +\n  geom_point(show.legend = FALSE, color = \"blue\") +\n  scale_size(range = c(2,25)) +\n  coord_flip() + \n  ylab(\"\") + xlab(\"\") # identical to labs(y = \"\", x = \"\")\n\n\n\n\n\n\n\n\nNot surprisingly, ggplot2 defaults to using area rather than radius. Of course, in this case, we really should not be using area at all since we can use position and length:\n\ngdp_data %&gt;%\n  filter(y == \"Area\") %&gt;%\n  ggplot(aes(Country, GDP)) +\n  geom_bar(stat = \"identity\", width = 0.5) +\n  labs(y = \"GDP in trillions of US dollars\")\n\n\n\n\n\n\n\n\n\n\nOrder categories by a meaningful value\nWhen one of the axes is used to show categories, as is done in barplots, the default ggplot2 behavior is to order the categories alphabetically when they are defined by character strings. If they are defined by factors, they are ordered by the factor levels. We rarely want to use alphabetical order. Instead, we should order by a meaningful quantity. In all the cases above, the barplots were ordered by the values being displayed. The exception was the graph showing barplots comparing browsers. In this case, we kept the order the same across the barplots to ease the comparison. Specifically, instead of ordering the browsers separately in the two years, we ordered both years by the average value of 2000 and 2015.\nWe can use the reorder function, which helps us achieve this goal. This will let us re-order the levels of a factor variable. It is different from using order() or arrange() because it only changes the order in which a factor variable is recorded. That is, it doesn’t change the order of the data, it changes the order of the factor levels, and those factor levels are used by ggplot to determine the plotting position.\nTo appreciate how the right order can help convey a message, suppose we want to create a plot to compare the murder rate across states. We are particularly interested in the most dangerous and safest states. Note the difference when we order alphabetically (the default) versus when we order by the actual rate:\n\ndata(murders)\np1 &lt;- murders %&gt;% mutate(murder_rate = total / population * 100000) %&gt;%\n  ggplot(aes(x = state, y = murder_rate)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  theme(axis.text.y = element_text(size = 8))  +\n  xlab(\"\")\n\np2 &lt;- murders %&gt;% mutate(murder_rate = total / population * 100000) %&gt;%\n  mutate(state = reorder(state, murder_rate)) %&gt;% # here's the magic!\n  ggplot(aes(x = state, y = murder_rate)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  theme(axis.text.y = element_text(size = 8))  +\n  xlab(\"\")\n\ngrid.arrange(p1, p2, ncol = 2) # we'll cover this later\n\n\n\n\n\n\n\n\nWe can make the second plot like this:\nThe reorder function lets us reorder groups as well. Earlier we saw an example related to income distributions across regions. Here are the two versions plotted against each other:\n\n\n\n\n\n\n\n\n\nThe first orders the regions alphabetically, while the second orders them by the group’s median.",
    "crumbs": [
      "Course Content",
      "Week 03",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "content/Week_03/03a.html#show-the-data",
    "href": "content/Week_03/03a.html#show-the-data",
    "title": "Effective Visualizations",
    "section": "Show the data",
    "text": "Show the data\nWe have focused on displaying single quantities across categories. We now shift our attention to displaying data, with a focus on comparing groups.\nTo motivate our first principle, “show the data”, we go back to our artificial example of describing heights to a person who is unaware of some basic facts about the population of interest (and is otherwise unsophisticated). This time let’s assume that this person is interested in the difference in heights between males and females. A commonly seen plot used for comparisons between groups, popularized by software such as Microsoft Excel, is the dynamite plot, which shows the average and standard errors.10 The plot looks like this:\n\n\n\n\n\n\n\n\n\nThe average of each group is represented by the top of each bar and the antennae extend out from the average to the average plus two standard errors. If all ET receives is this plot, he will have little information on what to expect if he meets a group of human males and females. The bars go to 0: does this mean there are tiny humans measuring less than one foot? Are all males taller than the tallest females? Is there a range of heights? ET can’t answer these questions since we have provided almost no information on the height distribution.\nThis brings us to our first principle: show the data. This simple ggplot2 code already generates a more informative plot than the barplot by simply showing all the data points:\n\n\n\n\n\n\n\n\n\nFor example, this plot gives us an idea of the range of the data. However, this plot has limitations as well, since we can’t really see all the 238 and 812 points plotted for females and males, respectively, and many points are plotted on top of each other. As we have previously described, visualizing the distribution is much more informative. But before doing this, we point out two ways we can improve a plot showing all the points.\nThe first is to add jitter, which adds a small random shift to each point. In this case, adding horizontal jitter does not alter the interpretation, since the point heights do not change, but we minimize the number of points that fall on top of each other and, therefore, get a better visual sense of how the data is distributed. A second improvement comes from using alpha blending: making the points somewhat transparent. The more points fall on top of each other, the darker the plot, which also helps us get a sense of how the points are distributed. Here is the same plot with jitter and alpha blending:\n\nheights %&gt;%\n  ggplot(aes(sex, height)) +\n  geom_jitter(width = 0.1, alpha = 0.2)\n\n\n\n\n\n\n\n\nNow we start getting a sense that, on average, males are taller than females. We also note dark horizontal bands of points, demonstrating that many report values that are rounded to the nearest integer.",
    "crumbs": [
      "Course Content",
      "Week 03",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "content/Week_03/03a.html#faceting",
    "href": "content/Week_03/03a.html#faceting",
    "title": "Effective Visualizations",
    "section": "Faceting",
    "text": "Faceting\nLooking at the previous plot, it’s easy to tell that males tend to be taller than females. Before, we showed how we can plot two distributions over each other using an aesthetic mapping. Something like this:\n\nheights %&gt;%\n  ggplot(aes(x = height, fill = sex)) +\n  geom_histogram(alpha = .5, show.legend = TRUE) +\n  labs(fill = 'Sex')\n\n\n\n\n\n\n\n\nSometimes, putting the plots on top of each other, even with a well-chosen alpha, does not clearly communicate the differences in the distribution. When we want to compare side-by-side, we will often use facets. Facets are a bit like supercharged aesthetic mapping because they let us separate plots based on categorical variables, but instead of putting them together, we can have side-by-side plots.\nTwo functions in ggplot give facets: facet_wrap and facet_grid. We’ll use facet_grid as this is a little more powerful.\nFacets are added as an additional layer like this: + facet_grid(. ~ sex). Inside the function, we have a “formula” that is written without quotes (which is unusual for R). Since facet_grid takes a “formula”, all we have to do to facet is decide how we want to lay out our plots. If we want each of the faceting groups to lie along the vertical axis, we put the variable on which we want to facet before the “~”, and after the “~” we simply put a period. If we want the groups to lie along the horizontal axis, we put the variable after the “~” and the period before. In the example, we’ll separate the histogram by drawing them side by side along the horizontal axis.\n\nheights %&gt;%\n  ggplot(aes(x = height)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(.~sex)\n\n\n\n\n\n\n\n\nThis would be the result if we took the females, plotted the histogram, then took the males, made another histogram, and then put them side by side. But we do it in one command by adding +facet_grid(...)\n\nUse common axes with facets\nSince we have plots side-by-side, they can have different scales along the x-axis (or along the y-axis if we were stacking with sex ~ .). We want to be careful here - if we don’t have matching scales on these axes, then it’ll be really hard to visually see differences in the distribution.\nAs an example of what not to do, and to show that we can use the scales argument in facet_grid, we can allow the x-axis to freely scale between the plots. This makes it hard to tell that males are, on average, taller because the average male height, despite being larger than the average female height (70 vs. 65 or so) falls in the same location within the plot box. Note that 80 is the extreme edge for the left plot, but not in the right plot.\n\nheights %&gt;%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(. ~ sex, scales = \"free_x\")\n\n\n\n\n\n\n\n\n\n\nAlign plots vertically to see horizontal changes and horizontally to see vertical changes\nIn these histograms, the visual cue related to decreases or increases in height are shifts to the left or right, respectively: horizontal changes. Aligning the plots vertically helps us see this change when the axes are fixed:\n\nheights %&gt;%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(. ~ sex)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis plot makes it much easier to notice that men’s heights are, on average, higher.\nThe sample size of females is smaller than of males – that is, we have more males in the data. Try table(heights$sex) to see this. It’s also clear from the above plot because the height of the bars on the y-axis (count) are smaller for females. If we are interested in the distribution within our sample, this is useful. If we’re interested in the distribution of females vs. the distribution of males, we might want to re-scale the y-axis. Here we use scales = 'free_y' to allow each of the y-axes to have their own scale. Pay close attention to the axis labels now!\n\np2 &lt;- heights %&gt;%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(sex~., scales = 'free_y')\np2\n\n\n\n\n\n\n\n\nWe still have count on the y-axis, so we didn’t switch to density (though it would look the same). Instead, we rescaled the y-axis, which gives us a different perspective but still contains the count information.\nIf we want the more compact summary provided by boxplots, we then align them horizontally since, by default, boxplots move up and down with changes in height. Following our show the data principle, we then overlay all the data points:\n\np3=heights %&gt;%\n  ggplot(aes(sex, height)) +\n  geom_boxplot(coef=3) +\n  geom_jitter(width = 0.1, alpha = 0.2) +\n  ylab(\"Height in inches\")\n\np3\n\n\n\n\n\n\n\n\nNow contrast and compare these three plots, based on exactly the same data:\n\n\n\n\n\n\n\n\n\nNotice how much more we learn from the two plots on the right. Barplots are useful for showing one number, but not very useful when we want to describe distributions.\n\n\nFacet grids\nAs the name implies, facet_grid can make more than just side-by-plots. If we specify variables on boths sides of the “~”, we get a grid of plots.\n\ngapminder::gapminder %&gt;%\n  filter(year %in% c(1952,1972, 1992, 2002)) %&gt;%\n  filter(continent != 'Oceania') %&gt;%\n  ggplot(aes(x = lifeExp)) + \n  geom_density() +\n  facet_grid(continent ~ year)\n\n\n\n\n\n\n\n\nThis makes it easy to read the life expectancy distribution over time (left-to-right) and across continents (up-and-down). It makes it easy to see that Africa has spread it’s life expectancy distribution (some improved, some didn’t), while Europe has become more clustered at the top end over time. Faceting in a grid is very helpful when you have a time dimension.\n\n\nVisual cues to be compared should be adjacent, continued\nFor each continent, let’s compare income in 1970 versus 2010. When comparing income data across regions between 1970 and 2010, we made a figure similar to the one below, but this time we investigate continents rather than regions.\nNote that there are two gapminder datasets, one in dslabs and one in the gapminder package. The dslabs version has more data, so I will switch to that here by using dslabs::gapminder as our data.\n\ndslabs::gapminder %&gt;%\n  filter(year %in% c(1970, 2010) & !is.na(gdp)) %&gt;%\n  mutate(dollars_per_day = gdp/population/365) %&gt;%\n  mutate(labels = paste(year, continent)) %&gt;%  # creating text labels\n  ggplot(aes(x = labels, y = dollars_per_day)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +\n  scale_y_continuous(trans = \"log2\") +\n  ylab(\"Income in dollars per day\")\n\n\n\n\n\n\n\n\nThe default in ggplot2 is to order labels alphabetically so the labels with 1970 come before the labels with 2010, making the comparisons challenging because a continent’s distribution in 1970 is visually far from its distribution in 2010. It is much easier to make the comparison between 1970 and 2010 for each continent when the boxplots for that continent are next to each other:\n\ndslabs::gapminder %&gt;%\n  filter(year %in% c(1970, 2010) & !is.na(gdp)) %&gt;%\n  mutate(dollars_per_day = gdp/population/365) %&gt;%\n  mutate(labels = paste(continent, year)) %&gt;%\n  ggplot(aes(labels, dollars_per_day)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25)) +\n  scale_y_continuous(trans = \"log2\") +\n  ylab(\"Income in dollars per day\") + xlab('Continent and Year') \n\n\n\n\n\n\n\n\n\n\nLeave some space\nThe design maven Edward Tufte emphasizes th eneed for clarifying and empty space. Resist the urge to pack everything into a small layout. Especially in digital formats, space can be inexpensive, and can help attract the eye to your work.\nWe can control the space around our plots using the theme() function. We’ll cover more details of theme() on Thursday. To add some space around a plot, we use theme(plot.margin = margin(t=2, r = 2, b = 2, l = 2, unit = 'cm')), where the arguments correspond to top, right, bottom, left (in that order). I’m also going to add a black border to the outside so that we can see the boundary of the frame.\n\ndslabs::gapminder %&gt;%\n  filter(year %in% c(1970, 2010) & !is.na(gdp)) %&gt;%\n  mutate(dollars_per_day = gdp/population/365) %&gt;%\n  mutate(labels = paste(continent, year)) %&gt;%\n  ggplot(aes(labels, dollars_per_day)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25)) +\n  scale_y_continuous(trans = \"log2\") +\n  ylab(\"Income in dollars per day\") + xlab('Continent and Year') +\n  theme(plot.margin = margin(t=2, r = 2, b = 2, l = 2, unit = 'cm'),\n        plot.background = element_rect(color = 'black', size = 1)) # Adds black border\n\n\n\n\n\n\n\n\n\n\nUse color\nThe comparison becomes even easier to make if we use color to denote the two things we want to compare. This is an “information carrying dimension” and implemented with an aesthetic mapping. Now we do not have to make the labels column and can just use continent on the x-axis:\n\n dslabs::gapminder %&gt;%\n  filter(year %in% c(1970, 2010) & !is.na(gdp)) %&gt;%\n  mutate(dollars_per_day = gdp/population/365, year = factor(year)) %&gt;%\n  ggplot(aes(x = continent, y = dollars_per_day, fill = year)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  scale_y_continuous(trans = \"log2\") +\n  ylab(\"Income in dollars per day\")",
    "crumbs": [
      "Course Content",
      "Week 03",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "content/Week_03/03a.html#think-of-the-color-blind",
    "href": "content/Week_03/03a.html#think-of-the-color-blind",
    "title": "Effective Visualizations",
    "section": "Think of the color blind",
    "text": "Think of the color blind\nAbout 10% of the population is color blind. Unfortunately, the default colors used in ggplot2 are not optimal for this group. However, ggplot2 does make it easy to change the color palette used in the plots. An example of how we can use a color blind friendly palette is described here: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette:\n\ncolor_blind_friendly_cols &lt;-\n  c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n    \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nHere are the colors\n\n\n\n\n\n\n\n\n\nFrom Seafood Prices Reveal Impacts of a Major Ecological Disturbance:\n\n\n\n\n\n\n\n\n\nThere are several resources that can help you select colors, for example this one: http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/.",
    "crumbs": [
      "Course Content",
      "Week 03",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "content/Week_03/03a.html#use-colorblind-friendly-colors-in-your-projects",
    "href": "content/Week_03/03a.html#use-colorblind-friendly-colors-in-your-projects",
    "title": "Effective Visualizations",
    "section": "Use colorblind friendly colors in your projects",
    "text": "Use colorblind friendly colors in your projects\nYou should get in the habit of using colorblind-friendly colors. It will be required on your group projects. Mastering the palette is a helpful skill.",
    "crumbs": [
      "Course Content",
      "Week 03",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "content/Week_03/03a.html#gridextra-and-grid.arrange",
    "href": "content/Week_03/03a.html#gridextra-and-grid.arrange",
    "title": "Effective Visualizations",
    "section": "gridExtra and grid.arrange",
    "text": "gridExtra and grid.arrange\nThe gridExtra package has been used a few times in this lesson to combine plots using the grid.arrange function. The use is pretty intuitive - you save your plots as objects plot1 &lt;- ggplot(data, aes(x = var1)) and plot2 &lt;- ggplot(data, aes(x = var2)), and then use grid.arrange(plot1, plot2) to combine. The function will align as best it can, and there are more advanced grob-based functions that can adjust and align axes between plots, but we won’t get into them. If we want to set the layout, we can specify nrow and ncol to set the rows and columns.\nThe very-useful patchwork package is quickly replacing grid.arrange and provides more flexibility.",
    "crumbs": [
      "Course Content",
      "Week 03",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "content/Week_03/03a.html#footnotes",
    "href": "content/Week_03/03a.html#footnotes",
    "title": "Effective Visualizations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttp://kbroman.org/↩︎\nhttps://www.biostat.wisc.edu/~kbroman/presentations/graphs2017.pdf↩︎\nhttps://github.com/kbroman/Talk_Graphs↩︎\nhttp://paldhous.github.io/ucb/2016/dataviz/index.html↩︎\nhttps://projecteuclid.org/download/pdf_1/euclid.ss/1177010488↩︎\nhttp://mediamatters.org/blog/2013/04/05/fox-news-newest-dishonest-chart-immigration-enf/193507↩︎\nhttp://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence/↩︎\nhttps://www.pakistantoday.com.pk/2018/05/18/whats-at-stake-in-venezuelan-presidential-vote↩︎\nhttps://www.youtube.com/watch?v=kl2g40GoRxg↩︎\nIf you’re unfamiliar, standard errors are defined later in the course—do not confuse them with the standard deviation of the data.↩︎",
    "crumbs": [
      "Course Content",
      "Week 03",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "content/Week_04/04a.html",
    "href": "content/Week_04/04a.html",
    "title": "Visualizations in Practice",
    "section": "",
    "text": "This page.\n\n\n\n\nToday we’re mostly learning some technical aspects of ggplot.\nWhy are we covering the material this way? (Good question. There’s an answer!)\n\n\n\n\nLoad up our murders data\n\nlibrary(dslabs)\nlibrary(ggplot2)\nlibrary(dplyr)\ndata(murders)\n\np &lt;- ggplot(data = murders, aes(x = population, y = total, label = abb))",
    "crumbs": [
      "Course Content",
      "Week 04",
      "Visualizations in Practice"
    ]
  },
  {
    "objectID": "content/Week_04/04a.html#readings",
    "href": "content/Week_04/04a.html#readings",
    "title": "Visualizations in Practice",
    "section": "",
    "text": "This page.\n\n\n\n\nToday we’re mostly learning some technical aspects of ggplot.\nWhy are we covering the material this way? (Good question. There’s an answer!)\n\n\n\n\nLoad up our murders data\n\nlibrary(dslabs)\nlibrary(ggplot2)\nlibrary(dplyr)\ndata(murders)\n\np &lt;- ggplot(data = murders, aes(x = population, y = total, label = abb))",
    "crumbs": [
      "Course Content",
      "Week 04",
      "Visualizations in Practice"
    ]
  },
  {
    "objectID": "content/Week_04/04a.html#scales-and-transformations",
    "href": "content/Week_04/04a.html#scales-and-transformations",
    "title": "Visualizations in Practice",
    "section": "Scales and transformations",
    "text": "Scales and transformations\n\nLog transformations\nLast lecture, we re-scaled our population by 10^6 (millions), but still had a lot of variation because some states are tiny and some are huge. Sometimes, we want to have one (or both) of our axes scaled non-linearly. For instance, if we wanted to have our x-axis be in log base 10, then each major tick would represent a factor of 10 over the last. This is not the default, so this change needs to be added through a scales layer. A quick look at the cheat sheet reveals the scale_x_continuous function lets us control the behavior of scales. We use them like this:\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 0.05) +\n  scale_x_continuous(trans = \"log10\") +\n  scale_y_continuous(trans = \"log10\")\n\n\n\n\n\n\n\n\nA couple of things here: adding things like scale_x_continuous(...) operates on the whole plot. In some cases, order matters, but it doesn’t here, so we can throw scale_x_continuous anywhere. Because we have altered the whole plot’s scale to be in the log-scale now, the nudge must be made smaller. It is in log-base-10 units. Using ?scale_x_continuous brings us to the help for both scale_x_continuous and scale_y_continuous, which shows us the options for transformations trans = ...\nThis particular transformation is so common that ggplot2 provides the specialized functions scale_x_log10 and scale_y_log10 which “inherit” (take the place of) the scale_x_continuous functions but have log base 10 as default. We can use these to rewrite the code like this:\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 0.05) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nThis can make a plot much easier to read, though one has to be sure to pay attention to the values on the axes. Plotting anything with very large outliers will almost always be better if done in log-scale. Adding the scale layer is an easy way to fix this.\nWe can also use one of many built-in transformations. Of note: reverse just inverts the scale, which can be helpful, log uses the natural log, sqrt takes the square root (dropping anything with a negative value), reciprocal takes 1/x. If your x-axis is in a date format, you can also scale to hms (hour-minute-second) or date.\n\n\nTransforming data vs. transforming using scale_...\nWe could simply take the log of population and log of total in the call and we’d get something very similar. Note that we had to override the aesthetic mapping set in p in each of the geometries:\n\np + geom_point(aes(x = log(population, base=10), y = log(total, base=10)), size = 3) +\n  geom_text(aes(x = log(population, base=10), y = log(total, base=10)), nudge_x = 0.05) \n\n\n\n\n\n\n\n\nThis avoids using scale_x_continuous or it’s child function scale_x_log10. One advantage to using scale_x... is that the axes are correctly labeled. When we transform the data directly, the axis labels only show the transformed values, so 7,000,000 becomes 7.0. This could be confusing! We could update the axis labels to say “total murders (log base 10)” and “total population (log base 10)”, but that’s cumbersome. Using scale_x... is a lot more refined and easy.",
    "crumbs": [
      "Course Content",
      "Week 04",
      "Visualizations in Practice"
    ]
  },
  {
    "objectID": "content/Week_04/04a.html#axis-labels-legends-and-titles",
    "href": "content/Week_04/04a.html#axis-labels-legends-and-titles",
    "title": "Visualizations in Practice",
    "section": "Axis labels, legends, and titles",
    "text": "Axis labels, legends, and titles\nBut let’s say we did want to re-name our x-axis label. Or maybe we don’t like that the variable column name is lower-case “p”.\nAs with many things in ggplot, there are many ways to get the same result. We’ll go over one way of changing titles and labels, but know that there are many more.\n\nChanging axis titles\nWe’ll use the labs(...) annotation layer to do this, which is pretty straightforward. ?labs shows us what we can change, and while it looks pretty basic, the real meat is in the ... argument, which the help says is “A list of new name-value pairs”. This means we can re-define the label on anything that is an aesthetic mapping. X and Y are aesthetic mappings, so…\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 0.05) +\n  scale_x_log10() +\n  scale_y_log10() + \n  labs(x = 'Population', y = 'Total murders')\n\n\n\n\n\n\n\n\nNow, let’s use an aesthetic mapping that generates a legend, like color, and see what labs renames:\n\np + geom_point(aes(color = region), size = 3) +\n  geom_text(nudge_x = 0.05) +\n  scale_x_log10() +\n  scale_y_log10() + \n  labs(x = 'Population', y = 'Total murders', color = 'US Region')\n\n\n\n\n\n\n\n\nWe can rename the aesthetic mapping-relevant label using labs. Even if there are multiple mapped aesthetics:\n\np + geom_point(aes(color = region, size = total/population)) +\n  geom_text(nudge_x = 0.05) +\n  scale_x_log10() +\n  scale_y_log10() + \n  labs(x = 'Population', y = 'Total murders', color = 'US Region', size = 'Murder rate')\n\n\n\n\n\n\n\n\n\n\nTitles\nIn ?labs, we also see some things that look like titles and captions. We can include those:\n\np + geom_point(aes(color = region, size = total/population)) +\n  geom_text(nudge_x = 0.05) +\n  scale_x_log10() +\n  scale_y_log10() + \n  labs(x = 'Population', y = 'Total murders', color = 'US Region', size = 'Murder rate',\n       title = 'This is a title', subtitle = 'This is a subtitle', caption = 'This is a caption', tag = 'This is a tag')\n\n\n\n\n\n\n\n\nNow that you know how, always label your plots with at least a title and have meaningful axis and legend labels.",
    "crumbs": [
      "Course Content",
      "Week 04",
      "Visualizations in Practice"
    ]
  },
  {
    "objectID": "content/Week_04/04a.html#axis-ticks",
    "href": "content/Week_04/04a.html#axis-ticks",
    "title": "Visualizations in Practice",
    "section": "Axis ticks",
    "text": "Axis ticks\nIn addition to the axis labels, we may want to format or change the axis tick labels (like “1e+06” above) or even where the tick marks and lines are drawn. If we don’t specify anything, the axis labels and tick marks are drawn as best as ggplot can do, but we can change this. This might be especially useful if our data has some meaningful cutoffs that aren’t found by the default, or we just don’t like where the marks fall or how they are labeled. This is easy to fix with ggplot.\nTo change the tick mark labels, we have to set the tick mark locations. Then we can set a label for each tick mark. Let’s go back to our murders data and, for simplicity, take the log transformation off the Y axis. We’ll use scale_y_continuous to tell R where to put the breaks (breaks =) and what to label the breaks. We have to give it one label for every break. Let’s say we just want a line at the 500’s and let’s say we want to (absurdly) use written numerics for each of the Y-axis lines. Since scale_y_log10 inherits from scale_y_continuous, we can just use that and add the breaks and labels:\n\np + geom_point(aes(color = region), size = 3) +\n  geom_text(nudge_x = .05) +\n  scale_x_log10() +\n  scale_y_log10(breaks = c(0,50, 100, 500,1000,1500), \n                     labels = c('Zero','Fifty','One hundred','Five hundred','One thousand','Fifteen hundred')) +\n  labs(x = 'Population', y = 'Total murders', color = 'US Region')\n\n\n\n\n\n\n\n\nWe have manually set both the location and the label for the y-axis. Note that R filled in the in-between “minor” tick lines, but we can take those out. Since we are setting the location of the lines, we can do anything we want:\n\np + geom_point(aes(color = region), size = 3) +\n  geom_text(nudge_x = .05) +\n  scale_x_log10() +\n  scale_y_log10(breaks = c(0,50, 100, 721, 1000,1500), \n                     labels = c('Zero','Fifty','One hundred','Seven hundred twenty one','One thousand','Fifteen hundred'),\n                     minor_breaks = NULL) +\n  labs(x = 'Population', y = 'Total murders', color = 'US Region')\n\n\n\n\n\n\n\n\nSo we can now define where axis tick lines should lie and how they should be labeled.\n\nFlexible axis tick labels\nAnyone else annoyed by the scientific 1e+06-style x-axis labels? If you’re not familiar, 1e+06 means 1 * 10^6 or 1,000,000. Of course, we could manually make the labels (and the breaks) like we did above, but that might be tedious, and what if we have different ranges of data? We’d end up forcing tick marks in areas that are far out of the support of the data (off the plot).\nWe can also use a labeler function. The most common ones are in the scales package, so you’ll want to install that and load it up using library(scales). Each labeler is a function that takes as it’s first argument a vector of numbers (or some expected format, like dates) and returns a formatted character vector of labels.\nSo the following will happen:\n\nscales::comma_format\n\n\nnumbers.vector = c(20000, 50000, 100000, 500000, 1000000) # 20k to 1M\n\nscales::label_comma(numbers.vector, big.mark = ',', decimal.mark = '.')\n\nfunction (x) \n{\n    number(x, accuracy = accuracy, scale = scale, prefix = prefix, \n        suffix = suffix, big.mark = big.mark, decimal.mark = decimal.mark, \n        style_positive = style_positive, style_negative = style_negative, \n        scale_cut = scale_cut, trim = trim, ...)\n}\n&lt;bytecode: 0x000001f413c03668&gt;\n&lt;environment: 0x000001f413c02750&gt;\n\n\nWha? That looks like the output…is a function?\nYes. Yes it is. The function returns….a function. And that function takes the number and returns the labels:\n\nscalefun = scales::label_comma(big.mark = ',', decimal.mark = '.')\n\nscalefun(numbers.vector)\n\n[1] \"20,000\"    \"50,000\"    \"100,000\"   \"500,000\"   \"1,000,000\"\n\n\nNow we have the appropriate format! Here are some others:\n\nscalefun.percent = scales::label_percent(accuracy = .001, scale=100)\n\nscalefun.percent(murders$total[1:5]/murders$population[1:5])\n\n[1] \"0.003%\" \"0.003%\" \"0.004%\" \"0.003%\" \"0.003%\"\n\nscalefun.dollar = scales::label_dollar(accuracy = 1, scale=1)\n\nscalefun.dollar(numbers.vector)\n\n[1] \"$20,000\"    \"$50,000\"    \"$100,000\"   \"$500,000\"   \"$1,000,000\"\n\n\nNote that we could set the accuracy and scale arguments in scales::label_percent and scales::label_dollar. The accuracy argument determines rounding. We don’t need cents when we have dollar values up to 1M, but we do need fractions of a percent when we use the murder rate (not multipled by 100,000).\n\nUsing labeler functions\nThe above was just an illustration of how the labelers work. To use it in practice, we need only give the function name to ggplot:\n\np + geom_point(aes(color = region), size = 3) +\n  geom_text(nudge_x = .05) +\n  scale_x_log10(labels= scales::label_comma()) +\n  scale_y_log10(breaks = c(0,50, 100, 721, 1000,1500), \n                     labels = scales::label_comma(accuracy=.01),\n                     minor_breaks = NULL) +\n  labs(x = 'Population', y = 'Total murders', color = 'US Region')\n\n\n\n\n\n\n\n\nWe don’t need the decimals on the y-axis labels, it’s just there for illustration.",
    "crumbs": [
      "Course Content",
      "Week 04",
      "Visualizations in Practice"
    ]
  },
  {
    "objectID": "content/Week_04/04a.html#additional-geometries",
    "href": "content/Week_04/04a.html#additional-geometries",
    "title": "Visualizations in Practice",
    "section": "Additional geometries",
    "text": "Additional geometries\nLet’s say we are happy with our axis tick locations, but we want to add a single additional line. Maybe we want to divide at 1,000,000 population (a vertical line at 1,000,000) becuase we think those over 1,000,000 are somehow different, and we want to call attention to the data around that point. As a more general example, if we were to plot, say, car accidents by age, we would maybe want to label age 21, when people can legally purchase alcohol (and subsequently cause car accidents).\ngeom_vline lets us add a single vertical line (without aesthetic mappings). If we look at ?geom_vline we see that it requires ones aesthetic:xintercept. It also takes aesthetics like color and size, and introduces the linetype aesthetic:\n\np + geom_point(aes(color = region), size = 3) +\n  geom_text(nudge_x = .05) +\n  geom_vline(aes(xintercept = 1000000), col = 'red', size = 2, linetype = 2) +\n  scale_x_log10() +\n  scale_y_log10(breaks = c(0,50, 100, 721, 1000,1500), \n                     labels = c('Zero','Fifty','One hundred','Seven hundred twenty one','One thousand','Fifteen hundred'),\n                     minor_breaks = NULL) +\n  labs(x = 'Population', y = 'Total murders', color = 'US Region')\n\n\n\n\n\n\n\n\nCombining geometries is as easy as adding the layers with +.\n\ngeom_line\nFor a good old line plot, we use the line geometry at geom_line. The help for ?geom_line tells us that we need an x and a y aesthetic (much like geom_points). Since our murders data isn’t really suited to a line graph, we’ll use a daily stock price. We’ll get this using tidyquant, which pulls stock prices from Yahoo Finance and maintains the “tidy” format. You’ll need to install.packages('tidyquant') before you run this the first time.\n\nlibrary(tidyquant)\nAAPL = tq_get(\"AAPL\", from = '2009-01-01', to = '2021-08-01', get = 'stock.prices')\nhead(AAPL)\n\n# A tibble: 6 × 8\n  symbol date        open  high   low close     volume adjusted\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 AAPL   2009-01-02  3.07  3.25  3.04  3.24  746015200     2.74\n2 AAPL   2009-01-05  3.33  3.43  3.31  3.38 1181608400     2.85\n3 AAPL   2009-01-06  3.43  3.47  3.30  3.32 1289310400     2.81\n4 AAPL   2009-01-07  3.28  3.30  3.22  3.25  753048800     2.74\n5 AAPL   2009-01-08  3.23  3.33  3.22  3.31  673500800     2.80\n6 AAPL   2009-01-09  3.33  3.34  3.22  3.23  546845600     2.73\n\n\nNow, we can plot a line graph of the Apple closing stock price over the requested date range. We want this to be a time series, so the x-axis will be the date and the y-axis will be the closing price.\n\nggplot(AAPL, aes(x = date, y = close)) +\n  geom_line() +\n  labs(x = 'Date', y = 'Closing price', title = 'Apple stock price')\n\n\n\n\n\n\n\n\nIn geom_line, R will automatically sort on the x-variable. If you don’t want this, then geom_path will use whatever order the data is in. Either way, if you have multiple observations for the same value on the x-axis, then you’ll get something pretty messy because R will try to connect, in some order, all the points. Let’s see an example with two stocks:\n\nAAPLNFLX = tq_get(c(\"AAPL\",\"NFLX\"), from = '2021-01-01', to = '2021-08-01', get = 'stock.prices')\nggplot(AAPLNFLX, aes(x = date, y = close)) +\n  geom_line() +\n  labs(x = 'Date', y = 'Closing price', title = 'Apple and Netflix stock price')\n\n\n\n\n\n\n\n\nThat looks kinda strange. That’s because, for every date, we have two values - the NFLX and the AAPL value, so each day has a vertical line drawn between the two prices. This is nonsense, especially since what we want to see is the history of NFLX and AAPL over time.\nAesthetics to the rescue! Remember, when we use an aesthetic mapping, we are able to separate out data by things like color or linetype. Let’s use color as the aesthetic here, and map it to the stock ticker:\n\nAAPLNFLX = tq_get(c(\"AAPL\",\"NFLX\"), from = '2021-01-01', to = '2021-08-01', get = 'stock.prices')\nggplot(AAPLNFLX, aes(x = date, y = close, color = symbol)) +\n  geom_line() +\n  labs(x = 'Date', y = 'Closing price', title = 'Apple and Netflix stock price')\n\n\n\n\n\n\n\n\nWell there we go! We can now see each stock price over time, with a convenient legend. Later on, we’ll learn how to change the color palatte. If we don’t necessarily want a different color but we do want to separate the lines, we can use the group aesthetic.\n\nAAPLNFLX = tq_get(c(\"AAPL\",\"NFLX\"), from = '2021-01-01', to = '2021-08-01', get = 'stock.prices')\nggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +\n  geom_line() + \n  labs(x = 'Date', y = 'Closing price', title = 'Apple and Netflix stock price')\n\n\n\n\n\n\n\n\nSimilar result as geom_line, but without the color difference (which makes it rather hard to tell what you’re looking at). But if we add labels using geom_label, we’ll get one label for every point, which will be overwhelming. The solution? Use some filtered data so that there is only one point for each label. But that means replacing the data in ggplot. Here’s how.\n\n\nUsing different data with different geometries\nJust as we can use different aesthetic mappings on each geometry, we can use different data entirely. This is useful when we want one geometry to have one set of data (like the stock prices above), but another geometry to only have a subset of the data. Why would we want that? Well, we’d like to label just one part of each of the lines in our plot, right? That means we want to label a subset of the stock data.\nTo replace data in a geometry, we just need to specify the data = argument separately:\n\nggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +\n  geom_line() +\n  geom_label(data = AAPLNFLX %&gt;% group_by(symbol) %&gt;% slice(100),\n             aes(label = symbol),\n             nudge_y = 20) + \n  labs(x = 'Date', y = 'Closing price', title = 'Apple and Netflix stock price')\n\n\n\n\n\n\n\n\nIn geom_label, we specified we wanted the 100th observation from each symbol to be the label location. Then, we nudged it up along y by 20 so that it’s clear of the line.\nR also has a very useful ggrepel package that gives us geom_label_repel which takes care of the nudging for us, even in complicated situations (lots of points, lines, etc.). It does a decent job here of moving the label to a point where it doesn’t cover a lot of data.\n\nlibrary(ggrepel)\nggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +\n  geom_line() +\n  geom_label_repel(data = AAPLNFLX %&gt;% group_by(symbol) %&gt;% slice(100),\n             aes(label = symbol)) + \n  labs(x = 'Date', y = 'Closing price', title = 'Apple and Netflix stock price')\n\n\n\n\n\n\n\n\nNow, we don’t lose a lot of space to a legend, and we haven’t had to use color to separate the stock symbols.\n\nSummarizing data and including in ggplot\nOften, we’ll want to have lines or highlights in places that are derived from the data. In the above example from murders, we added a line at an arbitrary x-value (population) of 1,000,000.\nLet’s say we want to show the average state population for each region. We’ll need to use two different data objects, one for the original, one for the summary. Create the summary first:\n\nmurders.summary = murders %&gt;%\n  group_by(region) %&gt;%\n  dplyr::summarize(meanpop = mean(population),\n                   meantot = mean(total),\n                   meanrate = 100000*sum(total)/sum(population))\n\nprint(murders.summary)\n\n# A tibble: 4 × 4\n  region         meanpop meantot meanrate\n  &lt;fct&gt;            &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Northeast     6146360     163.     2.66\n2 South         6804378.    247.     3.63\n3 North Central 5577250.    152.     2.73\n4 West          5534273.    147      2.66\n\n\nThe, use both murders and murders.summary in the plot:\n\np + geom_point(aes(color = region), size = 3) +\n  geom_text(nudge_x = .05) +\n  geom_vline(data = murders.summary, \n             aes(xintercept = meanpop, color=region), size = 2, linetype = 1, alpha=.5) +\n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = 'Population', y = 'Total murders', color = 'US Region')\n\n\n\n\n\n\n\n\nNote that we get the same color mapping – this isn’t necessarily automatic. Rather, it’s because the order in which R encounters each region is the exact same order in both murders and murders.summary. If it weren’t, we’d have to set the factor levels using factor and overwriting our old region column:\n\nmurders.summary = murders %&gt;%\n  group_by(region) %&gt;%\n  dplyr::summarize(meanpop = mean(population),\n                   meantot = mean(total),\n                   meanrate = 100000*sum(total)/sum(population)) %&gt;%\n  dplyr::mutate(region = factor(region, levels = levels(murders$region)))\n\nprint(murders.summary)\n\n# A tibble: 4 × 4\n  region         meanpop meantot meanrate\n  &lt;fct&gt;            &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Northeast     6146360     163.     2.66\n2 South         6804378.    247.     3.63\n3 North Central 5577250.    152.     2.73\n4 West          5534273.    147      2.66\n\n\nPerhaps we would like to summarize the regional average Population and Murders by adding representative points. Since we’re adding new data, we have to remove the label=abb part of the aes that inherits to each geom, which we do with label=NULL in the new point geometry:\n\np + geom_point(aes(color = region), size = 3) +\n  geom_text(nudge_x = .05) +\n  geom_point(data = murders.summary, aes(x = meanpop, y = meantot, color = region, label = NULL), size =12, shape=13) +\n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = 'Population', y = 'Total murders', color = 'US Region')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it!\n\n\n\nIn-Class Activity: Comparing 2008 Electoral Votes and Polling Data.\nIn this activity, we will use R to visualize and compare the actual electoral votes from the 2008 presidential election with the polling data prior to the election. Specifically, we will explore how accurately the polls predicted the election results. Yes, I know this is an older election… that’s kind of the point. This isn’t a politics class.\nSetup:\nYou’ll need to download two CSV files and put them somewhere on your computer (or upload it to RStudio.cloud if you’ve gone that direction)—preferably in a folder named data in your project folder. You can download the data from the link below:\n\n pres08.csv\n polls08.csv\n\nSteps Prior to Analysis:\n\nLoad the Data\n\n\n# Load the necessary libraries\nlibrary(tidyverse)\nelectoral_votes &lt;- read.csv(\"path_to/pres08.csv\")\npolling_data &lt;- read.csv(\"path_to/polls08.csv\") %&gt;%\n  dplyr::mutate(middate = as.Date(middate)) # we'll tackle dates later\n\nNote that you’ll have to replace path_to with the actual path to the files on your computer.\n\nData Preparation\n\nWe need to aggregate the polling data to get an average poll result for each state. We will then merge this with the electoral votes data.\n\n# Aggregating polling data\navg_polls &lt;- polling_data %&gt;%\n  group_by(state) %&gt;%\n  summarise(Obama_avg = mean(Obama), McCain_avg = mean(McCain))\n\n# Merging with electoral votes data\ncombined_data &lt;- merge(electoral_votes, avg_polls, by = \"state\")\n\n\nComplete Your Analysis\n\nYou are tasked with two things:\nA: Explore and visualize the discrepancies between polls and actual results using combined_data\nB: Visualize the relationship between the actual results and the poll results over time for the following state: c('MI','WI','IA','FL','NC','GA'). Does this relationship change over time? Or, put another way, were early polls or later polls more accurate?\nYou might find these code bits to be useful (hint: use them with case_when):\n\n# Define the regions\nNortheast = c('CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA')\nMidwest =  c('IL', 'IN', 'MI', 'OH', 'WI', 'IA', 'KS', 'MN', 'MO', 'NE', 'ND', 'SD')\nSouth = c('DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'DC', 'WV', 'AL', 'KY', 'MS', 'TN', 'AR', 'LA', 'OK', 'TX')\nWest = c('AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AK', 'CA', 'HI', 'OR', 'WA')\n\nMake sure you label your axes, legends, and other elements correctly. You may work in groups of up to three.\nUse https://bit.ly/EC242 to share. Happy visualizing!",
    "crumbs": [
      "Course Content",
      "Week 04",
      "Visualizations in Practice"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html",
    "href": "content/Week_05/05a.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "This page.\n\n\n\n\nHow can we reshape data into a useable tidy form?\nWhat is a join and why is it a common data wrangling maneuver?\nWhat is a primary key and why is it important to think about our data in this way?\nHow do we deal with messy date variables?",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html#required-reading",
    "href": "content/Week_05/05a.html#required-reading",
    "title": "Data Wrangling",
    "section": "",
    "text": "This page.\n\n\n\n\nHow can we reshape data into a useable tidy form?\nWhat is a join and why is it a common data wrangling maneuver?\nWhat is a primary key and why is it important to think about our data in this way?\nHow do we deal with messy date variables?",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html#gather",
    "href": "content/Week_05/05a.html#gather",
    "title": "Data Wrangling",
    "section": "gather",
    "text": "gather\nOne of the most used functions in the tidyr package is gather, which is useful for converting wide data into tidy data.\nLooking at the data above, the fundamental problem is that the column name contains data. This is a problem indeed! And a common one.\n\nThe gather arguments\nHere we want to reshape the wide_data dataset so that each row represents a country’s yearly fertility observation, which implies we need three columns to store the year, country, and the observed value. In its current form, data from different years are in different columns with the year values stored in the column names.\nAs with most tidyverse functions, the gather function’s first argument is the data frame that will be converted. Easy enough.\nThe second and third arguments will tell gather the column names we want to assign to the columns containing the current column names and the observed data, respectively. In this case a good choice for these two arguments would be something like year and fertility. These are not currenly column names in the data – they are names we want to use to hold data currently located in the “wide” columns. We know this is fertility data only because we deciphered it from the file name.\nThe fourth argument specifies the columns containing observed values; these are the columns that will be gathered. The default (if we leave out the 4th argument) is to gather all columns so, in most cases, we have to specify the columns. In our example we want columns 1960, 1961 up to 2015. Since these are non-standard column names, we have to backtick them so R doesn’t think they’re to be treated as integers:\nThe code to gather the fertility data therefore looks like this:\n\nnew_tidy_data &lt;- gather(wide_data, year, fertility, `1960`:`2015`)\n\nWe can also use the pipe like this:\n\nnew_tidy_data &lt;- wide_data %&gt;% gather(year, fertility, `1960`:`2015`)\n\nWe can see that the data have been converted to tidy format with columns year and fertility:\n\nhead(new_tidy_data)\n\n# A tibble: 6 × 3\n  country     year  fertility\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;\n1 Germany     1960       2.41\n2 South Korea 1960       6.16\n3 Germany     1961       2.44\n4 South Korea 1961       5.99\n5 Germany     1962       2.47\n6 South Korea 1962       5.79\n\n\nWe have the first column, which consists of “everything in the data that wasn’t gathered”, the next two columns are named based on the arguments we gave. The second column, named by our argument above, contains the column names of the wide data. The third column, also named by our argument above, contains the value that was in each corresponding colum x country pair. What you leave out of the 4th argument is important!\nNote that each year resulted in two rows since we have two countries and this column was not gathered. A somewhat quicker way to write this code is to specify which column will not be gathered, rather than all the columns that will be gathered:\n\nnew_tidy_data &lt;- wide_data %&gt;%\n  gather(year, fertility, -country)\n\nhead(new_tidy_data)\n\n# A tibble: 6 × 3\n  country     year  fertility\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;\n1 Germany     1960       2.41\n2 South Korea 1960       6.16\n3 Germany     1961       2.44\n4 South Korea 1961       5.99\n5 Germany     1962       2.47\n6 South Korea 1962       5.79\n\n\nThe new_tidy_data object looks like the original tidy_data we defined this way\n\ndata(\"gapminder\")\ntidy_data &lt;- gapminder %&gt;%\n  dplyr::filter(country %in% c(\"South Korea\", \"Germany\") & !is.na(fertility)) %&gt;%\n  dplyr::select(country, year, fertility)\n\nwith just one minor difference. Can you spot it? Look at the data type of the year column:\n\nclass(tidy_data$year)\n\n[1] \"integer\"\n\nclass(new_tidy_data$year)\n\n[1] \"character\"\n\n\nThe gather function assumes that column names are characters. So we need a bit more wrangling before we are ready to make a plot. We need to convert the year column to be numbers. The gather function includes the convert argument for this purpose:\n\nnew_tidy_data &lt;- wide_data %&gt;%\n  gather(year, fertility, -country, convert = TRUE)\nclass(new_tidy_data$year)\n\n[1] \"integer\"\n\n\nNote that we could have also used the mutate and as.numeric.\nNow that the data is tidy, we can use this relatively simple ggplot code:\n\nnew_tidy_data %&gt;% ggplot(aes(year, fertility, color = country)) +\n  geom_point()",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html#spread",
    "href": "content/Week_05/05a.html#spread",
    "title": "Data Wrangling",
    "section": "spread",
    "text": "spread\nAs we will see in later examples, it is sometimes useful for data wrangling purposes to convert tidy data into wide data. We often use this as an intermediate step in tidying up data. The spread function is basically the inverse of gather. The first argument is for the data, but since we are using the pipe, we don’t show it. The second argument tells spread which variable will be used as the column names. The third argument specifies which variable to use to fill out the cells:\n\nnew_wide_data &lt;- new_tidy_data %&gt;% spread(year, fertility)\ndplyr::select(new_wide_data, country, `1960`:`1967`)\n\n# A tibble: 2 × 9\n  country     `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Germany       2.41   2.44   2.47   2.49   2.49   2.48   2.44   2.37\n2 South Korea   6.16   5.99   5.79   5.57   5.36   5.16   4.99   4.85\n\n\nThe following diagram can help remind you how these two functions work:\n\n\n\n\n\n\n\n\n\n(Image courtesy of RStudio1. CC-BY-4.0 license2. Cropped from original.)",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html#separate",
    "href": "content/Week_05/05a.html#separate",
    "title": "Data Wrangling",
    "section": "separate",
    "text": "separate\nThe data wrangling shown above was simple compared to what is usually required. In our example spreadsheet files, we include an illustration that is slightly more complicated. It contains two variables: life expectancy and fertility. However, the way it is stored is not tidy and, as we will explain, not optimal.\n\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\n\nfilename &lt;- \"life-expectancy-and-fertility-two-countries-example.csv\"\nfilename &lt;-  file.path(path, filename)\n\nraw_dat &lt;- read_csv(filename)\ndplyr::select(raw_dat, 1:5)\n\n# A tibble: 2 × 5\n  country     `1960_fertility` `1960_life_expectancy` `1961_fertility`\n  &lt;chr&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;            &lt;dbl&gt;\n1 Germany                 2.41                   69.3             2.44\n2 South Korea             6.16                   53.0             5.99\n# ℹ 1 more variable: `1961_life_expectancy` &lt;dbl&gt;\n\n\nFirst, note that the data is in wide format. Second, notice that this table includes values for two variables, fertility and life expectancy, with the column name encoding which column represents which variable. Encoding information in the column names is not recommended but, unfortunately, it is quite common. We will put our wrangling skills to work to extract this information and store it in a tidy fashion.\nWe can start the data wrangling with the gather function, but we should no longer use the column name year for the new column since it also contains the variable type. We will call it key, the default, for now:\n\ndat &lt;- raw_dat %&gt;% gather(key, value, -country)\nhead(dat)\n\n# A tibble: 6 × 3\n  country     key                  value\n  &lt;chr&gt;       &lt;chr&gt;                &lt;dbl&gt;\n1 Germany     1960_fertility        2.41\n2 South Korea 1960_fertility        6.16\n3 Germany     1960_life_expectancy 69.3 \n4 South Korea 1960_life_expectancy 53.0 \n5 Germany     1961_fertility        2.44\n6 South Korea 1961_fertility        5.99\n\n\nThe result is not exactly what we refer to as tidy since each observation (year-country combination) is associated with two, not one, rows. We want to have the values from the two variables, fertility and life expectancy, in two separate columns. The first challenge to achieve this is to separate the key column into the year and the variable type. Notice that the entries in this column separate the year from the variable name with an underscore:\n\ndat$key[1:5]\n\n[1] \"1960_fertility\"       \"1960_fertility\"       \"1960_life_expectancy\"\n[4] \"1960_life_expectancy\" \"1961_fertility\"      \n\n\nEncoding multiple variables in a column name is such a common problem that the tidyverse package includes a function to separate these columns into two or more. Apart from the data, the separate function takes three arguments: the name of the column to be separated, the names to be used for the new columns, and the character that separates the variables. So, a first attempt at this is:\n\ndat %&gt;% separate(col = key, into = c(\"year\", \"variable_name\"), sep = \"_\")\n\nThe function does separate the values, but we run into a new problem. We receive the warning Additional pieces discarded in 112 rows [3, 4, 7,...]. (Earlier versions may give the error Too many values at 112 locations:) and that the life_expectancy variable is truncated to life. This is because the _ is used to separate life and expectancy, not just year and variable name! We could add a third column to catch this and let the separate function know which column to fill in with missing values, NA, when there is no third value. Here we tell it to fill the column on the right:\n\ndat %&gt;% separate(key, into = c(\"year\", \"first_variable_name\", \"second_variable_name\"), fill = \"right\")\n\n# A tibble: 224 × 5\n   country     year  first_variable_name second_variable_name value\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt;                &lt;dbl&gt;\n 1 Germany     1960  fertility           &lt;NA&gt;                  2.41\n 2 South Korea 1960  fertility           &lt;NA&gt;                  6.16\n 3 Germany     1960  life                expectancy           69.3 \n 4 South Korea 1960  life                expectancy           53.0 \n 5 Germany     1961  fertility           &lt;NA&gt;                  2.44\n 6 South Korea 1961  fertility           &lt;NA&gt;                  5.99\n 7 Germany     1961  life                expectancy           69.8 \n 8 South Korea 1961  life                expectancy           53.8 \n 9 Germany     1962  fertility           &lt;NA&gt;                  2.47\n10 South Korea 1962  fertility           &lt;NA&gt;                  5.79\n# ℹ 214 more rows\n\n\nHowever, if we read the separate help file, we find that a better approach is to merge the last two variables when there is an extra separation:\n\ndat %&gt;% separate(key, into = c(\"year\", \"variable_name\"), extra = \"merge\")\n\n# A tibble: 224 × 4\n   country     year  variable_name   value\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Germany     1960  fertility        2.41\n 2 South Korea 1960  fertility        6.16\n 3 Germany     1960  life_expectancy 69.3 \n 4 South Korea 1960  life_expectancy 53.0 \n 5 Germany     1961  fertility        2.44\n 6 South Korea 1961  fertility        5.99\n 7 Germany     1961  life_expectancy 69.8 \n 8 South Korea 1961  life_expectancy 53.8 \n 9 Germany     1962  fertility        2.47\n10 South Korea 1962  fertility        5.79\n# ℹ 214 more rows\n\n\nThis achieves the separation we wanted. However, we are not done yet. We need to create a column for each variable. As we learned, the spread function can do this:\n\ndat %&gt;%\n  separate(key, c(\"year\", \"variable_name\"), extra = \"merge\") %&gt;%\n  spread(variable_name, value)\n\n# A tibble: 112 × 4\n   country year  fertility life_expectancy\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1 Germany 1960       2.41            69.3\n 2 Germany 1961       2.44            69.8\n 3 Germany 1962       2.47            70.0\n 4 Germany 1963       2.49            70.1\n 5 Germany 1964       2.49            70.7\n 6 Germany 1965       2.48            70.6\n 7 Germany 1966       2.44            70.8\n 8 Germany 1967       2.37            71.0\n 9 Germany 1968       2.28            70.6\n10 Germany 1969       2.17            70.5\n# ℹ 102 more rows\n\n\nThe data is now in tidy format with one row for each observation with three variables: year, fertility, and life expectancy.",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html#unite",
    "href": "content/Week_05/05a.html#unite",
    "title": "Data Wrangling",
    "section": "unite",
    "text": "unite\nIt is sometimes useful to do the inverse of separate, unite two columns into one. To demonstrate how to use unite, we show code that, although not the optimal approach, serves as an illustration. Suppose that we did not know about extra and used this command to separate:\n\ndat %&gt;%\n  separate(key, c(\"year\", \"first_variable_name\", \"second_variable_name\"), fill = \"right\")\n\n# A tibble: 224 × 5\n   country     year  first_variable_name second_variable_name value\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt;                &lt;dbl&gt;\n 1 Germany     1960  fertility           &lt;NA&gt;                  2.41\n 2 South Korea 1960  fertility           &lt;NA&gt;                  6.16\n 3 Germany     1960  life                expectancy           69.3 \n 4 South Korea 1960  life                expectancy           53.0 \n 5 Germany     1961  fertility           &lt;NA&gt;                  2.44\n 6 South Korea 1961  fertility           &lt;NA&gt;                  5.99\n 7 Germany     1961  life                expectancy           69.8 \n 8 South Korea 1961  life                expectancy           53.8 \n 9 Germany     1962  fertility           &lt;NA&gt;                  2.47\n10 South Korea 1962  fertility           &lt;NA&gt;                  5.79\n# ℹ 214 more rows\n\n\nWe can achieve the same final result by uniting the second and third columns, then spreading the columns and renaming fertility_NA to fertility:\n\ndat %&gt;%\n  separate(key, c(\"year\", \"first_variable_name\", \"second_variable_name\"), fill = \"right\") %&gt;%\n  unite(variable_name, first_variable_name, second_variable_name) %&gt;%\n  spread(variable_name, value) %&gt;%\n  rename(fertility = fertility_NA)\n\n# A tibble: 112 × 4\n   country year  fertility life_expectancy\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1 Germany 1960       2.41            69.3\n 2 Germany 1961       2.44            69.8\n 3 Germany 1962       2.47            70.0\n 4 Germany 1963       2.49            70.1\n 5 Germany 1964       2.49            70.7\n 6 Germany 1965       2.48            70.6\n 7 Germany 1966       2.44            70.8\n 8 Germany 1967       2.37            71.0\n 9 Germany 1968       2.28            70.6\n10 Germany 1969       2.17            70.5\n# ℹ 102 more rows\n\n\n\n\n\n\n\n\nTRY IT\n\n\n\n\nRun the following command to define the co2_wide object using the co2 data built in to R (see ?co2):\n\n\nco2_wide &lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) %&gt;%\n  setNames(1:12) %&gt;%\n  mutate(year = as.character(1959:1997))\n\nUse the gather function to wrangle this into a tidy dataset. Call the column with the CO2 measurements co2 and call the month column month. Call the resulting object co2_tidy.\n\nPlot CO2 versus month with a different curve for each year using this code:\n\n\nco2_tidy %&gt;% ggplot(aes(month, co2, color = year)) + geom_line()\n\nIf the expected plot is not made, it is probably because co2_tidy$month is not numeric:\n\nclass(co2_tidy$month)\n\nRewrite the call to gather using an argument that assures the month column will be numeric. Then make the plot.\n\nWhat do we learn from this plot?\n\n\nCO2 measures increase monotonically from 1959 to 1997.\nCO2 measures are higher in the summer and the yearly average increased from 1959 to 1997.\nCO2 measures appear constant and random variability explains the differences.\nCO2 measures do not have a seasonal trend.\n\n\nNow load the admissions data set, which contains admission information for men and women across six majors and keep only the admitted percentage column:\n\n\nload(admissions)\ndat &lt;- admissions %&gt;% dplyr::select(-applicants)\n\nIf we think of an observation as a major, and that each observation has two variables (men admitted percentage and women admitted percentage) then this is not tidy. Use the spread function to wrangle into tidy shape: one row for each major.\n\nNow we will try a more advanced wrangling challenge. We want to wrangle the admissions data so that for each major we have 4 observations: admitted_men, admitted_women, applicants_men and applicants_women. The trick we perform here is actually quite common: first gather to generate an intermediate data frame and then spread to obtain the tidy data we want. We will go step by step in this and the next two exercises.\n\nUse the gather function to create a tmp data.frame with a column containing the type of observation admitted or applicants. Call the new columns key and value.\n\nNow you have an object tmp with columns major, gender, key and value. Note that if you combine the key and gender, we get the column names we want: admitted_men, admitted_women, applicants_men and applicants_women. Use the function unite to create a new column called column_name.\nNow use the spread function to generate the tidy data with four variables for each major.\nNow use the pipe to write a line of code that turns admissions to the table produced in the previous exercise.",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html#joins",
    "href": "content/Week_05/05a.html#joins",
    "title": "Data Wrangling",
    "section": "Joins",
    "text": "Joins\nThe join functions in the dplyr package (part of the tidyverse) make sure that the tables are combined so that matching rows are together. If you know SQL, you will see that the approach and syntax is very similar. The general idea is that one needs to identify one or more columns that will serve to match the two tables. Then a new table with the combined information is returned. Notice what happens if we join the two tables above by state using left_join (we will remove the others column and rename electoral_votes so that the tables fit on the page):\n\ntab &lt;- left_join(murders, results_us_election_2016, by = \"state\") %&gt;%\n  dplyr::select(-others) %&gt;% \n  rename(ev = electoral_votes)\nhead(tab)\n\n       state abb region population total ev clinton trump\n1    Alabama  AL  South    4779736   135  9    34.4  62.1\n2     Alaska  AK   West     710231    19  3    36.6  51.3\n3    Arizona  AZ   West    6392017   232 11    45.1  48.7\n4   Arkansas  AR  South    2915918    93  6    33.7  60.6\n5 California  CA   West   37253956  1257 55    61.7  31.6\n6   Colorado  CO   West    5029196    65  9    48.2  43.3\n\n\nThe data has been successfully joined and we can now, for example, make a plot to explore the relationship:\n\nlibrary(ggrepel)\ntab %&gt;% ggplot(aes(population/10^6, ev, label = abb)) +\n  geom_point() +\n  geom_text_repel() +\n  scale_x_continuous(trans = \"log2\") +\n  scale_y_continuous(trans = \"log2\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nWe see the relationship is close to linear with about 2 electoral votes for every million persons, but with very small states getting higher ratios.\nIn practice, it is not always the case that each row in one table has a matching row in the other. For this reason, we have several versions of join. To illustrate this challenge, we will take subsets of the tables above. We create the tables tab1 and tab2 so that they have some states in common but not all:\n\ntab_1 &lt;- slice(murders, 1:6) %&gt;% dplyr::select(state, population)\ntab_1\n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4   Arkansas    2915918\n5 California   37253956\n6   Colorado    5029196\n\ntab_2 &lt;- results_us_election_2016 %&gt;%\n  dplyr::filter(state%in%c(\"Alabama\", \"Alaska\", \"Arizona\",\n                    \"California\", \"Connecticut\", \"Delaware\")) %&gt;%\n  dplyr::select(state, electoral_votes) %&gt;% rename(ev = electoral_votes)\ntab_2\n\n        state ev\n1  California 55\n2     Arizona 11\n3     Alabama  9\n4 Connecticut  7\n5      Alaska  3\n6    Delaware  3\n\n\nWe will use these two tables as examples in the next sections.\n\nLeft join\nSuppose we want a table like tab_1, but adding electoral votes to whatever states we have available. For this, we use left_join with tab_1 as the first argument. We specify which column to use to match with the by argument.\n\nleft_join(tab_1, tab_2, by = \"state\")\n\n       state population ev\n1    Alabama    4779736  9\n2     Alaska     710231  3\n3    Arizona    6392017 11\n4   Arkansas    2915918 NA\n5 California   37253956 55\n6   Colorado    5029196 NA\n\n\nNote that NAs are added to the two states not appearing in tab_2. Also, notice that this function, as well as all the other joins, can receive the first arguments through the pipe:\n\ntab_1 %&gt;% left_join(tab_2, by = \"state\")\n\n\n\nRight join\nIf instead of a table with the same rows as first table, we want one with the same rows as second table, we can use right_join:\n\ntab_1 %&gt;% right_join(tab_2, by = \"state\")\n\n        state population ev\n1     Alabama    4779736  9\n2      Alaska     710231  3\n3     Arizona    6392017 11\n4  California   37253956 55\n5 Connecticut         NA  7\n6    Delaware         NA  3\n\n\nNow the NAs are in the column coming from tab_1.\n\n\nInner join\nIf we want to keep only the rows that have information in both tables, we use inner_join. You can think of this as an intersection:\n\ninner_join(tab_1, tab_2, by = \"state\")\n\n       state population ev\n1    Alabama    4779736  9\n2     Alaska     710231  3\n3    Arizona    6392017 11\n4 California   37253956 55\n\n\n\n\nFull join\nIf we want to keep all the rows and fill the missing parts with NAs, we can use full_join. You can think of this as a union:\n\nfull_join(tab_1, tab_2, by = \"state\")\n\n        state population ev\n1     Alabama    4779736  9\n2      Alaska     710231  3\n3     Arizona    6392017 11\n4    Arkansas    2915918 NA\n5  California   37253956 55\n6    Colorado    5029196 NA\n7 Connecticut         NA  7\n8    Delaware         NA  3\n\n\n\n\nSemi join\nThe semi_join function lets us keep the part of first table for which we have information in the second. It does not add the columns of the second. It isn’t often used:\n\nsemi_join(tab_1, tab_2, by = \"state\")\n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4 California   37253956\n\n\nThis gives the same result as:\n\ntab_1 %&gt;% \n  filter(state %in% tab_2$state) \n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4 California   37253956\n\n\n\n\nAnti join\nThe function anti_join is the opposite of semi_join. It keeps the elements of the first table for which there is no information in the second:\n\nanti_join(tab_1, tab_2, by = \"state\")\n\n     state population\n1 Arkansas    2915918\n2 Colorado    5029196\n\n\nThe following diagram summarizes the above joins:\n\n\n\n\n\n\n\n\n\n(Image courtesy of RStudio3. CC-BY-4.0 license4. Cropped from original.)",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html#binding",
    "href": "content/Week_05/05a.html#binding",
    "title": "Data Wrangling",
    "section": "Binding",
    "text": "Binding\nAlthough we have yet to use it in this book, another common way in which datasets are combined is by binding them. Unlike the join function, the binding functions do not try to match by a variable, but instead simply combine datasets. If the datasets don’t match by the appropriate dimensions, one obtains an error.\n\nBinding columns\nThe dplyr function bind_cols binds two objects by making them columns in a tibble. For example, we quickly want to make a data frame consisting of numbers we can use.\n\nbind_cols(a = 1:3, b = 4:6)\n\n# A tibble: 3 × 2\n      a     b\n  &lt;int&gt; &lt;int&gt;\n1     1     4\n2     2     5\n3     3     6\n\n\nThis function requires that we assign names to the columns. Here we chose a and b.\nNote that there is an R-base function cbind with the exact same functionality. An important difference is that cbind can create different types of objects, while bind_cols always produces a data frame.\nbind_cols can also bind two different data frames. For example, here we break up the tab data frame and then bind them back together:\n\ntab_1 &lt;- tab[, 1:3]\ntab_2 &lt;- tab[, 4:6]\ntab_3 &lt;- tab[, 7:8]\nnew_tab &lt;- bind_cols(tab_1, tab_2, tab_3)\nhead(new_tab)\n\n       state abb region population total ev clinton trump\n1    Alabama  AL  South    4779736   135  9    34.4  62.1\n2     Alaska  AK   West     710231    19  3    36.6  51.3\n3    Arizona  AZ   West    6392017   232 11    45.1  48.7\n4   Arkansas  AR  South    2915918    93  6    33.7  60.6\n5 California  CA   West   37253956  1257 55    61.7  31.6\n6   Colorado  CO   West    5029196    65  9    48.2  43.3\n\n\n\n\nBinding by rows\nThe bind_rows function is similar to bind_cols, but binds rows instead of columns:\n\ntab_1 &lt;- tab[1:2,]\ntab_2 &lt;- tab[3:4,]\nbind_rows(tab_1, tab_2)\n\n     state abb region population total ev clinton trump\n1  Alabama  AL  South    4779736   135  9    34.4  62.1\n2   Alaska  AK   West     710231    19  3    36.6  51.3\n3  Arizona  AZ   West    6392017   232 11    45.1  48.7\n4 Arkansas  AR  South    2915918    93  6    33.7  60.6\n\n\nThis is based on an R-base function rbind.",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html#set-operators",
    "href": "content/Week_05/05a.html#set-operators",
    "title": "Data Wrangling",
    "section": "Set operators",
    "text": "Set operators\nAnother set of commands useful for combining datasets are the set operators. When applied to vectors, these behave as their names suggest. Examples are intersect, union, setdiff, and setequal. However, if the tidyverse, or more specifically dplyr, is loaded, these functions can be used on data frames as opposed to just on vectors.\n\nIntersect\nYou can take intersections of vectors of any type, such as numeric:\n\nintersect(1:10, 6:15)\n\n[1]  6  7  8  9 10\n\n\nor characters:\n\nintersect(c(\"a\",\"b\",\"c\"), c(\"b\",\"c\",\"d\"))\n\n[1] \"b\" \"c\"\n\n\nThe dplyr package includes an intersect function that can be applied to tables with the same column names. This function returns the rows in common between two tables. To make sure we use the dplyr version of intersect rather than the base package version, we can use dplyr::intersect like this:\n\ntab_1 &lt;- tab[1:5,]\ntab_2 &lt;- tab[3:7,]\ndplyr::intersect(tab_1, tab_2)\n\n       state abb region population total ev clinton trump\n1    Arizona  AZ   West    6392017   232 11    45.1  48.7\n2   Arkansas  AR  South    2915918    93  6    33.7  60.6\n3 California  CA   West   37253956  1257 55    61.7  31.6\n\n\n\n\nUnion\nSimilarly union takes the union of vectors. For example:\n\nunion(1:10, 6:15)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\nunion(c(\"a\",\"b\",\"c\"), c(\"b\",\"c\",\"d\"))\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\n\nThe dplyr package includes a version of union that combines all the rows of two tables with the same column names.\n\ntab_1 &lt;- tab[1:5,]\ntab_2 &lt;- tab[3:7,]\ndplyr::union(tab_1, tab_2)\n\n        state abb    region population total ev clinton trump\n1     Alabama  AL     South    4779736   135  9    34.4  62.1\n2      Alaska  AK      West     710231    19  3    36.6  51.3\n3     Arizona  AZ      West    6392017   232 11    45.1  48.7\n4    Arkansas  AR     South    2915918    93  6    33.7  60.6\n5  California  CA      West   37253956  1257 55    61.7  31.6\n6    Colorado  CO      West    5029196    65  9    48.2  43.3\n7 Connecticut  CT Northeast    3574097    97  7    54.6  40.9\n\n\nNote that we get 7 unique rows from this. We do not get duplicated rows from the overlap in 1:5 and 3:7. If we were to bind_rows on the two subsets, we would get duplicates.\n\n\nsetdiff\nThe set difference between a first and second argument can be obtained with setdiff. Unlike intersect and union, this function is not symmetric:\n\nsetdiff(1:10, 6:15)\n\n[1] 1 2 3 4 5\n\nsetdiff(6:15, 1:10)\n\n[1] 11 12 13 14 15\n\n\nAs with the functions shown above, dplyr has a version for data frames:\n\ntab_1 &lt;- tab[1:5,]\ntab_2 &lt;- tab[3:7,]\ndplyr::setdiff(tab_1, tab_2)\n\n    state abb region population total ev clinton trump\n1 Alabama  AL  South    4779736   135  9    34.4  62.1\n2  Alaska  AK   West     710231    19  3    36.6  51.3\n\n\n\n\nsetequal\nFinally, the function setequal tells us if two sets are the same, regardless of order. So notice that:\n\nsetequal(1:5, 1:6)\n\n[1] FALSE\n\n\nbut:\n\nsetequal(1:5, 5:1)\n\n[1] TRUE\n\n\nWhen applied to data frames that are not equal, regardless of order, the dplyr version provides a useful message letting us know how the sets are different:\n\ndplyr::setequal(tab_1, tab_2)\n\n[1] FALSE\n\n\n\nTRY IT\n\nInstall and load the Lahman library. This database includes data related to baseball teams. It includes summary statistics about how the players performed on offense and defense for several years. It also includes personal information about the players.\n\nThe Batting data frame contains the offensive statistics for all players for many years. You can see, for example, the top 10 hitters by running this code:\n\nlibrary(Lahman)\n\ntop &lt;- Batting %&gt;%\n  dplyr::filter(yearID == 2016) %&gt;%\n  arrange(desc(HR)) %&gt;%\n  slice(1:10)\n\ntop %&gt;% as_tibble()\n\nBut who are these players? We see an ID, but not the names. The player names are in this table\n\nMaster %&gt;% as_tibble()\n\nWe can see column names nameFirst and nameLast. Use the left_join function to create a table of the top home run hitters. The table should have playerID, first name, last name, and number of home runs (HR). Rewrite the object top with this new table.\n\nNow use the Salaries data frame to add each player’s salary to the table you created in exercise 1. Note that salaries are different every year so make sure to filter for the year 2016, then use right_join. This time show first name, last name, team, HR, and salary.\nIn a previous exercise, we created a tidy version of the co2 dataset:\n\n\nco2_wide &lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) %&gt;%\n  setNames(1:12) %&gt;%\n  mutate(year = 1959:1997) %&gt;%\n  gather(month, co2, -year, convert = TRUE)\n\nWe want to see if the monthly trend is changing so we are going to remove the year effects and then plot the results. We will first compute the year averages. Use the group_by and summarize to compute the average co2 for each year. Save in an object called yearly_avg.\n\nNow use the left_join function to add the yearly average to the co2_wide dataset. Then compute the residuals: observed co2 measure - yearly average.\nMake a plot of the seasonal trends by year but only after removing the year effect.",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html#a-note-about-merging-and-duplicated-rows",
    "href": "content/Week_05/05a.html#a-note-about-merging-and-duplicated-rows",
    "title": "Data Wrangling",
    "section": "A note about merging and duplicated rows",
    "text": "A note about merging and duplicated rows\nWhen we merge data, we have to be very careful about duplicated rows. Specifically, we have to be certain that the fields we use to join on are unique or that we know they aren’t unique and intend to duplicate rows.\nWhen we have tidy data, we have one row per observation. When we join data that has more than one row per observation, the new dataset will no longer be tidy. Here, the notTidyData is not tidy *in relation to the tidyData as it does not have one observation per year and state. For instance:\n\ntidyData &lt;- bind_cols( state = c('MI','CA','MI','CA'), \n                       year = c(2001, 2002, 2001, 2002),  \n                       Arrests = c(10, 21, 30, 12))\n\nnotTidyData &lt;- bind_cols(state = c('MI','MI','MI','CA','CA','CA'), \n                         County = c('Ingham','Clinton','Wayne','Orange','Los Angeles','Kern'),\n                         InNOut_locations = c(0,0,0,20, 31, 8))\n\nhead(tidyData)\n\n# A tibble: 4 × 3\n  state  year Arrests\n  &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 MI     2001      10\n2 CA     2002      21\n3 MI     2001      30\n4 CA     2002      12\n\nhead(notTidyData)\n\n# A tibble: 6 × 3\n  state County      InNOut_locations\n  &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;\n1 MI    Ingham                     0\n2 MI    Clinton                    0\n3 MI    Wayne                      0\n4 CA    Orange                    20\n5 CA    Los Angeles               31\n6 CA    Kern                       8\n\n\nIf we use the only common field, state to merge tidyData, which is unique on state and year, to notTidyData, which is unique on county, then every time we see state in our “left” data, we will get all three counties in that state and for that year. Even worse, we will get the InNOut_locations tally repeated for every matching state!\n\njoinedData &lt;- left_join(tidyData, notTidyData, by = c('state'))\njoinedData\n\n# A tibble: 12 × 5\n   state  year Arrests County      InNOut_locations\n   &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n 1 MI     2001      10 Ingham                     0\n 2 MI     2001      10 Clinton                    0\n 3 MI     2001      10 Wayne                      0\n 4 CA     2002      21 Orange                    20\n 5 CA     2002      21 Los Angeles               31\n 6 CA     2002      21 Kern                       8\n 7 MI     2001      30 Ingham                     0\n 8 MI     2001      30 Clinton                    0\n 9 MI     2001      30 Wayne                      0\n10 CA     2002      12 Orange                    20\n11 CA     2002      12 Los Angeles               31\n12 CA     2002      12 Kern                       8\n\n\nBecause we asked for all columns of tidyData that matched (on state) in notTidyData, we get replicated Arrests - look at MI in 2001 in the first three rows. The original data had 10 Arrests in Michigan in 2001. Now, for every MI County in notTidyData, we have replicated the statewide arrests!\n\nsum(joinedData$Arrests)\n\n[1] 219\n\nsum(tidyData$Arrests)\n\n[1] 73\n\n\nYikes! We now have 3x the number of arrests in our data.\n\nsum(joinedData$InNOut_locations)\n\n[1] 118\n\nsum(notTidyData$InNOut_locations)\n\n[1] 59\n\n\nAnd while we might like that we have 3x the In-N-Out locations, we definitely think our data shouldn’t suddenly have more.\nThe reason this happens is that we do not have a unique key variable. In the WDI tip in Project 2, we didn’t have a single unique key variable - there were multiple values of iso2 country code in the data because we had multiple years for each iso2. Thus, when we merged, use used by = c('iso2','year') because the iso2 x year combination was the unique key.\nHere, each dataset would need to have a unique key (or “primary key”) on state and county and year to merge without risk of messing up our data. Since arrests aren’t broken out by county, we’d have to use summarize to total notTidyData at the state and year (to eliminate county). Then, we’d have to decide of the number of In-N-Outs are the same for each year since notTidyData doesn’t have year.\nThe lesson is this: always know what your join is doing. Know your unique keys. Use sum(duplicated(tidyData$key)) to see if all values are unique, or NROW(unique(tidyData %&gt;% dplyr::select(key1, key2))) to see if all rows are unique over the 2 keys (replacing “key1” and “key2” with your key fields).\nIf it shouldn’t add rows, then make sure the new data has the same number of rows as the old one, or use setequal to check.",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html#the-date-data-type",
    "href": "content/Week_05/05a.html#the-date-data-type",
    "title": "Data Wrangling",
    "section": "The date data type",
    "text": "The date data type\nWe have described three main types of vectors: numeric, character, and logical. In data science projects, we very often encounter variables that are dates. Although we can represent a date with a string, for example November 2, 2017, once we pick a reference day, referred to as the epoch, they can be converted to numbers by calculating the number of days since the epoch. Computer languages usually use January 1, 1970, as the epoch. So, for example, January 2, 2017 is day 1, December 31, 1969 is day -1, and November 2, 2017, is day 17,204.\nNow how should we represent dates and times when analyzing data in R? We could just use days since the epoch, but then it is almost impossible to interpret. If I tell you it’s November 2, 2017, you know what this means immediately. If I tell you it’s day 17,204, you will be quite confused. Similar problems arise with times and even more complications can appear due to time zones.\nFor this reason, R defines a data type just for dates and times. We saw an example in the polls data:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(\"polls_us_election_2016\")\npolls_us_election_2016$startdate %&gt;% head\n\n[1] \"2016-11-03\" \"2016-11-01\" \"2016-11-02\" \"2016-11-04\" \"2016-11-03\"\n[6] \"2016-11-03\"\n\n\nThese look like strings, but they are not:\n\nclass(polls_us_election_2016$startdate)\n\n[1] \"Date\"\n\n\nLook at what happens when we convert them to numbers:\n\nas.numeric(polls_us_election_2016$startdate) %&gt;% head\n\n[1] 17108 17106 17107 17109 17108 17108\n\n\nIt turns them into days since the epoch. The as.Date function can convert a character into a date. So to see that the epoch is day 0 we can type\n\nas.Date(\"1970-01-01\") %&gt;% as.numeric\n\n[1] 0\n\n\nPlotting functions, such as those in ggplot, are aware of the date format. This means that, for example, a scatterplot can use the numeric representation to decide on the position of the point, but include the string in the labels:\n\npolls_us_election_2016 %&gt;% dplyr::filter(pollster == \"Ipsos\" & state ==\"U.S.\") %&gt;%\n  ggplot(aes(startdate, rawpoll_trump)) +\n  geom_line()\n\n\n\n\n\n\n\n\nNote in particular that the month names are displayed, a very convenient feature.",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html#lubridate",
    "href": "content/Week_05/05a.html#lubridate",
    "title": "Data Wrangling",
    "section": "The lubridate package",
    "text": "The lubridate package\nThe tidyverse includes functionality for dealing with dates through the lubridate package.\n\nlibrary(lubridate)\n\nWe will take a random sample of dates to show some of the useful things one can do:\n\nset.seed(2002)\ndates &lt;- sample(polls_us_election_2016$startdate, 10) %&gt;% sort\ndates\n\n [1] \"2016-05-31\" \"2016-08-08\" \"2016-08-19\" \"2016-09-22\" \"2016-09-27\"\n [6] \"2016-10-12\" \"2016-10-24\" \"2016-10-26\" \"2016-10-29\" \"2016-10-30\"\n\n\nThe functions year, month and day extract those values:\n\ntibble(date = dates,\n       month = month(dates),\n       day = day(dates),\n       year = year(dates))\n\n# A tibble: 10 × 4\n   date       month   day  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1 2016-05-31     5    31  2016\n 2 2016-08-08     8     8  2016\n 3 2016-08-19     8    19  2016\n 4 2016-09-22     9    22  2016\n 5 2016-09-27     9    27  2016\n 6 2016-10-12    10    12  2016\n 7 2016-10-24    10    24  2016\n 8 2016-10-26    10    26  2016\n 9 2016-10-29    10    29  2016\n10 2016-10-30    10    30  2016\n\n\nWe can also extract the month labels:\n\nmonth(dates, label = TRUE)\n\n [1] May Aug Aug Sep Sep Oct Oct Oct Oct Oct\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\nAnother useful set of functions are the parsers that convert strings into dates. The function ymd assumes the dates are in the format YYYY-MM-DD and tries to parse as well as possible.\n\nx &lt;- c(20090101, \"2009-01-02\", \"2009 01 03\", \"2009-1-4\",\n       \"2009-1, 5\", \"Created on 2009 1 6\", \"200901 !!! 07\")\nymd(x)\n\n[1] \"2009-01-01\" \"2009-01-02\" \"2009-01-03\" \"2009-01-04\" \"2009-01-05\"\n[6] \"2009-01-06\" \"2009-01-07\"\n\n\nA further complication comes from the fact that dates often come in different formats in which the order of year, month, and day are different. The preferred format is to show year (with all four digits), month (two digits), and then day, or what is called the ISO 8601. Specifically we use YYYY-MM-DD so that if we order the string, it will be ordered by date. You can see the function ymd returns them in this format.\nBut, what if you encounter dates such as “09/01/02”? This could be September 1, 2002 or January 2, 2009 or January 9, 2002. In these cases, examining the entire vector of dates will help you determine what format it is by process of elimination. Once you know, you can use the many parses provided by lubridate.\nFor example, if the string is:\n\nx &lt;- \"09/01/02\"\n\nThe ymd function assumes the first entry is the year, the second is the month, and the third is the day, so it converts it to:\n\nymd(x)\n\n[1] \"2009-01-02\"\n\n\nThe mdy function assumes the first entry is the month, then the day, then the year:\n\nmdy(x)\n\n[1] \"2002-09-01\"\n\n\nThe lubridate package provides a function for every possibility:\n\nydm(x)\n\n[1] \"2009-02-01\"\n\nmyd(x)\n\n[1] \"2001-09-02\"\n\ndmy(x)\n\n[1] \"2002-01-09\"\n\ndym(x)\n\n[1] \"2001-02-09\"\n\n\nThe lubridate package is also useful for dealing with times. In R base, you can get the current time typing Sys.time(). The lubridate package provides a slightly more advanced function, now, that permits you to define the time zone:\n\nnow()\n\n[1] \"2025-01-14 09:59:09 EST\"\n\nnow(\"GMT\")\n\n[1] \"2025-01-14 14:59:09 GMT\"\n\n\nYou can see all the available time zones with OlsonNames() function.\nWe can also extract hours, minutes, and seconds:\n\nnow() %&gt;% hour()\n\n[1] 9\n\nnow() %&gt;% minute()\n\n[1] 59\n\nnow() %&gt;% second()\n\n[1] 9.763199\n\n\nThe package also includes a function to parse strings into times as well as parsers for time objects that include dates:\n\nx &lt;- c(\"12:34:56\")\nhms(x)\n\n[1] \"12H 34M 56S\"\n\nx &lt;- \"Nov/2/2012 12:34:56\"\nmdy_hms(x)\n\n[1] \"2012-11-02 12:34:56 UTC\"\n\n\nThis package has many other useful functions. We describe two of these here that we find particularly useful.\nThe make_date function can be used to quickly create a date object. It takes three arguments: year, month, day. make_datetime takes the same as make_date but also adds hour, minute, seconds, and time zone like ‘US/Michigan’ but defaulting to UTC. So create an date object representing, for example, July 6, 2019 we write:\n\nmake_date(2019, 7, 6)\n\n[1] \"2019-07-06\"\n\n\nTo make a vector of January 1 for the 80s we write:\n\nmake_date(1980:1989)\n\n [1] \"1980-01-01\" \"1981-01-01\" \"1982-01-01\" \"1983-01-01\" \"1984-01-01\"\n [6] \"1985-01-01\" \"1986-01-01\" \"1987-01-01\" \"1988-01-01\" \"1989-01-01\"\n\n\nAnother very useful function is the round_date. It can be used to round dates to nearest year, quarter, month, week, day, hour, minutes, or seconds. So if we want to group all the polls by week of the year we can do the following:\n\npolls_us_election_2016 %&gt;%\n  mutate(week = round_date(startdate, \"week\")) %&gt;%\n  group_by(week) %&gt;%\n  summarize(margin = mean(rawpoll_clinton - rawpoll_trump)) %&gt;%\n  qplot(week, margin, data = .)\n\n\n\n\n\n\n\n\nDate objects can be added to and subtracted from with hours, minutes, etc.\n\nstartDate &lt;- ymd_hms('2021-06-14 12:20:57')\n\nstartDate + seconds(4)\n\n[1] \"2021-06-14 12:21:01 UTC\"\n\nstartDate + hours(1) + days(2) - seconds(10)\n\n[1] \"2021-06-16 13:20:47 UTC\"\n\n\nYou can even calculate time differences in specific units:\n\nendDate = ymd_hms('2021-06-15 01:00:00')\n\nendDate - startDate\n\nTime difference of 12.65083 hours\n\ndifftime(endDate, startDate, units = 'days')\n\nTime difference of 0.5271181 days\n\n\nNote that both of these result in a difftime object. You can use as.numeric(difftime(endDate, startDate)) to get the numeric difference in times.\nSequences can be created as well:\n\nseq(from = startDate, to = endDate, by = 'hour')\n\n [1] \"2021-06-14 12:20:57 UTC\" \"2021-06-14 13:20:57 UTC\"\n [3] \"2021-06-14 14:20:57 UTC\" \"2021-06-14 15:20:57 UTC\"\n [5] \"2021-06-14 16:20:57 UTC\" \"2021-06-14 17:20:57 UTC\"\n [7] \"2021-06-14 18:20:57 UTC\" \"2021-06-14 19:20:57 UTC\"\n [9] \"2021-06-14 20:20:57 UTC\" \"2021-06-14 21:20:57 UTC\"\n[11] \"2021-06-14 22:20:57 UTC\" \"2021-06-14 23:20:57 UTC\"\n[13] \"2021-06-15 00:20:57 UTC\"",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_05/05a.html#footnotes",
    "href": "content/Week_05/05a.html#footnotes",
    "title": "Data Wrangling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://github.com/rstudio/cheatsheets↩︎\nhttps://github.com/rstudio/cheatsheets/blob/master/LICENSE↩︎\nhttps://github.com/rstudio/cheatsheets↩︎\nhttps://github.com/rstudio/cheatsheets/blob/master/LICENSE↩︎",
    "crumbs": [
      "Course Content",
      "Week 05",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "content/Week_06/06a.html",
    "href": "content/Week_06/06a.html",
    "title": "Foundations of Linear Regression",
    "section": "",
    "text": "This page.\n\n\n\n\n\n\nThe correlation between two variables tells us how they move together\nSample correlation is a random variable",
    "crumbs": [
      "Course Content",
      "Week 06",
      "Foundations of Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_06/06a.html#required-reading",
    "href": "content/Week_06/06a.html#required-reading",
    "title": "Foundations of Linear Regression",
    "section": "",
    "text": "This page.\n\n\n\n\n\n\nThe correlation between two variables tells us how they move together\nSample correlation is a random variable",
    "crumbs": [
      "Course Content",
      "Week 06",
      "Foundations of Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_06/06a.html#case-study-is-height-hereditary",
    "href": "content/Week_06/06a.html#case-study-is-height-hereditary",
    "title": "Foundations of Linear Regression",
    "section": "Case study: is height hereditary?",
    "text": "Case study: is height hereditary?\nWe have access to Galton’s family height data through the HistData package. This data contains heights on several dozen families: mothers, fathers, daughters, and sons. To imitate Galton’s analysis, we will create a dataset with the heights of fathers and a randomly selected son of each family:\n\nlibrary(tidyverse)\nlibrary(HistData)\ndata(\"GaltonFamilies\")\n\nset.seed(1983)\ngalton_heights &lt;- GaltonFamilies %&gt;%\n  filter(gender == \"male\") %&gt;%\n  group_by(family) %&gt;%\n  sample_n(1) %&gt;%\n  ungroup() %&gt;%\n  select(father, childHeight) %&gt;%\n  rename(son = childHeight)\n\nIn the exercises, we will look at other relationships including mothers and daughters.\nSuppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:\n\ngalton_heights %&gt;%\n  summarize(mean(father), sd(father), mean(son), sd(son))\n\n# A tibble: 1 × 4\n  `mean(father)` `sd(father)` `mean(son)` `sd(son)`\n           &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1           69.1         2.55        69.2      2.71\n\n\nHowever, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son.\n\ngalton_heights %&gt;% ggplot(aes(father, son)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\nWe will learn that the correlation coefficient is an informative summary of how two variables move together and then see how this can be used to predict one variable using the other.",
    "crumbs": [
      "Course Content",
      "Week 06",
      "Foundations of Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_06/06a.html#corr-coef",
    "href": "content/Week_06/06a.html#corr-coef",
    "title": "Foundations of Linear Regression",
    "section": "The correlation coefficient",
    "text": "The correlation coefficient\nThe correlation coefficient is defined for a list of pairs \\((x_1, y_1), \\dots, (x_n,y_n)\\) as the average of the product of the standardized values:\n\\[\n\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\n\\] with \\(\\mu_x, \\mu_y\\) the averages of \\(x_1,\\dots, x_n\\) and \\(y_1, \\dots, y_n\\), respectively, and \\(\\sigma_x, \\sigma_y\\) the standard deviations. The Greek letter \\(\\rho\\) is commonly used in statistics books to denote the correlation. The Greek letter for \\(r\\), \\(\\rho\\), because it is the first letter of regression. Soon we learn about the connection between correlation and regression. We can represent the formula above with R code using:\n\nrho &lt;- mean(scale(x) * scale(y))\n\nTo understand why this equation does in fact summarize how two variables move together, consider the \\(i\\)-th entry of \\(x\\) is \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\) SDs away from the average. Similarly, the \\(y_i\\) that is paired with \\(x_i\\), is \\(\\left( \\frac{y_1-\\mu_y}{\\sigma_y} \\right)\\) SDs away from the average \\(y\\). If \\(x\\) and \\(y\\) are unrelated, the product \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\\) will be positive ( \\(+ \\times +\\) and \\(- \\times -\\) ) as often as negative (\\(+ \\times -\\) and \\(- \\times +\\)) and will average out to about 0. This correlation is the average and therefore unrelated variables will have 0 correlation. If instead the quantities vary together, then we are averaging mostly positive products ( \\(+ \\times +\\) and \\(- \\times -\\)) and we get a positive correlation. If they vary in opposite directions, we get a negative correlation.\nThe correlation coefficient is always between -1 and 1. We can show this mathematically: consider that we can’t have higher correlation than when we compare a list to itself (perfect correlation) and in this case the correlation is:\n\\[\n\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)^2 =\n\\frac{1}{\\sigma_x^2} \\frac{1}{n} \\sum_{i=1}^n \\left( x_i-\\mu_x \\right)^2 =\n\\frac{1}{\\sigma_x^2} \\sigma^2_x =\n1\n\\]\nA similar derivation, but with \\(x\\) and its exact opposite, proves the correlation has to be bigger or equal to -1.\nFor other pairs, the correlation is in between -1 and 1. The correlation between father and son’s heights is about 0.5:\n\ngalton_heights %&gt;% summarize(r = cor(father, son)) %&gt;% pull(r)\n\n[1] 0.4334102\n\n\nTo see what data looks like for different values of \\(\\rho\\), here are six examples of pairs with correlations ranging from -0.9 to 0.99:\n\n\n\n\n\n\n\n\n\n\nSample correlation is a random variable\nBefore we continue connecting correlation to regression, let’s remind ourselves about random variability.\nIn most data science applications, we observe data that includes random variation. For example, in many cases, we do not observe data for the entire population of interest but rather for a random sample. As with the average and standard deviation, the sample correlation is the most commonly used estimate of the population correlation. This implies that the correlation we compute and use as a summary is a random variable.\nBy way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population. A less fortunate geneticist can only afford measurements from a random sample of 25 pairs. The sample correlation can be computed with:\n\nR &lt;- sample_n(galton_heights, 25, replace = TRUE) %&gt;%\n  summarize(r = cor(father, son)) %&gt;% pull(r)\n\nR is a random variable. We can run a Monte Carlo simulation to see its distribution:\n\nB &lt;- 1000\nN &lt;- 25\nR &lt;- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) %&gt;%\n    summarize(r=cor(father, son)) %&gt;%\n    pull(r)\n})\nqplot(R, geom = \"histogram\", binwidth = 0.05, color = I(\"black\"))\n\n\n\n\n\n\n\n\nWe see that the expected value of R is the “population” correlation:\n\nmean(R)\n\n[1] 0.4307393\n\n\nand that it has a relatively high standard error relative to the range of values R can take:\n\nsd(R)\n\n[1] 0.1609393\n\n\nSo, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.\nAlso, note that because the sample correlation is an average of independent draws, the central limit actually applies. Therefore, for large enough \\(N\\), the distribution of R is approximately normal with expected value \\(\\rho\\). The standard deviation, which is somewhat complex to derive, is \\(\\sqrt{\\frac{1-r^2}{N-2}}\\).\nIn our example, \\(N=25\\) does not seem to be large enough to make the approximation a good one:\n\nggplot(aes(sample=R), data = data.frame(R)) +\n  stat_qq() +\n  geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQQ plots\n\n\n\n\n\nThe above plot is called a qq plot. It compares two things: the quantiles of the numeric variable, and the quantiles that one would get from a normal distribution with the same mean and standard deviation as the numeric variable. Points on the 45 degree line indicate ranges of the data where the empirical quantile is equal to the theoretical (normal) quantile. Points off the line indicate ranges where the numeric variable is not normal. Small samples will be very noisy, but you should be able to see systematic deviations.\n\n\n\nIf you increase \\(N\\), you will see the distribution converging to normal.\n\n\nCorrelation is not always a useful summary\nCorrelation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82:\n\n\n\n\n\n\n\n\n\nCorrelation is only meaningful in a particular context. To help us understand when it is that correlation is meaningful as a summary statistic, we will return to the example of predicting a son’s height using his father’s height. This will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction.",
    "crumbs": [
      "Course Content",
      "Week 06",
      "Foundations of Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_06/06a.html#conditional-expectation",
    "href": "content/Week_06/06a.html#conditional-expectation",
    "title": "Foundations of Linear Regression",
    "section": "Conditional expectations",
    "text": "Conditional expectations\nSuppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height. Because the distribution of sons’ heights is approximately normal, we know the average height, 69.2, is the value with the highest proportion and would be the prediction with the highest chance of minimizing the error. But what if we are told that the father is taller than average, say 72 inches tall, do we still guess 69.2 for the son?\nIt turns out that if we were able to collect data from a very large number of fathers that are 72 inches, the distribution of their sons’ heights would be normally distributed. This implies that the average of the distribution computed on this subset would be our best prediction.\nIn general, we call this approach conditioning. The general idea is that we stratify a population into groups and compute summaries in each group. To provide a mathematical description of conditioning, consider we have a population of pairs of values \\((x_1,y_1),\\dots,(x_n,y_n)\\), for example all father and son heights in England. In the previous week’s content, we learned that if you take a random pair \\((X,Y)\\), the expected value and best predictor of \\(Y\\) is \\(\\mbox{E}(Y) = \\mu_y\\), the population average \\(1/n\\sum_{i=1}^n y_i\\). However, we are no longer interested in the general population, instead we are interested in only the subset of a population with a specific \\(x_i\\) value, 72 inches in our example. This subset of the population, is also a population and thus the same principles and properties we have learned apply. The \\(y_i\\) in the subpopulation have a distribution, referred to as the conditional distribution, and this distribution has an expected value referred to as the conditional expectation. In our example, the conditional expectation is the average height of all sons in England with fathers that are 72 inches. The statistical notation for the conditional expectation is\n\\[\n\\mbox{E}(Y \\mid X = x)\n\\]\nwith \\(x\\) representing the fixed value that defines that subset, for example 72 inches. Similarly, we denote the standard deviation of the strata with\n\\[\n\\mbox{SD}(Y \\mid X = x) = \\sqrt{\\mbox{Var}(Y \\mid X = x)}\n\\]\nBecause the conditional expectation \\(E(Y\\mid X=x)\\) is the best predictor for the random variable \\(Y\\) for an individual in the strata defined by \\(X=x\\), many data science challenges reduce to estimating this quantity. The conditional standard deviation quantifies the precision of the prediction.\nIn the example we have been considering, we are interested in computing the average son height conditioned on the father being 72 inches tall. We want to estimate \\(E(Y|X=72)\\) using the sample collected by Galton. We previously learned that the sample average is the preferred approach to estimating the population average. However, a challenge when using this approach to estimating conditional expectations is that for continuous data we don’t have many data points matching exactly one value in our sample. For example, we have only:\n\nsum(galton_heights$father == 72)\n\n[1] 8\n\n\nfathers that are exactly 72-inches. If we change the number to 72.5, we get even fewer data points:\n\nsum(galton_heights$father == 72.5)\n\n[1] 1\n\n\nA practical way to improve these estimates of the conditional expectations, is to define strata of with similar values of \\(x\\). In our example, we can round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall:\n\nconditional_avg &lt;- galton_heights %&gt;%\n  filter(round(father) == 72) %&gt;%\n  summarize(avg = mean(son)) %&gt;%\n  pull(avg)\nconditional_avg\n\n[1] 70.5\n\n\nNote that a 72-inch father is taller than average – specifically, 72 - 69.1/2.5 = 1.1 standard deviations taller than the average father. Our prediction 70.5 is also taller than average, but only 0.49 standard deviations larger than the average son. The sons of 72-inch fathers have regressed some to the average height. We notice that the reduction in how many SDs taller is about 0.5, which happens to be the correlation. As we will see in a later lecture, this is not a coincidence.\nIf we want to make a prediction of any height, not just 72, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group:\n\ngalton_heights %&gt;% mutate(father_strata = factor(round(father))) %&gt;%\n  ggplot(aes(father_strata, son)) +\n  geom_boxplot() +\n  geom_point()\n\n\n\n\n\n\n\n\nNot surprisingly, the centers of the groups are increasing with height. Furthermore, these centers appear to follow a linear relationship. Below we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:\n\n\n\n\n\n\n\n\n\nThe fact that these conditional averages follow a line is not a coincidence. In the next section, we explain that the line these averages follow is what we call the regression line, which improves the precision of our estimates. However, it is not always appropriate to estimate conditional expectations with the regression line so we also describe Galton’s theoretical justification for using the regression line.",
    "crumbs": [
      "Course Content",
      "Week 06",
      "Foundations of Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_06/06a.html#the-regression-line",
    "href": "content/Week_06/06a.html#the-regression-line",
    "title": "Foundations of Linear Regression",
    "section": "The regression line",
    "text": "The regression line\nIf we are predicting a random variable \\(Y\\) knowing the value of another \\(X=x\\) using a regression line, then we predict that for every standard deviation, \\(\\sigma_X\\), that \\(x\\) increases above the average \\(\\mu_X\\), \\(Y\\) increase \\(\\rho\\) standard deviations \\(\\sigma_Y\\) above the average \\(\\mu_Y\\) with \\(\\rho\\) the correlation between \\(X\\) and \\(Y\\). The formula for the regression is therefore:\n\\[\n\\left( \\frac{Y-\\mu_Y}{\\sigma_Y} \\right) = \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)\n\\]\nWe can rewrite it like this:\n\\[\nY = \\mu_Y + \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right) \\sigma_Y\n\\]\nIf there is perfect correlation, the regression line predicts an increase that is the same number of SDs. If there is 0 correlation, then we don’t use \\(x\\) at all for the prediction and simply predict the average \\(\\mu_Y\\). For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.\nNote that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, \\(x\\), is to the average of the \\(x\\)s. This is why we call it regression: the son regresses to the average height. In fact, the title of Galton’s paper was: Regression toward mediocrity in hereditary stature. To add regression lines to plots, we will need the above formula in the form:\n\\[\ny= b + mx \\mbox{ with slope } m = \\rho \\frac{\\sigma_y}{\\sigma_x} \\mbox{ and intercept } b=\\mu_y - m \\mu_x\n\\]\nHere we add the regression line to the original data:\n\nmu_x &lt;- mean(galton_heights$father)\nmu_y &lt;- mean(galton_heights$son)\ns_x &lt;- sd(galton_heights$father)\ns_y &lt;- sd(galton_heights$son)\nr &lt;- cor(galton_heights$father, galton_heights$son)\n\ngalton_heights %&gt;%\n  ggplot(aes(father, son)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x)\n\n\n\n\n\n\n\n\nThe regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\). You can make same plot, but using standard units like this:\n\ngalton_heights %&gt;%\n  ggplot(aes(scale(father), scale(son))) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = r)\n\n\nRegression improves precision\nLet’s compare the two approaches to prediction that we have presented:\n\nRound fathers’ heights to closest inch, stratify, and then take the average.\nCompute the regression line and use it to predict.\n\nWe use a Monte Carlo simulation sampling \\(N=50\\) families:\n\nB &lt;- 1000\nN &lt;- 50\n\nset.seed(1983)\nconditional_avg &lt;- replicate(B, {\n  dat &lt;- sample_n(galton_heights, N)\n  dat %&gt;% filter(round(father) == 72) %&gt;%\n    summarize(avg = mean(son)) %&gt;%\n    pull(avg)\n  })\n\nregression_prediction &lt;- replicate(B, {\n  dat &lt;- sample_n(galton_heights, N)\n  mu_x &lt;- mean(dat$father)\n  mu_y &lt;- mean(dat$son)\n  s_x &lt;- sd(dat$father)\n  s_y &lt;- sd(dat$son)\n  r &lt;- cor(dat$father, dat$son)\n  mu_y + r*(72 - mu_x)/s_x*s_y\n})\n\nAlthough the expected value of these two random variables is about the same:\n\nmean(conditional_avg, na.rm = TRUE)\n\n[1] 70.49368\n\nmean(regression_prediction)\n\n[1] 70.50941\n\n\nThe standard error for the regression prediction is substantially smaller:\n\nsd(conditional_avg, na.rm = TRUE)\n\n[1] 0.9635814\n\nsd(regression_prediction)\n\n[1] 0.4520833\n\n\nThe regression line is therefore much more stable than the conditional mean. There is an intuitive reason for this. The conditional average is computed on a relatively small subset: the fathers that are about 72 inches tall. In fact, in some of the permutations we have no data, which is why we use na.rm=TRUE. The regression always uses all the data.\nSo why not always use the regression for prediction? Because it is not always appropriate. For example, Anscombe provided cases for which the data does not have a linear relationship. So are we justified in using the regression line to predict? Galton answered this in the positive for height data. The justification, which we include in the next section, is somewhat more advanced than the rest of this reading.\n\n\nBivariate normal distribution (advanced)\nCorrelation and the regression slope are a widely used summary statistic, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases of dataset in which summarizing with correlation would be a mistake. But there are many more real-life examples.\nThe main way we motivate the use of correlation involves what is called the bivariate normal distribution.\nWhen a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals. These ovals can be thin (high correlation) or circle-shaped (no correlation).\n\nA more technical way to define the bivariate normal distribution is the following: if \\(X\\) is a normally distributed random variable, \\(Y\\) is also a normally distributed random variable, and the conditional distribution of \\(Y\\) for any \\(X=x\\) is approximately normal, then the pair is approximately bivariate normal.\nIf we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold:\n\ngalton_heights %&gt;%\n  mutate(z_father = round((father - mean(father)) / sd(father))) %&gt;%\n  filter(z_father %in% -2:2) %&gt;%\n  ggplot() +\n  stat_qq(aes(sample = son)) +\n  facet_wrap( ~ z_father)\n\n\n\n\n\n\n\n\nNow we come back to defining correlation. Galton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of \\(x\\), the expected value of the \\(Y\\) in pairs for which \\(X=x\\) is:\n\\[\n\\mbox{E}(Y | X=x) = \\mu_Y +  \\rho \\frac{X-\\mu_X}{\\sigma_X}\\sigma_Y\n\\]\nThis is the regression line, with slope \\[\\rho \\frac{\\sigma_Y}{\\sigma_X}\\] and intercept \\(\\mu_y - m\\mu_X\\). It is equivalent to the regression equation we showed earlier which can be written like this:\n\\[\n\\frac{\\mbox{E}(Y \\mid X=x)  - \\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X}\n\\]\nThis implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict.\nIn summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of \\(Y\\) given we know the value of \\(X\\), is given by the regression line.\n\n\nVariance explained\nThe bivariate normal theory also tells us that the standard deviation of the conditional distribution described above is:\n\\[\n\\mbox{SD}(Y \\mid X=x ) = \\sigma_Y \\sqrt{1-\\rho^2}\n\\]\nTo see why this is intuitive, notice that without conditioning, \\(\\mbox{SD}(Y) = \\sigma_Y\\), we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with a tall, 72-inch, father. This group will all tend to be somewhat tall so the standard deviation is reduced.\nSpecifically, it is reduced to \\(\\sqrt{1-\\rho^2} = \\sqrt{1 - 0.25}\\) = 0.87 of what it was originally. We could say that father heights “explain” 13% of the variability observed in son heights.\nThe statement “\\(X\\) explains such and such percent of the variability” is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by \\(1-\\rho^2\\), so we say that \\(X\\) explains \\(1- (1-\\rho^2)=\\rho^2\\) (the correlation squared) of the variance.\nBut it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution.\n\n\nWarning: there are two regression lines\nWe computed a regression line to predict the son’s height from father’s height. We used these calculations:\n\nmu_x &lt;- mean(galton_heights$father)\nmu_y &lt;- mean(galton_heights$son)\ns_x &lt;- sd(galton_heights$father)\ns_y &lt;- sd(galton_heights$son)\nr &lt;- cor(galton_heights$father, galton_heights$son)\nm_1 &lt;-  r * s_y / s_x\nb_1 &lt;- mu_y - m_1*mu_x\n\nwhich gives us the function \\(\\mbox{E}(Y\\mid X=x) =\\) 37.3 + 0.46 \\(x\\).\nWhat if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function: \\(x = \\{ \\mbox{E}(Y\\mid X=x) -\\) 37.3 \\(\\} /\\) 0.5.\nWe need to compute \\(\\mbox{E}(X \\mid Y=y)\\). Since the data is approximately bivariate normal, the theory described above tells us that this conditional expectation will follow a line with slope and intercept:\n\nm_2 &lt;-  r * s_x / s_y\nb_2 &lt;- mu_x - m_2 * mu_y\n\nSo we get \\(\\mbox{E}(X \\mid Y=y) =\\) 40.9 + 0.41y. Again we see regression to the average: the prediction for the father is closer to the father average than the son heights \\(y\\) is to the son average.\nHere is a plot showing the two regression lines, with blue for the predicting son heights with father heights and red for predicting father heights with son heights:\n\ngalton_heights %&gt;%\n  ggplot(aes(father, son)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = b_1, slope = m_1, col = \"blue\") +\n  geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTRY IT\n\n\n\n\nLoad the GaltonFamilies data from the HistData. The children in each family are listed by gender and then by height. Create a dataset called galton_heights by picking a male and female at random from each family (hint: group_by appropriately and use sample_n).\nMake a scatterplot for heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons. You may have to use gridExtra or patchwork)\nCompute the correlation in heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.",
    "crumbs": [
      "Course Content",
      "Week 06",
      "Foundations of Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_06/06a.html#footnotes",
    "href": "content/Week_06/06a.html#footnotes",
    "title": "Foundations of Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Francis_Galton↩︎",
    "crumbs": [
      "Course Content",
      "Week 06",
      "Foundations of Linear Regression"
    ]
  },
  {
    "objectID": "content/Week_07/07a.html",
    "href": "content/Week_07/07a.html",
    "title": "Linear Regression II",
    "section": "",
    "text": "This page.\n\n\n\n\n Chapter 3 in Introduction to Statistical Learning\n\n\n\n\n\nHow do we use R’s lm for regression?\nHow do we interpret linear regression outputs?\nHow are the standard errors derived?\nWhen should we turn to linear regression versus alternative approaches?\nWhy do we use linear regression so often in data analytics?",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression II"
    ]
  },
  {
    "objectID": "content/Week_07/07a.html#required-reading",
    "href": "content/Week_07/07a.html#required-reading",
    "title": "Linear Regression II",
    "section": "",
    "text": "This page.\n\n\n\n\n Chapter 3 in Introduction to Statistical Learning\n\n\n\n\n\nHow do we use R’s lm for regression?\nHow do we interpret linear regression outputs?\nHow are the standard errors derived?\nWhen should we turn to linear regression versus alternative approaches?\nWhy do we use linear regression so often in data analytics?",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression II"
    ]
  },
  {
    "objectID": "content/Week_07/07a.html#case-study-moneyball",
    "href": "content/Week_07/07a.html#case-study-moneyball",
    "title": "Linear Regression II",
    "section": "Case study: Moneyball",
    "text": "Case study: Moneyball\nMoneyball: The Art of Winning an Unfair Game is a book by Michael Lewis about the Oakland Athletics (A’s) baseball team and its general manager, the person tasked with building the team, Billy Beane.\nTraditionally, baseball teams use scouts to help them decide what players to hire. These scouts evaluate players by observing them perform. Scouts tend to favor athletic players with observable physical abilities. For this reason, scouts tend to agree on who the best players are and, as a result, these players tend to be in high demand. This in turn drives up their salaries.\nFrom 1989 to 1991, the A’s had one of the highest payrolls in baseball. They were able to buy the best players and, during that time, they were one of the best teams. However, in 1995 the A’s team owner changed and the new management cut the budget drastically, leaving then general manager, Sandy Alderson, with one of the lowest payrolls in baseball. He could no longer afford the most sought-after players. Alderson began using a statistical approach to find inefficiencies in the market. Alderson was a mentor to Billy Beane, who succeeded him in 1998 and fully embraced data science, as opposed to scouts, as a method for finding low-cost players that data predicted would help the team win. Today, this strategy has been adapted by most baseball teams. As we will see, regression plays a large role in this approach.\nAs motivation for this lecture, we will pretend it is 2002 (holy sh*t I’m old) and try to build a baseball team with a limited budget, just like the A’s had to do. To appreciate what you are up against, note that in 2002 the Yankees’ payroll of $125,928,583 more than tripled the Oakland A’s $39,679,746:\n\n\n\n\n\n\n\n\n\n\nSabermetrics\nStatistics have been used in baseball since its beginnings. The dataset we will be using, included in the Lahman library, goes back to the 19th century. For example, a summary statistic we will describe soon, the batting average, has been used for decades to summarize a batter’s success. Other statistics1 such as home runs (HR), runs batted in (RBI), and stolen bases (SB) are reported for each player in the game summaries included in the sports section of newspapers, with players rewarded for high numbers. Although summary statistics such as these were widely used in baseball, data analysis per se was not. These statistics were arbitrarily decided on without much thought as to whether they actually predicted anything or were related to helping a team win.\nThis changed with Bill James2. In the late 1970s, this aspiring writer and baseball fan started publishing articles describing more in-depth analysis of baseball data. He named the approach of using data to predict what outcomes best predicted if a team would win sabermetrics3. Until Billy Beane made sabermetrics the center of his baseball operation, Bill James’ work was mostly ignored by the baseball world. Currently, sabermetrics popularity is no longer limited to just baseball; other sports have started to use this approach as well.\nIn this lecture, to simplify the exercise, we will focus on scoring runs and ignore the two other important aspects of the game: pitching and fielding. We will see how regression analysis can help develop strategies to build a competitive baseball team with a constrained budget. The approach can be divided into two separate data analyses. In the first, we determine which recorded player-specific statistics predict runs. In the second, we examine if players were undervalued based on what our first analysis predicts.\n\n\nBaseball basics\nTo see how regression will help us find undervalued players, we actually don’t need to understand all the details about the game of baseball, which has over 100 rules. Here, we distill the sport to the basic knowledge one needs to know how to effectively attack the data science problem.\nThe goal of a baseball game is to score more runs (points) than the other team. Each team has 9 batters that have an opportunity to hit a ball with a bat in a predetermined order. After the 9th batter has had their turn, the first batter bats again, then the second, and so on. Each time a batter has an opportunity to bat, we call it a plate appearance (PA). At each PA, the other team’s pitcher throws the ball and the batter tries to hit it. The PA ends with an binary outcome: the batter either makes an out (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases). Each team gets nine tries, referred to as innings, to score runs and each inning ends after three outs (three failures).\nHere is a video showing a success: https://www.youtube.com/watch?v=HL-XjMCPfio. And here is one showing a failure: https://www.youtube.com/watch?v=NeloljCx-1g. In these videos, we see how luck is involved in the process. When at bat, the batter wants to hit the ball hard. If the batter hits it hard enough, it is a HR, the best possible outcome as the batter gets at least one automatic run. But sometimes, due to chance, the batter hits the ball very hard and a defender catches it, resulting in an out. In contrast, sometimes the batter hits the ball softly, but it lands just in the right place. The fact that there is chance involved hints at why probability models will be involved.\nNow there are several ways to succeed. Understanding this distinction will be important for our analysis. When the batter hits the ball, the batter wants to pass as many bases as possible. There are four bases with the fourth one called home plate. Home plate is where batters start by trying to hit, so the bases form a cycle.\n\n\n\n\n\n(Courtesy of Cburnett4. CC BY-SA 3.0 license5.)\n\n\n\n\n\nA batter who goes around the bases and arrives home, scores a run.\nWe are simplifying a bit, but there are five ways a batter can succeed, that is, not make an out:\n\nBases on balls (BB) - the pitcher fails to throw the ball through a predefined area considered to be hittable (the strikezone), so the batter is permitted to go to first base.\nSingle - Batter hits the ball and gets to first base.\nDouble (2B) - Batter hits the ball and gets to second base.\nTriple (3B) - Batter hits the ball and gets to third base.\nHome Run (HR) - Batter hits the ball and goes all the way home and scores a run.\n\nHere is an example of a HR: https://www.youtube.com/watch?v=xYxSZJ9GZ-w. If a batter gets to a base, the batter still has a chance of getting home and scoring a run if the next batter hits successfully. While the batter is on base, the batter can also try to steal a base (SB). If a batter runs fast enough, the batter can try to go from one base to the next without the other team tagging the runner. [Here] is an example of a stolen base: https://www.youtube.com/watch?v=JSE5kfxkzfk.\nAll these events are kept track of during the season and are available to us through the Lahman package. Now we will start discussing how data analysis can help us decide how to use these statistics to evaluate players.\n\n\nNo awards for BB\nHistorically, the batting average has been considered the most important offensive statistic. To define this average, we define a hit (H) and an at bat (AB). Singles, doubles, triples, and home runs are hits. The fifth way to be successful, BB, is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate. Today this success rate ranges from 20% to 38%. We refer to the batting average in thousands so, for example, if your success rate is 28%, we call it batting 280.\n\n\n\n\n\n(Picture courtesy of Keith Allison6. CC BY-SA 2.0 license7.)\n\n\n\n\nOne of Bill James’ first important insights is that the batting average ignores BB, but a BB is a success. He proposed we use the on base percentage (OBP) instead of batting average. He defined OBP as (H+BB)/(AB+BB) which is simply the proportion of plate appearances that don’t result in an out, a very intuitive measure. He noted that a player that gets many more BB than the average player might not be recognized if the batter does not excel in batting average. But is this player not helping produce runs? No award is given to the player with the most BB. However, bad habits are hard to break and baseball did not immediately adopt OBP as an important statistic. In contrast, total stolen bases were considered important and an award8 given to the player with the most. But players with high totals of SB also made more outs as they did not always succeed. Does a player with high SB total help produce runs? Can we use data science to determine if it’s better to pay for players with high BB or SB?\n\n\nBase on balls or stolen bases?\nOne of the challenges in this analysis is that it is not obvious how to determine if a player produces runs because so much depends on his teammates. We do keep track of the number of runs scored by a player. However, remember that if a player X bats right before someone who hits many HRs, batter X will score many runs. But these runs don’t necessarily happen if we hire player X but not his HR hitting teammate. However, we can examine team-level statistics. How do teams with many SB compare to teams with few? How about BB? We have data! Let’s examine some.\nLet’s start with an obvious one: HRs. Do teams that hit more home runs score more runs? We examine data from 1961 to 2001. The visualization of choice when exploring the relationship between two variables, such as HRs and wins, is a scatterplot:\n\nlibrary(Lahman)\n\nTeams %&gt;% filter(yearID %in% 1961:2001) %&gt;%\n  mutate(HR_per_game = HR / G, R_per_game = R / G) %&gt;%\n  ggplot(aes(HR_per_game, R_per_game)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\nThe plot shows a strong association: teams with more HRs tend to score more runs. Now let’s examine the relationship between stolen bases and runs:\n\nTeams %&gt;% filter(yearID %in% 1961:2001) %&gt;%\n  mutate(SB_per_game = SB / G, R_per_game = R / G) %&gt;%\n  ggplot(aes(SB_per_game, R_per_game)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\nHere the relationship is not as clear. Finally, let’s examine the relationship between BB and runs:\n\nTeams %&gt;% filter(yearID %in% 1961:2001) %&gt;%\n  mutate(BB_per_game = BB/G, R_per_game = R/G) %&gt;%\n  ggplot(aes(BB_per_game, R_per_game)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\nHere again we see a clear association. But does this mean that increasing a team’s BBs causes an increase in runs? One of the most important lessons you learn in this book is that association is not causation.\nIn fact, it looks like BBs and HRs are also associated:\n\nTeams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;%\n  mutate(HR_per_game = HR/G, BB_per_game = BB/G) %&gt;%\n  ggplot(aes(HR_per_game, BB_per_game)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\nWe know that HRs cause runs because, as the name “home run” implies, when a player hits a HR they are guaranteed at least one run. Could it be that HRs also cause BB and this makes it appear as if BB cause runs? When this happens we say there is confounding, an important concept we will learn more about throughout this lecture.\nLinear regression will help us parse all this out and quantify the associations. This will then help us determine what players to recruit. Specifically, we will try to predict things like how many more runs will a team score if we increase the number of BBs, but keep the HRs fixed? Regression will help us answer questions like this one.\n\n\nRegression applied to baseball statistics\nCan we use regression with these data? First, notice that the HR and Run data appear to be bivariate normal. We may have skipped the bivariate normal from before, but it just means that two variables share a joint normal distribution – each has it’s own mean, but conditional on some value of one, the other is normally distributed around it’s conditional value. We save the plot into the object p as we will use it again later.\n\nlibrary(Lahman)\np &lt;- Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;%\n  mutate(HR_per_game = HR/G, R_per_game = R/G) %&gt;%\n  ggplot(aes(HR_per_game, R_per_game)) +\n  geom_point(alpha = 0.5)\np\n\n\n\n\n\n\n\n\nThe qq-plots confirm that the normal approximation is useful here:\n\nTeams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;%\n  mutate(z_HR = round((HR - mean(HR))/sd(HR)),\n         R_per_game = R/G) %&gt;%\n  filter(z_HR %in% -2:3) %&gt;%\n  ggplot() +\n  stat_qq(aes(sample=R_per_game)) +\n  facet_wrap(~z_HR)\n\n\n\n\n\n\n\n\nNow we are ready to use linear regression to predict the number of runs a team will score if we know how many home runs the team hits. All we need to do is compute the five summary statistics:\n\nsummary_stats &lt;- Teams %&gt;%\n  filter(yearID %in% 1961:2001 ) %&gt;%\n  mutate(HR_per_game = HR/G, R_per_game = R/G) %&gt;%\n  summarize(avg_HR = mean(HR_per_game),\n            s_HR = sd(HR_per_game),\n            avg_R = mean(R_per_game),\n            s_R = sd(R_per_game),\n            r = cor(HR_per_game, R_per_game))\nsummary_stats\n\n     avg_HR      s_HR    avg_R       s_R         r\n1 0.8547104 0.2429707 4.355262 0.5885791 0.7615597\n\n\nand use the formulas given above to create the regression lines, as we did in Week 5’s Content, and adding the line to our plot p created earlier:\n\nreg_line &lt;- summary_stats %&gt;% summarize(slope = r*s_R/s_HR,\n                            intercept = avg_R - slope*avg_HR)\n\np + geom_abline(intercept = reg_line$intercept, slope = reg_line$slope)\n\n\n\n\n\n\n\n\nFor plotting, we can also use the argument method = \"lm\" which stands for linear model, the title of an upcoming section. So we can simplify the code above like this:\n\np + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nIn the example above, the slope is 1.8448241. So this tells us that teams that hit 1 more HR per game than the average team, score 1.8448241 more runs per game than the average team. Given that the most common final score is a difference of a run, this can certainly lead to a large increase in wins. Not surprisingly, HR hitters are very expensive. Because we are working on a budget, we will need to find some other way to increase wins. So in the next section we move our attention to BB.",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression II"
    ]
  },
  {
    "objectID": "content/Week_07/07a.html#confounding",
    "href": "content/Week_07/07a.html#confounding",
    "title": "Linear Regression II",
    "section": "Confounding",
    "text": "Confounding\nPreviously, we noted a strong relationship between Runs and BB. If we find the regression line for predicting runs from bases on balls, we a get slope of:\n\nlibrary(tidyverse)\nlibrary(Lahman)\nget_slope &lt;- function(x, y) cor(x, y) * sd(y) / sd(x)\n\nbb_slope &lt;- Teams %&gt;%\n  filter(yearID %in% 1961:2001 ) %&gt;%\n  mutate(BB_per_game = BB/G, R_per_game = R/G) %&gt;%\n  summarize(slope = get_slope(BB_per_game, R_per_game))\n\nbb_slope\n\n      slope\n1 0.7353288\n\n\nSo does this mean that if we go and hire low salary players with many BB, and who therefore increase the number of walks per game by 2, our team will score 1.5 more runs per game?\nWe are again reminded that association is not causation. The data does provide strong evidence that a team with two more BB per game than the average team, scores 1.5 runs per game. But this does not mean that BB are the cause.\nNote that if we compute the regression line slope for singles we get:\n\nsingles_slope &lt;- Teams %&gt;%\n  filter(yearID %in% 1961:2001 ) %&gt;%\n  mutate(Singles_per_game = (H-HR-X2B-X3B)/G, R_per_game = R/G) %&gt;%\n  summarize(slope = get_slope(Singles_per_game, R_per_game))\n\nsingles_slope\n\n      slope\n1 0.4494253\n\n\nwhich is a lower value than what we obtain for BB.\nAlso, notice that a single gets you to first base just like a BB. Those that know about baseball will tell you that with a single, runners on base have a better chance of scoring than with a BB. So how can BB be more predictive of runs? The reason this happen is because of confounding. Here we show the correlation between HR, BB, and singles:\n\nTeams %&gt;%\n  filter(yearID %in% 1961:2001 ) %&gt;%\n  mutate(Singles = (H-HR-X2B-X3B)/G, BB = BB/G, HR = HR/G) %&gt;%\n  summarize(cor(BB, HR), cor(Singles, HR), cor(BB, Singles))\n\n  cor(BB, HR) cor(Singles, HR) cor(BB, Singles)\n1   0.4039313       -0.1737435      -0.05603822\n\n\nIt turns out that pitchers, afraid of HRs, will sometimes avoid throwing strikes to HR hitters. As a result, HR hitters tend to have more BBs and a team with many HRs will also have more BBs. Although it may appear that BBs cause runs, it is actually the HRs that cause most of these runs. We say that BBs are confounded with HRs. Nonetheless, could it be that BBs still help? To find out, we somehow have to adjust for the HR effect. Regression can help with this as well.\n\nUnderstanding confounding through stratification\nA first approach is to keep HRs fixed at a certain value and then examine the relationship between BB and runs. As we did when we stratified fathers by rounding to the closest inch, here we can stratify HR per game to the closest ten. We filter out the strata with few points to avoid highly variable estimates:\n\ndat &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;%\n  mutate(HR_strata = round(HR/G, 1),\n         BB_per_game = BB / G,\n         R_per_game = R / G) %&gt;%\n  filter(HR_strata &gt;= 0.4 & HR_strata &lt;=1.2)\n\nand then make a scatterplot for each strata:\n\ndat %&gt;%\n  ggplot(aes(BB_per_game, R_per_game)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  facet_wrap( ~ HR_strata)\n\n\n\n\n\n\n\n# Note: we'll get a \"warning\"\n# telling us that ggplot has\n# used lm(y ~ x) where\n# y refers to the aesthetic mapping of y\n# x refers to the aesthetic mapping of x\n\nRemember that the regression slope for predicting runs with BB was 0.7. Once we stratify by HR, these slopes are substantially reduced:\n\ndat %&gt;%\n  group_by(HR_strata) %&gt;%\n  summarize(slope = get_slope(BB_per_game, R_per_game))\n\n# A tibble: 9 × 2\n  HR_strata slope\n      &lt;dbl&gt; &lt;dbl&gt;\n1       0.4 0.734\n2       0.5 0.566\n3       0.6 0.412\n4       0.7 0.285\n5       0.8 0.365\n6       0.9 0.261\n7       1   0.512\n8       1.1 0.454\n9       1.2 0.440\n\n\nThe slopes are reduced, but they are not 0, which indicates that BBs are helpful for producing runs, just not as much as previously thought. In fact, the values above are closer to the slope we obtained from singles, 0.45, which is more consistent with our intuition. Since both singles and BB get us to first base, they should have about the same predictive power.\nAlthough our understanding of the application tells us that HR cause BB but not the other way around, we can still check if stratifying by BB makes the effect of HR go down. To do this, we use the same code except that we swap HR and BBs to get this plot:\n\n\n\n\n\n\n\n\n\nIn this case, the slopes do not change much from the original:\n\ndat %&gt;% group_by(BB_strata) %&gt;%\n   summarize(slope = get_slope(HR_per_game, R_per_game))\n\n# A tibble: 12 × 2\n   BB_strata slope\n       &lt;dbl&gt; &lt;dbl&gt;\n 1       2.8  1.52\n 2       2.9  1.57\n 3       3    1.52\n 4       3.1  1.49\n 5       3.2  1.58\n 6       3.3  1.56\n 7       3.4  1.48\n 8       3.5  1.63\n 9       3.6  1.83\n10       3.7  1.45\n11       3.8  1.70\n12       3.9  1.30\n\n\nThey are reduced a bit, which is consistent with the fact that BB do in fact cause some runs.\n\nhr_slope &lt;- Teams %&gt;%\n  filter(yearID %in% 1961:2001 ) %&gt;%\n  mutate(HR_per_game = HR/G, R_per_game = R/G) %&gt;%\n  summarize(slope = get_slope(HR_per_game, R_per_game))\n\nhr_slope\n\n     slope\n1 1.844824\n\n\nRegardless, it seems that if we stratify by HR, we have bivariate distributions for runs versus BB. Similarly, if we stratify by BB, we have approximate bivariate normal distributions for HR versus runs.\n\n\nMultivariate regression\nIt is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this:\n\\[\n\\mbox{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1(x_2) x_1 + \\beta_2(x_1) x_2\n\\]\nwith the slopes for \\(x_1\\) changing for different values of \\(x_2\\) and vice versa (think of \\(\\beta_1(x_2)\\) as a function that gives a different slope depending on \\(x_2\\)). But is there an easier approach?\nIf we take random variability into account, the slopes in the strata don’t appear to change much. If these slopes are in fact the same, this implies that \\(\\beta_1(x_2)\\) and \\(\\beta_2(x_1)\\) are constants. This in turn implies that the expectation of runs conditioned on HR and BB can be written like this:\n\\[\n\\mbox{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\nThis model suggests that if the number of HR is fixed at \\(x_2\\), we observe a linear relationship between runs and BB with an intercept of \\(\\beta_0 + \\beta_2 x_2\\). Our exploratory data analysis suggested this. The model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by \\(\\beta_1 x_1\\).\nIn this analysis, referred to as multivariate regression, you will often hear people say that the BB slope \\(\\beta_1\\) is adjusted for the HR effect. If the model is correct then confounding has been accounted for. But how do we estimate \\(\\beta_1\\) and \\(\\beta_2\\) from the data? For this, we learn about linear models and least squares estimates.",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression II"
    ]
  },
  {
    "objectID": "content/Week_07/07a.html#lse",
    "href": "content/Week_07/07a.html#lse",
    "title": "Linear Regression II",
    "section": "Least squares estimates",
    "text": "Least squares estimates\nWe have described how if data is bivariate normal then the conditional expectations follow the regression line. The fact that the conditional expectation is a line is not an extra assumption but rather a derived result. However, in practice it is common to explicitly write down a model that describes the relationship between two or more variables using a linear model.\nWe note that “linear” here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combination of known quantities. In mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables. For example, \\(3x - 4y + 5z\\) is a linear combination of \\(x\\), \\(y\\), and \\(z\\). We can also add a constant so \\(2 + 3x - 4y + 5z\\) is also linear combination of \\(x\\), \\(y\\), and \\(z\\).\nSo \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\), is a linear combination of \\(x_1\\) and \\(x_2\\). The simplest linear model is a constant \\(\\beta_0\\); the second simplest is a line \\(\\beta_0 + \\beta_1 x\\). If we were to specify a linear model for Galton’s data, we would denote the \\(N\\) observed father heights with \\(x_1, \\dots, x_n\\), then we model the \\(N\\) son heights we are trying to predict with:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\, i=1,\\dots,N.\n\\]\nHere \\(x_i\\) is the father’s height, which is fixed (not random) due to the conditioning, and \\(Y_i\\) is the random son’s height that we want to predict. We further assume that \\(\\varepsilon_i\\) are independent from each other, have expected value 0 and the standard deviation of \\(\\varepsilon_i\\), call it \\(\\sigma\\), does not depend on \\(i\\).\nIn the above model, we know the \\(x_i\\), but to have a useful model for prediction, we need \\(\\beta_0\\) and \\(\\beta_1\\). We estimate these from the data. Once we do this, we can predict son’s heights for any father’s height \\(x\\). We show how to do this in the next section.\nNote that if we further assume that the \\(\\varepsilon\\) is normally distributed, then this model is exactly the same one we derived earlier by assuming bivariate normal data. A somewhat nuanced difference is that in the first approach we assumed the data was bivariate normal and that the linear model was derived, not assumed. In practice, linear models are just assumed without necessarily assuming normality: the distribution of the \\(\\varepsilon\\)’s is not specified. Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model.\n\nInterpreting linear models\nOne reason linear models are popular is that they are interpretable. In the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by \\(\\beta_1\\) for each inch we increase the father’s height \\(x\\). Because not all sons with fathers of height \\(x\\) are of equal height, we need the term \\(\\varepsilon\\), which explains the remaining variability. This remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness.\nGiven how we wrote the model above, the intercept \\(\\beta_0\\) is not very interpretable as it is the predicted height of a son with a father with no height. Due to regression to the mean, the prediction will usually be a bit larger than 0. To make the slope parameter more interpretable, we can rewrite the model slightly as:\n\\[\nY_i = \\beta_0 + \\beta_1 (x_i - \\bar{x}) + \\varepsilon_i, \\, i=1,\\dots,N\n\\]\nwith \\(\\bar{x} = 1/N \\sum_{i=1}^N x_i\\) the average of the \\(x\\). In this case \\(\\beta_0\\) represents the height when \\(x_i = \\bar{x}\\), which is the height of the son of an average father.\n\n\nLeast Squares Estimates (LSE)\nFor linear models to be useful, we have to estimate the unknown \\(\\beta\\)s. The standard approach in science is to find the values that minimize the distance of the fitted model to the data. The following is called the least squares (LS) equation and we will see it often in this lecture. For Galton’s data, we would write:\n\\[\nRSS = \\sum_{i=1}^n \\left\\{  y_i - \\left(\\beta_0 + \\beta_1 x_i \\right)\\right\\}^2\n\\]\nThis quantity is called the residual sum of squares (RSS). Once we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them with \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). Let’s demonstrate this with the previously defined dataset:\n\nlibrary(HistData)\ndata(\"GaltonFamilies\")\nset.seed(1983)\ngalton_heights &lt;- GaltonFamilies %&gt;%\n  filter(gender == \"male\") %&gt;%\n  group_by(family) %&gt;%\n  sample_n(1) %&gt;%\n  ungroup() %&gt;%\n  select(father, childHeight) %&gt;%\n  rename(son = childHeight)\n\nLet’s write a function that computes the RSS for any pair of values \\(\\beta_0\\) and \\(\\beta_1\\).\n\nrss &lt;- function(beta0, beta1, data){\n  resid &lt;- galton_heights$son - (beta0+beta1*galton_heights$father)\n  return(sum(resid^2))\n}\n\nSo for any pair of values, we get an RSS. Here is a plot of the RSS as a function of \\(\\beta_1\\) when we keep the \\(\\beta_0\\) fixed at 25.\n\nbeta1 = seq(0, 1, len=nrow(galton_heights))\nresults &lt;- data.frame(beta1 = beta1,\n                      rss = sapply(beta1, rss, beta0 = 25))\nresults %&gt;% ggplot(aes(beta1, rss)) + geom_line() +\n  geom_line(aes(beta1, rss))\n\n\n\n\n\n\n\n\nWe can see a clear minimum for \\(\\beta_1\\) at around 0.65. However, this minimum for \\(\\beta_1\\) is for when \\(\\beta_0 = 25\\), a value we arbitrarily picked. We don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs.\nTrial and error is not going to work in this case. We could search for a minimum within a fine grid of \\(\\beta_0\\) and \\(\\beta_1\\) values, but this is unnecessarily time-consuming since we can use calculus: take the partial derivatives, set them to 0 and solve for \\(\\beta_1\\) and \\(\\beta_2\\). Of course, if we have many parameters, these equations can get rather complex. But there are functions in R that do these calculations for us. We will learn these next. To learn the mathematics behind this, you can consult a book on linear models.\n\n\nThe lm function\nThe lm function is the workhorse for linear models in R. We want to fit the model:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\nwith \\(Y_i\\) the son’s height and \\(x_i\\) the father’s height as in our Galton Heights dataset. We can use lm() to obtain the least squares estimates.\n\nfit &lt;- lm(son ~ father, data = galton_heights)\nfit$coef\n\n(Intercept)      father \n  37.287605    0.461392 \n\n\nThere are two arguments here. The first is specifying the regression formula. We use the character ~ to let lm know which is the variable we are predicting (left of ~) and which we are using to predict (right of ~). The intercept is added automatically to the model that will be fit. Note that we just use the column names here – son not galton_heights$son. And note that we aren’t putting quotes around the names, even though R doesn’t have an object called son or father in its memory (it does have galton_heights$son and galton_heights$father of course).\nThe second argument is the key here. It is the data= argoument. Here, you’re telling R where to find the variables son and father.\nNote that while lm(galton_heights$son ~ galton_heights$father) will work, it will cause problems later when we try to predict using our model. Do not write your regressions like this. Use the formula and data = notation.\nThe object we named fit here is a “lm” object. Not a data frame or a tibble. It includes more information about the fit and lots of other stuff. We can use the function summary to extract more of this information. summary is usually how we’ll look at regression results:\n\nsummary(fit)\n\n\nCall:\nlm(formula = son ~ father, data = galton_heights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3543 -1.5657 -0.0078  1.7263  9.4150 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.28761    4.98618   7.478 3.37e-12 ***\nfather       0.46139    0.07211   6.398 1.36e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.45 on 177 degrees of freedom\nMultiple R-squared:  0.1878,    Adjusted R-squared:  0.1833 \nF-statistic: 40.94 on 1 and 177 DF,  p-value: 1.36e-09\n\n\nTo understand some of the information included in this summary we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables\n\n\nLSE are random variables\nThe LSE is derived from the data \\(y_1,\\dots,y_N\\), which are a realization of random variables \\(Y_1, \\dots, Y_N\\). This implies that our estimates are random variables. To see this, we can run a Monte Carlo simulation in which we assume the son and father height data defines a population, take a random sample of size \\(N=50\\), and compute the regression slope coefficient for each one:\n\nB &lt;- 1000\nN &lt;- 50\nlse &lt;- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) %&gt;%\n    lm(son ~ father, data = .) %&gt;%\n    .$coef\n})\nlse &lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,])\n\nWe can see the variability of the estimates by plotting their distributions:\n\n\n\n\n\n\n\n\n\nThe reason these look normal is because the central limit theorem applies here as well: for large enough \\(N\\), the least squares estimates will be approximately normal with expected value \\(\\beta_0\\) and \\(\\beta_1\\), respectively. The standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the lm function. Here it is for one of our simulated data sets:\n\n sample_n(galton_heights, N, replace = TRUE) %&gt;%\n  lm(son ~ father, data = .) %&gt;%\n  summary %&gt;% .$coef\n\n              Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 19.2791952 11.6564590 1.653950 0.1046637693\nfather       0.7198756  0.1693834 4.249977 0.0000979167\n\n\nYou can see that the standard errors estimates reported by the summary are close to the standard errors from the simulation:\n\nlse %&gt;% summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))\n\n     se_0      se_1\n1 8.83591 0.1278812\n\n\nThe summary function also reports t-statistics (t value) and p-values (Pr(&gt;|t|)). The t-statistic is not actually based on the central limit theorem but rather on the assumption that the \\(\\varepsilon\\)s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, \\(\\hat{\\beta}_0 / \\hat{\\mbox{SE}}(\\hat{\\beta}_0 )\\) and \\(\\hat{\\beta}_1 / \\hat{\\mbox{SE}}(\\hat{\\beta}_1 )\\), follow a t-distribution with \\(N-p\\) degrees of freedom, with \\(p\\) the number of parameters in our model. In the case of height \\(p=2\\), the two p-values are testing the null hypothesis that \\(\\beta_0 = 0\\) and \\(\\beta_1=0\\), respectively.\nRemember that, as we described in the section on t-tests for large enough \\(N\\), the CLT works and the t-distribution becomes almost the same as the normal distribution. Also, notice that we can construct confidence intervals, but we will soon learn about broom, an add-on package that makes this easy.\nAlthough we do not show examples in this section, hypothesis testing with regression models is commonly used in epidemiology and economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y, and Z”. However, several assumptions have to hold for these statements to be true.\n\n\nPredicted values are random variables\nOnce we fit our model, we can obtain prediction of \\(Y\\) by plugging in the estimates into the regression model. For example, if the father’s height is \\(x\\), then our prediction \\(\\hat{Y}\\) for the son’s height will be:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\]\nWhen we plot \\(\\hat{Y}\\) versus \\(x\\), we see the regression line.\nKeep in mind that the prediction \\(\\hat{Y}\\) is also a random variable and mathematical theory tells us what the standard errors are. If we assume the errors are normal, or have a large enough sample size, we can use theory to construct confidence intervals as well. In fact, the ggplot2 layer geom_smooth(method = \"lm\") that we previously used plots \\(\\hat{Y}\\) and surrounds it by confidence intervals:\n\ngalton_heights %&gt;% ggplot(aes(son, father)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nThe R function predict takes an lm object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided:\n\nfit &lt;- galton_heights %&gt;% lm(son ~ father, data = .)\n\ny_hat &lt;- predict(fit, se.fit = TRUE)\n\nnames(y_hat)\n\n[1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\"\n\n\n\n\nThe broom package\nOur original task was to provide an estimate and confidence interval for the slope estimates of each strata. The broom package will make this quite easy. While the most useful way of looking at results from a regression with lm is to use summary on the lm object, broom provides tools to extract info from the regression.\nThe broom package has three main functions, all of which extract information from the object returned by lm and return it in a tidyverse friendly data frame. These functions are tidy, glance, and augment. The tidy function returns estimates and related information as a data frame:\n\nlibrary(broom)\nfit &lt;- lm(R ~ BB, data = dat)\ntidy(fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  214.      23.5         9.10 6.46e-19\n2 BB             0.910    0.0447     20.3  5.30e-75\n\n\nWe can add other important summaries, such as confidence intervals:\n\ntidy(fit, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  214.      23.5         9.10 6.46e-19  168.      260.   \n2 BB             0.910    0.0447     20.3  5.30e-75    0.822     0.998\n\n\nBecause the outcome is a data frame, we can immediately use it with do to string together the commands that produce the table we are after. Because a data frame is returned, we can filter and select the rows and columns we want, which facilitates working with ggplot2:\n\ndat %&gt;%\n  group_by(HR) %&gt;%\n  do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) %&gt;%\n  filter(term == \"BB\") %&gt;%\n  select(HR, estimate, conf.low, conf.high) %&gt;%\n  ggplot(aes(HR, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_errorbar() +\n  geom_point()\n\n\n\n\n\n\n\n\nNow we return to discussing our original task of determining if slopes changed. The plot we just made, using do and tidy, shows that the confidence intervals overlap, which provides a nice visual confirmation that our assumption that the slope does not change is safe.\nThe other functions provided by broom, glance, and augment, relate to model-specific and observation-specific outcomes, respectively. Here, we can see the model fit summaries glance returns:\n\nglance(fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.331         0.330  79.9      413. 5.30e-75     1 -4865. 9735. 9750.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nYou can learn more about these summaries in any regression text book.\nWe will see an example of augment in the next section (and you used it already to predict values in an earlier assignment).",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression II"
    ]
  },
  {
    "objectID": "content/Week_07/07a.html#case-study-multiple-regression-and-moneyball-continued",
    "href": "content/Week_07/07a.html#case-study-multiple-regression-and-moneyball-continued",
    "title": "Linear Regression II",
    "section": "Case study: Multiple Regression and Moneyball (continued)",
    "text": "Case study: Multiple Regression and Moneyball (continued)\nIn trying to answer how well BBs predict runs, data exploration led us to a model, one with perhaps multiple variables:\n\\[\n\\mbox{E}[R \\mid BB = x_1, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\nHere, the data is approximately normal and conditional distributions were also normal. Thus, we are justified in using a linear model:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i\n\\]\nwith \\(Y_i\\) runs per game for team \\(i\\), \\(x_{i,1}\\) walks per game, and \\(x_{i,2}\\). To use lm here, we need to let the function know we have two predictor variables. So we use the + symbol as follows:\n\nfit &lt;- Teams %&gt;%\n  filter(yearID %in% 1961:2001) %&gt;%\n  mutate(BB = BB/G, HR = HR/G,  R = R/G) %&gt;%\n  lm(R ~ BB + HR, data = .)\n\nWe can use tidy to see a nice summary:\n\ntidy(fit, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.74     0.0824      21.2 7.62e- 83    1.58      1.91 \n2 BB             0.387    0.0270      14.3 1.20e- 42    0.334     0.440\n3 HR             1.56     0.0490      31.9 1.78e-155    1.47      1.66 \n\n\nWhen we fit the model with only one variable, the estimated slopes were 0.7353288 and 1.8448241 for BB and HR, respectively. Note that when fitting the multivariate model both go down, with the BB effect decreasing much more.\n\nInterpreting Multiple Regression Coefficients\nHow do we now interpret the coefficients from the regression? We are using regression to condition on values of the \\(x\\) variables (of which there are now two). So, the coefficient on \\(BB\\) is interpreted as the associated change in the \\(E[R]\\) per 1-unit increase in \\(BB\\), holding all else equal. That means holding the \\(\\varepsilon\\) present in every regression constant, and it means holding \\(HR\\) (the other variable) constant. The latin phrase ceteris paribus is often used, meaning “holding all else equal”.\n\n\nConstructing a metric to pick players\nNow we want to construct a metric to pick players, we need to consider singles, doubles, and triples as well. Can we build a model that predicts runs based on all these outcomes?\nWe now are going to take somewhat of a “leap of faith” and assume that these five variables are jointly normal. This means that if we pick any one of them, and hold the other four fixed, the relationship with the outcome is linear and the slope does not depend on the four values held constant. If this is true, then a linear model for our data is:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\beta_3 x_{i,3}+ \\beta_4 x_{i,4} + \\beta_5 x_{i,5} + \\varepsilon_i\n\\]\nwith \\(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\\) representing BB, singles, doubles, triples, and HR respectively.\nUsing lm, we can quickly find the LSE for the parameters using:\n\nfit &lt;- Teams %&gt;%\n  filter(yearID %in% 1961:2001) %&gt;%\n  mutate(BB = BB / G,\n         singles = (H - X2B - X3B - HR) / G,\n         doubles = X2B / G,\n         triples = X3B / G,\n         HR = HR / G,\n         R = R / G) %&gt;%\n  lm(R ~ BB + singles + doubles + triples + HR, data = .)\n\nWe can see the coefficients using tidy:\n\ncoefs &lt;- tidy(fit, conf.int = TRUE)\n\ncoefs\n\n# A tibble: 6 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -2.77     0.0862     -32.1 4.76e-157   -2.94     -2.60 \n2 BB             0.371    0.0117      31.6 1.87e-153    0.348     0.394\n3 singles        0.519    0.0127      40.8 8.67e-217    0.494     0.544\n4 doubles        0.771    0.0226      34.1 8.44e-171    0.727     0.816\n5 triples        1.24     0.0768      16.1 2.12e- 52    1.09      1.39 \n6 HR             1.44     0.0243      59.3 0            1.40      1.49 \n\n\nTo see how well our metric actually predicts runs, we can predict the number of runs for each team in 2002 using the function predict, then make a plot:\n\nTeams %&gt;%\n  filter(yearID %in% 2002) %&gt;%\n  mutate(BB = BB/G,\n         singles = (H-X2B-X3B-HR)/G,\n         doubles = X2B/G,\n         triples =X3B/G,\n         HR=HR/G,\n         R=R/G)  %&gt;%\n  mutate(R_hat = predict(fit, newdata = .)) %&gt;%\n  ggplot(aes(R_hat, R, label = teamID)) +\n  geom_point() +\n  geom_text(nudge_x=0.1, cex = 2) +\n  geom_abline()\n\n\n\n\n\n\n\n\nOur model does quite a good job as demonstrated by the fact that points from the observed versus predicted plot fall close to the identity line.\nSo instead of using batting average, or just number of HR, as a measure of picking players, we can use our fitted model to form a metric that relates more directly to run production. Specifically, to define a metric for player A, we imagine a team made up of players just like player A and use our fitted regression model to predict how many runs this team would produce. The formula would look like this: -2.7691857 + 0.3712147 \\(\\times\\) BB + 0.5193923 \\(\\times\\) singles + 0.7711444 \\(\\times\\) doubles + 1.2399696 \\(\\times\\) triples + 1.4433701 \\(\\times\\) HR.\nTo define a player-specific metric, we have a bit more work to do. A challenge here is that we derived the metric for teams, based on team-level summary statistics. For example, the HR value that is entered into the equation is HR per game for the entire team. If we compute the HR per game for a player, it will be much lower since the total is accumulated by 9 batters. Furthermore, if a player only plays part of the game and gets fewer opportunities than average, it is still considered a game played. For players, a rate that takes into account opportunities is the per-plate-appearance rate.\nTo make the per-game team rate comparable to the per-plate-appearance player rate, we compute the average number of team plate appearances per game:\n\npa_per_game &lt;- Batting %&gt;% filter(yearID == 2002) %&gt;%\n  group_by(teamID) %&gt;%\n  summarize(pa_per_game = sum(AB+BB)/max(G)) %&gt;%\n  pull(pa_per_game) %&gt;%\n  mean\n\nWe compute the per-plate-appearance rates for players available in 2002 on data from 1997-2001. To avoid small sample artifacts, we filter players with less than 1,000 plate appearances per year. Here is the entire calculation in one line:\n\nplayers &lt;- Batting %&gt;% filter(yearID %in% 1997:2001) %&gt;%\n  group_by(playerID) %&gt;%\n  mutate(PA = BB + AB) %&gt;%\n  summarize(G = sum(PA)/pa_per_game,\n    BB = sum(BB)/G,\n    singles = sum(H-X2B-X3B-HR)/G,\n    doubles = sum(X2B)/G,\n    triples = sum(X3B)/G,\n    HR = sum(HR)/G,\n    AVG = sum(H)/sum(AB),\n    PA = sum(PA)) %&gt;%\n  filter(PA &gt;= 1000) %&gt;%\n  select(-G) %&gt;%\n  mutate(R_hat = predict(fit, newdata = .))\n\nThe player-specific predicted runs computed here can be interpreted as the number of runs we predict a team will score if all batters are exactly like that player. The distribution shows that there is wide variability across players:\n\nqplot(R_hat, data = players, binwidth = 0.5, color = I(\"black\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTRY IT\n\n\n\nSince the 1980s, sabermetricians have used a summary statistic different from batting average to evaluate players. They realized walks were important and that doubles, triples, and HRs, should be weighed more than singles. As a result, they proposed the following metric:\n\\[\n\\frac{\\mbox{BB}}{\\mbox{PA}} + \\frac{\\mbox{Singles} + 2 \\mbox{Doubles} + 3 \\mbox{Triples} + 4\\mbox{HR}}{\\mbox{AB}}\n\\]\nThey called this on-base-percentage plus slugging percentage (OPS). Although the sabermetricians probably did not use regression, here we show how this metric is close to what one gets with regression.\n\nCompute the OPS for each team in the 2001 season. Then plot Runs per game versus OPS.\nFor every year since 1961, compute the correlation between runs per game and OPS; then plot these correlations as a function of year.\nNote that we can rewrite OPS as a weighted average of BBs, singles, doubles, triples, and HRs. We know that the weights for doubles, triples, and HRs are 2, 3, and 4 times that of singles. But what about BB? What is the weight for BB relative to singles? Hint: the weight for BB relative to singles will be a function of AB and PA.\nNote that the weight for BB, \\(\\frac{\\mbox{AB}}{\\mbox{PA}}\\), will change from team to team. To see how variable it is, compute and plot this quantity for each team for each year since 1961. Then plot it again, but instead of computing it for every team, compute and plot the ratio for the entire year. Then, once you are convinced that there is not much of a time or team trend, report the overall average.\nSo now we know that the formula for OPS is proportional to \\(0.91 \\times \\mbox{BB} + \\mbox{singles} + 2 \\times \\mbox{doubles} + 3 \\times \\mbox{triples} + 4 \\times \\mbox{HR}\\). Let’s see how these coefficients compare to those obtained with regression. Fit a regression model to the data after 1961, as done earlier: using per game statistics for each year for each team. After fitting this model, report the coefficients as weights relative to the coefficient for singles.\nWe see that our linear regression model coefficients follow the same general trend as those used by OPS, but with slightly less weight for metrics other than singles. For each team in years after 1961, compute the OPS, the predicted runs with the regression model and compute the correlation between the two as well as the correlation with runs per game.",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression II"
    ]
  },
  {
    "objectID": "content/Week_07/07a.html#bonus-but-im-really-into-moneyball",
    "href": "content/Week_07/07a.html#bonus-but-im-really-into-moneyball",
    "title": "Linear Regression II",
    "section": "BONUS: But I’m really into moneyball!",
    "text": "BONUS: But I’m really into moneyball!\nIf you’re interested in how we might build the best team for the dollar, keep reading.\n\nAdding salary and position information\nTo actually build the team, we will need to know their salaries as well as their defensive position. For this, we join the players data frame we just created with the player information data frame included in some of the other Lahman data tables. We will learn more about the join function (and we will discuss this further in a later lecture). For now, we just need to know that a join matches a “key” field that is shared between the two datasets. Here, it is playerID.\nEach join consists of two datasets, a shared key (or keys), and a type of join. A right join takes any rows in X that match rows in Y, and all rows in Y. A left join takes all rows of X and any rows of Y that match. With left and right joins, you may end up with observations where some columns do not have data. R will give these an “NA”.\nAn inner join takes only the rows in X and Y that match, so all observations will have data. A full join takes all rows in X and Y (and will give you lots of NAs if they don’t all match).\nIf more than one observation in Y matches the key field in X (or vice-versa), then you can get duplicated observations. We’ll cover joins more later. For now, we’ll just use right and left joins on data we know is safe to merge.\nStart by adding the 2002 salary of each player:\n\nplayers &lt;- Salaries %&gt;%\n  filter(yearID == 2002) %&gt;%\n  select(playerID, salary) %&gt;%\n  right_join(players, by=\"playerID\")\n\nNext, we add their defensive position. This is a somewhat complicated task because players play more than one position each year. The Lahman package table Appearances tells how many games each player played in each position, so we can pick the position that was most played using which.max on each row. We use apply to do this. However, because some players are traded, they appear more than once on the table, so we first sum their appearances across teams. Here, we pick the one position the player most played using the top_n function. To make sure we only pick one position, in the case of ties, we pick the first row of the resulting data frame. We also remove the OF position which stands for outfielder, a generalization of three positions: left field (LF), center field (CF), and right field (RF). We also remove pitchers since they don’t bat in the league in which the A’s play.\n\nposition_names &lt;-\n  paste0(\"G_\", c(\"p\",\"c\",\"1b\",\"2b\",\"3b\",\"ss\",\"lf\",\"cf\",\"rf\", \"dh\"))\n\ntmp &lt;- Appearances %&gt;%\n  filter(yearID == 2002) %&gt;%\n  group_by(playerID) %&gt;%\n  summarize_at(position_names, sum) %&gt;%\n  ungroup()\n\npos &lt;- tmp %&gt;%\n  select(all_of(position_names)) %&gt;% # all_of lets us use an external vector of position names to select\n  apply(., MAR = 1, which.max) # which.max gives us the column number of the position that is the max\n\nplayers &lt;- tibble(playerID = tmp$playerID, POS = position_names[pos]) %&gt;%\n  mutate(POS = str_to_upper(str_remove(POS, \"G_\"))) %&gt;%\n  filter(POS != \"P\") %&gt;%\n  right_join(players, by=\"playerID\") %&gt;%\n  filter(!is.na(POS)  & !is.na(salary))\n\nFinally, we add their first and last name:\n\nplayers &lt;- People %&gt;%\n  select(playerID, nameFirst, nameLast, debut) %&gt;%\n  mutate(debut = as.Date(debut)) %&gt;%\n  right_join(players, by=\"playerID\")\n\nIf you are a baseball fan (or were years ago), you will recognize the top 10 players:\n\nplayers %&gt;% select(nameFirst, nameLast, POS, salary, R_hat) %&gt;%\n  arrange(desc(R_hat)) %&gt;% top_n(10)\n\n   nameFirst nameLast POS   salary    R_hat\n1      Barry    Bonds  LF 15000000 8.441480\n2      Larry   Walker  RF 12666667 8.344316\n3       Todd   Helton  1B  5000000 7.764649\n4      Manny  Ramirez  LF 15462727 7.714582\n5      Sammy     Sosa  RF 15000000 7.559582\n6       Jeff  Bagwell  1B 11000000 7.405572\n7       Mike   Piazza   C 10571429 7.343984\n8      Jason   Giambi  1B 10428571 7.263690\n9      Edgar Martinez  DH  7086668 7.259399\n10       Jim    Thome  1B  8000000 7.231955\n\n\n\n\nPicking nine players\nOn average, players with a higher metric have higher salaries:\n\nplayers %&gt;% ggplot(aes(salary, R_hat, color = POS)) +\n  geom_point() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nWe can search for good deals by looking at players who produce many more runs than others with similar salaries. We can use this table to decide what players to pick and keep our total salary below the 40 million dollars Billy Beane had to work with. This can be done using what computer scientists call linear programming. This is not something we teach, but here are the position players selected with this approach:\n\n\n\n\n\nnameFirst\nnameLast\nPOS\nsalary\nR_hat\n\n\n\n\nTodd\nHelton\n1B\n5000000\n7.764649\n\n\nMike\nPiazza\nC\n10571429\n7.343984\n\n\nEdgar\nMartinez\nDH\n7086668\n7.259399\n\n\nJim\nEdmonds\nCF\n7333333\n6.552456\n\n\nJeff\nKent\n2B\n6000000\n6.391614\n\n\nPhil\nNevin\n3B\n2600000\n6.163936\n\n\nMatt\nStairs\nRF\n500000\n6.062372\n\n\nHenry\nRodriguez\nLF\n300000\n5.938315\n\n\nJohn\nValentin\nSS\n550000\n5.273441\n\n\n\n\n\n\n\nWe see that all these players have above average BB and most have above average HR rates, while the same is not true for singles. Here is a table with statistics standardized across players so that, for example, above average HR hitters have values above 0.\n\n\n\n\n\nnameLast\nBB\nsingles\ndoubles\ntriples\nHR\nAVG\nR_hat\n\n\n\n\nHelton\n0.9088340\n-0.2147828\n2.6489997\n-0.3105275\n1.5221254\n2.6704562\n2.5316660\n\n\nPiazza\n0.3281058\n0.4231217\n0.2037161\n-1.4181571\n1.8253653\n2.1990055\n2.0890701\n\n\nMartinez\n2.1352215\n-0.0051702\n1.2649044\n-1.2242578\n0.8079817\n2.2032836\n2.0000756\n\n\nEdmonds\n1.0706548\n-0.5579104\n0.7912381\n-1.1517126\n0.9730052\n0.8543566\n1.2562767\n\n\nKent\n0.2316321\n-0.7322902\n2.0113988\n0.4483097\n0.7658693\n0.7871932\n1.0870488\n\n\nNevin\n0.3066863\n-0.9051225\n0.4787634\n-1.1908955\n1.1927055\n0.1048721\n0.8475017\n\n\nStairs\n1.0996635\n-1.5127562\n-0.0460876\n-1.1285395\n1.1209081\n-0.5608456\n0.7406428\n\n\nRodriguez\n0.2011513\n-1.5963595\n0.3324557\n-0.7823620\n1.3202734\n-0.6723416\n0.6101181\n\n\nValentin\n0.1802855\n-0.9287069\n1.7940379\n-0.4348410\n-0.0452462\n-0.4717038\n-0.0894187",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression II"
    ]
  },
  {
    "objectID": "content/Week_07/07a.html#the-regression-fallacy",
    "href": "content/Week_07/07a.html#the-regression-fallacy",
    "title": "Linear Regression II",
    "section": "The regression fallacy",
    "text": "The regression fallacy\nWikipedia defines the sophomore slump as:\n\nA sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels).\n\nIn Major League Baseball, the rookie of the year (ROY) award is given to the first-year player who is judged to have performed the best. The sophmore slump phrase is used to describe the observation that ROY award winners don’t do as well during their second year. For example, this Fox Sports article9 asks “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”.\nDoes the data confirm the existence of a sophomore slump? Let’s take a look. Examining the data for batting average, we see that this observation holds true for the top performing ROYs:\n\n\n\n\n\n\n\n\n\nnameFirst\nnameLast\nrookie_year\nrookie\nsophomore\n\n\n\n\nWillie\nMcCovey\n1959\n0.3541667\n0.2384615\n\n\nIchiro\nSuzuki\n2001\n0.3497110\n0.3214838\n\n\nAl\nBumbry\n1973\n0.3370787\n0.2333333\n\n\nFred\nLynn\n1975\n0.3314394\n0.3136095\n\n\nAlbert\nPujols\n2001\n0.3288136\n0.3135593\n\n\n\n\n\n\n\nIn fact, the proportion of players that have a lower batting average their sophomore year is 0.7090909.\nSo is it “jitters” or “jinx”? To answer this question, let’s turn our attention to all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year).\n\nThe same pattern arises when we look at the top performers: batting averages go down for most of the top performers.\n\n\n\n\n\nnameFirst\nnameLast\n2013\n2014\n\n\n\n\nMiguel\nCabrera\n0.3477477\n0.3126023\n\n\nHanley\nRamirez\n0.3453947\n0.2828508\n\n\nMichael\nCuddyer\n0.3312883\n0.3315789\n\n\nScooter\nGennett\n0.3239437\n0.2886364\n\n\nJoe\nMauer\n0.3235955\n0.2769231\n\n\n\n\n\n\n\nBut these are not rookies! Also, look at what happens to the worst performers of 2013:\n\n\n\n\n\nnameFirst\nnameLast\n2013\n2014\n\n\n\n\nDanny\nEspinosa\n0.1582278\n0.2192192\n\n\nDan\nUggla\n0.1785714\n0.1489362\n\n\nJeff\nMathis\n0.1810345\n0.2000000\n\n\nB. J.\nUpton\n0.1841432\n0.2080925\n\n\nAdam\nRosales\n0.1904762\n0.2621951\n\n\n\n\n\n\n\nTheir batting averages mostly go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as the sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect:\n\n\n\n\n\n\n\n\n\nThe correlation is 0.460254 and the data look very much like a bivariate normal distribution, which means we predict a 2014 batting average \\(Y\\) for any given player that had a 2013 batting average \\(X\\) with:\n\\[ \\frac{Y - .255}{.032} = 0.46 \\left( \\frac{X - .261}{.023}\\right) \\]\nBecause the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It’s not a jinx; it’s just due to chance. The ROY are selected from the top values of \\(X\\) so it is expected that \\(Y\\) will regress to the mean.",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression II"
    ]
  },
  {
    "objectID": "content/Week_07/07a.html#footnotes",
    "href": "content/Week_07/07a.html#footnotes",
    "title": "Linear Regression II",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttp://mlb.mlb.com/stats/league_leaders.jsp↩︎\nhttps://en.wikipedia.org/wiki/Bill_James↩︎\nhttps://en.wikipedia.org/wiki/Sabermetrics↩︎\nhttps://en.wikipedia.org/wiki/User:Cburnett↩︎\nhttps://creativecommons.org/licenses/by-sa/3.0/deed.en↩︎\nhttps://www.flickr.com/people/27003603@N00↩︎\nhttps://creativecommons.org/licenses/by-sa/2.0↩︎\nhttp://www.baseball-almanac.com/awards/lou_brock_award.shtml↩︎\nhttp://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715↩︎",
    "crumbs": [
      "Course Content",
      "Week 07",
      "Linear Regression II"
    ]
  },
  {
    "objectID": "content/Week_08/08a.html",
    "href": "content/Week_08/08a.html",
    "title": "Linear Regression III",
    "section": "",
    "text": "This page.\n\n\n\n\nWhen can we make causal claims about the relationship between variables?\n\nAs with recent weeks, we will work with real data during the lecture. Please download the following dataset and load it into R.\n\n Ames.csv",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Linear Regression III"
    ]
  },
  {
    "objectID": "content/Week_08/08a.html#required-reading",
    "href": "content/Week_08/08a.html#required-reading",
    "title": "Linear Regression III",
    "section": "",
    "text": "This page.\n\n\n\n\nWhen can we make causal claims about the relationship between variables?\n\nAs with recent weeks, we will work with real data during the lecture. Please download the following dataset and load it into R.\n\n Ames.csv",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Linear Regression III"
    ]
  },
  {
    "objectID": "content/Week_08/08a.html#a-quick-r-note",
    "href": "content/Week_08/08a.html#a-quick-r-note",
    "title": "Linear Regression III",
    "section": "A quick R note:",
    "text": "A quick R note:\nWe learned that we can run a regression with all variables in a dataset using, for example, SalePrice ~ .\nIn your lab this week, you are asked to run 15 models (!) of increasing complexity, where “complexity” is defined in one question as adding an additional linear term. So if you had \\(X1\\), \\(X2\\), and \\(X3\\) as explanatory variables, you might want to have a model that is:\n\\[Y = \\beta_0 + \\beta_1 X1\\]\nand then\n\\[Y = \\beta_0 + \\beta_1 X1 + \\beta_2 X2\\]\nand so on. How you do so is up to you – you could do a list of formulas constructed with paste, then use lapply to that list. Or you could wisely use the ~ . part of the formula and change the columns in the data you pass to lm. Here are a few tips pertaining to some of the ways of doing this.\nIf you have a dataset called DesMoines and a character vector of all of your columns in DesMoines like so varnames = c('Y','X1','X2','X3'), and it’s in the order in which you’d like to add the variables to the model, then:\nDesMoines %&gt;% dplyr::select(varnames[1:N])\nwould select the first N variables. A loop on N from 2 to 4 would give you c('Y','X1') then c('Y','X1','X2') and so on.\nAnother handy tip is using paste with collapse = '+'. This will let you include interactions\n\nvarnames = c('Y','X1','X2','X3','X1:X3','X4*X5')\n\npaste(varnames[2:4], collapse = ' + ')\n\n[1] \"X1 + X2 + X3\"\n\n\nThis will put all of the things in the vector together with + between them. Wisely pasting Y ~ onto this, then using as.formula() lets you make a formula object from a character vector. Handy!",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Linear Regression III"
    ]
  },
  {
    "objectID": "content/Week_08/08a.html#spurious-correlation",
    "href": "content/Week_08/08a.html#spurious-correlation",
    "title": "Linear Regression III",
    "section": "Spurious correlation",
    "text": "Spurious correlation\nThe following comical example underscores that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption.\n\n\n\n\n\n\n\n\n\nDoes this mean that margarine causes divorces? Or do divorces cause people to eat more margarine? Of course the answer to both these questions is no. This is just an example of what we call a spurious correlation.\nYou can see many more absurd examples on the Spurious Correlations website1.\nThe cases presented in the spurious correlation site are all instances of what is generally called data dredging, data fishing, or data snooping. It’s basically a form of what in the US we call cherry picking. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.\nA Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble:\n\nN &lt;- 25\ng &lt;- 1000000\nsim_data &lt;- tibble(group = rep(1:g, each=N),\n                   x = rnorm(N * g),\n                   y = rnorm(N * g))\n\nThe first column denotes group. We created groups and for each one we generated a pair of independent vectors, \\(X\\) and \\(Y\\), with 25 observations each, stored in the second and third columns. Because we constructed the simulation, we know that \\(X\\) and \\(Y\\) are not correlated.\nNext, we compute the correlation between X and Y for each group and look at the max:\n\nres &lt;- sim_data %&gt;%\n  group_by(group) %&gt;%\n  summarize(r = cor(x, y)) %&gt;%\n  arrange(desc(r))\nres\n\n# A tibble: 1,000,000 × 2\n    group     r\n    &lt;int&gt; &lt;dbl&gt;\n 1 648278 0.852\n 2 404745 0.831\n 3 239061 0.768\n 4 979172 0.766\n 5 565886 0.763\n 6 496951 0.759\n 7 165110 0.756\n 8 916022 0.751\n 9 635400 0.749\n10 655316 0.749\n# ℹ 999,990 more rows\n\n\nWe see a maximum correlation of 0.852 and if you just plot the data from the group achieving this correlation, it shows a convincing plot that \\(X\\) and \\(Y\\) are in fact correlated:\n\nsim_data %&gt;% filter(group == res$group[which.max(res$r)]) %&gt;%\n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nRemember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation:\n\nres %&gt;% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = \"black\")\n\n\n\n\n\n\n\n\nIt’s just a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.2041623, the largest one will be close to 1.\nIf we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:\n\nlibrary(broom)\nsim_data %&gt;%\n  filter(group == res$group[which.max(res$r)]) %&gt;%\n  do(tidy(lm(y ~ x, data = .))) %&gt;%\n  filter(term == \"x\")\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic      p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 x        0.602    0.0773      7.79 0.0000000670\n\n\nNow, imagine that instead of a whole lot of simulated data, you had a whole lot of actual data and waded through enough of it to find two unrelated variables that happened to show up as correlated (like divorce rates and pounds of margarine consumed). This particular form of data dredging is referred to as p-hacking. P-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results. In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and a lot of different variables that represent exposures and report only the one exposure that resulted in a small p-value. Furthermore, they might try fitting several different models to account for confounding and pick the one that yields the smallest p-value. In experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported. This does not necessarily happen due to unethical behavior, but rather as a result of statistical ignorance or wishful thinking. In advanced statistics courses, you can learn methods to adjust for these multiple comparisons.",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Linear Regression III"
    ]
  },
  {
    "objectID": "content/Week_08/08a.html#outliers",
    "href": "content/Week_08/08a.html#outliers",
    "title": "Linear Regression III",
    "section": "Outliers",
    "text": "Outliers\nSuppose we take measurements from two independent outcomes, \\(X\\) and \\(Y\\), and we standardize the measurements. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using:\n\nset.seed(1985)\nx &lt;- rnorm(100,100,1)\ny &lt;- rnorm(100,84,1)\nx[-23] &lt;- scale(x[-23])\ny[-23] &lt;- scale(y[-23])\n\nThe data look like this:\n\nggplot() +\n  geom_point(aes(x,y))\n\n\n\n\n\n\n\n\nNot surprisingly, the correlation is very high:\n\ncor(x,y)\n\n[1] 0.9878382\n\n\nBut this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:\n\ncor(x[-23], y[-23])\n\n[1] -0.04419032\n\n\nPreviously, we (briefly) described alternatives to the average and standard deviation that are robust to outliers. There is also an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called Spearman correlation. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:\n\nggplot() +\n  geom_point(aes(x = rank(x), y = rank(y)))\n\n\n\n\n\n\n\n\nThe outlier is no longer associated with a very large value and the correlation comes way down:\n\ncor(rank(x), rank(y))\n\n[1] 0.002508251\n\n\nSpearman correlation can also be calculated like this:\n\ncor(x, y, method = \"spearman\")\n\n[1] 0.002508251\n\n\nThere are also methods for robust fitting of linear models which you can learn about in, for instance, this book: Robust Statistics: Edition 2 by Peter J. Huber & Elvezio M. Ronchetti.",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Linear Regression III"
    ]
  },
  {
    "objectID": "content/Week_08/08a.html#reversing-cause-and-effect",
    "href": "content/Week_08/08a.html#reversing-cause-and-effect",
    "title": "Linear Regression III",
    "section": "Reversing cause and effect",
    "text": "Reversing cause and effect\nAnother way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.\nA form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated2. Consider this quote from the article:\n\nWhen we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades… Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.\n\nA very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school.\nWe can easily construct an example of cause and effect reversal using the father and son height data. If we fit the model:\n\\[X_i = \\beta_0 + \\beta_1 y_i + \\varepsilon_i, i=1, \\dots, N\\]\nto the father and son height data, with \\(X_i\\) the father height and \\(y_i\\) the son height, we do get a statistically significant result:\n\nlibrary(HistData)\ndata(\"GaltonFamilies\")\nGaltonFamilies %&gt;%\n  filter(childNum == 1 & gender == \"male\") %&gt;%\n  select(father, childHeight) %&gt;%\n  rename(son = childHeight) %&gt;%\n  do(tidy(lm(father ~ son, data = .)))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   34.0      4.57        7.44 4.31e-12\n2 son            0.499    0.0648      7.70 9.47e-13\n\n\nThe model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted so as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it’s the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the interpretation.",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Linear Regression III"
    ]
  },
  {
    "objectID": "content/Week_08/08a.html#confounders",
    "href": "content/Week_08/08a.html#confounders",
    "title": "Linear Regression III",
    "section": "Confounders",
    "text": "Confounders\nConfounders are perhaps the most common reason that leads to associations begin misinterpreted.\nIf \\(X\\) and \\(Y\\) are correlated, we call \\(Z\\) a confounder if changes in \\(Z\\) causes changes in both \\(X\\) and \\(Y\\). Earlier, when studying baseball data, we saw how Home Runs was a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs. In some cases, we can use linear models to account for confounders. However, this is not always the case.\nIncorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions.\n\nExample: UC Berkeley admissions\nAdmission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). We can load the data and calculate the “headline” number:\n\nlibrary(dslabs)\ndata(admissions)\nadmissions %&gt;% group_by(gender) %&gt;%\n  summarize(percentage =\n              round(sum(admitted*applicants)/sum(applicants),1))\n\n# A tibble: 2 × 2\n  gender percentage\n  &lt;chr&gt;       &lt;dbl&gt;\n1 men          44.5\n2 women        30.3\n\n# Note: \"admitted\" is PERCENT admitted\n\nThe chi-squared test compares two groups with binary outcomes (like “admit” and “nonadmit”). The null hypothesis is that the groups are not differently distributed between the outcomes. Here’s what the data looks like going in – its in counts of “admitted” and “not admitted”.\n\nadmissions %&gt;% group_by(gender) %&gt;%\n  summarize(total_admitted = round(sum(admitted / 100 * applicants)),\n            not_admitted = sum(applicants) - sum(total_admitted)) %&gt;%\n  select(-gender) \n\n# A tibble: 2 × 2\n  total_admitted not_admitted\n           &lt;dbl&gt;        &lt;dbl&gt;\n1           1198         1493\n2            557         1278\n\n\nA low p-value rejects this hypothesis. Here, the test clearly rejects the hypothesis that gender and admission are independent:\n\nadmissions %&gt;% group_by(gender) %&gt;%\n  summarize(total_admitted = round(sum(admitted / 100 * applicants)),\n            not_admitted = sum(applicants) - sum(total_admitted)) %&gt;%\n  select(-gender) %&gt;%\n  do(tidy(chisq.test(.))) %&gt;% .$p.value\n\n[1] 1.055797e-21\n\n\nBut closer inspection shows a paradoxical result. Here are the percent admissions by major:\n\nadmissions %&gt;% select(major, gender, admitted) %&gt;%\n  spread(gender, admitted) %&gt;%\n  mutate(women_minus_men_pct = women - men)\n\n  major men women women_minus_men_pct\n1     A  62    82                  20\n2     B  63    68                   5\n3     C  37    34                  -3\n4     D  33    35                   2\n5     E  28    24                  -4\n6     F   6     7                   1\n\n\nFour out of the six majors favor women. More importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals.\nThe paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability.\nSo let’s define three variables: \\(X\\) is 1 for men and 0 for women, \\(Y\\) is 1 for those admitted and 0 otherwise, and \\(Z\\) quantifies the selectivity of the major. A gender bias claim would be based on the fact that \\(\\mbox{Pr}(Y=1 | X = x)\\) is higher for \\(x=1\\) than \\(x=0\\). However, \\(Z\\) is an important confounder to consider. Clearly \\(Z\\) is associated with \\(Y\\), as the more selective a major, the lower \\(\\mbox{Pr}(Y=1 | Z = z)\\). But is major selectivity \\(Z\\) associated with gender \\(X\\)?\nOne way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants:\n\nadmissions %&gt;%\n  group_by(major) %&gt;%\n  summarize(major_selectivity = sum(admitted * applicants)/sum(applicants),\n            percent_women_applicants = sum(applicants * (gender==\"women\")) /\n                                             sum(applicants) * 100) %&gt;%\n  ggplot(aes(major_selectivity, percent_women_applicants, label = major)) +\n  geom_text()\n\n\n\n\n\n\n\n\nThere seems to be association. The plot suggests that women were much more likely to apply to the four “hard” majors: gender and major selectivity are confounded. Compare, for example, major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women, while less than 30% of the applicants of major B were women.\n\n\nConfounding explained graphically\nThe following plot shows the number of applicants that were admitted and those that were not by major and gender:\n\n\n\n\n\n\n\n\n\n\nIt also breaks down the acceptances by major. This breakdown allows us to see that the majority of accepted men came from two majors: A and B. It also lets us see that few women applied to these majors.\n\n\nAverage after stratifying\nIn this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away:\n\nadmissions %&gt;%\n  ggplot(aes(major, admitted, col = gender, size = applicants)) +\n  geom_point() + \n  labs(x = 'Percent Admitted')\n\n\n\n\n\n\n\n\nNow we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest (least selective) majors, A and B.\nIf we average the difference by major, we find that the percent is actually 3.5% higher for women.\n\nadmissions %&gt;%  group_by(gender) %&gt;% summarize(average = mean(admitted))\n\n# A tibble: 2 × 2\n  gender average\n  &lt;chr&gt;    &lt;dbl&gt;\n1 men       38.2\n2 women     41.7",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Linear Regression III"
    ]
  },
  {
    "objectID": "content/Week_08/08a.html#simpsons-paradox",
    "href": "content/Week_08/08a.html#simpsons-paradox",
    "title": "Linear Regression III",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\nThe case we have just covered is an example of Simpson’s paradox. It is called a paradox because we see the sign of the correlation flip when comparing the entire publication and specific strata. As an illustrative example, suppose you have three random variables \\(X\\), \\(Y\\), and \\(Z\\) and that we observe realizations of these. Here is a plot of simulated observations for \\(X\\) and \\(Y\\) along with the sample correlation:\n\n\n\n\n\n\n\n\n\nYou can see that \\(X\\) and \\(Y\\) are negatively correlated. However, once we stratify by \\(Z\\) (shown in different colors below) another pattern emerges:\n\n\n\n\n\n\n\n\n\nIt is really \\(Z\\) that is negatively correlated with \\(X\\). If we stratify by \\(Z\\), the \\(X\\) and \\(Y\\) are actually positively correlated as seen in the plot above.\nHow do we stratify in regression? Depending on the relationship we think the variables have (conditional on \\(Z\\)). In the above example, it looks like an intercept shift would account for the differences across \\(Z\\) (note that the plot shows the correlation between \\(Y\\) and \\(X\\) for each value of \\(Z\\)):\n\nsummary(lm(y ~ x + as.factor(z), data = dat))\n\n\nCall:\nlm(formula = y ~ x + as.factor(z), data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6120 -0.5354  0.0269  0.5750  1.9138 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -5.78432    0.34026  -17.00   &lt;2e-16 ***\nx              0.78805    0.02999   26.28   &lt;2e-16 ***\nas.factor(z)2  3.66771    0.12976   28.27   &lt;2e-16 ***\nas.factor(z)3  7.24256    0.16855   42.97   &lt;2e-16 ***\nas.factor(z)4 10.73651    0.21520   49.89   &lt;2e-16 ***\nas.factor(z)5 14.43156    0.26816   53.82   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.824 on 494 degrees of freedom\nMultiple R-squared:  0.9301,    Adjusted R-squared:  0.9294 \nF-statistic:  1315 on 5 and 494 DF,  p-value: &lt; 2.2e-16\n\n\nIf we thought that the relationship between \\(Y\\) and \\(X\\) were different for each \\(Z\\) (not just the intercept), we could try allowing a slope-shift using interactions:\n\nsummary(lm(y ~ x*as.factor(z), data = dat))\n\n\nCall:\nlm(formula = y ~ x * as.factor(z), data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6383 -0.5431  0.0197  0.5581  1.9655 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -6.22140    0.72249  -8.611  &lt; 2e-16 ***\nx                0.82776    0.06520  12.696  &lt; 2e-16 ***\nas.factor(z)2    4.55366    0.95253   4.781 2.31e-06 ***\nas.factor(z)3    8.28606    0.88257   9.389  &lt; 2e-16 ***\nas.factor(z)4   11.02669    0.78302  14.082  &lt; 2e-16 ***\nas.factor(z)5   14.70047    0.76182  19.296  &lt; 2e-16 ***\nx:as.factor(z)2 -0.08900    0.09390  -0.948    0.344    \nx:as.factor(z)3 -0.12699    0.09712  -1.307    0.192    \nx:as.factor(z)4 -0.01018    0.08751  -0.116    0.907    \nx:as.factor(z)5  0.01720    0.10079   0.171    0.865    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8247 on 490 degrees of freedom\nMultiple R-squared:  0.9306,    Adjusted R-squared:  0.9293 \nF-statistic: 729.8 on 9 and 490 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Linear Regression III"
    ]
  },
  {
    "objectID": "content/Week_08/08a.html#footnotes",
    "href": "content/Week_08/08a.html#footnotes",
    "title": "Linear Regression III",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttp://tylervigen.com/spurious-correlations↩︎\nhttps://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated↩︎",
    "crumbs": [
      "Course Content",
      "Week 08",
      "Linear Regression III"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html",
    "href": "content/Week_09/09a.html",
    "title": "Probability and Statistics",
    "section": "",
    "text": "This page.\n\n\n\n\n Why It’s So Hard for Us to Visualize Uncertainty\n Amanda Cox’s keynote address at the 2017 OpenVis Conf\n Communicating Uncertainty When Lives Are on the Line\n Showing uncertainty during the live election forecast & Trolling the uncertainty dial\n\n\n\n\n\nWhy is uncertainty inherently a major part of data analytics?\nHow have past attempts to visualize uncertainty failed?\nWhat is the right way to visualize election uncertainty?",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html#required-reading",
    "href": "content/Week_09/09a.html#required-reading",
    "title": "Probability and Statistics",
    "section": "",
    "text": "This page.\n\n\n\n\n Why It’s So Hard for Us to Visualize Uncertainty\n Amanda Cox’s keynote address at the 2017 OpenVis Conf\n Communicating Uncertainty When Lives Are on the Line\n Showing uncertainty during the live election forecast & Trolling the uncertainty dial\n\n\n\n\n\nWhy is uncertainty inherently a major part of data analytics?\nHow have past attempts to visualize uncertainty failed?\nWhat is the right way to visualize election uncertainty?",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html#monte-carlo-simulations-for-categorical-data",
    "href": "content/Week_09/09a.html#monte-carlo-simulations-for-categorical-data",
    "title": "Probability and Statistics",
    "section": "Monte Carlo simulations for categorical data",
    "text": "Monte Carlo simulations for categorical data\nComputers provide a way to actually perform the simple random experiment described above: pick a bead at random from a bag that contains three blue beads and two red ones. Random number generators permit us to mimic the process of picking at random.\nAn example is the sample function in R. We demonstrate its use in the code below. First, we use the function rep to generate the urn:\n\nbeads &lt;- rep(c(\"red\", \"blue\"), times = c(2,3))\nbeads\n\n[1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n\n\nand then use sample to pick a bead at random:\n\nsample(beads, 1)\n\n[1] \"blue\"\n\n\nThis line of code produces one random outcome. We want to repeat this experiment an infinite number of times, but it is impossible to repeat forever. Instead, we repeat the experiment a large enough number of times to make the results practically equivalent to repeating forever. This is an example of a Monte Carlo simulation.\nMuch of what mathematical and theoretical statisticians study, which we do not cover in this class, relates to providing rigorous definitions of “practically equivalent” as well as studying how close a large number of experiments gets us to what happens in the limit. Later in this lecture, we provide a practical approach to deciding what is “large enough”.\nTo perform our first Monte Carlo simulation, we use the replicate function, which permits us to repeat the same task any number of times. Here, we repeat the random event \\(B =\\) 10,000 times:\n\nB &lt;- 10000\nevents &lt;- replicate(B, sample(beads, 1))\n\nWe can now see if our definition actually is in agreement with this Monte Carlo simulation approximation. We can use table to see the distribution:\n\ntab &lt;- table(events)\ntab\n\nevents\nblue  red \n6041 3959 \n\n\nand prop.table gives us the proportions:\n\nprop.table(tab)\n\nevents\n  blue    red \n0.6041 0.3959 \n\n\nThe numbers above are the estimated probabilities provided by this Monte Carlo simulation. Statistical theory, not covered here, tells us that as \\(B\\) gets larger, the estimates get closer to 3/5=.6 and 2/5=.4.\nAlthough this is a simple and not very useful example, we will use Monte Carlo simulations to estimate probabilities in cases in which it is harder to compute the exact ones. Before delving into more complex examples, we use simple ones to demonstrate the computing tools available in R.\n\nSetting the random seed\nBefore we continue, we will briefly explain the following important line of code:\n\nset.seed(1986)\n\nThroughout this class, we use random number generators. This implies that many of the results presented can actually change by chance, which then suggests that a frozen version of the class may show a different result than what you obtain when you try to code as shown in the class. This is actually fine since the results are random and change from time to time. However, if you want to ensure that results are exactly the same every time you run them, you can set R’s random number generation seed to a specific number. Above we set it to 1986. We want to avoid using the same seed everytime. A popular way to pick the seed is the year - month - day. For example, we picked 1986 on December 20, 2018: \\(2018 - 12 - 20 = 1986\\).\nYou can learn more about setting the seed by looking at the documentation:\n\n?set.seed\n\nIn the exercises, we may ask you to set the seed to assure that the results you obtain are exactly what we expect them to be.\n\n\nWith and without replacement\nThe function sample has an argument that permits us to pick more than one element from the urn. However, by default, this selection occurs without replacement: after a bead is selected, it is not put back in the bag. Notice what happens when we ask to randomly select five beads:\n\nsample(beads, 5)\n\n[1] \"red\"  \"blue\" \"blue\" \"blue\" \"red\" \n\nsample(beads, 5)\n\n[1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n\nsample(beads, 5)\n\n[1] \"blue\" \"red\"  \"blue\" \"red\"  \"blue\"\n\n\nThis results in rearrangements that always have three blue and two red beads. If we ask that six beads be selected, we get an error:\n\nsample(beads, 6)\n\nError in sample.int(length(x), size, replace, prob) :   cannot take a sample larger than the population when 'replace = FALSE'\nHowever, the sample function can be used directly, without the use of replicate, to repeat the same experiment of picking 1 out of the 5 beads, continually, under the same conditions. To do this, we sample with replacement: return the bead back to the urn after selecting it. We can tell sample to do this by changing the replace argument, which defaults to FALSE, to replace = TRUE:\n\nevents &lt;- sample(beads, B, replace = TRUE)\nprop.table(table(events))\n\nevents\n  blue    red \n0.6017 0.3983 \n\n\nNot surprisingly, we get results very similar to those previously obtained with replicate.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html#independence",
    "href": "content/Week_09/09a.html#independence",
    "title": "Probability and Statistics",
    "section": "Independence",
    "text": "Independence\nWe say two events are independent if the outcome of one does not affect the other. The classic example is coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement. In the example above, the probability of red is 0.40 regardless of previous draws.\nMany examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities: Ace, Deuce, Three, \\(\\dots\\), Ten, Jack, Queen, King, and Ace. Now if we deal a King for the first card, and don’t replace it into the deck, the probabilities of a second card being a King is less because there are only three Kings left: the probability is 3 out of 51. These events are therefore not independent: the first outcome affected the next one.\nTo see an extreme case of non-independent events, consider our example of drawing five beads at random without replacement:\n\nx &lt;- sample(beads, 5)\n\nIf you have to guess the color of the first bead, you will predict blue since blue has a 60% chance. But if I show you the result of the last four outcomes:\n\nx[2:5]\n\n[1] \"blue\" \"blue\" \"blue\" \"red\" \n\n\nwould you still guess blue? Of course not. Now you know that the probability of red is 1 since the only bead left is red. The events are not independent, so the probabilities change.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html#conditional-probabilities",
    "href": "content/Week_09/09a.html#conditional-probabilities",
    "title": "Probability and Statistics",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\nWhen events are not independent, conditional probabilities are useful. We already saw an example of a conditional probability: we computed the probability that a second dealt card is a King given that the first was a King. In probability, we use the following notation:\n\\[\n\\mbox{Pr}(\\mbox{Card 2 is a king} \\mid \\mbox{Card 1 is a king}) = 3/51\n\\]\nWe use the \\(\\mid\\) as shorthand for “given that” or “conditional on”.\nWhen two events, say \\(A\\) and \\(B\\), are independent, we have:\n\\[\n\\mbox{Pr}(A \\mid B) = \\mbox{Pr}(A)\n\\]\nThis is the mathematical way of saying: the fact that \\(B\\) happened does not affect the probability of \\(A\\) happening. In fact, this can be considered the mathematical definition of independence.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html#addition-and-multiplication-rules",
    "href": "content/Week_09/09a.html#addition-and-multiplication-rules",
    "title": "Probability and Statistics",
    "section": "Addition and multiplication rules",
    "text": "Addition and multiplication rules\n\nMultiplication rule\nIf we want to know the probability of two events, say \\(A\\) and \\(B\\), occurring, we can use the multiplication rule:\n\\[\n\\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\n\\] Let’s use Blackjack as an example. In Blackjack, you are assigned two random cards. After you see what you have, you can ask for more. The goal is to get closer to 21 than the dealer, without going over. Face cards are worth 10 points and Aces are worth 11 or 1 (you choose).\nSo, in a Blackjack game, to calculate the chances of getting a 21 by drawing an Ace and then a face card, we compute the probability of the first being an Ace and multiply by the probability of drawing a face card or a 10 given that the first was an Ace: \\(1/13 \\times 16/51 \\approx 0.025\\)\nThe multiplication rule also applies to more than two events. We can use induction to expand for more events:\n\\[\n\\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\\mbox{Pr}(C \\mid A \\mbox{ and } B)\n\\]\n\n\nMultiplication rule under independence\nWhen we have independent events, then the multiplication rule becomes simpler:\n\\[\n\\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B)\\mbox{Pr}(C)\n\\]\nBut we have to be very careful before using this since assuming independence can result in very different and incorrect probability calculations when we don’t actually have independence.\nAs an example, imagine a court case in which the suspect was described as having a mustache and a beard. The defendant has a mustache and a beard and the prosecution brings in an “expert” to testify that 1/10 men have beards and 1/5 have mustaches, so using the multiplication rule we conclude that only \\(1/10 \\times 1/5\\) or 0.02 have both.\nBut to multiply like this we need to assume independence! Say the conditional probability of a man having a mustache conditional on him having a beard is .95. So the correct calculation probability is much higher: \\(1/10 \\times 95/100 = 0.095\\).\nThe multiplication rule also gives us a general formula for computing conditional probabilities:\n\\[\n\\mbox{Pr}(B \\mid A) = \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)}\n\\]\nTo illustrate how we use these formulas and concepts in practice, we will use several examples related to card games.\n\n\nAddition rule\nThe addition rule tells us that:\n\\[\n\\mbox{Pr}(A \\mbox{ or } B) = \\mbox{Pr}(A) + \\mbox{Pr}(B) - \\mbox{Pr}(A \\mbox{ and } B)\n\\]\nThis rule is intuitive: think of a Venn diagram. If we simply add the probabilities, we count the intersection twice so we need to substract one instance.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html#combinations-and-permutations",
    "href": "content/Week_09/09a.html#combinations-and-permutations",
    "title": "Probability and Statistics",
    "section": "Combinations and permutations",
    "text": "Combinations and permutations\nIn our very first example, we imagined an urn with five beads. As a reminder, to compute the probability distribution of one draw, we simply listed out all the possibilities. There were 5 and so then, for each event, we counted how many of these possibilities were associated with the event. The resulting probability of choosing a blue bead is 3/5 because out of the five possible outcomes, three were blue.\nFor more complicated cases, the computations are not as straightforward. For instance, what is the probability that if I draw five cards without replacement, I get all cards of the same suit, what is known as a “flush” in poker? In a discrete probability course you learn theory on how to make these computations. Here we focus on how to use R code to compute the answers.\nFirst, let’s construct a deck of cards. For this, we will use the expand.grid and paste functions. We use paste to create strings by joining smaller strings. To do this, we take the number and suit of a card and create the card name like this:\n\nnumber &lt;- \"Three\"\nsuit &lt;- \"Hearts\"\npaste(number, suit)\n\n[1] \"Three Hearts\"\n\n\npaste also works on pairs of vectors performing the operation element-wise:\n\npaste(letters[1:5], as.character(1:5))\n\n[1] \"a 1\" \"b 2\" \"c 3\" \"d 4\" \"e 5\"\n\n\nThe function expand.grid gives us all the combinations of entries of two vectors. For example, if you have blue and black pants and white, grey, and plaid shirts, all your combinations are:\n\nexpand.grid(pants = c(\"blue\", \"black\"), shirt = c(\"white\", \"grey\", \"plaid\"))\n\n  pants shirt\n1  blue white\n2 black white\n3  blue  grey\n4 black  grey\n5  blue plaid\n6 black plaid\n\n\nHere is how we generate a deck of cards:\n\nsuits &lt;- c(\"Diamonds\", \"Clubs\", \"Hearts\", \"Spades\")\nnumbers &lt;- c(\"Ace\", \"Deuce\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\",\n             \"Eight\", \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\")\ndeck &lt;- expand.grid(number=numbers, suit=suits)\ndeck &lt;- paste(deck$number, deck$suit)\n\nWith the deck constructed, we can double check that the probability of a King in the first card is 1/13 by computing the proportion of possible outcomes that satisfy our condition:\n\nkings &lt;- paste(\"King\", suits)\nmean(deck %in% kings)\n\n[1] 0.07692308\n\n\nNow, how about the conditional probability of the second card being a King given that the first was a King? Earlier, we deduced that if one King is already out of the deck and there are 51 left, then this probability is 3/51. Let’s confirm by listing out all possible outcomes.\nTo do this, we can use the permutations function from the gtools package. For any list of size n, this function computes all the different combinations we can get when we select r items. Here are all the ways we can choose two numbers from a list consisting of 1,2,3:\n\nlibrary(gtools)\npermutations(3, 2)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    1\n[4,]    2    3\n[5,]    3    1\n[6,]    3    2\n\n\nNotice that the order matters here: 3,1 is different than 1,3. Also, note that (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again.\nOptionally, we can add a vector. If you want to see five random seven digit phone numbers out of all possible phone numbers (without repeats), you can type:\n\nall_phone_numbers &lt;- permutations(10, 7, v = 0:9)\nn &lt;- nrow(all_phone_numbers)\nindex &lt;- sample(n, 5)\nall_phone_numbers[index,]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]    1    3    8    0    6    7    5\n[2,]    2    9    1    6    4    8    0\n[3,]    5    1    6    0    9    8    2\n[4,]    7    4    6    0    2    8    1\n[5,]    4    6    5    9    2    8    0\n\n\nInstead of using the numbers 1 through 10, the default, it uses what we provided through v: the digits 0 through 9.\nTo compute all possible ways we can choose two cards when the order matters, we type:\n\nhands &lt;- permutations(52, 2, v = deck)\n\nThis is a matrix with two columns and 2652 rows. Here’s what it looks like:\n\nhead(hands)\n\n     [,1]        [,2]            \n[1,] \"Ace Clubs\" \"Ace Diamonds\"  \n[2,] \"Ace Clubs\" \"Ace Hearts\"    \n[3,] \"Ace Clubs\" \"Ace Spades\"    \n[4,] \"Ace Clubs\" \"Deuce Clubs\"   \n[5,] \"Ace Clubs\" \"Deuce Diamonds\"\n[6,] \"Ace Clubs\" \"Deuce Hearts\"  \n\n\nWith a matrix we can get the first and second cards like this:\n\nfirst_card &lt;- hands[,1]\nsecond_card &lt;- hands[,2]\n\nNow the cases for which the first hand was a King can be computed like this:\n\nkings &lt;- paste(\"King\", suits)\nsum(first_card %in% kings)\n\n[1] 204\n\n\nTo get the conditional probability, we compute what fraction of these have a King in the second card:\n\nsum(first_card%in%kings & second_card%in%kings) / sum(first_card%in%kings)\n\n[1] 0.05882353\n\n\nwhich is exactly 3/51, as we had already deduced. Notice that the code above is equivalent to:\n\nmean(first_card%in%kings & second_card%in%kings) / mean(first_card%in%kings)\n\n[1] 0.05882353\n\n\nwhich uses mean instead of sum and is an R version of:\n\\[\n\\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)}\n\\]\nHow about if the order doesn’t matter? For example, in Blackjack if you get an Ace and a face card in the first draw, it is called a Natural 21 and you win automatically. If we wanted to compute the probability of this happening, we would enumerate the combinations, not the permutations, since the order does not matter.\n\ncombinations(3,2)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    3\n\n\nIn the second line, the outcome does not include (2,1) because (1,2) already was enumerated. The same applies to (3,1) and (3,2).\nSo to compute the probability of a Natural 21 in Blackjack, we can do this:\n\naces &lt;- paste(\"Ace\", suits)\n\nfacecard &lt;- c(\"King\", \"Queen\", \"Jack\", \"Ten\")\nfacecard &lt;- expand.grid(number = facecard, suit = suits)\nfacecard &lt;- paste(facecard$number, facecard$suit)\n\nhands &lt;- combinations(52, 2, v = deck)\nmean(hands[,1] %in% aces & hands[,2] %in% facecard)\n\n[1] 0.04826546\n\n\nIn the last line, we assume the Ace comes first. This is only because we know the way combination enumerates possibilities and it will list this case first. But to be safe, we could have written this and produced the same answer:\n\nmean((hands[,1] %in% aces & hands[,2] %in% facecard) |\n       (hands[,2] %in% aces & hands[,1] %in% facecard))\n\n[1] 0.04826546\n\n\n\nMonte Carlo example\nInstead of using combinations to deduce the exact probability of a Natural 21, we can use a Monte Carlo to estimate this probability. In this case, we draw two cards over and over and keep track of how many 21s we get. We can use the function sample to draw two cards without replacements:\n\nhand &lt;- sample(deck, 2)\nhand\n\n[1] \"Queen Clubs\"  \"Seven Spades\"\n\n\nAnd then check if one card is an Ace and the other a face card or a 10. Going forward, we include 10 when we say face card. Now we need to check both possibilities:\n\n(hand[1] %in% aces & hand[2] %in% facecard) |\n  (hand[2] %in% aces & hand[1] %in% facecard)\n\n[1] FALSE\n\n\nIf we repeat this 10,000 times, we get a very good approximation of the probability of a Natural 21.\nLet’s start by writing a function that draws a hand and returns TRUE if we get a 21. The function does not need any arguments because it uses objects defined in the global environment.\n\nblackjack &lt;- function(){\n   hand &lt;- sample(deck, 2)\n  (hand[1] %in% aces & hand[2] %in% facecard) |\n    (hand[2] %in% aces & hand[1] %in% facecard)\n}\n\nHere we do have to check both possibilities: Ace first or Ace second because we are not using the combinations function. The function returns TRUE if we get a 21 and FALSE otherwise:\n\nblackjack()\n\n[1] FALSE\n\n\nNow we can play this game, say, 10,000 times:\n\nB &lt;- 10000\nresults &lt;- replicate(B, blackjack())\nmean(results)\n\n[1] 0.0475",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html#examples",
    "href": "content/Week_09/09a.html#examples",
    "title": "Probability and Statistics",
    "section": "Examples",
    "text": "Examples\nIn this section, we describe two discrete probability popular examples: the Monty Hall problem and the birthday problem. We’ll use R to help illustrate the mathematical concepts and to demostrate an idea that we’ll return to: simulation.\n\nMonty Hall problem\nIn the 1970s, there was a game show called “Let’s Make a Deal” and Monty Hall was the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door there was a prize. The other doors had a goat behind them to show the contestant they had lost. After the contestant picked a door, before revealing whether the chosen door contained a prize, Monty Hall would open one of the two remaining doors and show the contestant there was no prize behind that door. Then he would ask “Do you want to switch doors?” What would you do?\nWe can use probability to show that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3! This seems counterintuitive. Many people incorrectly think both chances are 1 in 2 since you are choosing between 2 options. You can watch a detailed mathematical explanation on Khan Academy2 or read one on Wikipedia3. Below we use a Monte Carlo simulation to see which strategy is better. Note that this code is written longer than it should be for pedagogical purposes.\nLet’s start with the stick strategy:\n\nB &lt;- 10000\nmonty_hall &lt;- function(strategy){\n  doors &lt;- as.character(1:3) \n  prize &lt;- sample(c(\"car\", \"goat\", \"goat\")) # no replacement\n  prize_door &lt;- doors[prize == \"car\"] # which door has the prize?\n  my_pick  &lt;- sample(doors, 1) # we pick a door at random\n  show &lt;- sample(doors[!doors %in% c(my_pick, prize_door)],1)\n  stick &lt;- my_pick\n  switch &lt;- doors[!doors%in%c(my_pick, show)]\n  choice &lt;- ifelse(strategy == \"stick\", stick, switch) # apply the strategy\n  choice == prize_door # did we win?\n}\nstick &lt;- replicate(B, monty_hall(\"stick\"))\nmean(stick)\n\n[1] 0.3416\n\nswitch &lt;- replicate(B, monty_hall(\"switch\"))\nmean(switch)\n\n[1] 0.6682\n\n\nAs we write the code, we note that the lines starting with my_pick and show have no influence on the last logical operation when we stick to our original choice anyway. From this we should realize that the chance is 1 in 3, what we began with. When we switch, the Monte Carlo estimate confirms the 2/3 calculation. This helps us gain some insight by showing that we are removing a door, show, that is definitely not a winner from our choices. We also see that unless we get it right when we first pick, you win: 1 - 1/3 = 2/3.\n\n\nBirthday problem\nSuppose you are in a classroom with 50 people. If we assume this is a randomly selected group of 50 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do this later. Here we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29. This actually doesn’t change the answer much.\nFirst, note that birthdays can be represented as numbers between 1 and 365, so a sample of 50 birthdays can be obtained like this:\n\nn &lt;- 50\nbdays &lt;- sample(1:365, n, replace = TRUE)\n\nTo check if in this particular set of 50 people we have at least two with the same birthday, we can use the function duplicated, which returns TRUE whenever an element of a vector is a duplicate. Here is an example:\n\nduplicated(c(1,2,3,1,4,3,5))\n\n[1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n\nThe second time 1 and 3 appear, we get a TRUE. So to check if two birthdays were the same, we simply use the any and duplicated functions like this:\n\nany(duplicated(bdays))\n\n[1] TRUE\n\n\nIn this case, we see that it did happen. At least two people had the same birthday.\nTo estimate the probability of a shared birthday in the group, we repeat this experiment by sampling sets of 50 birthdays over and over:\n\nB &lt;- 10000\nsame_birthday &lt;- function(n){\n  bdays &lt;- sample(1:365, n, replace=TRUE)\n  any(duplicated(bdays))\n}\nresults &lt;- replicate(B, same_birthday(50))\nmean(results)\n\n[1] 0.9691\n\n\nWere you expecting the probability to be this high?\nPeople tend to underestimate these probabilities. To get an intuition as to why it is so high, think about what happens when the group size is close to 365. At this stage, we run out of days and the probability is one.\nSay we want to use this knowledge to bet with friends about two people having the same birthday in a group of people. When are the chances larger than 50%? Larger than 75%?\nLet’s create a look-up table. We can quickly create a function to compute this for any group size:\n\ncompute_prob &lt;- function(n, B=10000){\n  results &lt;- replicate(B, same_birthday(n))\n  mean(results)\n}\n\nUsing the function sapply, we can perform element-wise operations on any function:\n\nn &lt;- seq(1,60)\nprob &lt;- sapply(n, compute_prob)\n\nWe can now make a plot of the estimated probabilities of two people having the same birthday in a group of size \\(n\\):\n\nlibrary(tidyverse)\nprob &lt;- sapply(n, compute_prob)\ndat = data.frame(n = n, prob = prob)\nggplot(dat, aes(x = n, y=prob)) + \n  geom_point()\n\n\n\n\n\n\n\n\nNow let’s compute the exact probabilities rather than use Monte Carlo approximations. Not only do we get the exact answer using math, but the computations are much faster since we don’t have to generate experiments.\nTo make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening. For this, we use the multiplication rule.\nLet’s start with the first person. The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one, is 364/365. Then, given that the first two people have unique birthdays, person 3 is left with 363 days to choose from. We continue this way and find the chances of all 50 people having a unique birthday is:\n\\[\n1 \\times \\frac{364}{365}\\times\\frac{363}{365} \\dots \\frac{365-n + 1}{365}\n\\]\nWe can write a function that does this for any number:\n\nexact_prob &lt;- function(n){\n  prob_unique &lt;- seq(365,365-n+1)/365\n  1 - prod( prob_unique)\n}\neprob &lt;- sapply(n, exact_prob)\ndat$eprob = eprob\nggplot(dat, aes(x = n, y = prob)) +\n  geom_point() +\n  geom_line(aes(y = eprob), col='red')\n\n\n\n\n\n\n\n\nThis plot shows that the Monte Carlo simulation provided a very good estimate of the exact probability. Had it not been possible to compute the exact probabilities, we would have still been able to accurately estimate the probabilities.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html#infinity-in-practice",
    "href": "content/Week_09/09a.html#infinity-in-practice",
    "title": "Probability and Statistics",
    "section": "Infinity in practice",
    "text": "Infinity in practice\nThe theory described here requires repeating experiments over and over forever. In practice we can’t do this. In the examples above, we used \\(B=10,000\\) Monte Carlo experiments and it turned out that this provided accurate estimates. The larger this number, the more accurate the estimate becomes until the approximaton is so good that your computer can’t tell the difference. But in more complex calculations, 10,000 may not be nearly enough. Also, for some calculations, 10,000 experiments might not be computationally feasible. In practice, we won’t know what the answer is, so we won’t know if our Monte Carlo estimate is accurate. We know that the larger \\(B\\), the better the approximation. But how big do we need it to be? This is actually a challenging question and answering it often requires advanced theoretical statistics training.\nOne practical approach we will describe here is to check for the stability of the estimate. The following is an example with the birthday problem for a group of 25 people.\n\nB &lt;- 10^seq(1, 5, len = 100)\ncompute_prob &lt;- function(B, n=25){\n  same_day &lt;- replicate(B, same_birthday(n))\n  mean(same_day)\n}\nprob &lt;- sapply(B, compute_prob)\ndat = data.frame(logB = log10(B),\n                 prob = prob)\nggplot(dat, aes(x = logB, y = prob)) +\n  geom_line()\n\n\n\n\n\n\n\n\nIn this plot, we can see that the values start to stabilize (that is, they vary less than .01) around 1000. Note that the exact probability, which we know in this case, is 0.5686997.\n\n\n\n\n\n\nTRY IT\n\n\n\n\nOne ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?\nWhat is the probability that the ball will not be cyan?\nInstead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\nNow repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\nTwo events \\(A\\) and \\(B\\) are independent if \\(\\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A) P(B)\\). Under which situation are the draws independent?\n\n\nYou don’t replace the draw.\nYou replace the draw.\nNeither\nBoth\n\n\nSay you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow?\nIf you roll a 6-sided die six times, what is the probability of not seeing a 6?\nTwo teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics win at least one game?\nCreate a Monte Carlo simulation to confirm your answer to the previous problem. Use B &lt;- 10000 simulations. Hint: use the following code to generate the results of the first four games:\n\n\nceltic_wins &lt;- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4))\n\nThe Celtics must win one of these 4 games.\n\nTwo teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series?\nConfirm the results of the previous question with a Monte Carlo simulation.\nTwo teams, \\(A\\) and \\(B\\), are playing a seven game series. Team \\(A\\) is better than team \\(B\\) and has a \\(p&gt;0.5\\) chance of winning each game. Given a value \\(p\\), the probability of winning the series for the underdog team \\(B\\) can be computed with the following function based on a Monte Carlo simulation:\n\n\nprob_win &lt;- function(p){\n  B &lt;- 10000\n  result &lt;- replicate(B, {\n    b_win &lt;- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p))\n    sum(b_win)&gt;=4\n  })\n  mean(result)\n}\n\nUse the function sapply to compute the probability, call it Pr, of winning for p &lt;- seq(0.5, 0.95, 0.025). Then plot the result.\n\nRepeat the exercise above, but now keep the probability fixed at p &lt;- 0.75 and compute the probability for different series lengths: best of 1 game, 3 games, 5 games,… Specifically, N &lt;- seq(1, 25, 2). Hint: use this function:\n\n\nprob_win &lt;- function(N, p=0.75){\n  B &lt;- 10000\n  result &lt;- replicate(B, {\n    b_win &lt;- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p))\n    sum(b_win)&gt;=(N+1)/2\n  })\n  mean(result)\n}\n\n\n\nIn previous lectures, we explained why when summarizing a list of numeric values, such as heights, it is not useful to construct a distribution that defines a proportion to each possible outcome. For example, if we measure every single person in a very large population of size \\(n\\) with extremely high precision, since no two people are exactly the same height, we need to assign the proportion \\(1/n\\) to each observed value and attain no useful summary at all. Similarly, when defining probability distributions, it is not useful to assign a very small probability to every single height.\nJust as when using distributions to summarize numeric data, it is much more practical to define a function that operates on intervals rather than single values. The standard way of doing this is using the cumulative distribution function (CDF).\nWe described empirical cumulative distribution function (eCDF) as a basic summary of a list of numeric values. As an example, we earlier defined the height distribution for adult male students. Here, we define the vector \\(x\\) to contain these heights:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(heights)\nx &lt;- heights %&gt;% filter(sex==\"Male\") %&gt;% pull(height)\n\nWe defined the empirical distribution function as:\n\nF &lt;- function(a) mean(x&lt;=a)\n\nwhich, for any value a, gives the proportion of values in the list x that are smaller or equal than a.\nKeep in mind that we have not yet introduced probability in the context of CDFs. Let’s do this by asking the following: if I pick one of the male students at random, what is the chance that he is taller than 70.5 inches? Because every student has the same chance of being picked, the answer to this is equivalent to the proportion of students that are taller than 70.5 inches. Using the CDF we obtain an answer by typing:\n\n1 - F(70)\n\n[1] 0.3768473\n\n\nOnce a CDF is defined, we can use this to compute the probability of any subset. For instance, the probability of a student being between height a and height b is:\n\nF(b)-F(a)\n\nBecause we can compute the probability for any possible event this way, the cumulative probability function defines the probability distribution for picking a height at random from our vector of heights x.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html#theoretical-continuous-distributions",
    "href": "content/Week_09/09a.html#theoretical-continuous-distributions",
    "title": "Probability and Statistics",
    "section": "Theoretical continuous distributions",
    "text": "Theoretical continuous distributions\nThe normal distribution is a useful approximation to many naturally occurring distributions, including that of height. The cumulative distribution for the normal distribution is defined by a mathematical formula which in R can be obtained with the function pnorm. We say that a random quantity is normally distributed with average m and standard deviation s if its probability distribution is defined by:\n\nF(a) = pnorm(a, m, s)\n\nThis is useful because if we are willing to use the normal approximation for, say, height, we don’t need the entire dataset to answer questions such as: what is the probability that a randomly selected student is taller then 70 inches? We just need the average height and standard deviation:\n\nm &lt;- mean(x)\ns &lt;- sd(x)\n1 - pnorm(70.5, m, s)\n\n[1] 0.371369\n\n\n\nTheoretical distributions as approximations\nThe normal distribution is derived mathematically: we do not need data to define it. For practicing data scientists, almost everything we do involves data. Data is always, technically speaking, discrete. For example, we could consider our height data categorical with each specific height a unique category. The probability distribution is defined by the proportion of students reporting each height. Here is a plot of that probability distribution:\n\n\n\n\n\n\n\n\n\nWhile most students rounded up their heights to the nearest inch, others reported values with more precision. One student reported his height to be 69.6850393700787, which is 177 centimeters. The probability assigned to this height is 0.0012315 or 1 in 812. The probability for 70 inches is much higher at 0.1059113, but does it really make sense to think of the probability of being exactly 70 inches as being different than 69.6850393700787? Clearly it is much more useful for data analytic purposes to treat this outcome as a continuous numeric variable, keeping in mind that very few people, or perhaps none, are exactly 70 inches, and that the reason we get more values at 70 is because people round to the nearest inch.\nWith continuous distributions, the probability of a singular value is not even defined. For example, it does not make sense to ask what is the probability that a normally distributed value is 70. Instead, we define probabilities for intervals. We thus could ask what is the probability that someone is between 69.5 and 70.5.\nIn cases like height, in which the data is rounded, the normal approximation is particularly useful if we deal with intervals that include exactly one round number. For example, the normal distribution is useful for approximating the proportion of students reporting values in intervals like the following three:\n\nmean(x &lt;= 68.5) - mean(x &lt;= 67.5)\n\n[1] 0.114532\n\nmean(x &lt;= 69.5) - mean(x &lt;= 68.5)\n\n[1] 0.1194581\n\nmean(x &lt;= 70.5) - mean(x &lt;= 69.5)\n\n[1] 0.1219212\n\n\nNote how close we get with the normal approximation:\n\npnorm(68.5, m, s) - pnorm(67.5, m, s)\n\n[1] 0.1031077\n\npnorm(69.5, m, s) - pnorm(68.5, m, s)\n\n[1] 0.1097121\n\npnorm(70.5, m, s) - pnorm(69.5, m, s)\n\n[1] 0.1081743\n\n\nHowever, the approximation is not as useful for other intervals. For instance, notice how the approximation breaks down when we try to estimate:\n\nmean(x &lt;= 70.9) - mean(x&lt;=70.1)\n\n[1] 0.02216749\n\n\nwith\n\npnorm(70.9, m, s) - pnorm(70.1, m, s)\n\n[1] 0.08359562\n\n\nIn general, we call this situation discretization. Although the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding. As long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool.\n\n\nThe probability density\nFor categorical distributions, we can define the probability of a category. For example, a roll of a die, let’s call it \\(X\\), can be 1,2,3,4,5 or 6. The probability of 4 is defined as:\n\\[\n\\mbox{Pr}(X=4) = 1/6\n\\]\nThe CDF can then easily be defined: \\[\nF(4) = \\mbox{Pr}(X\\leq 4) =  \\mbox{Pr}(X = 4) +  \\mbox{Pr}(X = 3) +  \\mbox{Pr}(X = 2) +  \\mbox{Pr}(X = 1)\n\\]\nAlthough for continuous distributions the probability of a single value \\(\\mbox{Pr}(X=x)\\) is not defined, there is a theoretical definition that has a similar interpretation. The probability density at \\(x\\) is defined as the function \\(f(a)\\) such that:\n\\[\nF(a) = \\mbox{Pr}(X\\leq a) = \\int_{-\\infty}^a f(x)\\, dx\n\\]\nFor those that know calculus, remember that the integral is related to a sum: it is the sum of bars with widths approximating 0. If you don’t know calculus, you can think of \\(f(x)\\) as a curve for which the area under that curve up to the value \\(a\\), gives you the probability \\(\\mbox{Pr}(X\\leq a)\\).\nFor example, to use the normal approximation to estimate the probability of someone being taller than 76 inches, we use:\n\n1 - pnorm(76, m, s)\n\n[1] 0.03206008\n\n\nwhich mathematically is the grey area below:\n\n\n\n\n\n\n\n\n\nThe curve you see is the probability density for the normal distribution. In R, we get this using the function dnorm.\nAlthough it may not be immediately obvious why knowing about probability densities is useful, understanding this concept will be essential to those wanting to fit models to data for which predefined functions are not available.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html#monte-carlo-simulations-for-continuous-variables",
    "href": "content/Week_09/09a.html#monte-carlo-simulations-for-continuous-variables",
    "title": "Probability and Statistics",
    "section": "Monte Carlo simulations for continuous variables",
    "text": "Monte Carlo simulations for continuous variables\nR provides functions to generate normally distributed outcomes. Specifically, the rnorm function takes three arguments: size, average (defaults to 0), and standard deviation (defaults to 1) and produces random numbers. Here is an example of how we could generate data that looks like our reported heights:\n\nn &lt;- length(x)\nm &lt;- mean(x)\ns &lt;- sd(x)\nsimulated_heights &lt;- rnorm(n, m, s)\n\nNot surprisingly, the distribution looks normal:\n\n\n\n\n\n\n\n\n\nThis is one of the most useful functions in R as it will permit us to generate data that mimics natural events and answers questions related to what could happen by chance by running Monte Carlo simulations.\nIf, for example, we pick 800 males at random, what is the distribution of the tallest person? How rare is a seven footer in a group of 800 males? The following Monte Carlo simulation helps us answer that question:\n\nB &lt;- 10000\ntallest &lt;- replicate(B, {\n  simulated_data &lt;- rnorm(800, m, s)\n  max(simulated_data)\n})\n\nHaving a seven footer is quite rare:\n\nmean(tallest &gt;= 7*12)\n\n[1] 0.0172\n\n\nHere is the resulting distribution:\n\n\n\n\n\n\n\n\n\nNote that it does not look normal.",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html#continuous-distributions",
    "href": "content/Week_09/09a.html#continuous-distributions",
    "title": "Probability and Statistics",
    "section": "Continuous distributions",
    "text": "Continuous distributions\nThe normal distribution is not the only useful theoretical distribution. Other continuous distributions that we may encounter are the student-t, Chi-square, exponential, gamma, beta, and beta-binomial. R provides functions to compute the density, the quantiles, the cumulative distribution functions and to generate Monte Carlo simulations. R uses a convention that lets us remember the names, namely using the letters d, q, p, and r in front of a shorthand for the distribution. We have already seen the functions dnorm, pnorm, and rnorm for the normal distribution. Remember that dnorm gives us the PDF, pnorm gives us the CDF, rnorm gives us random draws from the normal, and the function qnorm gives us the quantiles (the input to qnorm, then, must be between 0 and 1).\n\nqnorm(.25, mean = 0, sd = 1)\n\n[1] -0.6744898\n\nqnorm(.25, mean = -10, sd = 5)\n\n[1] -13.37245\n\nqnorm(.975, mean = 0, sd = 1)\n\n[1] 1.959964\n\n\n\n\n\n\n\n\nTRY IT\n\n\n\n\nAssume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?\nAssume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?\nAssume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?\nRepeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?\nNotice that the answer to the question does not change when you change units. This makes sense since the answer to the question should not be affected by what units we use. In fact, if you look closely, you notice that 61 and 64 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average.\nTo see the math that explains why the answers to questions 3, 4, and 5 are the same, suppose we have a random variable with average \\(m\\) and standard error \\(s\\). Suppose we ask the probability of \\(X\\) being smaller or equal to \\(a\\). Remember that, by definition, \\(a\\) is \\((a - m)/s\\) standard deviations \\(s\\) away from the average \\(m\\). The probability is:\n\n\\[\n\\mbox{Pr}(X \\leq a)\n\\]\nNow we subtract \\(\\mu\\) to both sides and then divide both sides by \\(\\sigma\\):\n\\[\n\\mbox{Pr}\\left(\\frac{X-m}{s} \\leq \\frac{a-m}{s} \\right)\n\\]\nThe quantity on the left is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it \\(Z\\):\n\\[\n\\mbox{Pr}\\left(Z \\leq \\frac{a-m}{s} \\right)\n\\]\nSo, no matter the units, the probability of \\(X\\leq a\\) is the same as the probability of a standard normal variable being less than \\((a - m)/s\\). If mu is the average and sigma the standard error, which of the following R code would give us the right answer in every situation:\n\nmean(X&lt;=a)\npnorm((a - m)/s)\npnorm((a - m)/s, m, s)\npnorm(a)\n\n\nImagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use qnorm.\nThe distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. Suppose you want to know the distribution of the highest IQ across all graduating classes if 10,000 people are born each in your school district. Run a Monte Carlo simulation with B=1000 generating 10,000 IQ scores and keeping the highest. Make a histogram.\n\n\n\nFall 2024: I have moved Random Variables to the top of Thursday’s Week 5 content",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_09/09a.html#footnotes",
    "href": "content/Week_09/09a.html#footnotes",
    "title": "Probability and Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Urn_problem↩︎\nhttps://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem↩︎\nhttps://en.wikipedia.org/wiki/Monty_Hall_problem↩︎",
    "crumbs": [
      "Course Content",
      "Week 09",
      "Probability and Statistics"
    ]
  },
  {
    "objectID": "content/Week_10/10a.html",
    "href": "content/Week_10/10a.html",
    "title": "Nonparametric Regression",
    "section": "",
    "text": "This page.\n\n\n\n\nHow to use k-nearest neighbors for regression through the use of the knnreg() function from the caret package\nHow to use decision trees for regression through the use of the rpart() function from the rpart package.\nHow “making predictions” can be thought of as estimating the regression function, that is, the conditional mean of the response given values of the features.\nThe difference between parametric and nonparametric methods.\nThe difference between model parameters and tuning parameters methods.\nHow these nonparametric methods deal with categorical variables and interactions.\nWhat is model flexibility?\nWhat is overfitting and how do we avoid it?",
    "crumbs": [
      "Course Content",
      "Week 10",
      "Nonparametric Regression"
    ]
  },
  {
    "objectID": "content/Week_10/10a.html#required-reading",
    "href": "content/Week_10/10a.html#required-reading",
    "title": "Nonparametric Regression",
    "section": "",
    "text": "This page.\n\n\n\n\nHow to use k-nearest neighbors for regression through the use of the knnreg() function from the caret package\nHow to use decision trees for regression through the use of the rpart() function from the rpart package.\nHow “making predictions” can be thought of as estimating the regression function, that is, the conditional mean of the response given values of the features.\nThe difference between parametric and nonparametric methods.\nThe difference between model parameters and tuning parameters methods.\nHow these nonparametric methods deal with categorical variables and interactions.\nWhat is model flexibility?\nWhat is overfitting and how do we avoid it?",
    "crumbs": [
      "Course Content",
      "Week 10",
      "Nonparametric Regression"
    ]
  },
  {
    "objectID": "content/Week_10/10a.html#r-setup",
    "href": "content/Week_10/10a.html#r-setup",
    "title": "Nonparametric Regression",
    "section": "R Setup",
    "text": "R Setup\n\nlibrary(tibble)     # data frame printing\nlibrary(dplyr)      # data manipulation\n\nlibrary(caret)      # fitting knn\nlibrary(rpart)      # fitting trees\nlibrary(rpart.plot) # plotting trees\n\nlibrary(knitr)      # creating tables\nlibrary(kableExtra) # styling tables",
    "crumbs": [
      "Course Content",
      "Week 10",
      "Nonparametric Regression"
    ]
  },
  {
    "objectID": "content/Week_10/10a.html#mathematical-setup",
    "href": "content/Week_10/10a.html#mathematical-setup",
    "title": "Nonparametric Regression",
    "section": "Mathematical Setup",
    "text": "Mathematical Setup\nLet’s return to the setup we defined in the previous lectures. Consider a random variable \\(Y\\) which represents a response variable, and \\(p\\) feature variables \\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\). We assume that the response variable \\(Y\\) is some function of the features, plus some random noise.\n\\[\nY = f(\\boldsymbol{X}) + \\epsilon\n\\]\nOur goal is to find some \\(f\\) such that \\(f(\\boldsymbol{X})\\) is close to \\(Y\\). More specifically we want to minimize the risk under squared error loss.\n\\[\n\\mathbb{E}_{\\boldsymbol{X}, Y} \\left[ (Y - f(\\boldsymbol{X})) ^ 2 \\right] = \\mathbb{E}_{\\boldsymbol{X}} \\mathbb{E}_{Y \\mid \\boldsymbol{X}} \\left[ ( Y - f(\\boldsymbol{X}) ) ^ 2 \\mid \\boldsymbol{X} = \\boldsymbol{x} \\right]\n\\]\nWe saw previously (see the slides from last two content days) that this risk is minimized by the conditional mean of \\(Y\\) given \\(\\boldsymbol{X}\\),\n\\[\n\\mu(\\boldsymbol{x}) \\triangleq \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}]\n\\]\nwhich we call the regression function.\nOur goal then is to estimate this regression function. Let’s use an example where we know the true probability model:\n\\[\nY = 1 - 2x - 3x ^ 2 + 5x ^ 3 + \\epsilon\n\\]\nwhere \\(\\epsilon \\sim \\text{N}(0, \\sigma^2)\\).\nRecall that this implies that the regression function is\n\\[\n\\mu(x) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] = 1 - 2x - 3x ^ 2 + 5x ^ 3\n\\]\nLet’s also pretend that we do not actually know this information, but instead have some data, \\((x_i, y_i)\\) for \\(i = 1, 2, \\ldots, n\\).\nWe simulate enough data to make the “pattern” clear-ish to recognize.\n\n\n\n\n\n\n\n\n\nWhen we use a linear model, we first need to make an assumption about the form of the regression function.\nFor example, we could assume that\n\\[\n\\mu(x) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\n\\]\nwhich is fit in R using the lm() function\n\nlm(y ~ x + I(x ^ 2) + I(x ^ 3), data = sim_slr_data)\n\n\nCall:\nlm(formula = y ~ x + I(x^2) + I(x^3), data = sim_slr_data)\n\nCoefficients:\n(Intercept)            x       I(x^2)       I(x^3)  \n     0.8397      -2.7257      -2.3752       6.0906  \n\n\nNotice that what is returned are (maximum likelihood or least squares) estimates of the unknown \\(\\beta\\) coefficients. That is, the “learning” that takes place with a linear models is “learning” the values of the coefficients.\nFor this reason, we call linear regression models parametric models. They have unknown model parameters, in this case the \\(\\beta\\) coefficients that must be learned from the data. The form of the regression function is assumed.\nWhat if we don’t want to make an assumption about the form of the regression function? While in this case, you might look at the plot and arrive at a reasonable guess of assuming a third order polynomial, what if it isn’t so clear? What if you have 100 features? Making strong assumptions might not work well.\nEnter nonparametric models. We will consider two examples: k-nearest neighbors and decision trees.",
    "crumbs": [
      "Course Content",
      "Week 10",
      "Nonparametric Regression"
    ]
  },
  {
    "objectID": "content/Week_10/10a.html#k-nearest-neighbors",
    "href": "content/Week_10/10a.html#k-nearest-neighbors",
    "title": "Nonparametric Regression",
    "section": "k-Nearest Neighbors",
    "text": "k-Nearest Neighbors\nWe’ll start with k-nearest neighbors which is possibly a more intuitive procedure than linear models.1\nIf our goal is to estimate the mean function,\n\\[\n\\mu(x) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}]\n\\]\nthe most natural approach would be to use\n\\[\n\\text{average}(\\{ y_i : x_i = x \\}).\n\\]\nThat is, to estimate the conditional mean at \\(x\\), average the \\(y_i\\) values for each data point where \\(x_i = x\\).\nWhile this sounds nice, it has an obvious flaw. For most values of \\(x\\) there will not be any \\(x_i\\) in the data where \\(x_i = x\\)!\nSo what’s the next best thing? Pick values of \\(x_i\\) that are “close” to \\(x\\).\n\\[\n\\text{average}( \\{ y_i : x_i \\text{ equal to (or very close to) x} \\} ).\n\\]\nThis is the main idea behind many nonparametric approaches. The details often just amount to very specifically defining what “close” means.\nIn the case of k-nearest neighbors we use\n\\[\n\\hat{\\mu}_k(x) = \\frac{1}{k} \\sum_{ \\{i \\ : \\ x_i \\in \\mathcal{N}_k(x, \\mathcal{D}) \\} } y_i\n\\]\nas our estimate of the regression function at \\(x\\). While this looks complicated, it is actually very simple. Here, we are using an average of the \\(y_i\\) values of for the \\(k\\) nearest neighbors to \\(x\\).\nThe \\(k\\) “nearest” neighbors are the \\(k\\) data points \\((x_i, y_i)\\) that have \\(x_i\\) values that are nearest to \\(x\\). We can define “nearest” using any distance we like, but unless otherwise noted, we are referring to euclidean distance.2 We are using the notation \\(\\{i \\ : \\ x_i \\in \\mathcal{N}_k(x, \\mathcal{D}) \\}\\) to define the \\(k\\) observations that have \\(x_i\\) values that are nearest to the value \\(x\\) in a dataset \\(\\mathcal{D}\\), in other words, the \\(k\\) nearest neighbors.\nThe plots below begin to illustrate this idea.\n\n\n\n\n\n\n\n\n\n\nIn the left plot, to estimate the mean of \\(Y\\) at \\(x = -0.5\\) we use the three nearest neighbors, which are highlighted with green. Our estimate is the average of the \\(y_i\\) values of these three points indicated by the black x.\nIn the middle plot, to estimate the mean of \\(Y\\) at \\(x = 0\\) we use the five nearest neighbors, which are highlighted with green. Our estimate is the average of the \\(y_i\\) values of these five points indicated by the black x.\nIn the right plot, to estimate the mean of \\(Y\\) at \\(x = 0.75\\) we use the nine nearest neighbors, which are highlighted with green. Our estimate is the average of the \\(y_i\\) values of these nine points indicated by the black x.\n\nYou might begin to notice a bit of an issue here. We have to do a new calculation each time we want to estimate the regression function at a different value of \\(x\\)! For this reason, k-nearest neighbors is often said to be “fast to train” and “slow to predict.” Training, is instant. You just memorize the data! Prediction involves finding the distance between the \\(x\\) considered and all \\(x_i\\) in the data!3\nSo, how then, do we choose the value of the tuning parameter \\(k\\)? We validate!\nFirst, let’s take a look at what happens with this data if we consider three different values of \\(k\\).\n\n\n\n\n\n\n\n\n\nFor each plot, the black dashed curve is the true mean function.\n\nIn the left plot we use \\(k = 25\\). The red “curve” is the estimate of the mean function for each \\(x\\) shown in the plot.\nIn the left plot we use \\(k = 5\\). The blue “curve” is the estimate of the mean function for each \\(x\\) shown in the plot.\nIn the left plot we use \\(k = 1\\). The green “curve” is the estimate of the mean function for each \\(x\\) shown in the plot.\n\nSome things to notice here:\n\nThe left plot with \\(k = 25\\) is performing poorly. The estimated “curve” does not “move” enough. This is an example of an inflexible model.\nThe right plot with \\(k = 1\\) might not perform too well. The estimated “curve” seems to “move” too much. (Notice, that it goes through each point. We’ve fit to the noise.) This is an example of a flexible model.\n\nWhile the middle plot with \\(k = 5\\) is not “perfect” it seems to roughly capture the “motion” of the true regression function. We can begin to see that if we generated new data, this estimated regression function would perform better than the other two.\nBut remember, in practice, we won’t know the true regression function, so we will need to determine how our model performs using only the available data!\nThis \\(k\\), the number of neighbors, is an example of a tuning parameter. Instead of being learned from the data, like model parameters such as the \\(\\beta\\) coefficients in linear regression, a tuning parameter tells us how to learn from data. It is user-specified. To determine the value of \\(k\\) that should be used, many models are fit to the estimation data, then evaluated on the validation. Using the information from the validation data, a value of \\(k\\) is chosen. (More on this in a bit.)\n\nModel parameters are “learned” using the same data that was used to fit the model.\nTuning parameters are “chosen” using data not used to fit the model.\n\nThis tuning parameter \\(k\\) also defines the flexibility of the model and plays a similar role as complexity, or the number of parameters, did in our linear regressions. In KNN, a small value of \\(k\\) is a flexible model, while a large value of \\(k\\) is inflexible.4\nBefore moving to an example of tuning a KNN model, we will first introduce decision trees.",
    "crumbs": [
      "Course Content",
      "Week 10",
      "Nonparametric Regression"
    ]
  },
  {
    "objectID": "content/Week_10/10a.html#decision-trees",
    "href": "content/Week_10/10a.html#decision-trees",
    "title": "Nonparametric Regression",
    "section": "Decision Trees",
    "text": "Decision Trees\nDecision trees are similar to k-nearest neighbors but instead of looking for neighbors, decision trees create neighborhoods. We won’t explore the full details of trees, but just start to understand the basic concepts, as well as learn to fit them in R.\nNeighborhoods are created via recursive binary partitions. In simpler terms, pick a feature and a possible cutoff value. Data that have a value less than the cutoff for the selected feature are in one neighborhood (the left) and data that have a value greater than the cutoff are in another (the right). Within these two neighborhoods, repeat this procedure until a stopping rule is satisfied. To make a prediction, check which neighborhood a new piece of data would belong to and predict the average of the \\(y_i\\) values of data in that neighborhood.\nWith the data above, which has a single feature \\(x\\), consider three possible cutoffs: -0.5, 0.0, and 0.75.\n\n\n\n\n\n\n\n\n\nFor each plot, the black vertical line defines the neighborhoods. The green horizontal lines are the average of the \\(y_i\\) values for the points in the left neighborhood. The red horizontal lines are the average of the \\(y_i\\) values for the points in the right neighborhood.\nWhat makes a cutoff good? Large differences in the average \\(y_i\\) between the two neighborhoods. More formally we want to find a cutoff value that minimizes\n\\[\n\\sum_{i \\in N_L} \\left( y_i - \\hat{\\mu}_{N_L} \\right) ^ 2 + \\sum_{i \\in N_R} \\left(y_i - \\hat{\\mu}_{N_R} \\right) ^ 2\n\\]\nwhere\n\n\\(N_L\\) are the data in the left neighborhood, that is \\(x &lt; c\\)\n\\(N_R\\) are the data in the right neighborhood, that is \\(x &gt; c\\)\n\\(\\hat{\\mu}_{N_L}\\) is the mean of the \\(y_i\\) for data in the left neighborhood\n\\(\\hat{\\mu}_{N_R}\\) is the mean of the \\(y_i\\) for data in the right neighborhood\n\nThis quantity is the sum of two sum of squared errors, one for the left neighborhood, and one for the right neighborhood.\n\n\n\n\n\nCutoff\nTotal SSE\nLeft SSE\nRight SSE\n\n\n\n\n-0.50\n45.02\n21.28\n23.74\n\n\n0.00\n58.94\n44.68\n14.26\n\n\n0.75\n56.71\n55.46\n1.25\n\n\n\n\n\n\n\nThe table above summarizes the results of the three potential splits. We see that (of the splits considered, which are not exhaustive5) the split based on a cutoff of \\(x = -0.50\\) creates the best partitioning of the space.\nNow let’s consider building a full tree.\n\n\n\n\n\n\n\n\n\nIn the plot above, the true regression function is the dashed black curve, and the solid orange curve is the estimated regression function using a decision tree. We see that there are two splits, which we can visualize as a tree.\n\n\n\n\n\n\n\n\n\nThe above “tree”6 shows the splits that were made. It informs us of the variable used, the cutoff value, and some summary of the resulting neighborhood. In “tree” terminology the resulting neighborhoods are “terminal nodes” of the tree. In contrast, “internal nodes” are neighborhoods that are created, but then further split.\nThe “root node” is the neighborhood contains all observations, before any splitting, and can be seen at the top of the image above. We see that this node represents 100% of the data. The other number, 0.21, is the mean of the response variable, in this case, \\(y_i\\).\nLooking at a terminal node, for example the bottom left node, we see that 23% of the data is in this node. The average value of the \\(y_i\\) in this node is -1, which can be seen in the plot above.\nWe also see that the first split is based on the \\(x\\) variable, and a cutoff of \\(x = -0.52\\). Note that because there is only one variable here, all splits are based on \\(x\\), but in the future, we will have multiple features that can be split and neighborhoods will no longer be one-dimensional. However, this is hard to plot.\nLet’s build a bigger, more flexible tree.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are two tuning parameters at play here which we will call by their names in R which we will see soon:\n\ncp or the “complexity parameter” as it is called.7 This parameter determines which splits are accepted. A split must improve the performance of the tree by more than cp in order to be used. When we get to R, we will see that the default value is 0.1.\nminsplit, the minimum number of observations in a node (neighborhood) in order to consider splitting within a neighborhood.\n\nThere are actually many more possible tuning parameters for trees, possibly differing depending on who wrote the code you’re using. We will limit discussion to these two.8 Note that they effect each other, and they effect other parameters which we are not discussing. The main takeaway should be how they effect model flexibility.\nFirst let’s look at what happens for a fixed minsplit by variable cp.\n\n\n\n\n\n\n\n\n\nWe see that as cp decreases, model flexibility increases. We see more splits, because the increase in performance needed to accept a split is smaller as cp is reduced.\nNow the reverse, fix cp and vary minsplit.\n\n\n\n\n\n\n\n\n\nWe see that as minsplit decreases, model flexibility increases. By allowing splits of neighborhoods with fewer observations, we obtain more splits, which results in a more flexible model.",
    "crumbs": [
      "Course Content",
      "Week 10",
      "Nonparametric Regression"
    ]
  },
  {
    "objectID": "content/Week_10/10a.html#example-credit-card-data",
    "href": "content/Week_10/10a.html#example-credit-card-data",
    "title": "Nonparametric Regression",
    "section": "Example: Credit Card Data",
    "text": "Example: Credit Card Data\n\nExample KNN: Credit Card Data\n\n# load data, coerce to tibble\ncrdt = as_tibble(ISLR::Credit)\n\nAgain, we are using the Credit data form the ISLR package. Note: this is not real data. It has been simulated.\n\n# data prep\ncrdt = crdt %&gt;%\n  select(-ID) %&gt;%\n  select(-Rating, everything())\n\nWe remove the ID variable as it should have no predictive power. We also move the Rating variable to the last column with a clever dplyr trick. This is in no way necessary, but is useful in creating some plots.\n\n# test-train split\nset.seed(1) \ncrdt_trn_idx = sample(nrow(crdt), size = 0.8 * nrow(crdt))\ncrdt_trn = crdt[crdt_trn_idx, ]\ncrdt_tst = crdt[-crdt_trn_idx, ]\n\nAfter train-test (with 80% of the data) and estimation-validation splitting the data, we look at the train data.\n\n# check data\nhead(crdt_trn, n = 10)\n\n# A tibble: 10 × 11\n   Income Limit Cards   Age Education Gender   Student Married Ethnicity Balance\n    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;       &lt;int&gt;\n 1  183.  13913     4    98        17 \" Male\"  No      Yes     Caucasian    1999\n 2   35.7  2880     2    35        15 \" Male\"  No      No      African …       0\n 3  123.   8376     2    89        17 \" Male\"  Yes     No      African …    1259\n 4   20.8  2672     1    70        18 \"Female\" No      No      African …       0\n 5   39.1  5565     4    48        18 \"Female\" No      Yes     Caucasian     772\n 6   36.5  3806     2    52        13 \" Male\"  No      No      African …     188\n 7   45.1  3762     3    80         8 \" Male\"  No      Yes     Caucasian      70\n 8   43.5  2906     4    69        11 \" Male\"  No      No      Caucasian       0\n 9   23.1  3476     2    50        15 \"Female\" No      No      Caucasian     209\n10   53.2  4943     2    46        16 \"Female\" No      Yes     Asian         382\n# ℹ 1 more variable: Rating &lt;int&gt;\n\n\nIn this simulated data, we would like to predict the Rating variable. For now, let’s try to use only demographic information as predictors.9 In particular, let’s focus on Age (numeric), Gender (categorical), and Student (categorical).\nLet’s fit KNN models with these features, and various values of \\(k\\). To do so, we use the knnreg() function from the caret package.10 Use ?knnreg for documentation and details.\n\ncrdt_knn_01 = knnreg(Rating ~ Age + Gender + Student, data = crdt_trn, k = 1)\ncrdt_knn_10 = knnreg(Rating ~ Age + Gender + Student, data = crdt_trn, k = 10)\ncrdt_knn_25 = knnreg(Rating ~ Age + Gender + Student, data = crdt_trn, k = 25)\ncrdt_knn_100= knnreg(Rating ~ Age + Gender + Student, data = crdt_trn, k = 75)\n\nHere, we fit three models to the estimation data. We supply the variables that will be used as features as we would with lm(). We also specify how many neighbors to consider via the k argument.\nBut wait a second, what is the distance from non-student to student? From male to female? In other words, how does KNN handle categorical variables? It doesn’t! Like lm() it creates dummy variables under the hood.\nNote: To this point, and until we specify otherwise, we will always coerce categorical variables to be factor variables in R. We will then let modeling functions such as lm() or knnreg() deal with the creation of dummy variables internally.\n\nhead(crdt_knn_10$learn$X)\n\n  Age GenderFemale StudentYes\n1  98            0          0\n2  35            0          0\n3  89            0          1\n4  70            1          0\n5  48            1          0\n6  52            0          0\n\n\nOnce these dummy variables have been created, we have a numeric \\(X\\) matrix, which makes distance calculations easy.11 For example, the distance between the 3rd and 4th observation here is 19.053.\n\ndist(head(crdt_knn_10$learn$X))\n\n          1         2         3         4         5\n2 63.000000                                        \n3  9.055385 54.009258                              \n4 28.017851 35.014283 19.052559                    \n5 50.009999 13.038405 41.024383 22.000000          \n6 46.000000 17.000000 37.013511 18.027756  4.123106\n\n\n\nsqrt(sum((crdt_knn_10$learn$X[3, ] - crdt_knn_10$learn$X[4, ]) ^ 2))\n\n[1] 19.05256\n\n\nWhat about interactions? Basically, you’d have to create them the same way as you do for linear models. We only mention this to contrast with trees in a bit.\nOK, so of these three models, which one performs best? (Where for now, “best” is obtaining the lowest validation RMSE.)\nFirst, note that we return to the predict() function as we did with lm().\n\npredict(crdt_knn_10, crdt_tst[1:5, ])\n\n[1] 334.9091 346.0000 459.3333 342.0909 450.8000\n\n\nThis uses the 10-NN (10 nearest neighbors) model to make predictions (estimate the regression function) given the first five observations of the validation data. Note: We did not name the second argument to predict(). Again, you’ve been warned.\nNow that we know how to use the predict() function, let’s calculate the validation RMSE for each of these models.\n\nknn_mod_list = list(\n  crdt_knn_01 = knnreg(Rating ~ Age + Gender + Student, data = crdt_trn, k = 1),\n  crdt_knn_10 = knnreg(Rating ~ Age + Gender + Student, data = crdt_trn, k = 10),\n  crdt_knn_25 = knnreg(Rating ~ Age + Gender + Student, data = crdt_trn, k = 25),\n  crdt_knn_100= knnreg(Rating ~ Age + Gender + Student, data = crdt_trn, k = 75)\n)\n\n\nknn_tst_pred = lapply(knn_mod_list, predict, crdt_tst)\n\n\ncalc_rmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\n\nsapply(knn_tst_pred, calc_rmse, crdt_tst$Rating)\n\n crdt_knn_01  crdt_knn_10  crdt_knn_25 crdt_knn_100 \n    163.9868     142.2242     135.3838     142.6786 \n\n\nSo, of these three values of \\(k\\), the model with \\(k = 25\\) achieves the lowest validation (test) RMSE.\nThis process, fitting a number of models with different values of the tuning parameter, in this case \\(k\\), and then finding the “best” tuning parameter value based on performance on the validation (test) data is called tuning. In practice, we would likely consider more values of \\(k\\), but this should illustrate the point.\nIn the next lectures, we will discuss the details of model flexibility and model tuning, and how these concepts are tied together. However, even though we will present some theory behind this relationship, in practice, you must tune and validate your models. There is no theory that will inform you ahead of tuning and validation which model will be the best. By teaching you how to fit KNN models in R and how to calculate validation RMSE, you already have all a set of tools you can use to find a good model.\nTuning isn’t necessarily something we’ve never done before – in our linear regression section, we fit our model using more and more variables (and transformations of those variables), which allowed more flexibility but, as we saw, resulted in over-fitting. The same principle applies to KNN when we allow \\(k\\) to decrease towards 1.\n\n\nExample Decision Tree: Credit Card Data\nLet’s turn to decision trees which we will fit with the rpart() function from the rpart package. Use ?rpart and ?rpart.control for documentation and details. In particular, ?rpart.control will detail the many tuning parameters of this implementation of decision tree models in R.\n\nset.seed(5)\ncrdt_trn_idx = sample(nrow(crdt), size = 0.8 * nrow(crdt))\ncrdt_trn = crdt[crdt_trn_idx, ]\ncrdt_tst = crdt[-crdt_trn_idx, ]\n\nWe’ll start by using default tuning parameters for minsplit, but I’m going to use a cp that gets us something interesting.\n\ncrdt_tree = rpart(Rating ~ Age + Gender + Student, data = crdt_trn, cp = .0095)\n\n\ncrdt_tree\n\nn= 320 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 320 7045031.0 350.6250  \n   2) Age&lt; 82.5 306 6586876.0 344.8170  \n     4) Age&lt; 35.5 54  471770.1 311.8704 *\n     5) Age&gt;=35.5 252 6043929.0 351.8770  \n      10) Age&gt;=69.5 68 1571017.0 326.0882  \n        20) Age&lt; 77.5 39  498793.0 287.3590 *\n        21) Age&gt;=77.5 29  935056.1 378.1724  \n          42) Gender= Male 11  445504.2 316.2727 *\n          43) Gender=Female 18  421648.0 416.0000 *\n      11) Age&lt; 69.5 184 4410974.0 361.4076 *\n   3) Age&gt;=82.5 14  222217.4 477.5714 *\n\n\nAbove we see the resulting tree printed, however, this is difficult to read. Instead, we use the rpart.plot() function from the rpart.plot package to better visualize the tree.\n\nrpart.plot(crdt_tree)\n\n\n\n\n\n\n\n\nAt each split, the variable used to split is listed together with a condition. If the condition is true for a data point, send it to the left neighborhood. Although the Gender available for creating splits, we only see splits based on Age and Student. This hints at the relative importance of these variables for prediction. More on this much later.\nCategorical variables are split based on potential categories! This is excellent. This means that trees naturally handle categorical features without needing to convert to numeric under the hood. We see a split that puts students into one neighborhood, and non-students into another.\nNotice that the splits happen in order. So for example, the third terminal node (with an average rating of 316) is based on splits of:\n\nAge &lt; 83\nAge &gt; 36\nAge &gt; 70\nAge &gt; 78\nGender = Male\n\nIn other words, individuals in this terminal node are students who are between the ages of 78 and 83. (Only 3% of the data is represented here.) This is basically an interaction between Age and Student without any need to directly specify it! What a great feature of trees.\nTo recap:\n\nTrees do not make assumptions about the form of the regression function.\nTrees automatically handle categorical features.\nTrees naturally incorporate interaction.\n\nNow let’s fit another tree that is more flexible by relaxing some tuning parameters. Recall that by default, cp = 0.1 and minsplit = 20.\n\ncrdt_tree_big = rpart(Rating ~ Age + Gender + Student, data = crdt_trn,\n                      cp = 0.0, minsplit = 20)\n\n\nrpart.plot(crdt_tree_big)\n\n\n\n\n\n\n\n\nTo make the tree even bigger, we could reduce minsplit, but in practice we mostly consider the cp parameter. Since minsplit has been kept the same, but cp was reduced, we see the same splits as the smaller tree, but many additional splits.\nNow let’s fit a bunch of trees, with different values of cp, for tuning.\n\ntree_mod_list = list(\n  crdt_tree_0000 = rpart(Rating ~ Age + Gender + Student, data = crdt_trn, cp = 0.000),\n  crdt_tree_0001 = rpart(Rating ~ Age + Gender + Student, data = crdt_trn, cp = 0.001),\n  crdt_tree_0010 = rpart(Rating ~ Age + Gender + Student, data = crdt_trn, cp = 0.010),\n  crdt_tree_0100 = rpart(Rating ~ Age + Gender + Student, data = crdt_trn, cp = 0.100)\n)\n\n\ntree_val_pred = lapply(tree_mod_list, predict, crdt_tst)\n\n\nsapply(tree_val_pred, calc_rmse, crdt_tst$Rating)\n\ncrdt_tree_0000 crdt_tree_0001 crdt_tree_0010 crdt_tree_0100 \n      182.0236       180.1045       172.6976       177.2816 \n\n\nHere we see the model, with cp = 0.010, performs best.\nNote that by only using these three features, we are severely limiting our models performance. Let’s quickly assess using all available predictors.\n\ncrdt_tree_all = rpart(Rating ~ ., data = crdt_trn)\n\n\nrpart.plot(crdt_tree_all)\n\n\n\n\n\n\n\n\nNotice that this model only splits based on Limit despite using all features. This should be a big hint about which variables are useful for prediction.\n\ncalc_rmse(\n  actual = crdt_tst$Rating,\n  predicted = predict(crdt_tree_all, crdt_tst)\n)\n\n[1] 45.17942\n\n\nThis model performs much better. You should try something similar with the KNN models above. Also, consider comparing this result to results from last lectures using linear models.\nNotice that we’ve been using that trusty predict() function here again.\n\npredict(crdt_tree_all, crdt_tst[1:5, ])\n\n       1        2        3        4        5 \n520.7812 645.4000 520.7812 440.7368 520.7812 \n\n\nWhat does this code do? It estimates the mean Rating given the feature information (the “x” values) from the first five observations from the validation data using a decision tree model with default tuning parameters. Hopefully a theme is emerging.",
    "crumbs": [
      "Course Content",
      "Week 10",
      "Nonparametric Regression"
    ]
  },
  {
    "objectID": "content/Week_10/10a.html#footnotes",
    "href": "content/Week_10/10a.html#footnotes",
    "title": "Nonparametric Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe chose to start with linear regression because most students the social sciences should already be familiar with the basic notion.↩︎\nThe usual distance when you hear distance. That is, unless you drive a taxicab.↩︎\nFor this reason, KNN is often not used in practice, but it is very useful learning tool.↩︎\nMany texts use the term complex instead of flexible. We feel this is confusing as complex is often associated with difficult. KNN with \\(k = 1\\) is actually a very simple model to understand, but it is very flexible as defined here.↩︎\nTo exhaust all possible splits of a variable, we would need to consider the midpoint between each of the order statistics of the variable. To exhaust all possible splits, we would need to do this for each of the feature variables.↩︎\nIt’s really an upside tree isn’t it?↩︎\nFlexibility parameter would be a better name.↩︎\nThe rpart function in R would allow us to use others, but we will always just leave their values as the default values.↩︎\nThere is a question of whether or not we should use these variables. For example, should men and women be given different ratings when all other variables are the same? Using the Gender variable allows for this to happen. Also, you might think, just don’t use the Gender variable. Unfortunately, it’s not that easy. There is an increasingly popular field of study centered around these ideas called machine learning fairness.↩︎\nThere are many other KNN functions in R. However, the operation and syntax of knnreg() better matches other functions we use in this course.↩︎\nWait. Doesn’t this sort of create an arbitrary distance between the categories? Why \\(0\\) and \\(1\\) and not \\(-42\\) and \\(51\\)? Good question. This hints at the notion of pre-processing. We’re going to hold off on this for now, but, often when performing k-nearest neighbors, you should try scaling all of the features to have mean \\(0\\) and variance \\(1\\).↩︎",
    "crumbs": [
      "Course Content",
      "Week 10",
      "Nonparametric Regression"
    ]
  },
  {
    "objectID": "content/Week_11/11a.html",
    "href": "content/Week_11/11a.html",
    "title": "Bias-Variance Tradeoff",
    "section": "",
    "text": "This page.\n Chapter 2 in Introduction to Statistical Learning with Applications in R.\n Chapter 8 (briefly) in Introduction to Statistical Learning with Applications in R.\n\n\n\n\nWhat is the relationship between bias, variance, and mean squared error?\nWhat is the relationship between model flexibility and training error?\nWhat is the relationship between model flexibility and validation (or test) error?\nWhat is a random forest?",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "content/Week_11/11a.html#required-reading",
    "href": "content/Week_11/11a.html#required-reading",
    "title": "Bias-Variance Tradeoff",
    "section": "",
    "text": "This page.\n Chapter 2 in Introduction to Statistical Learning with Applications in R.\n Chapter 8 (briefly) in Introduction to Statistical Learning with Applications in R.\n\n\n\n\nWhat is the relationship between bias, variance, and mean squared error?\nWhat is the relationship between model flexibility and training error?\nWhat is the relationship between model flexibility and validation (or test) error?\nWhat is a random forest?",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "content/Week_11/11a.html#illustration-of-bias-vs.-variance",
    "href": "content/Week_11/11a.html#illustration-of-bias-vs.-variance",
    "title": "Bias-Variance Tradeoff",
    "section": "Illustration of Bias vs. Variance",
    "text": "Illustration of Bias vs. Variance\nBias is about how close you are on average to the correct answer. Variance is about how scattered your estimates would be if you repeated your experiment with new data.\n\n\n\n\n\nImage from MachineLearningPlus.com\n\n\n\n\nWe care about these things because we usually only have our one dataset (when we’re not creating simulated data, that is), but need to know something about how bias and variance tend to look when we change our model complexity.",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "content/Week_11/11a.html#r-setup-and-source",
    "href": "content/Week_11/11a.html#r-setup-and-source",
    "title": "Bias-Variance Tradeoff",
    "section": "R Setup and Source",
    "text": "R Setup and Source\n\nlibrary(tibble)     # data frame printing\nlibrary(dplyr)      # data manipulation\n\nlibrary(caret)      # fitting knn\nlibrary(rpart)      # fitting trees\nlibrary(rpart.plot) # plotting trees",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "content/Week_11/11a.html#the-regression-setup",
    "href": "content/Week_11/11a.html#the-regression-setup",
    "title": "Bias-Variance Tradeoff",
    "section": "The Regression Setup",
    "text": "The Regression Setup\nConsider the general regression setup where we are given a random pair \\((X, Y) \\in \\mathbb{R}^p \\times \\mathbb{R}\\). We would like to “predict” \\(Y\\) with some function of \\(X\\), say, \\(f(X)\\).\nTo clarify what we mean by “predict,” we specify that we would like \\(f(X)\\) to be “close” to \\(Y\\). To further clarify what we mean by “close,” we define the squared error loss of estimating \\(Y\\) using \\(f(X)\\).\n\\[\nL(Y, f(X)) \\triangleq (Y - f(X)) ^ 2\n\\]\nNow we can clarify the goal of regression, which is to minimize the above loss, on average. We call this the risk of estimating \\(Y\\) using \\(f(X)\\).\n\\[\nR(Y, f(X)) \\triangleq \\mathbb{E}[L(Y, f(X))] = \\mathbb{E}_{X, Y}[(Y - f(X)) ^ 2]\n\\]\nBefore attempting to minimize the risk, we first re-write the risk after conditioning on \\(X\\).\n\\[\n\\mathbb{E}_{X, Y} \\left[ (Y - f(X)) ^ 2 \\right] = \\mathbb{E}_{X} \\mathbb{E}_{Y \\mid X} \\left[ ( Y - f(X) ) ^ 2 \\mid X = x \\right]\n\\]\nMinimizing the right-hand side is much easier, as it simply amounts to minimizing the inner expectation with respect to \\(Y \\mid X\\), essentially minimizing the risk pointwise, for each \\(x\\).\nIt turns out, that the risk is minimized by setting \\(f(x)\\) to be equal the conditional mean of \\(Y\\) given \\(X\\),\n\\[\nf(x) = \\mathbb{E}(Y \\mid X = x)\n\\]\nwhich we call the regression function.1\nNote that the choice of squared error loss is somewhat arbitrary. Suppose instead we chose absolute error loss.\n\\[\nL(Y, f(X)) \\triangleq | Y - f(X) |\n\\]\nThe risk would then be minimized setting \\(f(x)\\) equal to the conditional median.\n\\[\nf(x) = \\text{median}(Y \\mid X = x)\n\\]\nDespite this possibility, our preference will still be for squared error loss. The reasons for this are numerous, including: historical, ease of optimization, and protecting against large deviations.\nNow, given data \\(\\mathcal{D} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}\\), our goal becomes finding some \\(\\hat{f}\\) that is a good estimate of the regression function \\(f\\). We’ll see that this amounts to minimizing what we call the reducible error.",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "content/Week_11/11a.html#reducible-and-irreducible-error",
    "href": "content/Week_11/11a.html#reducible-and-irreducible-error",
    "title": "Bias-Variance Tradeoff",
    "section": "Reducible and Irreducible Error",
    "text": "Reducible and Irreducible Error\nSuppose that we obtain some \\(\\hat{f}\\), how well does it estimate \\(f\\)? We define the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\). A good \\(\\hat{f}\\) will have a low expected prediction error.\n\\[\n\\text{EPE}\\left(Y, \\hat{f}(X)\\right) \\triangleq \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  \\left( Y - \\hat{f}(X) \\right)^2 \\right]\n\\]\nThis expectation is over \\(X\\), \\(Y\\), and also \\(\\mathcal{D}\\). The estimate \\(\\hat{f}\\) is actually random depending on the data, \\(\\mathcal{D}\\), used to estimate \\(\\hat{f}\\). We could actually write \\(\\hat{f}(X, \\mathcal{D})\\) to make this dependence explicit, but our notation will become cumbersome enough as it is.\nLike before, we’ll condition on \\(X\\). This results in the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\) when \\(X = x\\).\n\\[\n\\text{EPE}\\left(Y, \\hat{f}(x)\\right) =\n\\mathbb{E}_{Y \\mid X, \\mathcal{D}} \\left[  \\left(Y - \\hat{f}(X) \\right)^2 \\mid X = x \\right] =\n\\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left[  \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]}_\\textrm{reducible error} +\n\\underbrace{\\mathbb{V}_{Y \\mid X} \\left[ Y \\mid X = x \\right]}_\\textrm{irreducible error}\n\\]\nA number of things to note here:\n\nThe expected prediction error is for a random \\(Y\\) given a fixed \\(x\\) and a random \\(\\hat{f}\\). As such, the expectation is over \\(Y \\mid X\\) and \\(\\mathcal{D}\\). Our estimated function \\(\\hat{f}\\) is random depending on the data, \\(\\mathcal{D}\\), which is used to perform the estimation.\nThe expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\) when \\(X = x\\) has been decomposed into two errors:\n\nThe reducible error, which is the expected squared error loss of estimation \\(f(x)\\) using \\(\\hat{f}(x)\\) at a fixed point \\(x\\). The only thing that is random here is \\(\\mathcal{D}\\), the data used to obtain \\(\\hat{f}\\). (Both \\(f\\) and \\(x\\) are fixed.) We’ll often call this reducible error the mean squared error of estimating \\(f(x)\\) using \\(\\hat{f}\\) at a fixed point \\(x\\). \\[ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) \\triangleq \\mathbb{E}_{\\mathcal{D}} \\left[  \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]\\]\nThe irreducible error. This is simply the variance of \\(Y\\) given that \\(X = x\\), essentially noise that we do not want to learn. This is also called the Bayes error.\n\n\nAs the name suggests, the reducible error is the error that we have some control over. But how do we control this error?",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "content/Week_11/11a.html#bias-variance-decomposition",
    "href": "content/Week_11/11a.html#bias-variance-decomposition",
    "title": "Bias-Variance Tradeoff",
    "section": "Bias-Variance Decomposition",
    "text": "Bias-Variance Decomposition\nAfter decomposing the expected prediction error into reducible and irreducible error, we can further decompose the reducible error.\nRecall the definition of the bias of an estimator.\n\\[\n\\text{bias}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left[\\hat{\\theta}\\right] - \\theta\n\\]\nAlso recall the definition of the variance of an estimator.\n\\[\n\\mathbb{V}(\\hat{\\theta}) = \\text{var}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left [ ( \\hat{\\theta} -\\mathbb{E}\\left[\\hat{\\theta}\\right] )^2 \\right]\n\\]\nUsing this, we further decompose the reducible error (mean squared error) into bias squared and variance.\n\\[\n\\text{MSE}\\left(f(x), \\hat{f}(x)\\right) =\n\\mathbb{E}_{\\mathcal{D}} \\left[  \\left(f(x) - \\hat{f}(x) \\right)^2 \\right] =\n\\underbrace{\\left(f(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right]  \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}(x) \\right)} +\n\\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}(x) \\right)}\n\\]\nThis is actually a common fact in estimation theory, but we have stated it here specifically for estimation of some regression function \\(f\\) using \\(\\hat{f}\\) at some point \\(x\\).\n\\[\n\\text{MSE}\\left(f(x), \\hat{f}(x)\\right) = \\text{bias}^2 \\left(\\hat{f}(x) \\right) + \\text{var} \\left(\\hat{f}(x) \\right)\n\\]\nIn a perfect world, we would be able to find some \\(\\hat{f}\\) which is unbiased, that is \\(\\text{bias}\\left(\\hat{f}(x) \\right) = 0\\), which also has low variance. In practice, this isn’t always possible.\nIt turns out, there is a bias-variance tradeoff. That is, often, the more bias in our estimation, the lesser the variance. Similarly, less variance is often accompanied by more bias. Flexible models tend to be unbiased, but highly variable. Simple models are often extremely biased, but have low variance.\nIn the context of regression, models are biased when:\n\nParametric: The form of the model does not incorporate all the necessary variables, or the form of the relationship is too simple. For example, a parametric model assumes a linear relationship, but the true relationship is quadratic.\nNon-parametric: The model provides too much smoothing.\n\nIn the context of regression, models are variable when:\n\nParametric: The form of the model incorporates too many variables, or the form of the relationship is too flexible. For example, a parametric model assumes a cubic relationship, but the true relationship is linear.\nNon-parametric: The model does not provide enough smoothing. It is very, “wiggly.” Recall our KNN model example from Content 08\n\nSo for us, to select a model that appropriately balances the tradeoff between bias and variance, and thus minimizes the reducible error, we need to select a model of the appropriate flexibility for the data.\nRecall that when fitting models, we’ve seen that train RMSE decreases as model flexibility is increasing. (Technically it is non-increasing.) For validation RMSE, we expect to see a U-shaped curve. Importantly, validation RMSE decreases, until a certain flexibility, then begins to increase.\nNow we can understand why this is happening. The expected test RMSE is essentially the expected prediction error, which we now known decomposes into (squared) bias, variance, and the irreducible Bayes error. The following plots show three examples of this.\n\n\n\n\n\n\n\n\n\nThe three plots show three examples of the bias-variance tradeoff. In the left panel, the variance influences the expected prediction error more than the bias. In the right panel, the opposite is true. The middle panel is somewhat neutral. In all cases, the difference between the Bayes error (the horizontal dashed grey line) and the expected prediction error (the solid black curve) is exactly the mean squared error, which is the sum of the squared bias (blue curve) and variance (orange curve). The vertical line indicates the flexibility that minimizes the prediction error.\nTo summarize, if we assume that irreducible error can be written as\n\\[\n\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2\n\\]\nthen we can write the full decomposition of the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}\\) when \\(X = x\\) as\n\\[\n\\text{EPE}\\left(Y, \\hat{f}(x)\\right) =\n\\underbrace{\\text{bias}^2\\left(\\hat{f}(x)\\right) + \\text{var}\\left(\\hat{f}(x)\\right)}_\\textrm{reducible error} + \\sigma^2.\n\\]\nAs model flexibility increases, bias decreases, while variance increases. By understanding the tradeoff between bias and variance, we can manipulate model flexibility to find a model that will predict well on unseen observations.\n\n\n\n\n\n\n\n\n\nTying this all together, the above image shows how we “expect” training and validation error to behavior in relation to model flexibility.2 In practice, we won’t always see such a nice “curve” in the validation error, but we expect to see the general trends.",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "content/Week_11/11a.html#using-simulation-to-estimate-bias-and-variance",
    "href": "content/Week_11/11a.html#using-simulation-to-estimate-bias-and-variance",
    "title": "Bias-Variance Tradeoff",
    "section": "Using Simulation to Estimate Bias and Variance",
    "text": "Using Simulation to Estimate Bias and Variance\nWe will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function \\(f(x) = x^2\\).\n\nf = function(x) {\n  x ^ 2\n}\n\nMore specifically, we’d like to predict an observation, \\(Y\\), given that \\(X = x\\) by using \\(\\hat{f}(x)\\) where\n\\[\n\\mathbb{E}[Y \\mid X = x] = f(x) = x^2\n\\]\nand\n\\[\n\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2.\n\\]\nAlternatively, we could write this as\n\\[\nY = f(X) + \\epsilon\n\\]\nwhere \\(\\mathbb{E}[\\epsilon] = 0\\) and \\(\\mathbb{V}[\\epsilon] = \\sigma ^ 2\\). In this formulation, we call \\(f(X)\\) the signal and \\(\\epsilon\\) the noise.\nTo carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following R code.\n\ngen_sim_data = function(f, sample_size = 100) {\n  x = runif(n = sample_size, min = 0, max = 1)\n  y = rnorm(n = sample_size, mean = f(x), sd = 0.3)\n  tibble(x, y)\n}\n\nAlso note that if you prefer to think of this situation using the \\(Y = f(X) + \\epsilon\\) formulation, the following code represents the same data generating process.\n\ngen_sim_data = function(f, sample_size = 100) {\n  x = runif(n = sample_size, min = 0, max = 1)\n  eps = rnorm(n = sample_size, mean = 0, sd = 0.3)\n  y = f(x) + eps\n  tibble(x, y)\n}\n\nTo completely specify the data generating process, we have made more model assumptions than simply \\(\\mathbb{E}[Y \\mid X = x] = x^2\\) and \\(\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2\\). In particular,\n\nThe \\(x_i\\) in \\(\\mathcal{D}\\) are sampled from a uniform distribution over \\([0, 1]\\).\nThe \\(x_i\\) and \\(\\epsilon\\) are independent.\nThe \\(y_i\\) in \\(\\mathcal{D}\\) are sampled from the conditional normal distribution.\n\n\\[\nY \\mid X \\sim N(f(x), \\sigma^2)\n\\]\nUsing this setup, we will generate datasets, \\(\\mathcal{D}\\), with a sample size \\(n = 100\\) and fit four models.\n\\[\n\\begin{aligned}\n\\texttt{predict(fit0, x)} &= \\hat{f}_0(x) = \\hat{\\beta}_0\\\\\n\\texttt{predict(fit1, x)} &= \\hat{f}_1(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\\\\n\\texttt{predict(fit2, x)} &= \\hat{f}_2(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 \\\\\n\\texttt{predict(fit9, x)} &= \\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9\n\\end{aligned}\n\\]\nTo get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.\n\nset.seed(1)\nsim_data = gen_sim_data(f)\n\n\nfit_0 = lm(y ~ 1,                   data = sim_data)\nfit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\nfit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\nfit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\n\nNote that technically we’re being lazy and using orthogonal polynomials, but the fitted values are the same, so this makes no difference for our purposes. These could be KNN, or decision trees just the same - the principle still applies.\nPlotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.\n\n\n\n\n\n\n\n\n\nThe following three plots were created using three additional simulated datasets. The zero predictor and ninth degree polynomial were fit to each.\n\n\n\n\n\n\n\n\n\nThis plot should make clear the difference between the bias and variance of these two models. The zero predictor model is clearly wrong, that is, biased, but nearly the same for each of the datasets, since it has very low variance.\nWhile the ninth degree model doesn’t appear to be correct for any of these three simulations, we’ll see that on average it is, and thus is performing unbiased estimation. These plots do however clearly illustrate that the ninth degree polynomial is extremely variable. Each dataset results in a very different fitted model. Correct on average isn’t the only goal we’re after, since in practice, we’ll only have a single dataset. This is why we’d also like our models to exhibit low variance.\nWe could have also fit \\(k\\)-nearest neighbors models to these three datasets.\n\n\n\n\n\n\n\n\n\nHere we see that when \\(k = 100\\) we have a biased model with very low variance.3 When \\(k = 5\\), we again have a highly variable model.\nThese two sets of plots reinforce our intuition about the bias-variance tradeoff. Flexible models (ninth degree polynomial and \\(k\\) = 5) are highly variable, and often unbiased. Simple models (zero predictor linear model and \\(k = 100\\)) are very biased, but have extremely low variance.\nWe will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates of \\(f(x)\\) given by these four models at the point \\(x = 0.90\\). We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.\n\nset.seed(1)\nn_sims = 250\nn_models = 4\nx = data.frame(x = 0.90) # fixed point at which we make predictions\npredictions = matrix(0, nrow = n_sims, ncol = n_models)\n\n\nfor (sim in 1:n_sims) {\n\n  # simulate new, random, training data\n  # this is the only random portion of the bias, var, and mse calculations\n  # this allows us to calculate the expectation over D\n  sim_data = gen_sim_data(f)\n\n  # fit models\n  fit_0 = lm(y ~ 1,                   data = sim_data)\n  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\n  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\n  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\n\n  # get predictions\n  predictions[sim, 1] = predict(fit_0, x)\n  predictions[sim, 2] = predict(fit_1, x)\n  predictions[sim, 3] = predict(fit_2, x)\n  predictions[sim, 4] = predict(fit_9, x)\n}\n\n\n\n\n\n\n\n\n\n\nThe above plot shows the predictions for each of the 250 simulations of each of the four models of different polynomial degrees. The truth, \\(f(x = 0.90) = (0.9)^2 = 0.81\\), is given by the solid black horizontal line.\nTwo things are immediately clear:\n\nAs flexibility increases, bias decreases. The mean of a model’s predictions is closer to the truth.\nAs flexibility increases, variance increases. The variance about the mean of a model’s predictions increases.\n\nThe goal of this simulation study is to show that the following holds true for each of the four models.\n\\[\n\\text{MSE}\\left(f(0.90), \\hat{f}_k(0.90)\\right) =\n\\underbrace{\\left(\\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] - f(0.90) \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}_k(0.90) \\right)} +\n\\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}_k(0.90) - \\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}_k(0.90) \\right)}\n\\]\nWe’ll use the empirical results of our simulations to estimate these quantities. (Yes, we’re using estimation to justify facts about estimation.) Note that we’ve actually used a rather small number of simulations. In practice we should use more, but for the sake of computation time, we’ve performed just enough simulations to obtain the desired results. (Since we’re estimating estimation, the bigger the sample size, the better.)\nTo estimate the mean squared error of our predictions, we’ll use\n\\[\n\\widehat{\\text{MSE}}\\left(f(0.90), \\hat{f}_k(0.90)\\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(f(0.90) - \\hat{f}_k^{[i]}(0.90) \\right)^2\n\\]\nwhere \\(\\hat{f}_k^{[i]}(0.90)\\) is the estimate of \\(f(0.90)\\) using the \\(i\\)-th from the polynomial degree \\(k\\) model.\nWe also write an accompanying R function.\n\nget_mse = function(truth, estimate) {\n  mean((estimate - truth) ^ 2)\n}\n\nSimilarly, for the bias of our predictions we use,\n\\[\n\\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right)  = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) \\right) - f(0.90)\n\\]\nAnd again, we write an accompanying R function.\n\nget_bias = function(estimate, truth) {\n  mean(estimate) - truth\n}\n\nLastly, for the variance of our predictions we have\n\\[\n\\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k^{[i]}(0.90) \\right)^2\n\\]\nWhile there is already R function for variance, the following is more appropriate in this situation.\n\nget_var = function(estimate) {\n  mean((estimate - mean(estimate)) ^ 2)\n}\n\nTo quickly obtain these results for each of the four models, we utilize the apply() function.\n\nbias = apply(predictions, MAR = 2, get_bias, truth = f(x = 0.90))\nvariance = apply(predictions, MAR = 2, get_var)\nmse = apply(predictions, MAR = 2, get_mse, truth = f(x = 0.90))\n\nWe summarize these results in the following table.\n\n\n\n\n\nDegree\nMean Squared Error\nBias Squared\nVariance\n\n\n\n\n0\n0.22643\n0.22476\n0.00167\n\n\n1\n0.00829\n0.00508\n0.00322\n\n\n2\n0.00387\n0.00005\n0.00381\n\n\n9\n0.01019\n0.00002\n0.01017\n\n\n\n\n\nA number of things to notice here:\n\nWe use squared bias in this table. Since bias can be positive or negative, squared bias is more useful for observing the trend as flexibility increases.\nThe squared bias trend which we see here is decreasing as flexibility increases, which we expect to see in general.\nThe exact opposite is true of variance. As model flexibility increases, variance increases.\nThe mean squared error, which is a function of the bias and variance, decreases, then increases. This is a result of the bias-variance tradeoff. We can decrease bias, by increasing variance. Or, we can decrease variance by increasing bias. By striking the correct balance, we can find a good mean squared error!\n\nWe can check for these trends with the diff() function in R.\n\nall(diff(bias ^ 2) &lt; 0)\n\n[1] TRUE\n\nall(diff(variance) &gt; 0)\n\n[1] TRUE\n\ndiff(mse) &lt; 0\n\n    1     2     9 \n TRUE  TRUE FALSE \n\n\nThe models with polynomial degrees 2 and 9 are both essentially unbiased. We see some bias here as a result of using simulation. If we increased the number of simulations, we would see both biases go down. Since they are both unbiased, the model with degree 2 outperforms the model with degree 9 due to its smaller variance.\nModels with degree 0 and 1 are biased because they assume the wrong form of the regression function. While the degree 9 model does this as well, it does include all the necessary polynomial degrees.\n\\[\n\\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9\n\\]\nThen, since least squares estimation is unbiased, importantly,\n\\[\n\\mathbb{E}\\left[\\hat{\\beta}_d\\right] = \\beta_d = 0\n\\]\nfor \\(d = 3, 4, \\ldots 9\\), we have\n\\[\n\\mathbb{E}\\left[\\hat{f}_9(x)\\right] = \\beta_0 + \\beta_1 x + \\beta_2 x^2\n\\]\nNow we can finally verify the bias-variance decomposition.\n\nbias ^ 2 + variance == mse\n\n    0     1     2     9 \nFALSE FALSE FALSE  TRUE \n\n\nBut wait, this says it isn’t true, except for the degree 9 model? It turns out, this is simply a computational issue. If we allow for some very small error tolerance, we see that the bias-variance decomposition is indeed true for predictions from these for models.\n\nall.equal(bias ^ 2 + variance, mse)\n\n[1] TRUE\n\n\nSee ?all.equal() for details.",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "content/Week_11/11a.html#model-flexibility",
    "href": "content/Week_11/11a.html#model-flexibility",
    "title": "Bias-Variance Tradeoff",
    "section": "Model Flexibility",
    "text": "Model Flexibility\nLet’s return to the simulated dataset we used occaisionally in the linear regression content. Recall there was a single feature \\(x\\) with the following properties:\n\n# define regression function\ncubic_mean = function(x) {\n  1 - 2 * x - 3 * x ^ 2 + 5 * x ^ 3\n}\n\nWe then generated some data around this function with some added noise:\n\n# define full data generating process\ngen_slr_data = function(sample_size = 100, mu) {\n  x = runif(n = sample_size, min = -1, max = 1)\n  y = mu(x) + rnorm(n = sample_size)\n  tibble(x, y)\n}\n\nAfter defining the data generating process, we generate and split the data.\n\n# simulate entire dataset\nset.seed(3)\nsim_slr_data = gen_slr_data(sample_size = 100, mu = cubic_mean)\n\n# test-train split\nslr_trn_idx = sample(nrow(sim_slr_data), size = 0.8 * nrow(sim_slr_data))\nslr_trn = sim_slr_data[slr_trn_idx, ]\nslr_tst = sim_slr_data[-slr_trn_idx, ]\n\n# estimation-validation split\nslr_est_idx = sample(nrow(slr_trn), size = 0.8 * nrow(slr_trn))\nslr_est = slr_trn[slr_est_idx, ]\nslr_val = slr_trn[-slr_est_idx, ]\n\n# check data\nhead(slr_trn, n = 10)\n\n# A tibble: 10 × 2\n        x      y\n    &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.573 -1.18 \n 2  0.807  0.576\n 3  0.272 -0.973\n 4 -0.813 -1.78 \n 5 -0.161  0.833\n 6  0.736  1.07 \n 7 -0.242  2.97 \n 8  0.520 -1.64 \n 9 -0.664  0.269\n10 -0.777 -2.02 \n\n\nFor validating models, we will use RMSE.\n\n# helper function for calculating RMSE\ncalc_rmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\nLet’s check how linear, k-nearest neighbors, and decision tree models fit to this data make errors, while paying attention to their flexibility.\n\n\n\n\n\n\n\n\n\nThis picture is an idealized version of what we expect to see, but we’ll illustrate the sorts of validate “curves” that we might see in practice.\nNote that in the following three sub-sections, a significant portion of the code is suppressed for visual clarity. See the source document for full details.\n\nLinear Models\nFirst up, linear models. We will fit polynomial models with degree from one to nine, and then validate.\n\n# fit polynomial models\npoly_mod_est_list = list(\n  poly_mod_1_est = lm(y ~ poly(x, degree = 1), data = slr_est),\n  poly_mod_2_est = lm(y ~ poly(x, degree = 2), data = slr_est),\n  poly_mod_3_est = lm(y ~ poly(x, degree = 3), data = slr_est),\n  poly_mod_4_est = lm(y ~ poly(x, degree = 4), data = slr_est),\n  poly_mod_5_est = lm(y ~ poly(x, degree = 5), data = slr_est),\n  poly_mod_6_est = lm(y ~ poly(x, degree = 6), data = slr_est),\n  poly_mod_7_est = lm(y ~ poly(x, degree = 7), data = slr_est),\n  poly_mod_8_est = lm(y ~ poly(x, degree = 8), data = slr_est),\n  poly_mod_9_est = lm(y ~ poly(x, degree = 9), data = slr_est)\n)\n\nThe plot below visualizes the results.\n\n\n\n\n\n\n\n\n\nWhat do we see here? As the polynomial degree increases:\n\nThe training error decreases.\nThe validation error decreases, then increases.\n\nThis more of less matches the idealized version above, but the validation “curve” is much more jagged. This is something that we can expect in practice.\nWe have previously noted that training error isn’t particularly useful for validating models. That is still true. However, it can be useful for checking that everything is working as planned. In this case, since we known that training error decreases as model flexibility increases, we can verify our intuition that a higher degree polynomial is indeed more flexible.4\n\n\nk-Nearest Neighbors\nNext up, k-nearest neighbors. We will consider values for \\(k\\) that are odd and between \\(1\\) and \\(45\\) inclusive.\n\n# helper function for fitting knn models\nfit_knn_mod = function(neighbors) {\n  knnreg(y ~ x, data = slr_est, k = neighbors)\n}\n\n\n# define values of tuning parameter k to evaluate\nk_to_try = seq(from = 1, to = 45, by = 2)\n\n# fit knn models\nknn_mod_est_list = lapply(k_to_try, fit_knn_mod)\n\nThe plot below visualizes the results.\n\n\n\n\n\n\n\n\n\nHere we see the “opposite” of the usual plot. Why? Because with k-nearest neighbors, a small value of \\(k\\) generates a flexible model compared to larger values of \\(k\\). So visually, this plot is flipped. That is we see that as \\(k\\) increases:\n\nThe training error increases.\nThe validation error decreases, then increases.\n\nImportant to note here: the pattern above only holds “in general,” that is, there can be minor deviations in the validation pattern along the way. This is due to the random nature of selection the data for the validate set.\n\n\nDecision Trees\nLastly, we evaluate some decision tree models. We choose some arbitrary values of cp to evaluate, while holding minsplit constant at 5. There are arbitrary choices that produce a plot that is useful for discussion.\n\n# helper function for fitting decision tree models\ntree_knn_mod = function(flex) {\n  rpart(y ~ x, data = slr_est, cp = flex, minsplit = 5)\n}\n\n\n# define values of tuning parameter cp to evaluate\ncp_to_try = c(0.5, 0.3, 0.1, 0.05, 0.01, 0.001, 0.0001)\n\n# fit decision tree models\ntree_mod_est_list = lapply(cp_to_try, tree_knn_mod)\n\nThe plot below visualizes the results.\n\n\n\n\n\n\n\n\n\nBased on this plot, how is cp related to model flexibility?5",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "content/Week_11/11a.html#footnotes",
    "href": "content/Week_11/11a.html#footnotes",
    "title": "Bias-Variance Tradeoff",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that in this section, we will refer to \\(f(x)\\) as the regression function instead of \\(\\mu(x)\\) for unimportant and arbitrary reasons.↩︎\nSomeday, someone will tell you this is a lie. They aren’t wrong. In modern deep learning, there is a concept called Deep Double Descent. See also @belkin2018reconciling.↩︎\nIt’s actually the same as the 0 predictor linear model. Can you see why?↩︎\nIn practice, if you already know how your model’s flexibility works, by checking that the training error goes down as you increase flexibility, you can check that you have done your coding and model training correctly.↩︎\nAs cp increases, model flexibility decreases.↩︎",
    "crumbs": [
      "Course Content",
      "Week 11",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "content/Week_12/12a.html",
    "href": "content/Week_12/12a.html",
    "title": "Classification",
    "section": "",
    "text": "This page.\n Chapter 4 in Introduction to Statistical Learning with Applications in R.\n\n\n\n\nHow do we make predictions about binary responses?\nWhy should we be concerned about using simple linear regression?\nWhat is the right way to assess the accuracy of such a model?",
    "crumbs": [
      "Course Content",
      "Week 12",
      "Classification"
    ]
  },
  {
    "objectID": "content/Week_12/12a.html#required-reading",
    "href": "content/Week_12/12a.html#required-reading",
    "title": "Classification",
    "section": "",
    "text": "This page.\n Chapter 4 in Introduction to Statistical Learning with Applications in R.\n\n\n\n\nHow do we make predictions about binary responses?\nWhy should we be concerned about using simple linear regression?\nWhat is the right way to assess the accuracy of such a model?",
    "crumbs": [
      "Course Content",
      "Week 12",
      "Classification"
    ]
  },
  {
    "objectID": "content/Week_12/12a.html#visualization-for-classification",
    "href": "content/Week_12/12a.html#visualization-for-classification",
    "title": "Classification",
    "section": "Visualization for Classification",
    "text": "Visualization for Classification\nOften, some simple visualizations can suggest simple classification rules. To quickly create some useful visualizations, we use the featurePlot() function from the caret() package.\n\nlibrary(caret)\n\nA density plot can often suggest a simple split based on a numeric predictor. Essentially this plot graphs a density estimate\n\\[\n\\hat{f}_{X_i}(x_i \\mid Y = k)\n\\]\nfor each numeric predictor \\(x_i\\) and each category \\(k\\) of the response \\(y\\).\n\nfeaturePlot(x = default_trn[, c(\"balance\", \"income\")],\n            y = default_trn$default,\n            plot = \"density\",\n            scales = list(x = list(relation = \"free\"),\n                          y = list(relation = \"free\")),\n            adjust = 1.5,\n            pch = \"|\",\n            layout = c(2, 1),\n            auto.key = list(columns = 2))\n\n\n\n\n\n\n\n\nSome notes about the arguments to this function:\n\nx is a data frame containing only numeric predictors. It would be nonsensical to estimate a density for a categorical predictor.\ny is the response variable. It needs to be a factor variable. If coded as 0 and 1, you will need to coerce to factor for plotting.\nplot specifies the type of plot, here density.\nscales defines the scale of the axes for each plot. By default, the axis of each plot would be the same, which often is not useful, so the arguments here, a different axis for each plot, will almost always be used.\nadjust specifies the amount of smoothing used for the density estimate.\npch specifies the plot character used for the bottom of the plot.\nlayout places the individual plots into rows and columns. For some odd reason, it is given as (col, row).\nauto.key defines the key at the top of the plot. The number of columns should be the number of categories.\n\nIs the income variable useful here? How about the balance variable? There seems to be a big difference in default status at a balance of about 1400. We will use this information shortly.\n\nfeaturePlot(x = default_trn[, c(\"balance\", \"income\")],\n            y = default_trn$student,\n            plot = \"density\",\n            scales = list(x = list(relation = \"free\"),\n                          y = list(relation = \"free\")),\n            adjust = 1.5,\n            pch = \"|\",\n            layout = c(2, 1),\n            auto.key = list(columns = 2))\n\n\n\n\n\n\n\n\nAbove, we create a similar plot, except with student as the response. It’s not that we want to predict student – rather, we want to see if student is correlated with balance or income. We see that students often carry a slightly larger balance, and have far lower income. This will be useful to know when making more complicated classifiers.\n\nfeaturePlot(x = default_trn[, c(\"student\", \"balance\", \"income\")],\n            y = default_trn$default,\n            plot = \"pairs\",\n            auto.key = list(columns = 2))\n\n\n\n\n\n\n\n\nWe can use plot = \"pairs\" to consider multiple variables at the same time. This plot reinforces using balance to create a classifier, and again shows that income seems not that useful.\n\nlibrary(ellipse)\nfeaturePlot(x = default_trn[, c(\"balance\", \"income\")],\n            y = default_trn$default,\n            plot = \"ellipse\",\n            auto.key = list(columns = 2))\n\n\n\n\n\n\n\n\nSimilar to pairs is a plot of type ellipse, which requires the ellipse package. Here we only use numeric predictors, as essentially we are assuming multivariate normality. The ellipses mark points of equal density.",
    "crumbs": [
      "Course Content",
      "Week 12",
      "Classification"
    ]
  },
  {
    "objectID": "content/Week_12/12a.html#a-simple-classifier",
    "href": "content/Week_12/12a.html#a-simple-classifier",
    "title": "Classification",
    "section": "A Simple Classifier",
    "text": "A Simple Classifier\nA very simple classifier is a rule based on a boundary \\(b\\) for a particular input variable \\(x\\).\n\\[\n\\hat{C}(x) =\n\\begin{cases}\n      1 & x &gt; b \\\\\n      0 & x \\leq b\n\\end{cases}\n\\]\nBased on the first plot, we believe we can use balance to create a reasonable classifier. In particular,\n\\[\n\\hat{C}(\\texttt{balance}) =\n\\begin{cases}\n      \\text{Yes} & \\texttt{balance} &gt; 1400 \\\\\n      \\text{No} & \\texttt{balance} \\leq 1400\n   \\end{cases}\n\\]\nSo we predict an individual is a defaulter if their balance is above 1400, and not a defaulter if the balance is 1400 or less.\n\nsimple_class = function(x, boundary, above = 1, below = 0) {\n  ifelse(x &gt; boundary, above, below)\n}\n\nWe write a simple R function that compares a variable to a boundary, then use it to make predictions on the train and test sets with our chosen variable and boundary.\n\ndefault_trn_pred = simple_class(x = default_trn$balance,\n                                boundary = 1400, above = \"Yes\", below = \"No\")\ndefault_tst_pred = simple_class(x = default_tst$balance,\n                                boundary = 1400, above = \"Yes\", below = \"No\")\nhead(default_tst_pred, n = 10)\n\n [1] \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\"",
    "crumbs": [
      "Course Content",
      "Week 12",
      "Classification"
    ]
  },
  {
    "objectID": "content/Week_12/12a.html#metrics-for-classification",
    "href": "content/Week_12/12a.html#metrics-for-classification",
    "title": "Classification",
    "section": "Metrics for Classification",
    "text": "Metrics for Classification\nIn the classification setting, there are a large number of metrics to assess how well a classifier is performing.\nOne of the most obvious things to do is arrange predictions and true values in a cross table.\n\n(trn_tab = table(predicted = default_trn_pred, actual = default_trn$default))\n\n         actual\npredicted   No  Yes\n      No  4337   23\n      Yes  496  144\n\n\n\n(tst_tab = table(predicted = default_tst_pred, actual = default_tst$default))\n\n         actual\npredicted   No  Yes\n      No  4343   29\n      Yes  491  137\n\n\nOften we give specific names to individual cells of these tables, and in the predictive setting, we would call this table a confusion matrix. Be aware, that the placement of Actual and Predicted values affects the names of the cells, and often the matrix may be presented transposed.\nIn statistics, we label the errors Type I and Type II, but these are hard to remember. False Positive and False Negative are more descriptive, so we choose to use these.\n\nThe confusionMatrix() function from the caret package can be used to obtain a wealth of additional information, which we see output below for the test data. Note that we specify which category is considered “positive.”\n\ntrn_con_mat  = confusionMatrix(trn_tab, positive = \"Yes\")\ntst_con_mat = confusionMatrix(tst_tab, positive = \"Yes\")\ntst_con_mat\n\nConfusion Matrix and Statistics\n\n         actual\npredicted   No  Yes\n      No  4343   29\n      Yes  491  137\n                                          \n               Accuracy : 0.896           \n                 95% CI : (0.8872, 0.9043)\n    No Information Rate : 0.9668          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.3088          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.8253          \n            Specificity : 0.8984          \n         Pos Pred Value : 0.2182          \n         Neg Pred Value : 0.9934          \n             Prevalence : 0.0332          \n         Detection Rate : 0.0274          \n   Detection Prevalence : 0.1256          \n      Balanced Accuracy : 0.8619          \n                                          \n       'Positive' Class : Yes             \n                                          \n\n\nThe most common, and most important metric is the classification error rate.\n\\[\n\\text{err}(\\hat{C}, \\text{Data}) = \\frac{1}{n}\\sum_{i = 1}^{n}I(y_i \\neq \\hat{C}(x_i))\n\\]\nHere, \\(I\\) is an indicator function, so we are essentially calculating the proportion of predicted classes that match the true class.\n\\[\nI(y_i \\neq \\hat{C}(x)) =\n\\begin{cases}\n  1 & y_i \\neq \\hat{C}(x) \\\\\n  0 & y_i = \\hat{C}(x) \\\\\n\\end{cases}\n\\]\nIt is also common to discuss the accuracy, which is simply one minus the error.\nLike regression, we often split the data, and then consider Train (Classification) Error and Test (Classification) Error will be used as a measure of how well a classifier will work on unseen future data.\n\\[\n\\text{err}_{\\texttt{trn}}(\\hat{C}, \\text{Train Data}) = \\frac{1}{n_{\\texttt{trn}}}\\sum_{i \\in \\texttt{trn}}^{}I(y_i \\neq \\hat{C}(x_i))\n\\]\n\\[\n\\text{err}_{\\texttt{tst}}(\\hat{C}, \\text{Test Data}) = \\frac{1}{n_{\\texttt{tst}}}\\sum_{i \\in \\texttt{tst}}^{}I(y_i \\neq \\hat{C}(x_i))\n\\]\nAccuracy values can be found by calling confusionMatrix(), or, if stored, can be accessed directly. Here, we use them to obtain error rates (1-Accuracy).\n\n 1 - trn_con_mat$overall[\"Accuracy\"]\n\nAccuracy \n  0.1038 \n\n# Note, R doesn't know to rename the result \"err\", so it keeps the name \"Accuracy\"\n\n\n1 - tst_con_mat$overall[\"Accuracy\"]\n\nAccuracy \n   0.104 \n\n# Note, R doesn't know to rename the result \"err\", so it keeps the name \"Accuracy\"\n\nWe can go back to the tst_con_mat table (called tst_tab) before and hand-calculate accuracy\n\nprint(tst_tab)\n\n         actual\npredicted   No  Yes\n      No  4343   29\n      Yes  491  137\n\n1 - ((4343 + 137) / 5000)\n\n[1] 0.104\n\n\nFirst some notation:\n\n\\(P\\) is the total number of actual positives\n\\(TP\\) is the total number of actual positives predicted to be positive\n\\(N\\) is the total number of actual negatives\n\\(TN\\) is the total number of actual negatives predicted to be negative\n\\(FP\\) and \\(FN\\) are the total number of false positives and false negatives\n\nWhich means…\n\n\\(P = TP + FN\\)\n\\(N = TN + FP\\)\n\nSometimes guarding against making certain errors, FP or FN, are more important than simply finding the best accuracy. Thus, sometimes we will consider sensitivity and specificity.\n\\[\n\\text{Sensitivity} = \\text{True Positive Rate} = \\frac{\\text{TP}}{\\text{P}} = \\frac{\\text{TP}}{\\text{TP + FN}}\n\\]\n\ntst_con_mat$byClass[\"Sensitivity\"]\n\nSensitivity \n  0.8253012 \n\n# 130/(130+28)\n\nThis is the share of actually-“yes” observations that were predicted by the model to be “yes”\n\\[\n\\text{Specificity} = \\text{True Negative Rate} = \\frac{\\text{TN}}{\\text{N}} = \\frac{\\text{TN}}{\\text{TN + FP}}\n\\]\n\ntst_con_mat$byClass[\"Specificity\"]\n\nSpecificity \n  0.8984278 \n\n# 4319/(4319/523)\n\nSpecificity is the share of actually-“no” observations that were predicted by the model to be “no”\nLike accuracy, these can easily be found using confusionMatrix().\n\nBalance and Prevalence\nWhen considering how well a classifier is performing, often, it is understandable to assume that any accuracy in a binary classification problem above 0.50 is a reasonable classifier. This however is not the case. We need to consider the balance of the classes. To do so, we look at the prevalence of positive cases.\n\\[\n\\text{Prev} = \\frac{\\text{P}}{\\text{Total Obs}}= \\frac{\\text{TP + FN}}{\\text{Total Obs}}\n\\]\n\ntrn_con_mat$byClass[\"Prevalence\"]\n\nPrevalence \n    0.0334 \n\ntst_con_mat$byClass[\"Prevalence\"]\n\nPrevalence \n    0.0332 \n\n# (29+137)/5000\n\nHere, we see an extremely low prevalence, which suggests an even simpler classifier than our current based on balance.\n\\[\n\\hat{C}(\\texttt{balance}) =\n\\begin{cases}\n      \\text{No} & \\texttt{balance} &gt; 1400 \\\\\n      \\text{No} & \\texttt{balance} \\leq 1400\n   \\end{cases}\n\\]\nThis classifier simply classifies all observations as negative cases.\n\npred_all_no = simple_class(default_tst$balance,\n                           boundary = 1400, above = \"No\", below = \"No\")\ntable(predicted = pred_all_no, actual = default_tst$default)\n\n         actual\npredicted   No  Yes\n       No 4834  166\n\n\nThe confusionMatrix() function won’t even accept this table as input, because it isn’t a full matrix, only one row, so we calculate error rates directly. To do so, we write a function.\n\ncalc_class_err = function(actual, predicted) {\n  mean(actual != predicted)\n}\n\n\ncalc_class_err(actual = default_tst$default,\n               predicted = pred_all_no)\n\n[1] 0.0332\n\n\nHere we see that the error rate is exactly the prevelance of the minority class.\n\ntable(default_tst$default) / length(default_tst$default)\n\n\n    No    Yes \n0.9668 0.0332 \n\n\nThis classifier does better than the previous. But the point is, in reality, to create a good classifier, we should obtain a test error better than the 0.033, which is obtained by simply manipulating the prevalences. Next section, we’ll introduce much better classifiers which should have no problem accomplishing this task. Point is, think carefully about what you’re putting your classifier up against. In March 2020 when we were very worried about COVID test accuracy, and when prevalance was, say, 1%, it was pointed out that we could make a 99% accurate COVID test by simply returning “No COVID” for every test! We’d be the new Theranos!",
    "crumbs": [
      "Course Content",
      "Week 12",
      "Classification"
    ]
  },
  {
    "objectID": "content/Week_12/12a.html#linear-regression-and-binary-responses",
    "href": "content/Week_12/12a.html#linear-regression-and-binary-responses",
    "title": "Classification",
    "section": "Linear Regression and Binary Responses",
    "text": "Linear Regression and Binary Responses\nBefore moving on to logistic regression, why not plain, old, linear regression? Let’s copy the data so that we can manipulate it, then go back to the original:\n\ndefault_trn_lm = default_trn\ndefault_tst_lm = default_tst\n\nSince linear regression expects a numeric response variable, we coerce the response to be numeric. Although these look like character strings, they are factors, and if you recall, factors are actually saved as integers and then have a mapping (levels) from each number to a value. We can look at str(default_trn_lm to see what the factors and levels are.\n\nstr(default_trn_lm)\n\n'data.frame':   5000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 2 1 1 2 2 1 ...\n $ balance: num  943 1035 1009 1195 729 ...\n $ income : num  30179 41714 8869 38453 17361 ...\n\n\n“No” is the first level, and it maps to 1. “Yes” is second, and it maps to 2. Notice that we also need to shift the results, as we require 0 and 1, not 1 and 2. Of course, we can always use case_when to do this in whatever way we wish.\n\ndefault_trn_lm$default = as.numeric(default_trn_lm$default) - 1\ndefault_tst_lm$default = as.numeric(default_tst_lm$default) - 1\n\nWhy would we think this should work? Recall that,\n\\[\n\\hat{\\mathbb{E}}[Y \\mid X = x] = X\\hat{\\beta}.\n\\]\nSince \\(Y\\) is limited to values of \\(0\\) and \\(1\\), we have\n\\[\n\\mathbb{E}[Y \\mid X = x] = P(Y = 1 \\mid X = x).\n\\]\nIt would then seem reasonable that \\(\\mathbf{X}\\hat{\\beta}\\) is a reasonable estimate of \\(P(Y = 1 \\mid X = x)\\). We test this on the Default data.\n\nmodel_lm = lm(default ~ balance, data = default_trn_lm)\n\nEverything seems to be working, until we plot the results.\n\nplot(default ~ balance, data = default_trn_lm,\n     col = \"darkorange\", pch = \"|\", ylim = c(-0.2, 1),\n     main = \"Using Linear Regression for Classification\")\nabline(h = 0, lty = 3)\nabline(h = 1, lty = 3)\nabline(h = 0.5, lty = 2)\nabline(model_lm, lwd = 3, col = \"dodgerblue\")\n\n\n\n\n\n\n\n\nTwo issues arise. First, all of the predicted probabilities are below 0.5. That means, we would classify every observation as a \"No\". This is certainly possible, but not what we would expect.\n\nall(predict(model_lm) &lt; 0.5)\n\n[1] TRUE\n\n\nThe next, and bigger issue, is predicted probabilities less than 0.\n\nany(predict(model_lm) &lt; 0)\n\n[1] TRUE",
    "crumbs": [
      "Course Content",
      "Week 12",
      "Classification"
    ]
  },
  {
    "objectID": "content/Week_12/12a.html#bayes-classifier",
    "href": "content/Week_12/12a.html#bayes-classifier",
    "title": "Classification",
    "section": "Bayes Classifier",
    "text": "Bayes Classifier\nWhy are we using a predicted probability of 0.5 as the cutoff for classification? Recall, the Bayes Classifier, which minimizes the classification error:\n\\[\nC^B(x) = \\underset{g}{\\mathrm{argmax}} \\ P(Y = g \\mid  X = x)\n\\]\nSo, in the binary classification problem, we will use predicted probabilities\n\\[\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid { X = x})\n\\]\nand\n\\[\n\\hat{P}(Y = 0 \\mid { X = x})\n\\]\nand then classify to the larger of the two. We actually only need to consider a single probability, usually \\(\\hat{P}(Y = 1 \\mid { X = x})\\). Since we use it so often, we give it the shorthand notation, \\(\\hat{p}(x)\\). Then the classifier is written,\n\\[\n\\hat{C}(x) =\n\\begin{cases}\n      1 & \\hat{p}(x) &gt; 0.5 \\\\\n      0 & \\hat{p}(x) \\leq 0.5\n\\end{cases}\n\\]\nThis classifier is essentially estimating the Bayes Classifier - it takes the value of \\(x\\), figures out which is larger, the \\(\\hat{P}(Y=1|X=x)\\) or \\(\\hat{P}(Y=0 | X=x)\\), and returns the classification \\(\\hat{C}(x)\\) as whichever probability is larger. Since there are only two values for \\(Y\\in\\{0,1\\}\\), the larger is always the one greater than \\(.50\\). Thus, since this is a Bayes Classifier, it minimizes classification errors.",
    "crumbs": [
      "Course Content",
      "Week 12",
      "Classification"
    ]
  },
  {
    "objectID": "content/Week_12/12a.html#logistic-regression-with-glm",
    "href": "content/Week_12/12a.html#logistic-regression-with-glm",
    "title": "Classification",
    "section": "Logistic Regression with glm()",
    "text": "Logistic Regression with glm()\nTo better estimate the probability\n\\[\np(x) = P(Y = 1 \\mid {X = x})\n\\] we turn to logistic regression. The model is written\n\\[\n\\log\\left(\\frac{p(x)}{1 - p(x)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p.\n\\]\nRearranging, we see the probabilities can be written as\n\\[\np(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p)}} = \\sigma(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots  + \\beta_p x_p)\n\\]\nNotice, we use the sigmoid function as shorthand notation, which appears often in deep learning literature. It takes any real input, and outputs a number between 0 and 1. How useful! (This is actualy a particular sigmoid function called the logistic function, but since it is by far the most popular sigmoid function, often sigmoid function is used to refer to the logistic function)\n\\[\n\\sigma(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}}\n\\]\nIt looks like this:\n\n\n\n\n\n\n\n\n\nThe model is fit by numerically maximizing the likelihood, which we will let R take care of. Essentially, R is going to try a whole bunch of guesses for \\(\\mathbf{\\beta}\\) and choose the one that best explains the data we have.\nWe start with a single predictor example, again using balance as our single predictor. Note that default_trn has a factor variable for default (No/Yes). Since R represents factor variables as numbers (here, 1 and 2), glm figures out that you mean No and Yes for 0 and 1.\n\nmodel_glm = glm(default ~ balance, data = default_trn, family = \"binomial\")\n\nFitting this model looks very similar to fitting a simple linear regression. Instead of lm() we use glm(). The only other difference is the use of family = \"binomial\" which indicates that we have a two-class categorical response. Using glm() with family = \"gaussian\" would perform the usual linear regression.\nFirst, we can obtain the fitted coefficients the same way we did with linear regression.\n\ncoef(model_glm)\n\n  (Intercept)       balance \n-10.493158288   0.005424994 \n\n\nThe next thing we should understand is how the predict() function works with glm(). So, let’s look at some predictions.\n\nhead(predict(model_glm))\n\n     2369      5273      9290      1252      8826       356 \n-5.376670 -4.875653 -5.018746 -4.007664 -6.538414 -6.601582 \n\n\nBy default, predict.glm() uses type = \"link\".\n\nhead(predict(model_glm, type = \"link\"))\n\n     2369      5273      9290      1252      8826       356 \n-5.376670 -4.875653 -5.018746 -4.007664 -6.538414 -6.601582 \n\n\nThat is, R is returning\n\\[\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots  + \\hat{\\beta}_p x_p\n\\] for each observation.\nImportantly, these are not predicted probabilities. To obtain the predicted probabilities\n\\[\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid X = x)\n\\]\nwe need to use type = \"response\"\n\nhead(predict(model_glm, type = \"response\"))\n\n       2369        5273        9290        1252        8826         356 \n0.004601914 0.007572331 0.006569370 0.017851333 0.001444691 0.001356375 \n\n\nNote that these are probabilities, not classifications. To obtain classifications, we will need to compare to the correct cutoff value with an ifelse() statement.\n\nmodel_glm_pred = ifelse(predict(model_glm, type = \"link\") &gt; 0, \"Yes\", \"No\")\n# model_glm_pred = ifelse(predict(model_glm, type = \"response\") &gt; 0.5, \"Yes\", \"No\")\n\nThe line that is run is performing\n\\[\n\\hat{C}(x) =\n\\begin{cases}\n      1 & \\hat{f}(x) &gt; 0 \\\\\n      0 & \\hat{f}(x) \\leq 0\n\\end{cases}\n\\]\nwhere\n\\[\n\\hat{f}(x) =\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots  + \\hat{\\beta}_p x_p.\n\\]\nThe commented line, which would give the same results, is performing\n\\[\n\\hat{C}(x) =\n\\begin{cases}\n      1 & \\hat{p}(x) &gt; 0.5 \\\\\n      0 & \\hat{p}(x) \\leq 0.5\n\\end{cases}\n\\]\nwhere\n\\[\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid X = x).\n\\]\nOnce we have classifications, we can calculate metrics such as the training classification error rate.\n\ncalc_class_err = function(actual, predicted) {\n  mean(actual != predicted)\n}\n\n\ncalc_class_err(actual = default_trn$default, predicted = model_glm_pred)\n\n[1] 0.0284\n\n\nAs we saw previously, the table() and confusionMatrix() functions can be used to quickly obtain many more metrics.\n\ntrain_tab = table(predicted = model_glm_pred, actual = default_trn$default)\nlibrary(caret)\ntrain_con_mat = confusionMatrix(train_tab, positive = \"Yes\")\nc(train_con_mat$overall[\"Accuracy\"],\n  train_con_mat$byClass[\"Sensitivity\"],\n  train_con_mat$byClass[\"Specificity\"])\n\n   Accuracy Sensitivity Specificity \n  0.9716000   0.2941176   0.9954451 \n\n\nWe could also write a custom function for the error for use with trained logistic regression models.\n\nget_logistic_error = function(mod, data, res, pos = 1, neg = 0, cut = 0.5) {\n  probs = predict(mod, newdata = data, type = \"response\")\n  preds = ifelse(probs &gt; cut, pos, neg)\n  calc_class_err(actual = data[, res], predicted = preds)\n}\n\nThis function will be useful later when calculating train and test errors for several models at the same time.\n\nget_logistic_error(model_glm, data = default_trn,\n                   res = \"default\", pos = \"Yes\", neg = \"No\", cut = 0.5)\n\n[1] 0.0284\n\n\nTo see how much better logistic regression is for this task, we create the same plot we used for linear regression.\n\nplot(default ~ balance, data = default_trn_lm,\n     col = \"darkorange\", pch = \"|\", ylim = c(-0.2, 1),\n     main = \"Using Logistic Regression for Classification\")\nabline(h = 0, lty = 3)\nabline(h = 1, lty = 3)\nabline(h = 0.5, lty = 2)\ncurve(predict(model_glm, data.frame(balance = x), type = \"response\"),\n      add = TRUE, lwd = 3, col = \"dodgerblue\")\nabline(v = -coef(model_glm)[1] / coef(model_glm)[2], lwd = 2)\n\n\n\n\n\n\n\n\nThis plot contains a wealth of information.\n\nThe orange | characters are the data, \\((x_i, y_i)\\).\nThe blue “curve” is the predicted probabilities given by the fitted logistic regression. That is, \\[\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid { X = x})\n\\]\nThe solid vertical black line represents the decision boundary, the balance that obtains a predicted probability of 0.5. In this case balance = 1934.2247145.\n\nThe decision boundary is found by solving for points that satisfy\n\\[\n\\hat{p}(x) = \\hat{P}(Y = 1 \\mid { X = x}) = 0.5\n\\]\nThis is equivalent to point that satisfy\n\\[\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 = 0.\n\\] Thus, for logistic regression with a single predictor, the decision boundary is given by the point\n\\[\nx_1 = \\frac{-\\hat{\\beta}_0}{\\hat{\\beta}_1}.\n\\]\nThe following is not run, but an alternative way to add the logistic curve to the plot.\n\ngrid = seq(0, max(default_trn$balance), by = 0.01)\n\nsigmoid = function(x) {\n  1 / (1 + exp(-x))\n}\n\nlines(grid, sigmoid(coef(model_glm)[1] + coef(model_glm)[2] * grid), lwd = 3)\n\nUsing the usual formula syntax, it is easy to add or remove complexity from logistic regressions.\n\nmodel_1 = glm(default ~ 1, data = default_trn, family = \"binomial\")\nmodel_2 = glm(default ~ ., data = default_trn, family = \"binomial\")\nmodel_3 = glm(default ~ . ^ 2 + I(balance ^ 2),\n              data = default_trn, family = \"binomial\")\n\nNote that, using polynomial transformations of predictors will allow a linear model to have non-linear decision boundaries.\n\nmodel_list = list(model_1, model_2, model_3)\ntrain_errors = sapply(model_list, get_logistic_error, data = default_trn,\n                      res = \"default\", pos = \"Yes\", neg = \"No\", cut = 0.5)\ntest_errors  = sapply(model_list, get_logistic_error, data = default_tst,\n                      res = \"default\", pos = \"Yes\", neg = \"No\", cut = 0.5)\n\nknitr::kable(cbind(train_errors, test_errors))\n\n\n\n\ntrain_errors\ntest_errors\n\n\n\n\n0.0340\n0.0326\n\n\n0.0274\n0.0258\n\n\n0.0274\n0.0264\n\n\n\n\n\nHere we see the misclassification error rates for each model. The train (weakly) decreases, and the test decreases, until it starts to increases. Everything we learned about the bias-variance tradeoff for regression also applies here.\n\ndiff(train_errors)\n\n[1] -0.0066  0.0000\n\ndiff(test_errors)\n\n[1] -0.0068  0.0006\n\n\nWe call model_2 the additive logistic model, which we will use quite often.",
    "crumbs": [
      "Course Content",
      "Week 12",
      "Classification"
    ]
  },
  {
    "objectID": "content/Week_12/12a.html#roc-curves",
    "href": "content/Week_12/12a.html#roc-curves",
    "title": "Classification",
    "section": "ROC Curves",
    "text": "ROC Curves\nLet’s return to our simple model with only balance as a predictor.\n\nmodel_glm = glm(default ~ balance, data = default_trn, family = \"binomial\")\n\nWe write a function which allows use to make predictions based on different probability cutoffs.\n\nget_logistic_pred = function(mod, data, res = \"y\", pos = 1, neg = 0, cut = 0.5) {\n  probs = predict(mod, newdata = data, type = \"response\")\n  ifelse(probs &gt; cut, pos, neg)\n}\n\n\\[\n\\hat{C}(x) =\n\\begin{cases}\n      1 & \\hat{p}(x) &gt; c \\\\\n      0 & \\hat{p}(x) \\leq c\n\\end{cases}\n\\]\nLet’s use this to obtain predictions using a low, medium, and high cutoff. (0.1, 0.5, and 0.9)\n\ntest_pred_10 = get_logistic_pred(model_glm, data = default_tst, res = \"default\",\n                                 pos = \"Yes\", neg = \"No\", cut = 0.1)\ntest_pred_50 = get_logistic_pred(model_glm, data = default_tst, res = \"default\",\n                                 pos = \"Yes\", neg = \"No\", cut = 0.5)\ntest_pred_90 = get_logistic_pred(model_glm, data = default_tst, res = \"default\",\n                                 pos = \"Yes\", neg = \"No\", cut = 0.9)\n\nNow we evaluate accuracy, sensitivity, and specificity for these classifiers.\n\ntest_tab_10 = table(predicted = test_pred_10, actual = default_tst$default)\ntest_tab_50 = table(predicted = test_pred_50, actual = default_tst$default)\ntest_tab_90 = table(predicted = test_pred_90, actual = default_tst$default)\n\ntest_con_mat_10 = confusionMatrix(test_tab_10, positive = \"Yes\")\ntest_con_mat_50 = confusionMatrix(test_tab_50, positive = \"Yes\")\ntest_con_mat_90 = confusionMatrix(test_tab_90, positive = \"Yes\")\n\n\nmetrics = rbind(\n\n  c(test_con_mat_10$overall[\"Accuracy\"],\n    test_con_mat_10$byClass[\"Sensitivity\"],\n    test_con_mat_10$byClass[\"Specificity\"]),\n\n  c(test_con_mat_50$overall[\"Accuracy\"],\n    test_con_mat_50$byClass[\"Sensitivity\"],\n    test_con_mat_50$byClass[\"Specificity\"]),\n\n  c(test_con_mat_90$overall[\"Accuracy\"],\n    test_con_mat_90$byClass[\"Sensitivity\"],\n    test_con_mat_90$byClass[\"Specificity\"])\n\n)\n\nrownames(metrics) = c(\"c = 0.10\", \"c = 0.50\", \"c = 0.90\")\nmetrics\n\n         Accuracy Sensitivity Specificity\nc = 0.10   0.9328  0.71779141   0.9400455\nc = 0.50   0.9730  0.31288344   0.9952450\nc = 0.90   0.9688  0.04294479   1.0000000\n\n\nWe see then sensitivity decreases as the cutoff is increased. Conversely, specificity increases as the cutoff increases. This is useful if we are more interested in a particular error, instead of giving them equal weight.\nNote that usually the best accuracy will be seen near \\(c = 0.50\\). This is not always true, and in your lab, you may need to change your cutoff.\nInstead of manually checking cutoffs, we can create an ROC curve (receiver operating characteristic curve) which will sweep through all possible cutoffs, and plot the sensitivity and specificity.\n\nWhere on this curve would you think is the “best” place to be? Why?\nWhere on this curve would you think is the “worst” place to be? Why?\n\n\nlibrary(pROC)\ntest_prob = predict(model_glm, newdata = default_tst, type = \"response\")\ntest_roc = roc(default_tst$default ~ test_prob, plot = TRUE, print.auc = TRUE)\n\n\n\n\n\n\n\nas.numeric(test_roc$auc)\n\n[1] 0.9492866\n\n\nThe AUC is the “area under the curve”. One interpretation of the AUC is that it is “the probability that the model ranks a randomly selected positive more highly than a randomly selected negative.” A good model will have a high AUC. A high AUC has a high sensitivity and a high specificity over all of the cutoff values.",
    "crumbs": [
      "Course Content",
      "Week 12",
      "Classification"
    ]
  },
  {
    "objectID": "content/Week_12/12a.html#multinomial-logistic-regression",
    "href": "content/Week_12/12a.html#multinomial-logistic-regression",
    "title": "Classification",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\nWhat if the response contains more than two categories? For that we need multinomial logistic regression.\n\\[\nP(Y = k \\mid { X = x}) = \\frac{e^{\\beta_{0k} + \\beta_{1k} x_1 + \\cdots +  + \\beta_{pk} x_p}}{\\sum_{g = 1}^{G} e^{\\beta_{0g} + \\beta_{1g} x_1 + \\cdots + \\beta_{pg} x_p}}\n\\]\nWe will omit the details, as ISL has as well. If you are interested, the Wikipedia page provides a rather thorough coverage. Also note that the above is an example of the softmax function.\nAs an example of a dataset with a three category response, we use the iris dataset, which is so famous, it has its own Wikipedia entry. It is also a default dataset in R, so no need to load it.\nBefore proceeding, we test-train split this data.\n\nset.seed(430)\niris_obs = nrow(iris)\niris_idx = sample(iris_obs, size = trunc(0.50 * iris_obs))\niris_trn = iris[iris_idx, ]\niris_test = iris[-iris_idx, ]\n\nTo perform multinomial logistic regression, we use the multinom function from the nnet package. Training using multinom() is done using similar syntax to lm() and glm(). We add the trace = FALSE argument to suppress information about updates to the optimization routine as the model is trained.\n\nlibrary(nnet)\nmodel_multi = multinom(Species ~ ., data = iris_trn, trace = FALSE)\nsummary(model_multi)$coefficients\n\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    16.77474    -7.855576   -13.98668     25.13860    4.270375\nvirginica    -33.94895   -37.519645   -94.22846     97.82691   73.487162\n\n\nNotice we are only given coefficients for two of the three class, much like only needing coefficients for one class in logistic regression.\nA difference between glm() and multinom() is how the predict() function operates.\n\nhead(predict(model_multi, newdata = iris_trn))\n\n[1] setosa     versicolor versicolor setosa     virginica  versicolor\nLevels: setosa versicolor virginica\n\nhead(predict(model_multi, newdata = iris_trn, type = \"prob\"))\n\n          setosa   versicolor     virginica\n1   1.000000e+00 1.910554e-16 6.118616e-176\n92  8.542846e-22 1.000000e+00  1.372168e-18\n77  8.343856e-23 1.000000e+00  2.527471e-14\n38  1.000000e+00 1.481126e-16 5.777917e-180\n108 1.835279e-73 1.403654e-36  1.000000e+00\n83  1.256090e-16 1.000000e+00  2.223689e-32\n\n\nNotice that by default, classifications are returned. When obtaining probabilities, we are given the predicted probability for each class.\nInterestingly, you’ve just fit a neural network, and you didn’t even know it! (Hence the nnet package.) Later we will discuss the connections between logistic regression, multinomial logistic regression, and simple neural networks.",
    "crumbs": [
      "Course Content",
      "Week 12",
      "Classification"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html",
    "href": "content/Week_13/13a.html",
    "title": "Text as Data",
    "section": "",
    "text": "This page.\n\n\n\n\nWhat are some common issues with string data?\nWhat are the key ways to wrangle strings?\nWhat are regular expressions and why are they magic",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#required-reading",
    "href": "content/Week_13/13a.html#required-reading",
    "title": "Text as Data",
    "section": "",
    "text": "This page.\n\n\n\n\nWhat are some common issues with string data?\nWhat are the key ways to wrangle strings?\nWhat are regular expressions and why are they magic",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#the-stringr-package",
    "href": "content/Week_13/13a.html#the-stringr-package",
    "title": "Text as Data",
    "section": "The stringr package",
    "text": "The stringr package\n\nlibrary(tidyverse)\nlibrary(stringr)\n\nIn general, string processing tasks can be divided into detecting, locating, extracting, or replacing patterns in strings. We will see several examples. The table below includes the functions available to you in the stringr package. We split them by task. We also include the R-base equivalent when available.\nAll these functions take a character vector as first argument. Also, for each function, operations are vectorized: the operation gets applied to each string in the vector.\nFinally, note that in this table we mention groups. These will be explained later on.\n\n\n\n\n\n\n\n\n\nstringr\nTask\nDescription\nR-base\n\n\n\n\nstr_detect\nDetect\nIs the pattern in the string?\ngrepl\n\n\nstr_which\nDetect\nReturns the index of entries that contain the pattern.\ngrep\n\n\nstr_subset\nDetect\nReturns the subset of strings that contain the pattern.\ngrep with value = TRUE\n\n\nstr_locate\nLocate\nReturns positions of first occurrence of pattern in a string.\nregexpr\n\n\nstr_locate_all\nLocate\nReturns position of all occurrences of pattern in a string.\ngregexpr\n\n\nstr_view\nLocate\nShow the first part of the string that matches pattern.\n\n\n\nstr_view_all\nLocate\nShow me all the parts of the string that match the pattern.\n\n\n\nstr_extract\nExtract\nExtract the first part of the string that matches the pattern.\n\n\n\nstr_extract_all\nExtract\nExtract all parts of the string that match the pattern.\n\n\n\nstr_match\nExtract\nExtract first part of the string that matches the groups and the patterns defined by the groups.\n\n\n\nstr_match_all\nExtract\nExtract all parts of the string that matches the groups and the patterns defined by the groups.\n\n\n\nstr_sub\nExtract\nExtract a substring.\nsubstring\n\n\nstr_split\nExtract\nSplit a string into a list with parts separated by pattern.\nstrsplit\n\n\nstr_split_fixed\nExtract\nSplit a string into a matrix with parts separated by pattern.\nstrsplit with fixed = TRUE\n\n\nstr_count\nDescribe\nCount number of times a pattern appears in a string.\n\n\n\nstr_length\nDescribe\nNumber of character in string.\nnchar\n\n\nstr_replace\nReplace\nReplace first part of a string matching a pattern with another.\n\n\n\nstr_replace_all\nReplace\nReplace all parts of a string matching a pattern with another.\ngsub\n\n\nstr_to_upper\nReplace\nChange all characters to upper case.\ntoupper\n\n\nstr_to_lower\nReplace\nChange all characters to lower case.\ntolower\n\n\nstr_to_title\nReplace\nChange first character to upper and rest to lower.\n\n\n\nstr_replace_na\nReplace\nReplace all NAs to a new value.\n\n\n\nstr_trim\nReplace\nRemove white space from start and end of string.\n\n\n\nstr_c\nManipulate\nJoin multiple strings.\npaste0\n\n\nstr_conv\nManipulate\nChange the encoding of the string.\n\n\n\nstr_sort\nManipulate\nSort the vector in alphabetical order.\nsort\n\n\nstr_order\nManipulate\nIndex needed to order the vector in alphabetical order.\norder\n\n\nstr_trunc\nManipulate\nTruncate a string to a fixed size.\n\n\n\nstr_pad\nManipulate\nAdd white space to string to make it a fixed size.\n\n\n\nstr_dup\nManipulate\nRepeat a string.\nrep then paste\n\n\nstr_wrap\nManipulate\nWrap things into formatted paragraphs.\n\n\n\nstr_interp\nManipulate\nString interpolation.\nsprintf",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#case-study-1-us-murders-data",
    "href": "content/Week_13/13a.html#case-study-1-us-murders-data",
    "title": "Text as Data",
    "section": "Case study 1: US murders data",
    "text": "Case study 1: US murders data\nIn this section we introduce some of the more simple string processing challenges with the following datasets as an example:\n\nlibrary(rvest)\nurl &lt;- paste0(\"https://en.wikipedia.org/w/index.php?title=\",\n              \"Gun_violence_in_the_United_States_by_state\",\n              \"&direction=prev&oldid=810166167\")\nmurders_raw &lt;- read_html(url) %&gt;%\n  html_node(\"table\") %&gt;%\n  html_table() %&gt;%\n  setNames(c(\"state\", \"population\", \"total\", \"murder_rate\"))\n\nhead(murders_raw)\n\n# A tibble: 6 × 4\n  state      population total murder_rate\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt;\n1 Alabama    4,853,875  348           7.2\n2 Alaska     737,709    59            8  \n3 Arizona    6,817,565  309           4.5\n4 Arkansas   2,977,853  181           6.1\n5 California 38,993,940 1,861         4.8\n6 Colorado   5,448,819  176           3.2\n\n\nThe code above shows the first step in constructing the dataset\n\nlibrary(dslabs)\ndata(murders)\n\nfrom the raw data, which was extracted from a Wikipedia page.\nIn general, string processing involves a string and a pattern. In R, we usually store strings in a character vector such as murders$population. The first three strings in this vector defined by the population variable are:\n\nmurders_raw$population[1:3]\n\n[1] \"4,853,875\" \"737,709\"   \"6,817,565\"\n\n\nThe usual coercion does not work here:\n\nas.numeric(murders_raw$population[1:3])\n\n[1] NA NA NA\n\n\nThis is because of the commas ,. The string processing we want to do here is remove the pattern, ,, from the strings in murders_raw$population and then coerce to numbers. We can use the str_detect function to see that two of the three columns have commas in the entries:\n\ncommas &lt;- function(x) any(str_detect(x, \",\"))\nmurders_raw %&gt;% summarize_all(commas)\n\n# A tibble: 1 × 4\n  state population total murder_rate\n  &lt;lgl&gt; &lt;lgl&gt;      &lt;lgl&gt; &lt;lgl&gt;      \n1 FALSE TRUE       TRUE  FALSE      \n\n\nWe can then use the str_replace_all function to remove them:\n\ntest_1 &lt;- str_replace_all(murders_raw$population, \",\", \"\")\ntest_1 &lt;- as.numeric(test_1)\n\nThe stringr function str_replace_all replaces all of the first argument’s instances of , (the second argument) with '' (the third argument). The first input for the function is the character vector on which you’d like to operate.\nWe can then use mutate_all to apply this operation to each column, since it won’t affect the columns without commas. You may notice the use of mutate_all and summarize_all here. These are versions of mutate and summarize that apply a given function to every column in the data. Above, the function we apply to every column is the commas function. Note that in using summarize_all we pipe the data in, but we do not list the columns we wish to create via summarize. To apply as.numeric to each column in murders_raw, we’d use: murders_raw %&gt;% mutate_all(str_replace_all, ',', '') %&gt;% mutate_at(2:3, as.numeric).\nNote, the str_replace_all function is NOT an analog to mutate_all that applies to each column in the data!\nIt turns out that this operation is so common that readr includes the function parse_number specifically meant to remove non-numeric characters before coercing:\n\ntest_2 &lt;- parse_number(murders_raw$population)\nidentical(test_1, test_2)\n\n[1] TRUE\n\n\nSo we can obtain our desired table using:\n\nmurders_new &lt;- murders_raw %&gt;% mutate_at(2:3, parse_number)\nhead(murders_new)\n\n# A tibble: 6 × 4\n  state      population total murder_rate\n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 Alabama       4853875   348         7.2\n2 Alaska         737709    59         8  \n3 Arizona       6817565   309         4.5\n4 Arkansas      2977853   181         6.1\n5 California   38993940  1861         4.8\n6 Colorado      5448819   176         3.2\n\n\nThis case is relatively simple compared to the string processing challenges that we typically face in data science. The next example is a rather complex one and it provides several challenges that will permit us to learn many string processing techniques.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#case-study-2-self-reported-heights",
    "href": "content/Week_13/13a.html#case-study-2-self-reported-heights",
    "title": "Text as Data",
    "section": "Case study 2: self-reported heights",
    "text": "Case study 2: self-reported heights\nThe dslabs package includes the raw data from which the heights dataset was obtained. You can load it like this:\n\ndata(reported_heights)\n\nThese heights were obtained using a web form in which students were asked to enter their heights. They could enter anything, but the instructions asked for height in inches, a number. We compiled 1,095 submissions, but unfortunately the column vector with the reported heights had several non-numeric entries and as a result became a character vector:\n\nclass(reported_heights$height)\n\n[1] \"character\"\n\n\nIf we try to parse it into numbers, we get a warning:\n\nx &lt;- as.numeric(reported_heights$height)\n\nAlthough most values appear to be height in inches as requested:\n\nhead(x)\n\n[1] 75 70 68 74 61 65\n\n\nwe do end up with many NAs:\n\nsum(is.na(x))\n\n[1] 81\n\n\nWe can see some of the entries that are not successfully converted by using filter to keep only the entries resulting in NAs:\n\nreported_heights %&gt;%\n  dplyr::mutate(new_height = as.numeric(height)) %&gt;%\n  dplyr::filter(is.na(new_height)) %&gt;%\n  head(n=10)\n\n            time_stamp    sex                 height new_height\n1  2014-09-02 15:16:28   Male                  5' 4\"         NA\n2  2014-09-02 15:16:37 Female                  165cm         NA\n3  2014-09-02 15:16:52   Male                    5'7         NA\n4  2014-09-02 15:16:56   Male                  &gt;9000         NA\n5  2014-09-02 15:16:56   Male                   5'7\"         NA\n6  2014-09-02 15:17:09 Female                   5'3\"         NA\n7  2014-09-02 15:18:00   Male 5 feet and 8.11 inches         NA\n8  2014-09-02 15:19:48   Male                   5'11         NA\n9  2014-09-04 00:46:45   Male                  5'9''         NA\n10 2014-09-04 10:29:44   Male                 5'10''         NA\n\n\nWe immediately see what is happening. Some of the students did not report their heights in inches as requested. We could discard these data and continue. However, many of the entries follow patterns that, in principle, we can easily convert to inches. For example, in the output above, we see various cases that use the format x'y'' with x and y representing feet and inches, respectively. Each one of these cases can be read and converted to inches by a human, for example 5'4'' is 5*12 + 4 = 64. So we could fix all the problematic entries by hand. However, humans are prone to making mistakes, so an automated approach is preferable. Also, because we plan on continuing to collect data, it will be convenient to write code that automatically does this.\nA first step in this type of task is to survey the problematic entries and try to define specific patterns followed by a large groups of entries. The larger these groups, the more entries we can fix with a single programmatic approach. We want to find patterns that can be accurately described with a rule, such as “a digit, followed by a feet symbol, followed by one or two digits, followed by an inches symbol”.\nTo look for such patterns, it helps to remove the entries that are consistent with being in inches and to view only the problematic entries. We thus write a function to automatically do this. We keep entries that either result in NAs when applying as.numeric or are outside a range of plausible heights. We permit a range that covers about 99.9999% of the adult population. We also use suppressWarnings to avoid the warning message we know as.numeric will gives us.\n\nnot_inches &lt;- function(x, smallest = 50, tallest = 84){\n  inches &lt;- suppressWarnings(as.numeric(x))\n  ind &lt;- is.na(inches) | inches &lt; smallest | inches &gt; tallest\n  ind\n}\n\nWe apply this function and find the number of problematic entries:\n\nproblems &lt;- reported_heights %&gt;%\n  dplyr::filter(not_inches(height)) %&gt;%\n  pull(height)\nlength(problems)\n\n[1] 292\n\n\nWe can now view all the cases by simply printing them. We don’t do that here because there are 292 problems, but after surveying them carefully, we see that three patterns can be used to define three large groups within these exceptions.\n1. A pattern of the form x'y or x' y'' or x'y\" with x and y representing feet and inches, respectively. Here are ten examples:\n\n\n5' 4\" 5'7 5'7\" 5'3\" 5'11 5'9'' 5'10'' 5' 10 5'5\" 5'2\"\n\n\n2. A pattern of the form x.y or x,y with x feet and y inches. Here are ten examples:\n\n\n5.3 5.5 6.5 5.8 5.6 5,3 5.9 6,8 5.5 6.2\n\n\n3. Entries that were reported in centimeters rather than inches. Here are ten examples:\n\n\n150 175 177 178 163 175 178 165 165 180\n\n\nOnce we see these large groups following specific patterns, we can develop a plan of attack. Remember that there is rarely just one way to perform these tasks. Here we pick one that helps us teach several useful techniques. But surely there is a more efficient way of performing the task.\nPlan of attack: we will convert entries fitting the first two patterns into a standardized one. We will then leverage the standardization to extract the feet and inches and convert to inches. We will then define a procedure for identifying entries that are in centimeters and convert them to inches. After applying these steps, we will then check again to see what entries were not fixed and see if we can tweak our approach to be more comprehensive.\nAt the end, we hope to have a script that makes web-based data collection methods robust to the most common user mistakes.\nTo achieve our goal, we will use a technique that enables us to accurately detect patterns and extract the parts we want: regular expressions (regex). But first, we quickly describe how to escape the function of certain characters so that they can be included in strings.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#how-to-escape-when-defining-strings",
    "href": "content/Week_13/13a.html#how-to-escape-when-defining-strings",
    "title": "Text as Data",
    "section": "How to escape when defining strings",
    "text": "How to escape when defining strings\nTo define strings in R, we can use either double quotes:\n\ns &lt;- \"Hello!\"\n\nor single quotes:\n\ns &lt;- 'Hello!'\n\nMake sure you choose the correct single quote since using the back quote will give you an error:\n\ns &lt;- `Hello`\n\nError: object 'Hello' not found\nNow, what happens if the string we want to define includes double quotes? For example, if we want to write 10 inches like this 10\"? In this case you can’t use:\n\ns &lt;- \"10\"\"\n\nbecause this is just the string 10 followed by a double quote. If you type this into R, you get an error because you have an unclosed double quote. To avoid this, we can use the single quotes:\n\ns &lt;- '10\"'\n\nIf we print out s we see that the double quotes are escaped with the backslash \\.\n\ns\n\n[1] \"10\\\"\"\n\n\nIn fact, escaping with the backslash provides a way to define the string while still using the double quotes to define strings:\n\ns &lt;- \"10\\\"\"\n\nIn R, the function cat lets us see what the string actually looks like:\n\ncat(s)\n\n10\"\n\n\nNow, what if we want our string to be 5 feet written like this 5'? In this case, we can use the double quotes:\n\ns &lt;- \"5'\"\ncat(s)\n\n5'\n\n\nSo we’ve learned how to write 5 feet and 10 inches separately, but what if we want to write them together to represent 5 feet and 10 inches like this 5'10\"? In this case, neither the single nor double quotes will work. This:\n\ns &lt;- '5'10\"'\n\ncloses the string after 5 and this:\n\ns &lt;- \"5'10\"\"\n\ncloses the string after 10. Keep in mind that if we type one of the above code snippets into R, it will get stuck waiting for you to close the open quote and you will have to exit the execution with the esc button.\nIn this situation, we need to escape the function of the quotes with the backslash \\. You can escape either character like this:\n\ns &lt;- '5\\'10\"'\ncat(s)\n\n5'10\"\n\n\nor like this:\n\ns &lt;- \"5'10\\\"\"\ncat(s)\n\n5'10\"\n\n\nEscaping characters is something we often have to use when processing strings.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#regular-expressions",
    "href": "content/Week_13/13a.html#regular-expressions",
    "title": "Text as Data",
    "section": "Regular expressions",
    "text": "Regular expressions\nA regular expression (regex) is a way to describe specific patterns of characters of text. They can be used to determine if a given string matches the pattern. A set of rules has been defined to do this efficiently and precisely and here we show some examples. We can learn more about these rules by reading a detailed tutorials1 2. This RStudio cheat sheet3 is also very useful.\nThe patterns supplied to the stringr functions can be a regex rather than a standard string. We will learn how this works through a series of examples.\nThroughout this section you will see that we create strings to test out our regex. To do this, we define patterns that we know should match and also patterns that we know should not. We will call them yes and no, respectively. This permits us to check for the two types of errors: failing to match and incorrectly matching.\n\nStrings are a regex\nTechnically any string is a regex, perhaps the simplest example is a single character. So the comma , used in the next code example is a simple example of searching with regex.\n\npattern &lt;- \",\"\nstr_detect(murders_raw$total, pattern)\n\nWe suppress the output which is logical vector telling us which entries have commas.\nAbove, we noted that an entry included a cm. This is also a simple example of a regex. We can show all the entries that used cm like this:\n\nstr_subset(reported_heights$height, \"cm\")\n\n[1] \"165cm\"  \"170 cm\"\n\n\n\n\nSpecial characters\nNow let’s consider a slightly more complicated example. Which of the following strings contain the pattern cm or inches?\n\nyes &lt;- c(\"180 cm\", \"70 inches\")\nno &lt;- c(\"180\", \"70''\")\ns &lt;- c(yes, no)\n\n\nstr_detect(s, \"cm\") | str_detect(s, \"inches\")\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nHowever, we don’t need to do this. The main feature that distinguishes the regex language from plain strings is that we can use special characters. These are characters with a meaning. We start by introducing | which means or. So if we want to know if either cm or inches appears in the strings, we can use the regex cm|inches:\n\nstr_detect(s, \"cm|inches\")\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nand obtain the correct answer.\nAnother special character that will be useful for identifying feet and inches values is \\d which means any digit: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. The backslash is used to distinguish it from the character d. In R, we have to escape the backslash \\ so we actually have to use \\\\d to represent digits. Here is an example:\n\nyes &lt;- c(\"5\", \"6\", \"5'10\", \"5 feet\", \"4'11\")\nno &lt;- c(\"\", \".\", \"Five\", \"six\")\ns &lt;- c(yes, no)\npattern &lt;- \"\\\\d\"\nstr_detect(s, pattern)\n\n[1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\nWe take this opportunity to introduce the str_view function, which is helpful for troubleshooting as it shows us the first match for each string:\n\nstr_view(s, pattern)\n\n\n\n\n\n\n\n\n\n\nand str_view_all shows us all the matches, so 3'2 has two matches and 5'10 has three.\n\nstr_view_all(s, pattern)\n\n\n\n\n\n\n\n\n\n\nThere are many other special characters. We will learn some others below, but you can see most or all of them in the cheat sheet4 mentioned earlier.\n\n\nCharacter classes\nCharacter classes are used to define a series of characters that can be matched. We define character classes with square brackets []. So, for example, if we want the pattern to match only if we have a 5 or a 6, we use the regex [56]:\n\nstr_view(s, \"[56]\")\n\n\n\n\n\n\n\n\n\n\nSuppose we want to match values between 4 and 7. A common way to define character classes is with ranges. So, for example, [0-9] is equivalent to \\\\d. The pattern we want is therefore [4-7].\n\nyes &lt;- as.character(4:7)\nno &lt;- as.character(1:3)\ns &lt;- c(yes, no)\nstr_detect(s, \"[4-7]\")\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\nHowever, it is important to know that in regex everything is a character; there are no numbers. So 4 is the character 4 not the number four. Notice, for example, that [1-20] does not mean 1 through 20, it means the characters 1 through 2 or the character 0. So [1-20] simply means the character class composed of 0, 1, and 2.\nKeep in mind that characters do have an order and the digits do follow the numeric order. So 0 comes before 1 which comes before 2 and so on. For the same reason, we can define lower case letters as [a-z], upper case letters as [A-Z], and [a-zA-z] as both.\n\n\nAnchors\nWhat if we want a match when we have exactly 1 digit? This will be useful in our case study since feet are never more than 1 digit so a restriction will help us. One way to do this with regex is by using anchors, which let us define patterns that must start or end at a specific place. The two most common anchors are ^ and $ which represent the beginning and end of a string, respectively. So the pattern ^\\\\d$ is read as “start of the string followed by one digit followed by end of string”.\nThis pattern now only detects the strings with exactly one digit:\n\npattern &lt;- \"^\\\\d$\"\nyes &lt;- c(\"1\", \"5\", \"9\")\nno &lt;- c(\"12\", \"123\", \" 1\", \"a4\", \"b\")\ns &lt;- c(yes, no)\nstr_view_all(s, pattern)\n\n\n\n\n\n\n\n\n\n\nThe 1 does not match because it does not start with the digit but rather with a space, which is actually not easy to see.\n\n\nQuantifiers\nFor the inches part, we can have one or two digits. This can be specified in regex with quantifiers. This is done by following the pattern with curly brackets containing the number of times the previous entry can be repeated. We use an example to illustrate. The pattern for one or two digits is:\n\npattern &lt;- \"^\\\\d{1,2}$\"\nyes &lt;- c(\"1\", \"5\", \"9\", \"12\")\nno &lt;- c(\"123\", \"a4\", \"b\")\nstr_view(c(yes, no), pattern)\n\n\n\n\n\n\n\n\n\n\nIn this case, 123 does not match, but 12 does. So to look for our feet and inches pattern, we can add the symbols for feet ' and inches \" after the digits.\nWith what we have learned, we can now construct an example for the pattern x'y\\\" with x feet and y inches.\n\npattern &lt;- \"^[4-7]'\\\\d{1,2}\\\"$\"\n\nThe pattern is now getting complex, but you can look at it carefully and break it down:\n\n^ = start of the string\n[4-7] = one digit, either 4,5,6 or 7\n' = feet symbol\n\\\\d{1,2} = one or two digits\n\\\" = inches symbol\n$ = end of the string\n\nLet’s test it out:\n\nyes &lt;- c(\"5'7\\\"\", \"6'2\\\"\",  \"5'12\\\"\")\nno &lt;- c(\"6,2\\\"\", \"6.2\\\"\",\"I am 5'11\\\"\", \"3'2\\\"\", \"64\")\nstr_detect(yes, pattern)\n\n[1] TRUE TRUE TRUE\n\nstr_detect(no, pattern)\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\n\nFor now, we are permitting the inches to be 12 or larger. We will add a restriction later as the regex for this is a bit more complex than we are ready to show.\n\n\nWhite space \\s\nAnother problem we have are spaces. For example, our pattern does not match 5' 4\" because there is a space between ' and 4 which our pattern does not permit. Spaces are characters and R does not ignore them:\n\nidentical(\"Hi\", \"Hi \")\n\n[1] FALSE\n\n\nIn regex, \\s represents white space. To find patterns like 5' 4, we can change our pattern to:\n\npattern_2 &lt;- \"^[4-7]'\\\\s\\\\d{1,2}\\\"$\"\nstr_subset(problems, pattern_2)\n\n[1] \"5' 4\\\"\"  \"5' 11\\\"\" \"5' 7\\\"\" \n\n\nHowever, this will not match the patterns with no space. So do we need more than one regex pattern? It turns out we can use a quantifier for this as well.\n\n\nQuantifiers: *, ?, +\nWe want the pattern to permit spaces but not require them. Even if there are several spaces, like in this example 5'   4, we still want it to match. There is a quantifier for exactly this purpose. In regex, the character * means zero or more instances of the previous character. Here is an example:\n\nyes &lt;- c(\"AB\", \"A1B\", \"A11B\", \"A111B\", \"A1111B\")\nno &lt;- c(\"A2B\", \"A21B\")\nstr_detect(yes, \"A1*B\")\n\n[1] TRUE TRUE TRUE TRUE TRUE\n\nstr_detect(no, \"A1*B\")\n\n[1] FALSE FALSE\n\n\nThe above matches the first string which has zero 1s and all the strings with one or more 1. We can then improve our pattern by adding the * after the space character \\s.\nThere are two other similar quantifiers. For none or once, we can use ?, and for one or more, we can use +. You can see how they differ with this example:\n\ndata.frame(string = c(\"AB\", \"A1B\", \"A11B\", \"A111B\", \"A1111B\"),\n           none_or_more = str_detect(yes, \"A1*B\"),\n           nore_or_once = str_detect(yes, \"A1?B\"),\n           once_or_more = str_detect(yes, \"A1+B\"))\n\n  string none_or_more nore_or_once once_or_more\n1     AB         TRUE         TRUE        FALSE\n2    A1B         TRUE         TRUE         TRUE\n3   A11B         TRUE        FALSE         TRUE\n4  A111B         TRUE        FALSE         TRUE\n5 A1111B         TRUE        FALSE         TRUE\n\n\nWe will actually use all three in our reported heights example, but we will see these in a later section.\n\n\nNot\nTo specify patterns that we do not want to detect, we can use the ^ symbol but only inside square brackets. Remember that outside the square bracket ^ means the start of the string. So, for example, if we want to detect digits that are preceded by anything except a letter we can do the following:\n\npattern &lt;- \"[^a-zA-Z]\\\\d\"\nyes &lt;- c(\".3\", \"+2\", \"-0\",\"*4\")\nno &lt;- c(\"A3\", \"B2\", \"C0\", \"E4\")\nstr_detect(yes, pattern)\n\n[1] TRUE TRUE TRUE TRUE\n\nstr_detect(no, pattern)\n\n[1] FALSE FALSE FALSE FALSE\n\n\nAnother way to generate a pattern that searches for everything except is to use the upper case of the special character. For example \\\\D means anything other than a digit, \\\\S means anything except a space, and so on.\n\n\nGroups\nGroups are a powerful aspect of regex that permits the extraction of values. Groups are defined using parentheses. They don’t affect the pattern matching per se. Instead, it permits tools to identify specific parts of the pattern so we can extract them.\nWe want to change heights written like 5.6 to 5'6.\nTo avoid changing patterns such as 70.2, we will require that the first digit be between 4 and 7 [4-7] and that the second be none or more digits \\\\d*. Let’s start by defining a simple pattern that matches this:\n\npattern_without_groups &lt;- \"^[4-7],\\\\d*$\"\n\nWe want to extract the digits so we can then form the new version using a period. These are our two groups, so we encapsulate them with parentheses:\n\npattern_with_groups &lt;-  \"^([4-7]),(\\\\d*)$\"\n\nWe encapsulate the part of the pattern that matches the parts we want to keep for later use. Adding groups does not affect the detection, since it only signals that we want to save what is captured by the groups. Note that both patterns return the same result when using str_detect:\n\nyes &lt;- c(\"5,9\", \"5,11\", \"6,\", \"6,1\")\nno &lt;- c(\"5'9\", \",\", \"2,8\", \"6.1.1\")\ns &lt;- c(yes, no)\nstr_detect(s, pattern_without_groups)\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nstr_detect(s, pattern_with_groups)\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\nOnce we define groups, we can use the function str_match to extract the values these groups define:\n\nstr_match(s, pattern_with_groups)\n\n     [,1]   [,2] [,3]\n[1,] \"5,9\"  \"5\"  \"9\" \n[2,] \"5,11\" \"5\"  \"11\"\n[3,] \"6,\"   \"6\"  \"\"  \n[4,] \"6,1\"  \"6\"  \"1\" \n[5,] NA     NA   NA  \n[6,] NA     NA   NA  \n[7,] NA     NA   NA  \n[8,] NA     NA   NA  \n\n\nNotice that the second and third columns contain feet and inches, respectively. The first column is the part of the string matching the pattern. If no match occurred, we see an NA.\nNow we can understand the difference between the functions str_extract and str_match: str_extract extracts only strings that match a pattern, not the values defined by groups:\n\nstr_extract(s, pattern_with_groups)\n\n[1] \"5,9\"  \"5,11\" \"6,\"   \"6,1\"  NA     NA     NA     NA",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#search-and-replace-with-regex",
    "href": "content/Week_13/13a.html#search-and-replace-with-regex",
    "title": "Text as Data",
    "section": "Search and replace with regex",
    "text": "Search and replace with regex\nEarlier we defined the object problems containing the strings that do not appear to be in inches. We can see that not too many of our problematic strings match the pattern:\n\npattern &lt;- \"^[4-7]'\\\\d{1,2}\\\"$\"\nsum(str_detect(problems, pattern))\n\n[1] 14\n\n\nTo see why this is, we show some examples that expose why we don’t have more matches:\n\nproblems[c(2, 10, 11, 12, 15)] %&gt;% str_view(pattern)\n\n\n\n\n\n\n\n\n\n\nAn initial problem we see immediately is that some students wrote out the words “feet” and “inches”. We can see the entries that did this with the str_subset function:\n\nstr_subset(problems, \"inches\")\n\n[1] \"5 feet and 8.11 inches\" \"Five foot eight inches\" \"5 feet 7inches\"        \n[4] \"5ft 9 inches\"           \"5 ft 9 inches\"          \"5 feet 6 inches\"       \n\n\nWe also see that some entries used two single quotes '' instead of a double quote \".\n\nstr_subset(problems, \"''\")\n\n [1] \"5'9''\"   \"5'10''\"  \"5'10''\"  \"5'3''\"   \"5'7''\"   \"5'6''\"   \"5'7.5''\"\n [8] \"5'7.5''\" \"5'10''\"  \"5'11''\"  \"5'10''\"  \"5'5''\"  \n\n\nTo correct this, we can replace the different ways of representing inches and feet with a uniform symbol. We will use ' for feet, whereas for inches we will simply not use a symbol since some entries were of the form x'y. Now, if we no longer use the inches symbol, we have to change our pattern accordingly:\n\npattern &lt;- \"^[4-7]'\\\\d{1,2}$\"\n\nIf we do this replacement before the matching, we get many more matches:\n\nproblems %&gt;%\n  str_replace(\"feet|ft|foot\", \"'\") %&gt;% # replace feet, ft, foot with '\n  str_replace(\"inches|in|''|\\\"\", \"\") %&gt;% # remove all inches symbols\n  str_detect(pattern) %&gt;%\n  sum()\n\n[1] 48\n\n\nHowever, we still have many cases to go.\nNote that in the code above, we leveraged the stringr consistency and used the pipe.\nFor now, we improve our pattern by adding \\\\s* in front of and after the feet symbol ' to permit space between the feet symbol and the numbers. Now we match a few more entries:\n\npattern &lt;- \"^[4-7]\\\\s*'\\\\s*\\\\d{1,2}$\"\nproblems %&gt;%\n  str_replace(\"feet|ft|foot\", \"'\") %&gt;% # replace feet, ft, foot with '\n  str_replace(\"inches|in|''|\\\"\", \"\") %&gt;% # remove all inches symbols\n  str_detect(pattern) %&gt;%\n  sum\n\n[1] 53\n\n\nWe might be tempted to avoid doing this by removing all the spaces with str_replace_all. However, when doing such an operation we need to make sure that it does not have unintended effects. In our reported heights examples, this will be a problem because some entries are of the form x y with space separating the feet from the inches. If we remove all spaces, we will incorrectly turn x y into xy which implies that a 6 1 would become 61 inches instead of 73 inches.\nThe second large type of problematic entries were of the form x.y, x,y and x y. We want to change all these to our common format x'y. But we can’t just do a search and replace because we would change values such as 70.5 into 70'5. Our strategy will therefore be to search for a very specific pattern that assures us feet and inches are being provided and then, for those that match, replace appropriately.\n\nSearch and replace using groups\nAnother powerful aspect of groups is that you can refer to the extracted values in a regex when searching and replacing.\nThe regex special character for the i-th group is \\\\i. So \\\\1 is the value extracted from the first group, \\\\2 the value from the second and so on. As a simple example, note that the following code will replace a comma with period, but only if it is between two digits:\n\npattern_with_groups &lt;-  \"^([4-7]),(\\\\d*)$\"\nyes &lt;- c(\"5,9\", \"5,11\", \"6,\", \"6,1\")\nno &lt;- c(\"5'9\", \",\", \"2,8\", \"6.1.1\")\ns &lt;- c(yes, no)\nstr_replace(s, pattern_with_groups, \"\\\\1'\\\\2\")\n\n[1] \"5'9\"   \"5'11\"  \"6'\"    \"6'1\"   \"5'9\"   \",\"     \"2,8\"   \"6.1.1\"\n\n\nWe can use this to convert cases in our reported heights.\nWe are now ready to define a pattern that helps us convert all the x.y, x,y and x y to our preferred format. We need to adapt pattern_with_groups to be a bit more flexible and capture all the cases.\n\npattern_with_groups &lt;-\"^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$\"\n\nLet’s break this one down:\n\n^ = start of the string\n[4-7] = one digit, either 4, 5, 6, or 7\n\\\\s* = none or more white space\n[,\\\\.\\\\s+] = feet symbol is either ,, . or at least one space\n\\\\s* = none or more white space\n\\\\d* = none or more digits\n$ = end of the string\n\nWe can see that it appears to be working:\n\nstr_subset(problems, pattern_with_groups) %&gt;% head()\n\n[1] \"5.3\"  \"5.25\" \"5.5\"  \"6.5\"  \"5.8\"  \"5.6\" \n\n\nand will be able to perform the search and replace:\n\nstr_subset(problems, pattern_with_groups) %&gt;%\n  str_replace(pattern_with_groups, \"\\\\1'\\\\2\") %&gt;% head\n\n[1] \"5'3\"  \"5'25\" \"5'5\"  \"6'5\"  \"5'8\"  \"5'6\" \n\n\nAgain, we will deal with the inches-larger-than-twelve challenge later.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#testing-and-improving",
    "href": "content/Week_13/13a.html#testing-and-improving",
    "title": "Text as Data",
    "section": "Testing and improving",
    "text": "Testing and improving\nDeveloping the right regex on the first try is often difficult. Trial and error is a common approach to finding the regex pattern that satisfies all desired conditions. In the previous sections, we have developed a powerful string processing technique that can help us catch many of the problematic entries. Here we will test our approach, search for further problems, and tweak our approach for possible improvements. Let’s write a function that captures all the entries that can’t be converted into numbers remembering that some are in centimeters (we will deal with those later):\n\nnot_inches_or_cm &lt;- function(x, smallest = 50, tallest = 84){\n  inches &lt;- suppressWarnings(as.numeric(x))\n  ind &lt;- !is.na(inches) &\n    ((inches &gt;= smallest & inches &lt;= tallest) |\n       (inches/2.54 &gt;= smallest & inches/2.54 &lt;= tallest))\n  !ind\n}\n\nproblems &lt;- reported_heights %&gt;%\n  dplyr::filter(not_inches_or_cm(height)) %&gt;%\n  pull(height)\nlength(problems)\n\n[1] 200\n\n\nLet’s see what proportion of these fit our pattern after the processing steps we developed above:\n\nconverted &lt;- problems %&gt;%\n  str_replace(\"feet|foot|ft\", \"'\") %&gt;% # convert feet symbols to '\n  str_replace(\"inches|in|''|\\\"\", \"\") %&gt;%  # remove inches symbols\n  str_replace(\"^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$\", \"\\\\1'\\\\2\")# change format\n\npattern &lt;- \"^[4-7]\\\\s*'\\\\s*\\\\d{1,2}$\"\nindex &lt;- str_detect(converted, pattern)\nmean(index)\n\n[1] 0.615\n\n\nNote how we leveraged the pipe, one of the advantages of using stringr. This last piece of code shows that we have matched well over half of the strings. Let’s examine the remaining cases:\n\nconverted[!index]\n\n [1] \"6\"             \"165cm\"         \"511\"           \"6\"            \n [5] \"2\"             \"&gt;9000\"         \"5 ' and 8.11 \" \"11111\"        \n [9] \"6\"             \"103.2\"         \"19\"            \"5\"            \n[13] \"300\"           \"6'\"            \"6\"             \"Five ' eight \"\n[17] \"7\"             \"214\"           \"6\"             \"0.7\"          \n[21] \"6\"             \"2'33\"          \"612\"           \"1,70\"         \n[25] \"87\"            \"5'7.5\"         \"5'7.5\"         \"111\"          \n[29] \"5' 7.78\"       \"12\"            \"6\"             \"yyy\"          \n[33] \"89\"            \"34\"            \"25\"            \"6\"            \n[37] \"6\"             \"22\"            \"684\"           \"6\"            \n[41] \"1\"             \"1\"             \"6*12\"          \"87\"           \n[45] \"6\"             \"1.6\"           \"120\"           \"120\"          \n[49] \"23\"            \"1.7\"           \"6\"             \"5\"            \n[53] \"69\"            \"5' 9 \"         \"5 ' 9 \"        \"6\"            \n[57] \"6\"             \"86\"            \"708,661\"       \"5 ' 6 \"       \n[61] \"6\"             \"649,606\"       \"10000\"         \"1\"            \n[65] \"728,346\"       \"0\"             \"6\"             \"6\"            \n[69] \"6\"             \"100\"           \"88\"            \"6\"            \n[73] \"170 cm\"        \"7,283,465\"     \"5\"             \"5\"            \n[77] \"34\"           \n\n\nFour clear patterns arise:\n\nMany students measuring exactly 5 or 6 feet did not enter any inches, for example 6', and our pattern requires that inches be included.\nSome students measuring exactly 5 or 6 feet entered just that number.\nSome of the inches were entered with decimal points. For example 5'7.5''. Our pattern only looks for two digits.\nSome entries have spaces at the end, for example 5 ' 9.\n\nAlthough not as common, we also see the following problems:\n\nSome entries are in meters and some of these use European decimals: 1.6, 1,70.\nTwo students added cm.\nA student spelled out the numbers: Five foot eight inches.\n\nIt is not necessarily clear that it is worth writing code to handle these last three cases since they might be rare enough. However, some of them provide us with an opportunity to learn a few more regex techniques, so we will build a fix.\nFor case 1, if we add a '0 after the first digit, for example, convert all 6 to 6'0, then our previously defined pattern will match. This can be done using groups:\n\nyes &lt;- c(\"5\", \"6\", \"5\")\nno &lt;- c(\"5'\", \"5''\", \"5'4\")\ns &lt;- c(yes, no)\nstr_replace(s, \"^([4-7])$\", \"\\\\1'0\")\n\n[1] \"5'0\" \"6'0\" \"5'0\" \"5'\"  \"5''\" \"5'4\"\n\n\nThe pattern says it has to start (^) with a digit between 4 and 7 and end there ($). The parenthesis defines the group that we pass as \\\\1 to generate the replacement regex string.\nWe can adapt this code slightly to handle the case 2 as well, which covers the entry 5'. Note 5' is left untouched. This is because the extra ' makes the pattern not match since we have to end with a 5 or 6. We want to permit the 5 or 6 to be followed by 0 or 1 feet sign. So we can simply add '{0,1} after the ' to do this. However, we can use the none or once special character ?. As we saw above, this is different from * which is none or more. We now see that the fourth case is also converted:\n\nstr_replace(s, \"^([56])'?$\", \"\\\\1'0\")\n\n[1] \"5'0\" \"6'0\" \"5'0\" \"5'0\" \"5''\" \"5'4\"\n\n\nHere we only permit 5 and 6, but not 4 and 7. This is because 5 and 6 feet tall is quite common, so we assume those that typed 5 or 6 really meant 60 or 72 inches. However, 4 and 7 feet tall are so rare that, although we accept 84 as a valid entry, we assume 7 was entered in error.\nWe can use quantifiers to deal with case 3. These entries are not matched because the inches include decimals and our pattern does not permit this. We need to allow the second group to include decimals not just digits. This means we must permit zero or one period . then zero or more digits. So we will be using both ? and *. Also remember that, for this particular case, the period needs to be escaped since it is a special character (it means any character except line break). Here is a simple example of how we can use *.\nSo we can adapt our pattern, currently ^[4-7]\\\\s*'\\\\s*\\\\d{1,2}$ to permit a decimal at the end:\n\npattern &lt;- \"^[4-7]\\\\s*'\\\\s*(\\\\d+\\\\.?\\\\d*)$\"\n\nCase 4, meters using commas, we can approach similarly to how we converted the x.y to x'y. A difference is that we require that the first digit be 1 or 2:\n\nyes &lt;- c(\"1,7\", \"1, 8\", \"2, \" )\nno &lt;- c(\"5,8\", \"5,3,2\", \"1.7\")\ns &lt;- c(yes, no)\nstr_replace(s, \"^([12])\\\\s*,\\\\s*(\\\\d*)$\", \"\\\\1\\\\.\\\\2\")\n\n[1] \"1.7\"   \"1.8\"   \"2.\"    \"5,8\"   \"5,3,2\" \"1.7\"  \n\n\nWe will later check if the entries are meters using their numeric values. We will come back to the case study after introducing two widely used functions in string processing that will come in handy when developing our final solution for the self-reported heights.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#trimming",
    "href": "content/Week_13/13a.html#trimming",
    "title": "Text as Data",
    "section": "Trimming",
    "text": "Trimming\nIn general, spaces at the start or end of the string are uninformative. These can be particularly deceptive because sometimes they can be hard to see:\n\ns &lt;- \"Hi \"\ncat(s)\n\nHi \n\nidentical(s, \"Hi\")\n\n[1] FALSE\n\n\nThis is a general enough problem that there is a function dedicated to removing them: str_trim.\n\nstr_trim(\"5 ' 9 \")\n\n[1] \"5 ' 9\"",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#changing-lettercase",
    "href": "content/Week_13/13a.html#changing-lettercase",
    "title": "Text as Data",
    "section": "Changing lettercase",
    "text": "Changing lettercase\nNotice that regex is case sensitive. Often we want to match a word regardless of case. One approach to doing this is to first change everything to lower case and then proceeding ignoring case. As an example, note that one of the entries writes out numbers as words Five foot eight inches. Although not efficient, we could add 13 extra str_replace calls to convert zero to 0, one to 1, and so on. To avoid having to write two separate operations for Zero and zero, One and one, etc., we can use the str_to_lower function to make all works lower case first:\n\ns &lt;- c(\"Five feet eight inches\")\nstr_to_lower(s)\n\n[1] \"five feet eight inches\"\n\n\nOther related functions are str_to_upper and str_to_title. These are especially useful when you’re trying to merge two datasets – you’ll often see state names in various capitalization forms.\nWe are now ready to define a procedure that converts all the problematic cases to inches.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#case-study-2-self-reported-heights-continued",
    "href": "content/Week_13/13a.html#case-study-2-self-reported-heights-continued",
    "title": "Text as Data",
    "section": "Case study 2: self-reported heights (continued)",
    "text": "Case study 2: self-reported heights (continued)\nWe now put all of what we have learned together into a function that takes a string vector and tries to convert as many strings as possible to one format. We write a function that puts together what we have done above.\n\nconvert_format &lt;- function(s){\n  s %&gt;%\n    str_replace(\"feet|foot|ft\", \"'\") %&gt;%\n    str_replace_all(\"inches|in|''|\\\"|cm|and\", \"\") %&gt;%\n    str_replace(\"^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$\", \"\\\\1'\\\\2\") %&gt;%\n    str_replace(\"^([56])'?$\", \"\\\\1'0\") %&gt;%\n    str_replace(\"^([12])\\\\s*,\\\\s*(\\\\d*)$\", \"\\\\1\\\\.\\\\2\") %&gt;%\n    str_trim()\n}\n\nWe can also write a function that converts words to numbers:\n\nlibrary(english)\nwords_to_numbers &lt;- function(s){\n  s &lt;- str_to_lower(s)\n  for(i in 0:11)\n    s &lt;- str_replace_all(s, words(i), as.character(i))\n  s\n}\n\nNote that we can perform the above operation more efficiently with the function recode. Now we can see which problematic entries remain:\n\nconverted &lt;- problems %&gt;% words_to_numbers() %&gt;% convert_format()\nremaining_problems &lt;- converted[not_inches_or_cm(converted)]\npattern &lt;- \"^[4-7]\\\\s*'\\\\s*\\\\d+\\\\.?\\\\d*$\"\nindex &lt;- str_detect(remaining_problems, pattern)\nremaining_problems[!index]\n\n [1] \"511\"       \"2\"         \"&gt;9000\"     \"11111\"     \"103.2\"     \"19\"       \n [7] \"300\"       \"7\"         \"214\"       \"0.7\"       \"2'33\"      \"612\"      \n[13] \"1.70\"      \"87\"        \"111\"       \"12\"        \"yyy\"       \"89\"       \n[19] \"34\"        \"25\"        \"22\"        \"684\"       \"1\"         \"1\"        \n[25] \"6*12\"      \"87\"        \"1.6\"       \"120\"       \"120\"       \"23\"       \n[31] \"1.7\"       \"86\"        \"708,661\"   \"649,606\"   \"10000\"     \"1\"        \n[37] \"728,346\"   \"0\"         \"100\"       \"88\"        \"7,283,465\" \"34\"       \n\n\napart from the cases reported as meters, which we will fix below, they all seem to be cases that are impossible to fix.\n\nThe extract function\nThe extract function is a useful tidyverse function for string processing that we will use in our final solution, so we introduce it here. In a previous section, we constructed a regex that lets us identify which elements of a character vector match the feet and inches pattern. However, we want to do more. We want to extract and save the feet and number values so that we can convert them to inches when appropriate.\nIf we have a simpler case like this:\n\ns &lt;- c(\"5'10\", \"6'1\")\ntab &lt;- data.frame(x = s)\n\nIn an earlier Content section we learned about the separate function, which can be used to achieve our current goal:\n\ntab %&gt;% separate(x, c(\"feet\", \"inches\"), sep = \"'\")\n\n  feet inches\n1    5     10\n2    6      1\n\n\nThe extract function from the tidyr package lets us use regex groups to extract the desired values. Here is the equivalent to the code above using separate but using extract:\n\nlibrary(tidyr)\ntab %&gt;% tidyr::extract(x, c(\"feet\", \"inches\"), regex = \"(\\\\d)'(\\\\d{1,2})\")\n\n  feet inches\n1    5     10\n2    6      1\n\n\nSo why do we even need the new function extract? We have seen how small changes can throw off exact pattern matching. Groups in regex give us more flexibility. For example, if we define:\n\ns &lt;- c(\"5'10\", \"6'1\\\"\",\"5'8inches\")\ntab &lt;- data.frame(x = s)\n\nand we only want the numbers, separate fails:\n\ntab %&gt;% separate(x, c(\"feet\",\"inches\"), sep = \"'\", fill = \"right\")\n\n  feet  inches\n1    5      10\n2    6      1\"\n3    5 8inches\n\n\nHowever, we can use extract. The regex here is a bit more complicated since we have to permit ' with spaces and feet. We also do not want the \" included in the value, so we do not include that in the group:\n\ntab %&gt;% tidyr::extract(x, c(\"feet\", \"inches\"), regex = \"(\\\\d)'(\\\\d{1,2})\")\n\n  feet inches\n1    5     10\n2    6      1\n3    5      8\n\n\n\n\nPutting it all together\nWe are now ready to put it all together and wrangle our reported heights data to try to recover as many heights as possible. The code is complex, but we will break it down into parts.\nWe start by cleaning up the height column so that the heights are closer to a feet’inches format. We added an original heights column so we can compare before and after.\nNow we are ready to wrangle our reported heights dataset:\n\npattern &lt;- \"^([4-7])\\\\s*'\\\\s*(\\\\d+\\\\.?\\\\d*)$\"\n\nsmallest &lt;- 50\ntallest &lt;- 84\nnew_heights &lt;- reported_heights %&gt;%\n  dplyr::mutate(original = height,\n         height = words_to_numbers(height) %&gt;% convert_format()) %&gt;%\n  tidyr::extract(height, c(\"feet\", \"inches\"), regex = pattern, remove = FALSE) %&gt;%\n  dplyr::mutate_at(c(\"height\", \"feet\", \"inches\"), as.numeric) %&gt;%\n  dplyr::mutate(guess = 12 * feet + inches) %&gt;%\n  dplyr::mutate(height = case_when(\n    is.na(height) ~ as.numeric(NA),\n    between(height, smallest, tallest) ~ height,  #inches\n    between(height/2.54, smallest, tallest) ~ height/2.54, #cm\n    between(height*100/2.54, smallest, tallest) ~ height*100/2.54, #meters\n    TRUE ~ as.numeric(NA))) %&gt;%\n  dplyr::mutate(height = ifelse(is.na(height) &\n                           inches &lt; 12 & between(guess, smallest, tallest),\n                         guess, height)) %&gt;%\n  dplyr::select(-guess)\n\nWe can check all the entries we converted by typing:\n\nnew_heights %&gt;%\n  dplyr::filter(not_inches(original)) %&gt;%\n  dplyr::select(original, height) %&gt;%\n  arrange(height) %&gt;%\n  View()\n\nA final observation is that if we look at the shortest students in our course:\n\nnew_heights %&gt;% arrange(height) %&gt;% head(n=7)\n\n           time_stamp    sex height feet inches original\n1 2017-07-04 01:30:25   Male  50.00   NA     NA       50\n2 2017-09-07 10:40:35   Male  50.00   NA     NA       50\n3 2014-09-02 15:18:30 Female  51.00   NA     NA       51\n4 2016-06-05 14:07:20 Female  52.00   NA     NA       52\n5 2016-06-05 14:07:38 Female  52.00   NA     NA       52\n6 2014-09-23 03:39:56 Female  53.00   NA     NA       53\n7 2015-01-07 08:57:29   Male  53.77   NA     NA    53.77\n\n\nWe see heights of 53, 54, and 55. In the originals, we also have 51 and 52. These short heights are rare and it is likely that the students actually meant 5'1, 5'2, 5'3, 5'4, and 5'5. Because we are not completely sure, we will leave them as reported. The object new_heights contains our final solution for this case study.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#string-splitting",
    "href": "content/Week_13/13a.html#string-splitting",
    "title": "Text as Data",
    "section": "String splitting",
    "text": "String splitting\nAnother very common data wrangling operation is string splitting. To illustrate how this comes up, we start with an illustrative example. Suppose we did not have the function read_csv or read.csv available to us. We instead have to read a csv file using the base R function readLines like this:\n\nfilename &lt;- system.file(\"extdata/murders.csv\", package = \"dslabs\")\nlines &lt;- readLines(filename)\n\nThis function reads-in the data line-by-line to create a vector of strings. In this case, one string for each row in the spreadsheet. The first six lines are:\n\nlines %&gt;% head()\n\n[1] \"state,abb,region,population,total\" \"Alabama,AL,South,4779736,135\"     \n[3] \"Alaska,AK,West,710231,19\"          \"Arizona,AZ,West,6392017,232\"      \n[5] \"Arkansas,AR,South,2915918,93\"      \"California,CA,West,37253956,1257\" \n\n\nWe want to extract the values that are separated by a comma for each string in the vector. The command str_split does exactly this:\n\nx &lt;- str_split(lines, \",\")\nx %&gt;% head(2)\n\n[[1]]\n[1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"     \n\n[[2]]\n[1] \"Alabama\" \"AL\"      \"South\"   \"4779736\" \"135\"    \n\n\nNote that the first entry has the column names, so we can separate that out:\n\ncol_names &lt;- x[[1]]\nx &lt;- x[-1]\n\nTo convert our list into a data frame, we can use a shortcut provided by the map functions in the purrr package. The map function applies the same function to each element in a list. So if we want to extract the first entry of each element in x, we can write:\n\nlibrary(purrr)\nmap(x, function(y) y[1]) %&gt;% head(2)\n\n[[1]]\n[1] \"Alabama\"\n\n[[2]]\n[1] \"Alaska\"\n\n\nHowever, because this is such a common task, purrr provides a shortcut. If the second argument receives an integer instead of a function, it assumes we want that entry. So the code above can be written more efficiently like this:\n\nmap(x, 1)\n\nTo force map to return a character vector instead of a list, we can use map_chr. Similarly, map_int returns integers. So to create our data frame, we can use:\n\ndat &lt;- tibble(map_chr(x, 1),\n              map_chr(x, 2),\n              map_chr(x, 3),\n              map_chr(x, 4),\n              map_chr(x, 5)) %&gt;%\n  mutate_all(parse_guess) %&gt;%\n  setNames(col_names)\ndat %&gt;% head\n\n# A tibble: 6 × 5\n  state      abb   region population total\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Alabama    AL    South     4779736   135\n2 Alaska     AK    West       710231    19\n3 Arizona    AZ    West      6392017   232\n4 Arkansas   AR    South     2915918    93\n5 California CA    West     37253956  1257\n6 Colorado   CO    West      5029196    65\n\n\nIf you learn more about the purrr package, you will learn that you perform the above with the following, more efficient, code:\n\ndat &lt;- x %&gt;%\n  transpose() %&gt;%\n  map( ~ parse_guess(unlist(.))) %&gt;%\n  setNames(col_names) %&gt;%\n  as_tibble()\n\nIt turns out that we can avoid all the work shown above after the call to str_split. Specifically, if we know that the data we are extracting can be represented as a table, we can use the argument simplify=TRUE and str_split returns a matrix instead of a list:\n\nx &lt;- str_split(lines, \",\", simplify = TRUE)\ncol_names &lt;- x[1,]\nx &lt;- x[-1,]\ncolnames(x) &lt;- col_names\nx %&gt;% as_tibble() %&gt;%\n  mutate_all(parse_guess) %&gt;%\n  head(5)\n\n# A tibble: 5 × 5\n  state      abb   region population total\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Alabama    AL    South     4779736   135\n2 Alaska     AK    West       710231    19\n3 Arizona    AZ    West      6392017   232\n4 Arkansas   AR    South     2915918    93\n5 California CA    West     37253956  1257",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#case-study-3-extracting-tables-from-a-pdf",
    "href": "content/Week_13/13a.html#case-study-3-extracting-tables-from-a-pdf",
    "title": "Text as Data",
    "section": "Case study 3: extracting tables from a PDF",
    "text": "Case study 3: extracting tables from a PDF\nOne of the datasets provided in dslabs shows scientific funding rates by gender in the Netherlands:\n\nlibrary(dslabs)\ndata(\"research_funding_rates\")\nresearch_funding_rates %&gt;%\n  dplyr::select(\"discipline\", \"success_rates_men\", \"success_rates_women\")\n\n           discipline success_rates_men success_rates_women\n1   Chemical sciences              26.5                25.6\n2   Physical sciences              19.3                23.1\n3             Physics              26.9                22.2\n4          Humanities              14.3                19.3\n5  Technical sciences              15.9                21.0\n6   Interdisciplinary              11.4                21.8\n7 Earth/life sciences              24.4                14.3\n8     Social sciences              15.3                11.5\n9    Medical sciences              18.8                11.2\n\n\nThe data comes from a paper published in the Proceedings of the National Academy of Science (PNAS)5, a widely read scientific journal. However, the data is not provided in a spreadsheet; it is in a table in a PDF document. Here is a screenshot of the table:\n\n\n\n\n\n\n\n\n\n(Source: Romy van der Lee and Naomi Ellemers, PNAS 2015 112 (40) 12349-123536.)\nWe could extract the numbers by hand, but this could lead to human error. Instead, we can try to wrangle the data using R. We start by downloading the pdf document, then importing into R:\n\nlibrary(\"pdftools\")\ntemp_file &lt;- tempfile()\nurl &lt;- paste0(\"https://www.pnas.org/content/suppl/2015/09/16/\",\n              \"1510159112.DCSupplemental/pnas.201510159SI.pdf\")\ndownload.file(url, temp_file)\ntxt &lt;- pdf_text(temp_file)\nfile.remove(temp_file)\n\nIf we examine the object text, we notice that it is a character vector with an entry for each page. So we keep the page we want:\n\nraw_data_research_funding_rates &lt;- txt[2]\n\nThe steps above can actually be skipped because we include this raw data in the dslabs package as well:\n\ndata(\"raw_data_research_funding_rates\")\n\nExamining the object raw_data_research_funding_rates we see that it is a long string and each line on the page, including the table rows, are separated by the symbol for newline: \\n. We therefore can create a list with the lines of the text as elements as follows:\n\ntab &lt;- str_split(raw_data_research_funding_rates, \"\\n\")\n\nBecause we start off with just one element in the string, we end up with a list with just one entry.\n\ntab &lt;- tab[[1]]\n\nBy examining tab we see that the information for the column names is the third and fourth entries:\n\nthe_names_1 &lt;- tab[3]\nthe_names_2 &lt;- tab[4]\n\nThe first of these rows looks like this:\n\n\n                                                      Applications, n           \n\n\n          Awards, n                      Success rates, %\n\n\nWe want to create one vector with one name for each column. Using some of the functions we have just learned, we do this. Let’s start with the_names_1, shown above. We want to remove the leading space and anything following the comma. We use regex for the latter. Then we can obtain the elements by splitting strings separated by space. We want to split only when there are 2 or more spaces to avoid splitting Success rates. So we use the regex \\\\s{2,}\n\nthe_names_1 &lt;- the_names_1 %&gt;%\n  str_trim() %&gt;%\n  str_replace_all(\",\\\\s.\", \"\") %&gt;%\n  str_split(\"\\\\s{2,}\", simplify = TRUE)\nthe_names_1\n\n     [,1]           [,2]     [,3]           \n[1,] \"Applications\" \"Awards\" \"Success rates\"\n\n\nNow we will look at the_names_2:\n\n\n                        Discipline              Total     Men      Women        \n\n\n  Total    Men       Women          Total    Men      Women\n\n\nHere we want to trim the leading space and then split by space as we did for the first line:\n\nthe_names_2 &lt;- the_names_2 %&gt;%\n  str_trim() %&gt;%\n  str_split(\"\\\\s+\", simplify = TRUE)\nthe_names_2\n\n     [,1]         [,2]    [,3]  [,4]    [,5]    [,6]  [,7]    [,8]    [,9] \n[1,] \"Discipline\" \"Total\" \"Men\" \"Women\" \"Total\" \"Men\" \"Women\" \"Total\" \"Men\"\n     [,10]  \n[1,] \"Women\"\n\n\nWe can then join these to generate one name for each column:\n\ntmp_names &lt;- str_c(rep(the_names_1, each = 3), the_names_2[-1], sep = \"_\")\nthe_names &lt;- c(the_names_2[1], tmp_names) %&gt;%\n  str_to_lower() %&gt;%\n  str_replace_all(\"\\\\s\", \"_\")\nthe_names\n\n [1] \"discipline\"          \"applications_total\"  \"applications_men\"   \n [4] \"applications_women\"  \"awards_total\"        \"awards_men\"         \n [7] \"awards_women\"        \"success_rates_total\" \"success_rates_men\"  \n[10] \"success_rates_women\"\n\n\nNow we are ready to get the actual data. By examining the tab object, we notice that the information is in lines 6 through 14. We can use str_split again to achieve our goal:\n\nnew_research_funding_rates &lt;- tab[6:14] %&gt;%\n  str_trim %&gt;%\n  str_split(\"\\\\s{2,}\", simplify = TRUE) %&gt;%\n  data.frame(stringsAsFactors = FALSE) %&gt;%\n  setNames(the_names) %&gt;%\n  mutate_at(-1, parse_number)\nnew_research_funding_rates %&gt;% as_tibble()\n\n# A tibble: 9 × 10\n  discipline applications_total applications_men applications_women awards_total\n  &lt;chr&gt;                   &lt;dbl&gt;            &lt;dbl&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n1 Chemical …                122               83                 39           32\n2 Physical …                174              135                 39           35\n3 Physics                    76               67                  9           20\n4 Humanities                396              230                166           65\n5 Technical…                251              189                 62           43\n6 Interdisc…                183              105                 78           29\n7 Earth/lif…                282              156                126           56\n8 Social sc…                834              425                409          112\n9 Medical s…                505              245                260           75\n# ℹ 5 more variables: awards_men &lt;dbl&gt;, awards_women &lt;dbl&gt;,\n#   success_rates_total &lt;dbl&gt;, success_rates_men &lt;dbl&gt;,\n#   success_rates_women &lt;dbl&gt;\n\n\nWe can see that the objects are identical:\n\nidentical(research_funding_rates, new_research_funding_rates)\n\n[1] TRUE",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#recoding",
    "href": "content/Week_13/13a.html#recoding",
    "title": "Text as Data",
    "section": "Recoding",
    "text": "Recoding\nAnother common operation involving strings is recoding the names of categorical variables. Let’s say you have really long names for your levels and you will be displaying them in plots, you might want to use shorter versions of these names. For example, in character vectors with country names, you might want to change “United States of America” to “USA” and “United Kingdom” to UK, and so on. We can do this with case_when, although the tidyverse offers an option that is specifically designed for this task: the recode function.\nHere is an example that shows how to rename countries with long names:\n\nlibrary(dslabs)\ndata(\"gapminder\")\n\nSuppose we want to show life expectancy time series by country for the Caribbean:\n\ngapminder %&gt;%\n  dplyr::filter(region == \"Caribbean\") %&gt;%\n  ggplot(aes(year, life_expectancy, color = country)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThe plot is what we want, but much of the space is wasted to accommodate some of the long country names.  We have four countries with names longer than 12 characters. These names appear once for each year in the Gapminder dataset. Once we pick nicknames, we need to change them all consistently. The recode function can be used to do this:\n\ngapminder %&gt;% dplyr::filter(region==\"Caribbean\") %&gt;%\n  mutate(country = recode(country,\n                          `Antigua and Barbuda` = \"Barbuda\",\n                          `Dominican Republic` = \"DR\",\n                          `St. Vincent and the Grenadines` = \"St. Vincent\",\n                          `Trinidad and Tobago` = \"Trinidad\")) %&gt;%\n  ggplot(aes(year, life_expectancy, color = country)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThere are other similar functions in other R packages, such as recode_factor and fct_recoder in the forcats package.\n\n\n\n\n\n\nTRY IT\n\n\n\n\nComplete all lessons and exercises in the https://regexone.com/ online interactive tutorial.\nIn the extdata directory of the dslabs package, you will find a PDF file containing daily mortality data for Puerto Rico from Jan 1, 2015 to May 31, 2018. You can find the file like this:\n\n\nfn &lt;- system.file(\"extdata\", \"RD-Mortality-Report_2015-18-180531.pdf\",\n                  package=\"dslabs\")\n\nFind and open the file or open it directly from RStudio. On a Mac, you can type:\n\nsystem2(\"open\", args = fn)\n\nand on Windows, you can type:\n\nsystem(\"cmd.exe\", input = paste(\"start\", fn))\n\nWhich of the following best describes this file:\n\nIt is a table. Extracting the data will be easy.\nIt is a report written in prose. Extracting the data will be impossible.\nIt is a report combining graphs and tables. Extracting the data seems possible.\nIt shows graphs of the data. Extracting the data will be difficult.\n\n\nWe are going to create a tidy dataset with each row representing one observation. The variables in this dataset will be year, month, day, and deaths. Start by installing and loading the pdftools package:\n\n\ninstall.packages(\"pdftools\")\nlibrary(pdftools)\n\nNow read-in fn using the pdf_text function and store the results in an object called txt. Which of the following best describes what you see in txt?\n\nA table with the mortality data.\nA character string of length 12. Each entry represents the text in each page. The mortality data is in there somewhere.\nA character string with one entry containing all the information in the PDF file.\nAn html document.\n\n\nExtract the ninth page of the PDF file from the object txt, then use the str_split from the stringr package so that you have each line in a different entry. Call this string vector s. Then look at the result and choose the one that best describes what you see.\n\n\nIt is an empty string.\nI can see the figure shown in page 1.\nIt is a tidy table.\nI can see the table! But there is a bunch of other stuff we need to get rid of.\n\n\nWhat kind of object is s and how many entries does it have?\nWe see that the output is a list with one component. Redefine s to be the first entry of the list. What kind of object is s and how many entries does it have?\nWhen inspecting the string we obtained above, we see a common problem: white space before and after the other characters. Trimming is a common first step in string processing. These extra spaces will eventually make splitting the strings hard so we start by removing them. We learned about the command str_trim that removes spaces at the start or end of the strings. Use this function to trim s.\nWe want to extract the numbers from the strings stored in s. However, there are many non-numeric characters that will get in the way. We can remove these, but before doing this we want to preserve the string with the column header, which includes the month abbreviation. Use the str_which function to find the rows with a header. Save these results to header_index. Hint: find the first string that matches the pattern 2015 using the str_which function.\nNow we are going to define two objects: month will store the month and header will store the column names. Identify which row contains the header of the table. Save the content of the row into an object called header, then use str_split to help define the two objects we need. Hints: the separator here is one or more spaces. Also, consider using the simplify argument.\nNotice that towards the end of the page you see a totals row followed by rows with other summary statistics. Create an object called tail_index with the index of the totals entry.\nBecause our PDF page includes graphs with numbers, some of our rows have just one number (from the y-axis of the plot). Use the str_count function to create an object n with the number of numbers in each each row. Hint: you can write a regex for number like this \\\\d+.\nWe are now ready to remove entries from rows that we know we don’t need. The entry header_index and everything before it should be removed. Entries for which n is 1 should also be removed, and the entry tail_index and everything that comes after it should be removed as well.\nNow we are ready to remove all the non-numeric entries. Do this using regex and the str_remove_all function. Hint: remember that in regex, using the upper case version of a special character usually means the opposite. So \\\\D means “not a digit”. Remember you also want to keep spaces.\nTo convert the strings into a table, use the str_split_fixed function. Convert s into a data matrix with just the day and death count data. Hints: note that the separator is one or more spaces. Make the argument n a value that limits the number of columns to the values in the 4 columns and the last column captures all the extra stuff. Then keep only the first four columns.\nNow you are almost ready to finish. Add column names to the matrix, including one called day. Also, add a column with the month. Call the resulting object dat. Finally, make sure the day is an integer not a character. Hint: use only the first five columns.\nNow finish it up by tidying tab with the gather function.\nMake a plot of deaths versus day with color to denote year. Exclude 2018 since we do not have data for the entire year.\nNow that we have wrangled this data step-by-step, put it all together in one R chunk, using the pipe as much as possible. Hint: first define the indexes, then write one line of code that does all the string processing.\nAdvanced: let’s return to the MLB Payroll example from the web scraping section. Use what you have learned in the web scraping and string processing chapters to extract the payroll for the New York Yankees, Boston Red Sox, and Oakland A’s and plot them as a function of time.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#html",
    "href": "content/Week_13/13a.html#html",
    "title": "Text as Data",
    "section": "HTML",
    "text": "HTML\nBecause this code is accessible, we can download the HTML file, import it into R, and then write programs to extract the information we need from the page. However, once we look at HTML code, this might seem like a daunting task. But we will show you some convenient tools to facilitate the process. To get an idea of how it works, here are a few lines of code from the Wikipedia page that provides the US murders data:\n&lt;table class=\"wikitable sortable\"&gt;\n&lt;tr&gt;\n&lt;th&gt;State&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"/wiki/List_of_U.S._states_and_territories_by_population\"\ntitle=\"List of U.S. states and territories by population\"&gt;Population&lt;/a&gt;&lt;br /&gt;\n&lt;small&gt;(total inhabitants)&lt;/small&gt;&lt;br /&gt;\n&lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=\"cite_ref-1\" class=\"reference\"&gt;\n&lt;a href=\"#cite_note-1\"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/th&gt;\n&lt;th&gt;Murders and Nonnegligent\n&lt;p&gt;Manslaughter&lt;br /&gt;\n&lt;small&gt;(total deaths)&lt;/small&gt;&lt;br /&gt;\n&lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=\"cite_ref-2\" class=\"reference\"&gt;\n&lt;a href=\"#cite_note-2\"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;\n&lt;/th&gt;\n&lt;th&gt;Murder and Nonnegligent\n&lt;p&gt;Manslaughter Rate&lt;br /&gt;\n&lt;small&gt;(per 100,000 inhabitants)&lt;/small&gt;&lt;br /&gt;\n&lt;small&gt;(2015)&lt;/small&gt;&lt;/p&gt;\n&lt;/th&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Alabama\" title=\"Alabama\"&gt;Alabama&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;4,853,875&lt;/td&gt;\n&lt;td&gt;348&lt;/td&gt;\n&lt;td&gt;7.2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Alaska\" title=\"Alaska\"&gt;Alaska&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;737,709&lt;/td&gt;\n&lt;td&gt;59&lt;/td&gt;\n&lt;td&gt;8.0&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\nYou can actually see the data, except data values are surrounded by html code such as &lt;td&gt;. We can also see a pattern of how it is stored. If you know HTML, you can write programs that leverage knowledge of these patterns to extract what we want. We also take advantage of a language widely used to make webpages look “pretty” called Cascading Style Sheets (CSS).\nAlthough we provide tools that make it possible to scrape data without knowing HTML, as a data scientist it is quite useful to learn some HTML and CSS. Not only does this improve your scraping skills, but it might come in handy if you are creating a webpage to showcase your work. There are plenty of online courses and tutorials for learning these. Two examples are Codeacademy9 and W3schools10.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#the-rvest-package",
    "href": "content/Week_13/13a.html#the-rvest-package",
    "title": "Text as Data",
    "section": "The rvest package",
    "text": "The rvest package\nThe tidyverse provides a web harvesting package called rvest. The first step using this package is to import the webpage into R. The package makes this quite simple:\n\nlibrary(tidyverse)\nlibrary(rvest)\nh &lt;- read_html(url)\n\nNote that the entire Murders in the US Wikipedia webpage is now contained in h. The class of this object is:\n\nclass(h)\n\n[1] \"xml_document\" \"xml_node\"    \n\n\nThe rvest package is actually more general; it handles XML documents. XML is a general markup language (that’s what the ML stands for) that can be used to represent any kind of data. HTML is a specific type of XML specifically developed for representing webpages. Here we focus on HTML documents.\nNow, how do we extract the table from the object h? If we print h, we don’t really see much:\n\nh\n\n{html_document}\n&lt;html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-toc-available\" lang=\"en\" dir=\"ltr\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body class=\"skin--responsive skin-vector skin-vector-search-vue mediawik ...\n\n\nWe can see all the code that defines the downloaded webpage using the html_text function like this:\n\nhtml_text(h)\n\nWe don’t show the output here because it includes thousands of characters, but if we look at it, we can see the data we are after are stored in an HTML table: you can see this in this line of the HTML code above &lt;table class=\"wikitable sortable\"&gt;. The different parts of an HTML document, often defined with a message in between &lt; and &gt; are referred to as nodes. The rvest package includes functions to extract nodes of an HTML document: html_nodes extracts all nodes of different types and html_node extracts the first one. To extract the tables from the html code we use:\n\ntab &lt;- h %&gt;% html_nodes(\"table\")\n\nNow, instead of the entire webpage, we just have the html code for the tables in the page:\n\ntab\n\n{xml_nodeset (2)}\n[1] &lt;table class=\"wikitable sortable\"&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;State\\n&lt;/th&gt;\\n&lt;th&gt;\\n ...\n[2] &lt;table class=\"nowraplinks hlist mw-collapsible mw-collapsed navbox-inner\" ...\n\n\nThe table we are interested is the first one:\n\ntab[[1]]\n\n{html_node}\n&lt;table class=\"wikitable sortable\"&gt;\n[1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;State\\n&lt;/th&gt;\\n&lt;th&gt;\\n&lt;a href=\"/wiki/List_of_U.S._states ...\n\n\nThis is clearly not a tidy dataset, not even a data frame. In the code above, you can definitely see a pattern and writing code to extract just the data is very doable. In fact, rvest includes a function just for converting HTML tables into data frames:\n\ntab &lt;- tab[[1]] %&gt;% html_table\nclass(tab)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWe are now much closer to having a usable data table:\n\ntab &lt;- tab %&gt;% setNames(c(\"state\", \"population\", \"total\", \"murder_rate\"))\nhead(tab)\n\n# A tibble: 6 × 4\n  state      population total murder_rate\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt;\n1 Alabama    4,853,875  348           7.2\n2 Alaska     737,709    59            8  \n3 Arizona    6,817,565  309           4.5\n4 Arkansas   2,977,853  181           6.1\n5 California 38,993,940 1,861         4.8\n6 Colorado   5,448,819  176           3.2\n\n\nWe still have some wrangling to do. For example, we need to remove the commas and turn characters into numbers. Before continuing with this, we will learn a more general approach to extracting information from web sites.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#css-selectors",
    "href": "content/Week_13/13a.html#css-selectors",
    "title": "Text as Data",
    "section": "CSS selectors",
    "text": "CSS selectors\nThe default look of a webpage made with the most basic HTML is quite unattractive. The aesthetically pleasing pages we see today are made using CSS to define the look and style of webpages. The fact that all pages for a company have the same style usually results from their use of the same CSS file to define the style. The general way these CSS files work is by defining how each of the elements of a webpage will look. The title, headings, itemized lists, tables, and links, for example, each receive their own style including font, color, size, and distance from the margin. CSS does this by leveraging patterns used to define these elements, referred to as selectors. An example of such a pattern, which we used above, is table, but there are many, many more.\nIf we want to grab data from a webpage and we happen to know a selector that is unique to the part of the page containing this data, we can use the html_nodes function. However, knowing which selector can be quite complicated. In fact, the complexity of webpages has been increasing as they become more sophisticated. For some of the more advanced ones, it seems almost impossible to find the nodes that define a particular piece of data. However, selector gadgets actually make this possible.\nSelectorGadget11 is piece of software that allows you to interactively determine what CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables from html pages, we highly recommend you install it. A Chrome extension is available which permits you to turn on the gadget and then, as you click through the page, it highlights parts and shows you the selector you need to extract these parts. There are various demos of how to do this including rvest author Hadley Wickham’s vignette12 and other tutorials based on the vignette13 14.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#json",
    "href": "content/Week_13/13a.html#json",
    "title": "Text as Data",
    "section": "JSON",
    "text": "JSON\nSharing data on the internet has become more and more common. Unfortunately, providers use different formats, which makes it harder for data scientists to wrangle data into R. Yet there are some standards that are also becoming more common. Currently, a format that is widely being adopted is the JavaScript Object Notation or JSON. Because this format is very general, it is nothing like a spreadsheet. This JSON file looks more like the code you use to define a list. Here is an example of information stored in a JSON format:\n\n\n[\n  {\n    \"name\": \"Miguel\",\n    \"student_id\": 1,\n    \"exam_1\": 85,\n    \"exam_2\": 86\n  },\n  {\n    \"name\": \"Sofia\",\n    \"student_id\": 2,\n    \"exam_1\": 94,\n    \"exam_2\": 93\n  },\n  {\n    \"name\": \"Aya\",\n    \"student_id\": 3,\n    \"exam_1\": 87,\n    \"exam_2\": 88\n  },\n  {\n    \"name\": \"Cheng\",\n    \"student_id\": 4,\n    \"exam_1\": 90,\n    \"exam_2\": 91\n  }\n] \n\n\nThe file above actually represents a data frame. To read it, we can use the function fromJSON from the jsonlite package. Note that JSON files are often made available via the internet. Several organizations provide a JSON API or a web service that you can connect directly to and obtain data. Here is an example:\n\nlibrary(jsonlite)\nres &lt;- fromJSON('http://ergast.com/api/f1/2004/1/results.json')\n\nciti_bike &lt;- fromJSON(\"http://citibikenyc.com/stations/json\")\n\nThis downloads a list. The first argument tells you when you downloaded it:\n\nciti_bike$executionTime\n\nand the second is a data table:\n\nciti_bike$stationBeanList %&gt;% as_tibble()\n\nYou can learn much more by examining tutorials and help files from the jsonlite package. This package is intended for relatively simple tasks such as converting data into tables. For more flexibility, we recommend rjson.",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_13/13a.html#footnotes",
    "href": "content/Week_13/13a.html#footnotes",
    "title": "Text as Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.regular-expressions.info/tutorial.html↩︎\nhttp://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions↩︎\nhttps://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf↩︎\nhttps://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf↩︎\nhttp://www.pnas.org/content/112/40/12349.abstract↩︎\nhttp://www.pnas.org/content/112/40/12349↩︎\nhttps://en.wikipedia.org/w/index.php?title=Gun_violence_in_the_United_States_by_state&direction=prev&oldid=810166167↩︎\nhttps://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License↩︎\nhttps://www.codecademy.com/learn/learn-html↩︎\nhttps://www.w3schools.com/↩︎\nhttp://selectorgadget.com/↩︎\nhttps://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html↩︎\nhttps://stat4701.github.io/edav/2015/04/02/rvest_tutorial/↩︎\nhttps://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/↩︎",
    "crumbs": [
      "Course Content",
      "Week 13",
      "Text as Data"
    ]
  },
  {
    "objectID": "content/Week_14/14a.html",
    "href": "content/Week_14/14a.html",
    "title": "Geospatial with R",
    "section": "",
    "text": "This page.\n\n\n\n\nWhat are the building blocks of geospatial data?\nHow do we handle uniquely geospatial properties like distance or spatial correlation?",
    "crumbs": [
      "Course Content",
      "Week 14",
      "Geospatial with R"
    ]
  },
  {
    "objectID": "content/Week_14/14a.html#required-reading",
    "href": "content/Week_14/14a.html#required-reading",
    "title": "Geospatial with R",
    "section": "",
    "text": "This page.\n\n\n\n\nWhat are the building blocks of geospatial data?\nHow do we handle uniquely geospatial properties like distance or spatial correlation?",
    "crumbs": [
      "Course Content",
      "Week 14",
      "Geospatial with R"
    ]
  },
  {
    "objectID": "content/Week_14/14a.html#vector-vs.-raster",
    "href": "content/Week_14/14a.html#vector-vs.-raster",
    "title": "Geospatial with R",
    "section": "Vector vs. Raster",
    "text": "Vector vs. Raster\nThere are two ways of storing 2-D mapped spatial data, raster and vector. A vector representation of a 2-D shape is best described as an irregular polygon with points defining vertices. A square plotted in cartesian coordinates is a vector representation. Conversely, a raster image is a grid of cells where each cell is defined as “in” or “out” of the square. Most computer graphics like JPEG and TIFF are raster graphics and each pixel has an assigned color. To make a raster image of a blue square, we’d make a big grid of pixels, and then color some blue based on their location. To make a blue square in vector form, we’d record just the location of the corners and add instructions to color inside the polygon formed by those corners blue.\n\n\n\n\n\n\n\n\n\n\nVectors are scalable. Rasters are not\nRasters are great for detail, like pixels in a picture, but they do not scale up very well. Vectors are great for things that do need to scale up. They are also smaller and easier to work with when they aren’t trying to replicate photo-realistic images. Vectors can handle curves by recording the properties of the curve (e.g. bezier curves), while rasters have to approximate curves along the grid of cells, so if you want a smooth curve, you need lots of cells.\nGeospatial work is almost always done in vectors because (1) it is easier to store data as vectors, and (2) it is easier to manipulate, project, intersect, or connect vector points, lines, and polygons.\nWe are going to work entirely with vectors today.",
    "crumbs": [
      "Course Content",
      "Week 14",
      "Geospatial with R"
    ]
  },
  {
    "objectID": "content/Week_14/14a.html#spatial-merges",
    "href": "content/Week_14/14a.html#spatial-merges",
    "title": "Geospatial with R",
    "section": "Spatial merges",
    "text": "Spatial merges\nCombining spatial data is the strength of geospatial analysis. We have our map of MI, and we have out points. Let’s “merge” the points to the map, meaning let’s connect the elements in our map (the state of MI) to the elements in our points (gas plants). This is a point-to-polygon merge.\n\nPoint-to-polygon merges\nWe’ll use st_join to produce an inner join, so we keep only those points that are “in” (spatially) the state of Michigan. I’m specifying join = st_intersects though this is the default. Note that all the points that remain in the merged MI.gasplants are in Michigan, and note that all the data columns from MI are now in gasplants. We’ll use a county map of MI here so we will have the county data for each county containing a gas plant.\n\nMI.counties = counties(state = 'MI', cb = TRUE, progress_bar = FALSE)\n\nMI.gasplants = gasplants %&gt;%\n  st_transform(st_crs(MI.counties)) %&gt;%\n  st_join(MI.counties, left = FALSE,\n          join = st_intersects)\n\nhead(MI.gasplants)\n\nSimple feature collection with 6 features and 13 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -86.32218 ymin: 43.79656 xmax: -84.01813 ymax: 44.69001\nGeodetic CRS:  NAD83\n                       name STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID\n20           Aztec Manistee      26      101 01622993 0500000US26101 26101\n28             Beaver Creek      26      039 01622962 0500000US26039 26039\n148                Fraser 8      26      017 01622951 0500000US26017 26017\n162    Goose Lake Gas Plant      26      133 01623009 0500000US26133 26133\n212 Kalkaska Gas Processing      26      079 01622982 0500000US26079 26079\n268        Marion Gas Plant      26      133 01623009 0500000US26133 26133\n        NAME        NAMELSAD STUSPS STATE_NAME LSAD      ALAND     AWATER\n20  Manistee Manistee County     MI   Michigan   06 1404616367 1912438858\n28  Crawford Crawford County     MI   Michigan   06 1441108421   17887265\n148      Bay      Bay County     MI   Michigan   06 1145834939  487713370\n162  Osceola  Osceola County     MI   Michigan   06 1466674406   17425113\n212 Kalkaska Kalkaska County     MI   Michigan   06 1449729130   28020186\n268  Osceola  Osceola County     MI   Michigan   06 1466674406   17425113\n                      geometry\n20  POINT (-86.32218 44.26337)\n28      POINT (-84.818 44.559)\n148 POINT (-84.01813 43.79656)\n162   POINT (-85.4091 44.1106)\n212 POINT (-85.19667 44.69001)\n268 POINT (-85.09891 44.08532)\n\n\nNow, we can plot the counties map with the gasplants over it. We can even use aes(...) to fill the counties:\n\nggplot() + \n  geom_sf(data = MI.counties, aes(fill = NAME), show.legend = F) +  \n  geom_sf(data = MI.gasplants) + \n  theme_minimal()  \n\n\n\n\n\n\n\n\nHuh. Most gas plants in Michigan are to the north of here. Interesting.\n\n\nMapview\nSometimes, we want to be able to zoom in and out. ggplot is static, so that won’t work too well. Thanks to the leaflet engine, the mapview packages is stellar for exploration of spatial data. You can specify zcol = Name if you want to color by the Name field. I can’t embed this in the website, but you can run this at home. It will appear in the “Viewer” pane, not in the “Plots” pane. Unlike the static image here, you will be able to zoom and pan.\nmapview(MI.gasplants)\n\n\n\n\n\n\n\n\n\nIn an actual mapview window (not this static image here), clicking on the points or polygons will bring up a pop-up of the data for that row. Mapview is very useful for exploring your spatial data. It is not useful for presenting your data. Please, never use mapview as an output from a markdown code chunk. Use it for exploring, then use geom_sf() to present your analysis.\nWhile Mapview has many features that you will likely find interesting, one of the more useful features is that you can set the display color, which helps for data exploration.\nmapview(MI.counties, zcol = 'NAME')\nThis will set each county in Michigan to a different color (using the viridis colors). Another very useful feature of mapview is that you can layer plots with +. Again, I’m including a static image. Using this command in Rstudio will open an interactive map:\nmapview(MI.gasplants) + mapview(MI.counties, zcol = 'NAME')\n\n\n\n\n\n\n\n\n\n\n\nPolygon-to-polygon merges\nThe gas plants and state merge, above, was very simple because points are always either within or not within a polygon. Worst that can happen is some of your points are not over any polygon at all (resulting in NA values). But what if you’re merging polygons to polygons?\nFirst, let’s load some (overlapping) polygons. We can load up all of our states again (dropping the territories). We’ll also use a map of watersheds (which cross state boundaries). This is the HUC-4 map of the Rockies from the US Geological Survey. The HUC-4 is a definition of a watershed where the area of the HUC-4 is the area drained by a major tributary:\n\nUS = states(cb=TRUE) %&gt;%\n  dplyr::filter(!STUSPS %in% c('PR','GU','VI','MP','AS','AK','HI'))\n\nHUC4 = st_read('https://opendata.arcgis.com/datasets/7f8632f3e3114623b4f5c8f97d935dca_1.kml') %&gt;%\n  st_transform(st_crs(US)) %&gt;%\n  dplyr::mutate(randomData = rpois(n(), 20))\n\nReading layer `USGS_NHD_Hydrologic_Units__HUCs_' from data source \n  `https://opendata.arcgis.com/datasets/7f8632f3e3114623b4f5c8f97d935dca_1.kml' \n  using driver `KML'\nSimple feature collection with 12 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -121.5779 ymin: 31.5082 xmax: -109.7625 ymax: 45.25827\nGeodetic CRS:  WGS 84\n\nggplot() + \n  geom_sf(data = US, col = 'gray50') +\n  geom_sf(data = HUC4, aes(fill = Name), show.legend = FALSE) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThese watersheds clearly overlap state boundaries. So what happens if we merge them? sf will create a new obsveration (row) for every HUC-4 / State combination\n\npoly.merge = HUC4 %&gt;%\n  st_join(US, left = TRUE)\n\nhead(poly.merge)\n\nSimple feature collection with 6 features and 12 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -115.7061 ymin: 31.5082 xmax: -111.5061 ymax: 39.30285\nGeodetic CRS:  NAD83\n                        Name Description randomData STATEFP  STATENS\n1             Lower Colorado                     17      32 01779793\n1.1           Lower Colorado                     17      06 01779778\n1.2           Lower Colorado                     17      04 01779777\n2   Lower Colorado-Lake Mead                     18      49 01455989\n2.1 Lower Colorado-Lake Mead                     18      32 01779793\n2.2 Lower Colorado-Lake Mead                     18      04 01779777\n       AFFGEOID GEOID STUSPS       NAME LSAD        ALAND      AWATER\n1   0400000US32    32     NV     Nevada   00 284537290201  1839636284\n1.1 0400000US06    06     CA California   00 403671756816 20293573058\n1.2 0400000US04    04     AZ    Arizona   00 294363973043   855871553\n2   0400000US49    49     UT       Utah   00 213355072799  6529973239\n2.1 0400000US32    32     NV     Nevada   00 284537290201  1839636284\n2.2 0400000US04    04     AZ    Arizona   00 294363973043   855871553\n                          geometry\n1   POLYGON ((-114.6233 36.0304...\n1.1 POLYGON ((-114.6233 36.0304...\n1.2 POLYGON ((-114.6233 36.0304...\n2   POLYGON ((-115.0786 39.3005...\n2.1 POLYGON ((-115.0786 39.3005...\n2.2 POLYGON ((-115.0786 39.3005...\n\n\nNow, every HUC-4 like “Lower Colorado” has multiple observations, one for each STUSPS that it touches. When we plot it, though, each of those observations are still attached to the same HUC-4 polygon. Technically, R is plotting the same HUC multiple times (once for each state) on top of each other, so we don’t see them. This is the equivalent of merging your data in a way that duplicates observations.\n\nggplot(poly.merge) + geom_sf() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nWe have another option in our join - we can ask st_join to keep just the largest:\n\npoly.merge.largest = HUC4 %&gt;%\n  st_join(US, left = TRUE, largest = TRUE)\n\nhead(poly.merge.largest)\n\nSimple feature collection with 6 features and 12 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -121.255 ymin: 31.5082 xmax: -111.5061 ymax: 42.3448\nGeodetic CRS:  NAD83\n                          Name Description randomData STATEFP  STATENS\n1               Lower Colorado                     17      04 01779777\n2     Lower Colorado-Lake Mead                     18      04 01779777\n3    Northern Mojave-Mono Lake                     21      06 01779778\n4 Central Nevada Desert Basins                     23      32 01779793\n5               North Lahontan                     16      06 01779778\n6   Black Rock Desert-Humboldt                     23      32 01779793\n     AFFGEOID GEOID STUSPS       NAME LSAD        ALAND      AWATER\n1 0400000US04    04     AZ    Arizona   00 294363973043   855871553\n2 0400000US04    04     AZ    Arizona   00 294363973043   855871553\n3 0400000US06    06     CA California   00 403671756816 20293573058\n4 0400000US32    32     NV     Nevada   00 284537290201  1839636284\n5 0400000US06    06     CA California   00 403671756816 20293573058\n6 0400000US32    32     NV     Nevada   00 284537290201  1839636284\n                        geometry\n1 POLYGON ((-114.6233 36.0304...\n2 POLYGON ((-115.0786 39.3005...\n3 POLYGON ((-118.7594 38.3208...\n4 POLYGON ((-114.7211 41.2410...\n5 POLYGON ((-120.1835 41.9743...\n6 POLYGON ((-117.9693 42.3430...\n\n\nNow, there is only one observation per HUC-4, and it corresponds to the state that has the most overlap area-wise. For Lower Colorado, Arizona has the most overlap. There are lots of things besides st_intersect we can use to call two things “joined”. ?st_join tells you about them. For instance, we can use join = st_covers and we will only get a merge when HUC-4 completely covers the state.\n\nHUC4 %&gt;%\n  st_join(US, left = TRUE, join = st_covers) %&gt;%\n  head()\n\nSimple feature collection with 6 features and 12 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -121.255 ymin: 31.5082 xmax: -111.5061 ymax: 42.3448\nGeodetic CRS:  NAD83\n                          Name Description randomData STATEFP STATENS AFFGEOID\n1               Lower Colorado                     17    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;\n2     Lower Colorado-Lake Mead                     18    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;\n3    Northern Mojave-Mono Lake                     21    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;\n4 Central Nevada Desert Basins                     23    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;\n5               North Lahontan                     16    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;\n6   Black Rock Desert-Humboldt                     23    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;\n  GEOID STUSPS NAME LSAD ALAND AWATER                       geometry\n1  &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    NA     NA POLYGON ((-114.6233 36.0304...\n2  &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    NA     NA POLYGON ((-115.0786 39.3005...\n3  &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    NA     NA POLYGON ((-118.7594 38.3208...\n4  &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    NA     NA POLYGON ((-114.7211 41.2410...\n5  &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    NA     NA POLYGON ((-120.1835 41.9743...\n6  &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    NA     NA POLYGON ((-117.9693 42.3430...\n\n\nNone of the HUC-4’s completely cover a state, so we get NA for all the state data.\nThe other thing we can do is ask R to create separate polygons - one for every HUC-4 / state combination. That isn’t a merge, but it plays a similar role. Note this uses st_intersection:\n\npoly.int = HUC4 %&gt;%\n  st_intersection(US) %&gt;%\n  arrange(Name)\n\nhead(poly.int)\n\nSimple feature collection with 6 features and 12 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -120.4649 ymin: 35.40541 xmax: -114.1471 ymax: 42.3448\nGeodetic CRS:  NAD83\n                            Name Description randomData STATEFP  STATENS\n6     Black Rock Desert-Humboldt                     23      32 01779793\n6.1   Black Rock Desert-Humboldt                     23      06 01779778\n6.2   Black Rock Desert-Humboldt                     23      41 01155107\n7               Central Lahontan                     20      32 01779793\n7.1             Central Lahontan                     20      06 01779778\n4   Central Nevada Desert Basins                     23      32 01779793\n       AFFGEOID GEOID STUSPS       NAME LSAD        ALAND      AWATER\n6   0400000US32    32     NV     Nevada   00 284537290201  1839636284\n6.1 0400000US06    06     CA California   00 403671756816 20293573058\n6.2 0400000US41    41     OR     Oregon   00 248628414476  6170965739\n7   0400000US32    32     NV     Nevada   00 284537290201  1839636284\n7.1 0400000US06    06     CA California   00 403671756816 20293573058\n4   0400000US32    32     NV     Nevada   00 284537290201  1839636284\n                          geometry\n6   MULTIPOLYGON (((-119.9997 4...\n6.1 MULTIPOLYGON (((-119.9997 4...\n6.2 MULTIPOLYGON (((-118.7203 4...\n7   POLYGON ((-120.0065 39.2721...\n7.1 POLYGON ((-120.0016 39.5794...\n4   POLYGON ((-115.1379 35.4054...\n\n\nNow, we have a unique polygon for every combination of HUC-4 and State (with the US):\n\nggplot() + \n  geom_sf(data = US, fill = 'gray50') +\n  geom_sf(data = poly.int, aes(fill = STUSPS), show.legend = FALSE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHere, I’ve set the fill to the state, but you can see that the HUC-4’s have boundaries at the state line.\n\n\nSummarizing\nOur summarize function let us collapse by groups and calculate interseting things like average (over a group or region). The neat part is that it works on spatial data as well. Let’s look at the data again:\n\nhead(poly.int %&gt;% \n       dplyr::select(Name, randomData, STUSPS) %&gt;%\n       arrange(STUSPS))\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -120.4649 ymin: 32.2603 xmax: -111.5061 ymax: 41.10578\nGeodetic CRS:  NAD83\n                            Name randomData STUSPS\n1.2               Lower Colorado         17     AZ\n2.2     Lower Colorado-Lake Mead         18     AZ\n6.1   Black Rock Desert-Humboldt         23     CA\n7.1             Central Lahontan         20     CA\n4.1 Central Nevada Desert Basins         23     CA\n1.1               Lower Colorado         17     CA\n                          geometry\n1.2 POLYGON ((-114.8165 32.5069...\n2.2 POLYGON ((-114.7368 36.0159...\n6.1 MULTIPOLYGON (((-119.9997 4...\n7.1 POLYGON ((-120.0016 39.5794...\n4.1 MULTIPOLYGON (((-118.7021 3...\n1.1 POLYGON ((-115.1379 35.4054...\n\n\nSo AZ has two HUC-4’s in it - Lower Colorado and Lower Coloardo - Lake Mead (you can see them above). Summarize on geospatial data works just like regular data - we can group_by(STUSPS), and we can summarize() any of the data. I threw some random data into HUC-4 so we can summarize that.\nBut how do we combine data specific to each HUC-4 in AZ? We could:\n\nJust take the average of all of the randomData values within the state.\nTake a weighted average of randomData where the area is the weight\nTake some other function (min, max, etc.) of randomData.\n\nWe can implement any of these using sf. Let’s do the second since it nests the first. First, we’ll add the area of the State x HUC-4 using st_area, which gives a units object. We can turn that into a numeric:\n\npoly.int.summary = poly.int %&gt;%\n  dplyr::mutate(State.HUC.area = as.numeric(st_area(.))) %&gt;%\n  group_by(STUSPS) %&gt;%\n  dplyr::summarize(mean.randomData = weighted.mean(randomData, w = State.HUC.area))\n\nhead(poly.int.summary)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -121.5779 ymin: 32.2603 xmax: -111.0436 ymax: 45.25827\nGeodetic CRS:  NAD83\n# A tibble: 6 × 3\n  STUSPS mean.randomData                                                geometry\n  &lt;chr&gt;            &lt;dbl&gt;                                          &lt;GEOMETRY [°]&gt;\n1 AZ                17.6 POLYGON ((-114.7368 36.01591, -114.737 36.01577, -114.…\n2 CA                20.1 POLYGON ((-118.9235 38.25064, -118.9497 38.26894, -118…\n3 ID                23.2 POLYGON ((-117.243 44.39097, -117.2351 44.37385, -117.…\n4 MT                24   MULTIPOLYGON (((-111.3842 44.75446, -111.3846 44.75484…\n5 NV                22.1 POLYGON ((-118.7021 38.09324, -118.6216 38.03439, -118…\n6 OR                21.5 POLYGON ((-116.7012 45.24284, -116.7016 45.24301, -116…\n\n\nsf with the tidyverse makes it really easy to apply spatial versions of summarize and mutate. Very useful.\nIf we had wanted to just take the average (ignoring area), we’d just leave out the w = State.HUC.area or just used mean. If we had wanted to take, say, the minimum, we would use min(randomData) instead of weighted.mean. We can use whatever function we want in summarize, just as we did with non-spatial data.",
    "crumbs": [
      "Course Content",
      "Week 14",
      "Geospatial with R"
    ]
  },
  {
    "objectID": "content/Week_14/14a.html#cropping-vs.-merging",
    "href": "content/Week_14/14a.html#cropping-vs.-merging",
    "title": "Geospatial with R",
    "section": "Cropping vs. merging",
    "text": "Cropping vs. merging\nSometimes, we wish to only crop to a region rather than merging. sf has the st_crop function to do this. Let’s crop our HUC-4 data to just the bounding box of the state of Nevada\n\nHUC4.nv = HUC4 %&gt;%\n  st_crop(US %&gt;% dplyr::filter(STUSPS=='NV'))\n\nggplot(HUC4.nv) + geom_sf(aes(fill = Name), show.legend = F) +\n  geom_sf(data = US %&gt;% dplyr::filter(STUSPS=='NV'),  fill = NA, col = 'gray20', lwd = 3 ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIf we wanted to actually crop to the state of Nevada (st_crop only uses the bounding box of Nevada), we’d use st_intersection but do the intersection with just the state of Nevada:\n\nHUC4.nv = HUC4 %&gt;%\n  st_intersection(US %&gt;% dplyr::filter(STUSPS=='NV'))\n\nggplot(HUC4.nv) + geom_sf(aes(fill = Name), show.legend = F) +\n  geom_sf(data = US %&gt;% dplyr::filter(STUSPS=='NV'),  fill = NA, col = 'gray20', lwd = 3 ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote that the data for each HUC-4 remains unchanged. If there were something in that data (like “population” or “area”) that is specific to the entire polygon representing the HUC, then cropping the HUC outside of Nevada may lead to misleading data. The moral of the story is: be careful and thoughtful!\n\nBounding boxes\nThis introduces a useful concept: the bounding box. The bounding box is defined by the closest 4 points that form a box that perfectly encloses the object (even when the object is not a rectangle). The extent of the above plot is the bounding box for Nevada.\n\nNevada.bbox = st_bbox(US %&gt;% dplyr::filter(STUSPS=='NV'))\nNevada.bbox\n\n      xmin       ymin       xmax       ymax \n-120.00646   35.00186 -114.03965   42.00221 \n\n\nThe bounding box can be used to frame a “window” in a ggplot using geom_sf(). That is, sometimes, we want to plot just a subsection of a map, but we still want the data to be the whole map. Here’s an example using the HOLC Redlining Maps, which were created in the 1930’s and were used to segregate US housing up until the 1970’s. They are available at the University of Richmond’s Mapping Inequality site. We can load Lansing and Detroit using the code below:\n\nlansing = st_read('https://dsl.richmond.edu/panorama/redlining/static/citiesData/MILansing19XX/geojson.json')\n\nReading layer `geojson' from data source \n  `https://dsl.richmond.edu/panorama/redlining/static/citiesData/MILansing19XX/geojson.json' \n  using driver `GeoJSON'\nSimple feature collection with 5 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.60737 ymin: 42.67877 xmax: -84.45292 ymax: 42.77022\nGeodetic CRS:  WGS 84\n\ndetroit = st_read('https://dsl.richmond.edu/panorama/redlining/static/citiesData/MIDetroit1939/geojson.json')\n\nReading layer `geojson' from data source \n  `https://dsl.richmond.edu/panorama/redlining/static/citiesData/MIDetroit1939/geojson.json' \n  using driver `GeoJSON'\nSimple feature collection with 239 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -83.40036 ymin: 42.13495 xmax: -82.87287 ymax: 42.56012\nGeodetic CRS:  WGS 84\n\nmi.redlining = bind_rows(lansing, detroit) %&gt;%\n  st_transform(st_crs(MI))\n\nggplot(mi.redlining) +\n  geom_sf(aes(fill = grade, col = grade)) + \n  geom_sf(data = MI, fill = NA, col = 'gray50') +\n  scale_fill_manual(values = c('A' = 'darkgreen', 'B' = 'blue', 'C' = 'yellow', 'D' = 'red'),\n                    aesthetics = c('color','fill')) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can tell that our polygons have plotted, but since we have the whole state of Michigan, they’re almost unreadable. We need to set our window over the lower part of the lower peninsula. We’ll use coord_sf to do this, but first we need to define a window. Since windows are almost always rectangular, we can use the st_bbox(mi.redlining), but we have to pull out the xlim (xmin, xmax) and ylim (ymin, ymax):\n\nggplot(mi.redlining) +\n  geom_sf(aes(fill = grade, col = grade)) + \n  geom_sf(data = MI, fill = NA, col = 'gray50') +\n  scale_fill_manual(values = c('A' = 'green', 'B' = 'blue', 'C' = 'yellow', 'D' = 'red'),\n                    aesthetics = c('color','fill')) +\n  theme_minimal() +\n  coord_sf(xlim = st_bbox(mi.redlining)[c(1, 3)],\n           ylim = st_bbox(mi.redlining)[c(2, 4)])",
    "crumbs": [
      "Course Content",
      "Week 14",
      "Geospatial with R"
    ]
  },
  {
    "objectID": "content/Week_14/14a.html#distance-matrices",
    "href": "content/Week_14/14a.html#distance-matrices",
    "title": "Geospatial with R",
    "section": "Distance matrices",
    "text": "Distance matrices\nOne of the most common spatial statistics we’d use in data analytics is the distance matrix. If we have a set of points and we think that we can explain some data about those points (unemployment, ag production, murders per capita) based on the distance to some explanatory source (gas plants, superfund site, etc.), then we might want to include distance to gas plants in our model as a predictor. Frequently, we’ll use inverse distance, \\(\\frac{1}{d}\\), so that closer things can have more of an impact. To do this, we need a distance matrix.\nLet’s combine our gasplants and our ourCities to get the distance from each of our cities to the nearest gas plant. Maybe we have city-level data on student achievements and we want to see if gas plants lower student achievement. While we would need a lot more information to make this model, we can look at what we have for now.\nWe will use st_distance, which will generate a special type of object that contains the distance information. MI.gasplants has 9 observations, and ourCities has 6, so for each row in ourCities we will get 9 distances, one to each gasplant. This forms a distance matrix where each row is an object in ourCities and each column is an object in MI.gasplants. We are going to take only a few MI.gasplants so we can easily view the results:\n\nourCities.spatial = ourCities.spatial %&gt;% \n  st_transform(st_crs(MI.gasplants))\n\nMI.gasplants.small = MI.gasplants[1:4,]\n\nourDistance = st_distance(x = ourCities.spatial, y = MI.gasplants.small)\nourDistance\n\nUnits: [m]\n          [,1]      [,2]     [,3]      [,4]\n[1,] 341143.11 285982.39 181009.4 275255.21\n[2,] 221918.02 204053.49 125901.8 167922.72\n[3,] 153864.33 190110.91 162360.3 129274.21\n[4,] 227166.32 259648.90 210370.0 202766.55\n[5,]  78661.48  67024.32 166385.3  74135.81\n[6,] 263562.31 303561.81 408893.5 312655.34\n\n\nWe get a units matrix, which has extra properties that allow us to convert the units. The units will be in whatever the CRS of the objects is in - st_crs(ourCities.spatial) tells us the units are meters.\nWhat if we wanted to find the closest gas plant to each city? That is akin to looking at each row, and finding the column that is the smallest, right? We will use apply, and we will note that the order of the columns is the same as the order in MI.gasplants.small, so we can use MI.gasplants.small$name to tell us the name of the closest gas plant. We will apply over each row (MAR=1) and use the which.min function, which returns the index number of the maximum column.\n\nmax.index = apply(ourDistance, MAR = 1, which.min)\n\nWe can combine this index with the MI.gasplants.small object to get the names of the closest gas plant for each of the cities. We’ll make a nice, neat tibble with the city name (in the order from ourCities.spatial), the closest gas plant name, and the distance to that plant:\n\ntibble(City = ourCities.spatial$City,\n       Closest.gasplant = MI.gasplants.small$name[max.index],\n       Distance.to.closest = ourDistance[cbind(1:length(max.index), max.index)])\n\n# A tibble: 6 × 3\n  City          Closest.gasplant     Distance.to.closest\n  &lt;chr&gt;         &lt;chr&gt;                                [m]\n1 Detroit       Fraser 8                         181009.\n2 Lansing       Fraser 8                         125902.\n3 Grand Rapids  Goose Lake Gas Plant             129274.\n4 Kalamazoo     Goose Lake Gas Plant             202767.\n5 Traverse City Beaver Creek                      67024.\n6 Marquette     Aztec Manistee                   263562.\n\n\nBut wait, what is going on in the last line of code there? Well, recall our distance matrix and max.index:\n\nourDistance\n\nUnits: [m]\n          [,1]      [,2]     [,3]      [,4]\n[1,] 341143.11 285982.39 181009.4 275255.21\n[2,] 221918.02 204053.49 125901.8 167922.72\n[3,] 153864.33 190110.91 162360.3 129274.21\n[4,] 227166.32 259648.90 210370.0 202766.55\n[5,]  78661.48  67024.32 166385.3  74135.81\n[6,] 263562.31 303561.81 408893.5 312655.34\n\n#\nmax.index\n\n[1] 3 3 4 4 2 1\n\n\nwe want to select from our distance matrix the 1st row, 3rd column; the 2nd row, 3rd column; 3rd row, 1st column; 4th row, 3rd column; 5th row, 4th column; and 6th row, 4th column. This means the row index and column index are not ranges, but are paired. Using cbind(1:6, max.index) makes them paired entries, and we can select specific row x column combinations that way.\n\nst_nearest_feature\nAs is common in R, there is a function that will get the closest points between to spatial objects. st_nearest_points takes two geometries, and returns the neaarest point in y for every point in x, which is what we did with the MI gas plants.\n\nst_nearest_feature(x = ourCities.spatial, y = MI.gasplants.small)\n\n[1] 3 3 4 4 2 1\n\n\nThis is exactly our max.index and can be used on the y object, MI.gasplants.small, to pull the names, subset, get distances etc.\nst_nearest_feature also works for points and polygons, or polygons and polygons, where it returns the index of the polygon that contains the nearest point to the features in x. Let’s find the nearest Great Lake for each of our cities using a KML shapefile of the Great Lakes from WI DNR Note that the GeoJSON link is under “API” on this site. We run into a complex geometry problem, and add st_make_valid() to fix it:\n\nGL = st_read('https://opendata.arcgis.com/datasets/a8bb79fc10e64eee8c3a9db97cc5dc80_4.geojson') %&gt;%\n  st_transform(st_crs(ourCities.spatial)) %&gt;% st_make_valid()\n\nReading layer `Great_Lakes' from data source \n  `https://opendata.arcgis.com/datasets/a8bb79fc10e64eee8c3a9db97cc5dc80_4.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 15 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -92.10222 ymin: 41.38576 xmax: -75.95819 ymax: 49.00535\nGeodetic CRS:  WGS 84\n\nclosest.GL.index = st_nearest_feature(x = ourCities.spatial, y = GL)\nourCities %&gt;% dplyr::mutate(Closest.GreatLake = GL$FEAT_NAME[closest.GL.index])\n\n           City      lat       lon Closest.GreatLake\n1       Detroit 42.33155 -83.04664         Lake Erie\n2       Lansing 42.73383 -84.55463        Lake Huron\n3  Grand Rapids 42.96324 -85.66786     Lake Michigan\n4     Kalamazoo 42.29171 -85.58723     Lake Michigan\n5 Traverse City 44.76065 -85.61660     Lake Michigan\n6     Marquette 46.44815 -87.63059     Lake Superior",
    "crumbs": [
      "Course Content",
      "Week 14",
      "Geospatial with R"
    ]
  },
  {
    "objectID": "content/Week_14/14a.html#other-resources",
    "href": "content/Week_14/14a.html#other-resources",
    "title": "Geospatial with R",
    "section": "Other resources",
    "text": "Other resources\n\nClaudia Engel’s “Using Spatial Data with R” is a very useful resource. It covers sf and an older geospatial library called sp that has similar functionality but was not tidyverse-friendly.\nThe Rstudio Spatial Cheat Sheet.\n\nThere are lots of useful RStudio cheat sheets, actually.\n\nIf you want to remove the Great Lakes from your map, here’s how you do it\n\nFirst, retrieve the geojson file of the great lakes. Only WI DNR seems to have a public shapefile of the Great Lakes (an informal complaint has been registered with MI-EGLE, grrrr) which you can download as a geojson from here – see the little cloud download button, and select geojson. This one doesn’t have a link to take it directly, so download the file to your local drive, keep it with your .RMD file, and load it direction from your local path.\nSecond, use the code below as an example. You may have a different shapefile you want to clip to the Great Lakes – put it in place of MI below. Note that we use a spatial version of the difference function, which gives us “everything in the first that isn’t in the second”.\n\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tigris)\n\n\nMI = counties(state = 'MI', year = 2019) # your map here\nGreatLakes = st_read('/PUT YOUR LOCAL FILEPATH TO THE GEOJSON HERE.geojson') %&gt;%  # put your local filepath in here\n  st_transform(st_crs(MI)) %&gt;%  # need them to be in the same projection\n  st_make_valid() %&gt;%  # this fixes an issue with the shapefile after projection\n  st_union()  # we will put all the lakes together as we don't care which lake is being used to erase county/district boundaries\n\n\nMIx = st_difference(MI, GreatLakes)  # spatial version of the difference function\nor, try the erase_water() function from tigris. This erases any body of water, no matter how small, so on a whole state, it can take a long time. The area_threshold argument (0 to 1) tells R how detailed you want the erasure to be. A threshold of .999 will remove Great Lakes and big lakes, but leave most inland lakes and won’t take as long.\n\nMI = counties(state = 'MI', progress_bar = FALSE) %&gt;%\nerase_water(area_threshold = .999)\n\nggplot(MI) + geom_sf()",
    "crumbs": [
      "Course Content",
      "Week 14",
      "Geospatial with R"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Course Content",
    "section": "",
    "text": "Each week has two sets of required readings (pages in the sidebar) that you should complete before coming to lecture. Read the first before our first meeting of the week, and read the second before the second meetings. That is, you should complete the first reading, attend Tuesday class, then do the associated exercises contained within the reading, and read the second reading before Thursday. You will be working each week’s lab between Thursday afternoon and Monday at 11:59 PM (when the labs are due). Don’t forget your weekly writing in between, due Saturday at 11:59pm.\nThe course content is structured as follows. For each topic, we begin with a set of questions that might guide your reading and help frame your thoughts. These questions can serve as helpful starting places for your thinking; they are not representative of the totality of the content and are not intended to be limiting. You should not try to respond to all of these (or any of them if you don’t want to)—they’ll just help you know what to look for and think about as you read. The first reading is generally a “principles” reading, discussing the concepts for the week. The second is generally an “applications” reading, meant to give concrete examples and code. This isn’t always the case, but holds in general.",
    "crumbs": [
      "Syllabus",
      "Course Content"
    ]
  },
  {
    "objectID": "content/index.html#content-navigation",
    "href": "content/index.html#content-navigation",
    "title": "Course Content",
    "section": "Content navigation",
    "text": "Content navigation\nUse the links on the sidebar to navigate between required reading. Readings are ordered by week, and within each week, by order of the class meetings.",
    "crumbs": [
      "Syllabus",
      "Course Content"
    ]
  },
  {
    "objectID": "groupproject/project1.html",
    "href": "groupproject/project1.html",
    "title": "Project 1",
    "section": "",
    "text": "Only one member of your group should turn in your project on D2L. The copy turned in must have each member’s name at the top.\nOne group member should turn in your project on D2L under “Project 1” no later than Saturday, March 1st, 11:59PM",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "groupproject/project1.html#part-1-rats-rats-rats.",
    "href": "groupproject/project1.html#part-1-rats-rats-rats.",
    "title": "Project 1",
    "section": "Part 1: Rats, rats, rats.",
    "text": "Part 1: Rats, rats, rats.\nNew York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots. Rats in NYC are plentiful, but they also deliver food, so they’re useful too.\n\nNYC keeps incredibly detailed data regarding animal sightings, including rats, and it makes this data publicly available.\nFor this first project, pretend that you are an analyst with the NYC Mayor’s office, and you have been tasked with getting a better understanding of the rat problem. Your job is to use R and ggplot2 to tell an interesting story hidden in the data. You must create a story by looking carefully at the data, finding some insight into where or when rat sightings occur, and describing to the Mayor how this insight may inform a strategy to address the rats. Your assignment will take the form of a professional memo that presents the data and insights along with appropriate visualizations that communicate the story in-line. Your memo should be approximately 2-4 pages (including plots) and, of course, will use Rmarkdown rendered to PDF using LaTeX.\n\nInstructions\nHere’s what you need to do:\n\nDownload New York City’s database of rat sightings since 2010:\n\n Rat_sightings.csv\n\nSummarize the data somehow. The raw data has more than 150,000 rows, which means you’ll need to aggregate the data (filter(), group_by(), and summarize() will be your friends). Consider looking at the number of sightings per borough, per year, per dwelling type, etc., or a combination of these, like the change in the number sightings across the 5 boroughs between 2010 and 2024. Consider the meaning of variables you create – remember how the total numbers of murders in a state was mostly just a function of size, but that murder rate was a more useful variable. (Of course, I just gave you these ideas, so you’ll need to be a bit more creative than just doing exactly what I just said.)\nExplore the data further. Once you have summarized the data in one way, use what you find to further explore other patterns you see in the data. You may bring in other data (it is not hard to find and add to your data the population of each borough or the average daily temperature by month, etc.). You can use case_when() to add data that’s at a coarse level (year, borough, etc.), or merge() if you’re comfortable with it. Be create, but do so to further explore and analyze. Drill down on something you see in the data to tell a story. Remember, as analysts, we are looking for “patterns in the data” that are not easily written in closed form.\nCreate appropriate visualizations based on the data you summarized and explored. Pay attention to every detail - color choice, labels, axis text, etc. Use proper capitalization. Make sure everything lays out in your final product. This is not trivial - it will take you some time to get everything just right.\nWrite a polished, professional memo presenting your analysis to the Mayor. The memo should tell the story you found in the data. It should not narrarate your discovery process (e.g. it should not say “we tried looking at…and then ran ggplot(…))” but rather should weave your exploration into a narrarative driven by your graphics, with the text filling in the context. We are specifically looking for a discussion of the following:\n\nWhat story are you telling with your graphics?\nWhat new insight have you found?\nHow have you applied reasonable standards in visual storytelling?\nWhat policy implication is there (if any)?\n\nUpload the following outputs to D2L:\n\nA PDF file of your memo with your graphics embedded in it.1 This means you’ll need to do all your coding in an R Markdown file and embed your code in chunks.\nNote that Part 2 of this project should be included in this PDF in it’s own section (see below).\nNothing else. No .Rmd, no code, nothing but your clean, polished memo with Part 1 and Part 2.\n\n\nSome important notes on your assignment\n\nYour assignment should be clean and polished as if you were a city employee and you were turning in a work product. It should should flow nicely and use subsections (using ### at the start of the line) as appropriate.\nAgain, do not “annotate” your thought process (e.g. do not write “we tried X but there were too many NA’s, so we did Y instead”). This should be a polished memo suitable for turning in as a work product. Each graphic should guide the analysis towards the next.\nYour code should not appear in your output - it should be only your plots and memo writing.\n\nTo turn off code echoing, add echo = FALSE in each of your code chunk options (e.g. {r setup, echo = FALSE}), or set it globally in the first code chunk inside the knitr::opts_chunk$set function.\n\nMake sure you take a look at the Evaluation critera, below.\n\n\n\nStarter code\nI’ve provided some starter code below. A couple comments about it:\n\nBy default, read_csv() treats cells that are empty or “NA” as missing values. This rat dataset uses “N/A” to mark missing values, so we need to add that as a possible marker of missingness (hence na = c(\"\", \"NA\", \"N/A\"))\nTo make life easier, I’ve renamed some of the key variables you might work with. You can rename others if you want.\nI’ve also created a few date-related variables (sighting_year, sighting_month, sighting_day, and sighting_weekday). You don’t have to use them, but they’re there if you need them. The functions that create these, like year() and wday() are part of the lubridate library.\nThe date/time variables are formatted like 04/03/2017 12:00:00 AM, which R is not able to automatically parse as a date when reading the CSV file. You can use the mdy_hms() function in the lubridate library to parse dates that are structured as “month-day-year-hour-minute”. There are also a bunch of other iterations of this function, like ymd(), dmy(), etc., for other date formats.\nThere’s one row with an unspecified borough, so I filter that out.\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nrats_raw &lt;- read_csv(\"data/Rat_Sightings.csv\", na = c(\"\", \"NA\", \"N/A\"))\n# If you get an error that says \"All formats failed to parse. No formats\n# found\", it's because the mdy_hms function couldn't parse the date. The date\n# variable *should* be in this format: \"04/03/2017 12:00:00 AM\", but in some\n# rare instances, it might load without the seconds as \"04/03/2017 12:00 AM\".\n# If there are no seconds, use mdy_hm() instead of mdy_hms().\nrats_clean &lt;- rats_raw %&gt;%\n  rename(created_date = `Created Date`,\n         location_type = `Location Type`,\n         borough = Borough) %&gt;%\n  mutate(created_date = mdy_hms(created_date)) %&gt;%\n  mutate(sighting_year = year(created_date),\n         sighting_month = month(created_date),\n         sighting_day = day(created_date),\n         sighting_weekday = wday(created_date, label = TRUE, abbr = FALSE)) %&gt;%\n  filter(borough != \"Unspecified\")\n\nYou’ll summarize the data with functions from dplyr in the tidyverse, including stuff like count(), arrange(), filter(), group_by(), summarize(), and mutate(). As mentioned before, if you’re bringing in outside data (e.g. population of a borough), you can use case_when() to add it, or try using merge() and seek help at office hours if it’s not clear to you.",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "groupproject/project1.html#part-2-data-hunting",
    "href": "groupproject/project1.html#part-2-data-hunting",
    "title": "Project 1",
    "section": "Part 2: Data Hunting",
    "text": "Part 2: Data Hunting\nFor the second part of the project, your task is simple. Your group must identify three different data sources2 for potential use in your final project. You are not bound to this decision.\nDo not use Kaggle data. While Kaggle is useful for learning data science, part of this assignment is learning how to find actual data in the wild. I give zero credit for data links to Kaggle data. You must find your data from the source.\nFor each dataset, you must write a single short paragraph about what about this data interests the group and what general questions one could answer with it. Add this to the memo from Part 1, using ## to make a new header in Rmarkdown for this section.",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "groupproject/project1.html#evaluations",
    "href": "groupproject/project1.html#evaluations",
    "title": "Project 1",
    "section": "Evaluations",
    "text": "Evaluations\nI will only give top marks to those groups showing initiative and cleverness. I will use the following weights for final scores:\nPart 1\n\nTechnical difficulty: Does the final project show mastery of the R and ggplot skills we’ve discussed thus far? Did the underlying data analysis use and expand on the tools we learned in class? Full credit requires going beyond the basic tidyverse filtering / grouping / summarizing. (10 points)\nClarity / use of ggplot: Are all of the design elements property executed in the plots and memo. Are axes and legends appropriately labeled and are colorblind-friendly colors used? Does the plot display properly in the memo without any cut-off axes? Is everything properly spelled? (5 points)\nAppropriateness of visuals: Do the visualizations communicate the analysis? Does it use visual cues to show the analysis? (10 points)\nStorytelling: Does your memo clearly convey the analysis? Does it flow appropriately with the visuals? Does it include a policy recommendation? (10 points)\n\nPart 2\nEach piece of data and description is worth 5 points. (15 points total)",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "groupproject/project1.html#footnotes",
    "href": "groupproject/project1.html#footnotes",
    "title": "Project 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can approach this in a couple different ways—you can write the memo and then include the full figure and code at the end, similar to this blog post, or you can write the memo in an incremental way, describing the analytical steps that each figure illuminates, ultimately arriving at a final figure, like this blog post.↩︎\nThe three different sources need not be different websites or from different organizations. For example, three different tables from the US Census would be sufficient↩︎",
    "crumbs": [
      "Assignments",
      "Group Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Social Science Data Analytics\n        ",
    "section": "",
    "text": "EC242 - Spring 2025\n        \n        \n            Tu/Th 1:00 - 2:20pmNatSci 204Michigan State UniversityDepartment of Economics\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nInstructor\n\n Ben Bushong\n Marshall-Adams Hall #25E\n bbushong@msu.edu\n Slack\n\n\n\nCourse details\n\n Tu/Th 1:00 - 2:20pm\n NatSci 204\n\n\n\nContacting me\nFor individual questions, email is the best way to get in contact. So that I know the context, please include [EC242] at the start of your message. Questions regarding material, problem sets, or R coding are best directed to a course channel on Slack so that our TA and others may address your question quickly. I will reply to all queries usually within 24 hours."
  },
  {
    "objectID": "resource/data.html",
    "href": "resource/data.html",
    "title": "Data",
    "section": "",
    "text": "There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n\nData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\nGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.\nKaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test. Note: when you are asked in an assignment or project to find data, Kaggle is not a valid source. You learn nothing about farming by getting fruit from the market. Similarly, you learn nothing about data analysis by getting pre-packaged data from Kaggle.\n360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\nUS City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\nPolitical science and economics datasets: There’s a wealth of data available for political science- and economics-related topics:\n\nFrançois Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities.\nThomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.).\nErik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)\n\nGeospatial datasets: For our geospatial unit, many entities (government agencies, etc.) post repositories of spatial data via arcgis.com, which is owned by ESRI, the company that makes the predominant commercial GIS software. The data owner decides what data to post and how to present it, but arcgis.com provides the hosting. R has excellent geospatial functionalities, so we cover (when time permits) geospatial data. It is easy to find geospatial data to import into R. Here are a few:\n\nState of Michigan Open Data Portal\nHomeland Infrastructure Foundation-Level Data (HIFLD)\nCity of East Lansing Data Viewer",
    "crumbs": [
      "Other Useful Stuff",
      "Data"
    ]
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Helpful resources",
    "section": "",
    "text": "In these reference pages, I’ve included some useful resources to help you in the course."
  },
  {
    "objectID": "resource/markdown.html",
    "href": "resource/markdown.html",
    "title": "Using Markdown",
    "section": "",
    "text": "Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#basic-markdown-formatting",
    "href": "resource/markdown.html#basic-markdown-formatting",
    "title": "Using Markdown",
    "section": "Basic Markdown formatting",
    "text": "Basic Markdown formatting\n\n\n\n              Type…\n    …or…\n            …to get\n\n\n\n\nSome text in a paragraph.\n\nMore text in the next paragraph. Always\nuse empty lines between paragraphs.\n\nSome text in a paragraph.\nMore text in the next paragraph. Always use empty lines between paragraphs.\n\n\n*Italic*\n_Italic_\nItalic\n\n\n**Bold**\n__Bold__\nBold\n\n\n# Heading 1\n\n\nHeading 1\n\n\n\n## Heading 2\n\n\nHeading 2\n\n\n\n### Heading 3\n\n\nHeading 3\n\n\n\n(Go up to heading level 6 with ######)\n\n\n\n\n[Link text](http://www.example.com)\n\nLink text\n\n\n![Image caption](/path/to/image.png)\n\n |\n\n\n`Inline code` with backticks\n\nInline code with backticks\n\n\n&gt; Blockquote\n\n\nBlockquote\n\n\n\n- Things in\n- an unordered\n- list\n* Things in\n* an unordered\n* list\n\nThings in\nan unordered\nlist\n\n\n\n1. Things in\n2. an ordered\n3. list\n1) Things in\n2) an ordered\n3) list\n\nThings in\nan ordered\nlist\n\n\n\nHorizontal line\n\n---\nHorizontal line\n\n***\nHorizontal line",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#math",
    "href": "resource/markdown.html#math",
    "title": "Using Markdown",
    "section": "Math",
    "text": "Math\nMarkdown uses LaTeX to create fancy mathematical equations. This is useful for things as simple as making a \\(\\beta\\), or for writing out long, complex equations. In fact, Markdown does not like, and will error out on, “unicode” characters like the version of \\(\\beta\\) you get by using the text symbols. Needless to say, don’t do that – use LaTeX.\nIn LaTeX, there are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here, but the basic usage is easy.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n\n\n\n\n\n\n\n                            Type…\n                        …to get\n\n\n\n\nBased on the DAG, the regression model for\nestimating the effect of education on wages\nis $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, or\n$\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon$.\nBased on the DAG, the regression model for estimating the effect of education on wages is \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or \\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).\n\n\n\nTo put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math:\n\n$$\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n$$\n\nBut now we just use computers to solve for $x$.\n…to get…\n\nThe quadratic equation was an important part of high school math:\n\\[\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n\\]\nBut now we just use computers to solve for \\(x\\).\n\n\nBecause dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”.",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#superscripts-and-subscripts",
    "href": "resource/markdown.html#superscripts-and-subscripts",
    "title": "Using Markdown",
    "section": "Superscripts and Subscripts",
    "text": "Superscripts and Subscripts\nSubscripts and superscripts are used often in this class. They are made by adding _{sub} or ^{super}. See the \\beta_0 in the table above, which made a nice \\(\\beta_{0}\\). If you have only one character or number, you don’t need to use the {}. That is beta_i,j will give \\(\\beta_i,j\\), and beta_{i,j} will give \\(\\beta_{i,j}\\).",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#tables-1",
    "href": "resource/markdown.html#tables-1",
    "title": "Using Markdown",
    "section": "Tables",
    "text": "Tables\nThere are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like pander (use pandoc.table()) or knitr (use kable()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n  Right     Left     Center     Default\n-------     ------ ----------   -------\n     12     12        12            12\n    123     123       123          123\n      1     1          1             1\n\nTable: Caption goes here\n…to get…\n\nCaption goes here\n\n\nRight\nLeft\nCenter\nDefault\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\nFor pipe tables, type…\n| Right | Left | Default | Center |\n|------:|:-----|---------|:------:|\n|   12  |  12  |    12   |    12  |\n|  123  |  123 |   123   |   123  |\n|    1  |    1 |     1   |     1  |\n\nTable: Caption goes here\n…to get…\n\nCaption goes here\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#footnotes",
    "href": "resource/markdown.html#footnotes",
    "title": "Using Markdown",
    "section": "Footnotes",
    "text": "Footnotes\nThere are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags].\n\n[^1]: This is a note.\n\n[^note-on-dags]: DAGs are neat.\n\nAnd here's more of the document.\n…to get…\n\nHere is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n\n\n\n\n\nThis is a note.↩︎\n\n\n\n\nDAGs are neat.↩︎\n\n\n\n\n\n\nYou can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!]\n…to get…\n\nCausal inference is neat.1\n\n\n\n\n\nBut it can be hard too!↩︎",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#front-matter",
    "href": "resource/markdown.html#front-matter",
    "title": "Using Markdown",
    "section": "Front matter",
    "text": "Front matter\nYou can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n---\ntitle: Title of your document\ndate: \"January 13, 2020\"\nauthor: \"Your name\"\n---\nYou can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n---\ntitle: \"My cool title: a subtitle\"\n---\nIf you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n---\ntitle: 'An evaluation of \"scare quotes\"'\n---",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#citations",
    "href": "resource/markdown.html#citations",
    "title": "Using Markdown",
    "section": "Citations",
    "text": "Citations\nSSC442 and EC420, you won’t use this part\nOne of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\n\nAdd a bibliography: entry to the YAML metadata:\n---\ntitle: Title of your document\ndate: \"January 13, 2020\"\nauthor: \"Your name\"\nbibliography: name_of_file.bib\n---\nChoose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n---\ntitle: Title of your document\ndate: \"January 13, 2020\"\nauthor: \"Your name\"\nbibliography: name_of_file.bib\ncsl: \"https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\"\n---\nSome of the most common CSLs are:\n\nChicago author-date\nChicago note-bibliography\nChicago full note-bibliography (no shortened notes or ibids)\nAPA 7th edition\nMLA 8th edition\n\nCite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n\n\n\n\n\n\n\nType…\n…to get…\n\n\n\n\nCausal inference is neat [@Rohrer:2018; @AngristPischke:2015].\nCausal inference is neat (Rohrer 2018; Angrist and Pischke 2015).\n\n\nCausal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1].\nCausal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).\n\n\nAngrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018].\nAngrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).\n\n\n@AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees.\nAngrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.\n\n\n\nAfter compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n\nAngrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/markdown.html#other-references",
    "href": "resource/markdown.html#other-references",
    "title": "Using Markdown",
    "section": "Other references",
    "text": "Other references\nThese websites have additional details and examples and practice tools:\n\nCommonMark’s Markdown tutorial: A quick interactive Markdown tutorial.\nMarkdown tutorial: Another interactive tutorial to practice using Markdown.\nMarkdown cheatsheet: Useful one-page reminder of Markdown syntax.\nThe Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.",
    "crumbs": [
      "Syllabus",
      "Markdown",
      "Using Markdown"
    ]
  },
  {
    "objectID": "resource/spatial.html",
    "href": "resource/spatial.html",
    "title": "Spatial Basics",
    "section": "",
    "text": "R with the sf package makes seamless the handling of spatial data (data that is indexed to, or specific to, spatially-defined locations). We start our GIS unit by showing spatial data for a row contained within the geometry column of a spatial data frame. Each row’s entry in geometry corresponds to the point, line, or polygon that row of data refers to. What we do not show is how those points, lines, or polygons come to be. This spatial basics supplemental details this, briefly.",
    "crumbs": [
      "Syllabus",
      "Other Useful Stuff",
      "Spatial Basics"
    ]
  },
  {
    "objectID": "resource/spatial.html#vector-vs.-raster",
    "href": "resource/spatial.html#vector-vs.-raster",
    "title": "Spatial Basics",
    "section": "Vector vs. Raster",
    "text": "Vector vs. Raster\nThere are two ways of storing 2-D mapped spatial data, raster and vector. A vector representation of a 2-D shape is best described as an irregular polygon with points defining vertices. A square plotted in cartesian coordinates is a vector representation. Conversely, a raster image is a grid of cells where each cell is defined as “in” or “out” of the square. Most computer graphics like JPEG and TIFF are raster graphics and each pixel has an assigned color. To make a raster image of a blue square, we’d make a big grid of pixels, and then color some blue based on their location. To make a blue square in vector form, we’d record just the location of the corners and add instructions to color inside the polygon formed by those corners blue.\n\n\n\n\n\n\n\n\n\n\nVectors are scalable. Rasters are not\nRasters are great for detail, like pixels in a picture, but they do not scale up very well. Vectors are great for things that do need to scale up. They are also smaller and easier to work with when they aren’t trying to replicate photo-realistic images. Vectors can handle curves by recording the properties of the curve (e.g. bezier curves), while rasters have to approximate curves along the grid of cells, so if you want a smooth curve, you need lots of cells.\nGeospatial work is almost always done in vectors because (1) it is easier to store data as vectors, and (2) it is easier to manipulate, project, intersect, or connect vector points, lines, and polygons.\nWe are going to work entirely with vectors today.",
    "crumbs": [
      "Syllabus",
      "Other Useful Stuff",
      "Spatial Basics"
    ]
  },
  {
    "objectID": "resource/unzipping.html",
    "href": "resource/unzipping.html",
    "title": "Unzipping files",
    "section": "",
    "text": "Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.",
    "crumbs": [
      "Syllabus",
      "Other Useful Stuff",
      "Unzipping files"
    ]
  },
  {
    "objectID": "resource/unzipping.html#unzipping-files-on-macos",
    "href": "resource/unzipping.html#unzipping-files-on-macos",
    "title": "Unzipping files",
    "section": "Unzipping files on macOS",
    "text": "Unzipping files on macOS\nDouble click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.",
    "crumbs": [
      "Syllabus",
      "Other Useful Stuff",
      "Unzipping files"
    ]
  },
  {
    "objectID": "resource/unzipping.html#unzipping-files-on-windows",
    "href": "resource/unzipping.html#unzipping-files-on-windows",
    "title": "Unzipping files",
    "section": "Unzipping files on Windows",
    "text": "Unzipping files on Windows\ntl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\n\n\n\n\n\n\n\n\n\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\n\n\n\n\n\n\n\n\n\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\n\n\n\n\n\n\n\n\n\nThen choose where you want to unzip all the files and click on “Extract”\n\n\n\n\n\n\n\n\n\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.",
    "crumbs": [
      "Syllabus",
      "Other Useful Stuff",
      "Unzipping files"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Below is a roadmap for the semester. Note that this will inevitably change from the first day you access this course. However, whatever is listed below should be considered canon. Accordingly, you should visit this page frequently throughout the term.\nAs mentioned in the syllabus, the course is structured by topics; each week introduces a new topic. Moreover, every week is divided into four important sections that you should engage with: principles, applications, weekly writings, and assignments."
  },
  {
    "objectID": "schedule.html#course-calendar",
    "href": "schedule.html#course-calendar",
    "title": "Schedule",
    "section": " Course Calendar",
    "text": "Course Calendar\n\n\n\n\n\nWeek\nDates\nProgramming Foundations\nPrinciples\nApplications\nAssignment\n\n\n\n\n1\nJan 14 & Jan 16\n(Re-)introduction to R\n\n\n\n\n\n2\nJan 21 & Jan 23\nProgramming basics, tidyverse, and visualization\n\n\n\n\n\n3\nJan 28 & Jan 30\nVisualization II\n\n\n\n\n\n4\nFeb 4 & Feb 6\nVisualization III\n\n\n\n\n\n5\nFeb 11\nWrangling Data\n\n\n\n\n\n\n\nData Analysis Foundations\nPrinciples\nApplications\nAssignment\n\n\n6\nFeb 18 & Feb 20\nLinear Regression I\n\n\n\n\n\n7\nFeb 25 & Feb 27\nLinear Regression II\n\n\n\n\n\n\nMarch 1st\nProject 1 Due\n\n\n\n\n\n8\nMar 11 & Mar 13\nLinear Regression III\n\n\n\n\n\n\n\nApplications of Data Analysis\nPrinciples\nApplications\nAssignment\n\n\n9\nMar 18 & Mar 20\nUncertainty and Probability in R\n\n\n\n\n\n10\nMar 25 & Mar 27\nNonlinear Regression\n\n\n\n\n\n11\nApr 1 & Apr 3\nFeature Selection and the Bias Variance Tradeoff\n\n\n\n\n\n12\nApr 8 & Apr 10\nClassification\n\n\n\n\n\n\nApril 12th\nProject 2 Due\n\n\n\n\n\n\n\nFurther Extensions\nPrinciples\nApplications\nAssignment\n\n\n13\nApr 15 & Apr 17\nText as Data\n\n\n\n\n\n14\nApr 22 & Apr 24\nGeospatial in R (Last lab)\n\n\n\n\n\n\n\nConclusions\nPrinciples\nApplications\nAssignment\n\n\n\nMay 1st\nFinal Project Due"
  }
]