[
  {
    "objectID": "assignment/01-assignment.html",
    "href": "assignment/01-assignment.html",
    "title": "1: Programming Basics in R",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, September 2nd, 11:59pm\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.\nIf you have not yet done so, you’ll need to install both R and RStudio. See the Installing page of our course resources for instructions.\nI have created a video walkthrough for the basics of using R for another course, but it is useful here. You can see part A here (labeled “Part 2a”) here ] and part B here (labeled “Part 2b”) . You should already be at this level of familiarity with R, but if you need a review, this is a good place to start."
  },
  {
    "objectID": "assignment/01-assignment.html#conditionals",
    "href": "assignment/01-assignment.html#conditionals",
    "title": "1: Programming Basics in R",
    "section": "Conditional expressions",
    "text": "Conditional expressions\nConditional expressions are one of the basic features of programming. They are used for what is called flow control. The most common conditional expression is the if-else statement. In R, we can actually perform quite a bit of data analysis without conditionals. However, they do come up occasionally, and you will need them once you start writing your own functions and packages.\nHere is a very simple example showing the general structure of an if-else statement. The basic idea is to print the reciprocal of a unless a is 0:\n\na <- 0\n\nif(a!=0){\n  print(1/a)\n} else{\n  print(\"No reciprocal for 0.\")\n}\n\n[1] \"No reciprocal for 0.\"\n\n\nLet’s look at one more example using the US murders data frame:\n\nlibrary(dslabs)\ndata(murders)\nmurder_rate <- murders$total / murders$population*100000\n\nHere is a very simple example that tells us which states, if any, have a murder rate lower than 0.5 per 100,000. The if statement protects us from the case in which no state satisfies the condition.\n\nind <- which.min(murder_rate)\n\nif(murder_rate[ind] < 0.5){\n  print(murders$state[ind])\n} else{\n  print(\"No state has murder rate that low\")\n}\n\n[1] \"Vermont\"\n\n\nIf we try it again with a rate of 0.25, we get a different answer:\n\nif(murder_rate[ind] < 0.25){\n  print(murders$state[ind])\n} else{\n  print(\"No state has a murder rate that low.\")\n}\n\n[1] \"No state has a murder rate that low.\"\n\n\nA related function that is very useful is ifelse. This function takes three arguments: a logical and two possible answers. If the logical is TRUE, the value in the second argument is returned and if FALSE, the value in the third argument is returned. Here is an example:\n\na <- 0\nifelse(a > 0, 1/a, NA)\n\n[1] NA\n\n\nThe function is particularly useful because it works on vectors. It examines each entry of the logical vector and returns elements from the vector provided in the second argument, if the entry is TRUE, or elements from the vector provided in the third argument, if the entry is FALSE.\n\na <- c(0, 1, 2, -4, 5)\nresult <- ifelse(a > 0, 1/a, NA)\n\nThis table helps us see what happened:\n\n\n\n\n \n  \n    a \n    is_a_positive \n    answer1 \n    answer2 \n    result \n  \n \n\n  \n    0 \n    FALSE \n    Inf \n    NA \n    NA \n  \n  \n    1 \n    TRUE \n    1.00 \n    NA \n    1.0 \n  \n  \n    2 \n    TRUE \n    0.50 \n    NA \n    0.5 \n  \n  \n    -4 \n    FALSE \n    -0.25 \n    NA \n    NA \n  \n  \n    5 \n    TRUE \n    0.20 \n    NA \n    0.2 \n  \n\n\n\n\n\nHere is an example of how this function can be readily used to replace all the missing values in a vector with zeros:\n\ndata(na_example)\nno_nas <- ifelse(is.na(na_example), 0, na_example)\nsum(is.na(no_nas))\n\n[1] 0\n\n\nTwo other useful functions are any and all. The any function takes a vector of logicals and returns TRUE if any of the entries is TRUE. The all function takes a vector of logicals and returns TRUE if all of the entries are TRUE. Here is an example:\n\nz <- c(TRUE, TRUE, FALSE)\nany(z)\n\n[1] TRUE\n\nall(z)\n\n[1] FALSE"
  },
  {
    "objectID": "assignment/01-assignment.html#defining-functions",
    "href": "assignment/01-assignment.html#defining-functions",
    "title": "1: Programming Basics in R",
    "section": "Defining functions",
    "text": "Defining functions\nAs you become more experienced, you will find yourself needing to perform the same operations over and over. A simple example is computing averages. We can compute the average of a vector x using the sum and length functions: sum(x)/length(x). Because we do this repeatedly, it is much more efficient to write a function that performs this operation. This particular operation is so common that someone already wrote the mean function and it is included in base R. However, you will encounter situations in which the function does not already exist, so R permits you to write your own. A simple version of a function that computes the average can be defined like this:\n\navg <- function(x){\n  s <- sum(x)\n  n <- length(x)\n  s/n\n}\n\nNow avg is a function that computes the mean:\n\nx <- 1:100\nidentical(mean(x), avg(x))\n\n[1] TRUE\n\n\nNotice that variables defined inside a function are not saved in the workspace. So while we use s and n when we call avg, the values are created and changed only during the call. Here is an illustrative example:\n\ns <- 3\navg(1:10)\n\n[1] 5.5\n\ns\n\n[1] 3\n\n\nNote how s is still 3 after we call avg.\nIn general, functions are objects, so we assign them to variable names with <-. The function function tells R you are about to define a function. The general form of a function definition looks like this:\n\nmy_function <- function(VARIABLE_NAME){\n  perform operations on VARIABLE_NAME and calculate VALUE\n  VALUE\n}\n\nThe functions you define can have multiple arguments as well as default values. For example, we can define a function that computes either the arithmetic or geometric average depending on a user defined variable like this:\n\navg <- function(x, arithmetic = TRUE){\n  n <- length(x)\n  ifelse(arithmetic, sum(x)/n, prod(x)^(1/n))\n}\n\nWe will learn more about how to create functions through experience as we face more complex tasks."
  },
  {
    "objectID": "assignment/01-assignment.html#namespaces",
    "href": "assignment/01-assignment.html#namespaces",
    "title": "1: Programming Basics in R",
    "section": "Namespaces",
    "text": "Namespaces\nOnce you start becoming more of an R expert user, you will likely need to load several add-on packages for some of your analysis. Once you start doing this, it is likely that two packages use the same name for two different functions. And often these functions do completely different things. In fact, you have already encountered this because both dplyr and the R-base stats package define a filter function. There are five other examples in dplyr. We know this because when we first load dplyr we see the following message:\nThe following objects are masked from ‘package:stats’:\n\n    filter, lag\n\nThe following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\nSo what does R do when we type filter? Does it use the dplyr function or the stats function? From our previous work we know it uses the dplyr one. But what if we want to use the stats version?\nThese functions live in different namespaces. R will follow a certain order when searching for a function in these namespaces. You can see the order by typing:\n\nsearch()\n\nThe first entry in this list is the global environment which includes all the objects you define.\nSo what if we want to use the stats filter instead of the dplyr filter but dplyr appears first in the search list? You can force the use of a specific namespace by using double colons (::) like this:\n\nstats::filter\n\nIf we want to be absolutely sure that we use the dplyr filter, we can use\n\ndplyr::filter\n\nAlso note that if we want to use a function in a package without loading the entire package, we can use the double colon as well.\nFor more on this more advanced topic we recommend the R packages book1."
  },
  {
    "objectID": "assignment/01-assignment.html#for-loops",
    "href": "assignment/01-assignment.html#for-loops",
    "title": "1: Programming Basics in R",
    "section": "For-loops",
    "text": "For-loops\nIf we had to write this section in a single sentence, it would be: Don’t use for-loops. Looping is intuitive, but R is designed to provide more computationally efficient solutions. For-loops should be considered a quick-and-dirty way to get an answer. But, hey, you live your own life. Below we provide a brief overview to for-looping.\nThe formula for the sum of the series \\(1+2+\\dots+n\\) is \\(n(n+1)/2\\). What if we weren’t sure that was the right function? How could we check? Using what we learned about functions we can create one that computes the \\(S_n\\):\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  sum(x)\n}\n\nHow can we compute \\(S_n\\) for various values of \\(n\\), say \\(n=1,\\dots,25\\)? Do we write 25 lines of code calling compute_s_n? No, that is what for-loops are for in programming. In this case, we are performing exactly the same task over and over, and the only thing that is changing is the value of \\(n\\). For-loops let us define the range that our variable takes (in our example \\(n=1,\\dots,10\\)), then change the value and evaluate expression as you loop.\nPerhaps the simplest example of a for-loop is this useless piece of code:\n\nfor(i in 1:5){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nHere is the for-loop we would write for our \\(S_n\\) example:\n\nm <- 25\ns_n <- vector(length = m) # create an empty vector\nfor(n in 1:m){\n  s_n[n] <- compute_s_n(n)\n}\n\nIn each iteration \\(n=1\\), \\(n=2\\), etc…, we compute \\(S_n\\) and store it in the \\(n\\)th entry of s_n.\nNow we can create a plot to search for a pattern:\n\nn <- 1:m\nplot(n, s_n)\n\n\n\n\n\n\nIf you noticed that it appears to be a quadratic, you are on the right track because the formula is \\(n(n+1)/2\\)."
  },
  {
    "objectID": "assignment/01-assignment.html#vectorization",
    "href": "assignment/01-assignment.html#vectorization",
    "title": "1: Programming Basics in R",
    "section": "Vectorization and functionals",
    "text": "Vectorization and functionals\nAlthough for-loops are an important concept to understand, in R we rarely use them. As you learn more R, you will realize that vectorization is preferred over for-loops since it results in shorter and clearer code. (It’s also vastly more efficient computationally, which can matter as your data grows.) A vectorized function is a function that will apply the same operation on each of the vectors.\n\nx <- 1:10\nsqrt(x)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\ny <- 1:10\nx*y\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\nTo make this calculation, there is no need for for-loops. However, not all functions work this way. For instance, the function we just wrote, compute_s_n, does not work element-wise since it is expecting a scalar. This piece of code does not run the function on each entry of n:\n\nn <- 1:25\ncompute_s_n(n)\n\nFunctionals are functions that help us apply the same function to each entry in a vector, matrix, data frame, or list. Here we cover the functional that operates on numeric, logical, and character vectors: sapply.\nThe function sapply permits us to perform element-wise operations on any function. Here is how it works:\n\nx <- 1:10\nsapply(x, sqrt)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\n\nEach element of x is passed on to the function sqrt and the result is returned. These results are concatenated. In this case, the result is a vector of the same length as the original x. This implies that the for-loop above can be written as follows:\n\nn <- 1:25\ns_n <- sapply(n, compute_s_n)\n\nOther functionals are apply, lapply, tapply, mapply, vapply, and replicate. We mostly use sapply, apply, and replicate in this book, but we recommend familiarizing yourselves with the others as they can be very useful."
  },
  {
    "objectID": "assignment/01-assignment.html#exercises",
    "href": "assignment/01-assignment.html#exercises",
    "title": "1: Programming Basics in R",
    "section": "Exercises",
    "text": "Exercises\nThis is your first weekly lab assignment. Each lab assignment will need to be done in Rmarkdown using the lab template, just right-click and Save As…Start a new folder on your drive for this course, and inside that a new folder for lab assignments, and inside that a new folder for Lab No. 0. Rmarkdown will place some intermediate files in that folder, so leaving .Rmd files on your desktop will make things messy, fast.\nOnce you’ve saved the file, open it up in Rstudio.\n\nChange the title to “Lab 0”\nPut your name on it\nLeave the date alone. That little `r Sys.time(...)` will ask R to return the date (with M-D-Y formatting), which Rmarkdown will put in as if you had typed in the actual date. Note - if your system time is rendered in unicode characters (which may be the case in some languages), you will get an error. If this happens, you’ll have to manually change the date in the header along with your name.\nWhen you type ## 1. Text of..., Markdown will recognize “1. Text of” as a header and will automatically make it big.\n\nSo please copy the number and text of the question you are answering here.\n\nNext will be the ```{r q1} text that will be in gray. R will recognize this as code and will treat it as such. Anything run in that block will have an output.\n\nIf you want to see what the code will do, copy the code and paste it into the gray area. Then, click the green right arrow in the top-right corner of the gray code chunk. It should show you the results.\nUse the results (plus your understanding of the code) to answer the questions below.\n\nWith each completed question, click the “Knit” button up above the script window. Rmarkdown will create a .pdf for you of your work (as long as it doesn’t hit any R errors). Knit often to make sure you haven’t hit an error!\nThe \\newpage line is a Latex command (the program that makes the typesetting look nice). It will start a new pdf page.\nOn the next page, copy question #2 to a new header using ##.\nKeep going until done. Render one last .pdf, proofread it, and turn it in on D2L!\n\nFor more on using R Markdown, see the R Markdown page of our course resources. You can also learn about the general Markdown language on our Markdown page of our course resources\n\nEXERCISES\n\nIn your first code chunk, load the package library tidyverse, which you will need for Question 8. Always load all your package libraries at the top, in the first code chunk!\nWhat will this conditional expression return and why?\n\n\nx <- c(1,2,-3,4)\n\nif(all(x>0)){\n  print(\"All Postives\")\n} else{\n  print(\"Not all positives\")\n}\n\n\nWhich of the following expressions is always FALSE when at least one entry of a logical vector x is TRUE?\n\n\nall(x)\nany(x)\nany(!x)\nall(!x)\n\n\nThe function nchar tells you how many characters long a character vector is. Write a line of code that assigns a new column in murders called new_names that is equal to the state column. Then, replace new_names with the corresponding state abbreviation when the state name is longer than 8 characters.\nCreate a function sum_n that for any given value, say \\(n\\), computes the sum of the integers from 1 to n (inclusive). Use the function to determine the sum of integers from 1 to 5,000.\nCreate a function altman_plot that takes two arguments, x and y, and plots the difference against the sum. That is, the difference should be on the y-axis and the and the sum should be on the x-axis. Use it to make an altman plot of x <- c(5,7,9) and y <- c(10,11,12). When your function creates the plot, it will output automatically in your Rmarkdown knitted .pdf.\nAfter running the code below, what is the value of x and why?\n\n\nx <- 3\nmy_func <- function(y){\n  x <- 5\n  y+5\n}\n\n\nWrite a function compute_s_n that for any given \\(n\\) computes the sum \\(S_n = 1^2 + 2^2 + 3^2 + \\dots n^2\\). Report the value of the sum when \\(n=10\\).\nDefine an empty numerical vector s_n of size 25 using s_n <- vector(\"numeric\", 25) and store in the results of \\(S_1, S_2, \\dots S_{25}\\) using a for-loop.\nRepeat exercise 8, but this time use sapply.\nRepeat exercise 8, but this time use map_dbl.\nPlot \\(S_n\\) versus \\(n\\). Use points defined by \\(n=1,\\dots,25\\).\nConfirm that the equivalent formula for this sum is \\(S_n= n(n+1)(2n+1)/6\\). To do so, compare whether compute_s_n gives the same result for the vector s_n as the equivalent formula for the sum."
  },
  {
    "objectID": "assignment/02-assignment.html",
    "href": "assignment/02-assignment.html",
    "title": "2: More Programming in R",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, September 9th, 11:59pm\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative.\nIf you have not yet done so, you’ll need to install both R and RStudio. See the Installing page of our course resources for instructions.\nI have created a video walkthrough for the basics of using R for another course, but it is useful here. You can see part A here (labeled “Part 2a”) here ] and part B here (labeled “Part 2b”) . You should already be at this level of familiarity with R, but if you need a review, this is a good place to start."
  },
  {
    "objectID": "assignment/02-assignment.html#conditionals",
    "href": "assignment/02-assignment.html#conditionals",
    "title": "2: More Programming in R",
    "section": "Conditional expressions",
    "text": "Conditional expressions\nConditional expressions are one of the basic features of programming. They are used for what is called flow control. The most common conditional expression is the if-else statement. In R, we can actually perform quite a bit of data analysis without conditionals. However, they do come up occasionally, and you will need them once you start writing your own functions and packages.\nHere is a very simple example showing the general structure of an if-else statement. The basic idea is to print the reciprocal of a unless a is 0:\n\na <- 0\n\nif(a!=0){\n  print(1/a)\n} else{\n  print(\"No reciprocal for 0.\")\n}\n\n[1] \"No reciprocal for 0.\"\n\n\nLet’s look at one more example using the US murders data frame:\n\nlibrary(dslabs)\ndata(murders)\nmurder_rate <- murders$total / murders$population*100000\n\nHere is a very simple example that tells us which states, if any, have a murder rate lower than 0.5 per 100,000. The if statement protects us from the case in which no state satisfies the condition.\n\nind <- which.min(murder_rate)\n\nif(murder_rate[ind] < 0.5){\n  print(murders$state[ind])\n} else{\n  print(\"No state has murder rate that low\")\n}\n\n[1] \"Vermont\"\n\n\nIf we try it again with a rate of 0.25, we get a different answer:\n\nif(murder_rate[ind] < 0.25){\n  print(murders$state[ind])\n} else{\n  print(\"No state has a murder rate that low.\")\n}\n\n[1] \"No state has a murder rate that low.\"\n\n\nA related function that is very useful is ifelse. This function takes three arguments: a logical and two possible answers. If the logical is TRUE, the value in the second argument is returned and if FALSE, the value in the third argument is returned. Here is an example:\n\na <- 0\nifelse(a > 0, 1/a, NA)\n\n[1] NA\n\n\nThe function is particularly useful because it works on vectors. It examines each entry of the logical vector and returns elements from the vector provided in the second argument, if the entry is TRUE, or elements from the vector provided in the third argument, if the entry is FALSE.\n\na <- c(0, 1, 2, -4, 5)\nresult <- ifelse(a > 0, 1/a, NA)\n\nThis table helps us see what happened:\n\n\n\n\n \n  \n    a \n    is_a_positive \n    answer1 \n    answer2 \n    result \n  \n \n\n  \n    0 \n    FALSE \n    Inf \n    NA \n    NA \n  \n  \n    1 \n    TRUE \n    1.00 \n    NA \n    1.0 \n  \n  \n    2 \n    TRUE \n    0.50 \n    NA \n    0.5 \n  \n  \n    -4 \n    FALSE \n    -0.25 \n    NA \n    NA \n  \n  \n    5 \n    TRUE \n    0.20 \n    NA \n    0.2 \n  \n\n\n\n\n\nHere is an example of how this function can be readily used to replace all the missing values in a vector with zeros:\n\ndata(na_example)\nno_nas <- ifelse(is.na(na_example), 0, na_example)\nsum(is.na(no_nas))\n\n[1] 0\n\n\nTwo other useful functions are any and all. The any function takes a vector of logicals and returns TRUE if any of the entries is TRUE. The all function takes a vector of logicals and returns TRUE if all of the entries are TRUE. Here is an example:\n\nz <- c(TRUE, TRUE, FALSE)\nany(z)\n\n[1] TRUE\n\nall(z)\n\n[1] FALSE"
  },
  {
    "objectID": "assignment/02-assignment.html#defining-functions",
    "href": "assignment/02-assignment.html#defining-functions",
    "title": "2: More Programming in R",
    "section": "Defining functions",
    "text": "Defining functions\nAs you become more experienced, you will find yourself needing to perform the same operations over and over. A simple example is computing averages. We can compute the average of a vector x using the sum and length functions: sum(x)/length(x). Because we do this repeatedly, it is much more efficient to write a function that performs this operation. This particular operation is so common that someone already wrote the mean function and it is included in base R. However, you will encounter situations in which the function does not already exist, so R permits you to write your own. A simple version of a function that computes the average can be defined like this:\n\navg <- function(x){\n  s <- sum(x)\n  n <- length(x)\n  s/n\n}\n\nNow avg is a function that computes the mean:\n\nx <- 1:100\nidentical(mean(x), avg(x))\n\n[1] TRUE\n\n\nNotice that variables defined inside a function are not saved in the workspace. So while we use s and n when we call avg, the values are created and changed only during the call. Here is an illustrative example:\n\ns <- 3\navg(1:10)\n\n[1] 5.5\n\ns\n\n[1] 3\n\n\nNote how s is still 3 after we call avg.\nIn general, functions are objects, so we assign them to variable names with <-. The function function tells R you are about to define a function. The general form of a function definition looks like this:\n\nmy_function <- function(VARIABLE_NAME){\n  perform operations on VARIABLE_NAME and calculate VALUE\n  VALUE\n}\n\nThe functions you define can have multiple arguments as well as default values. For example, we can define a function that computes either the arithmetic or geometric average depending on a user defined variable like this:\n\navg <- function(x, arithmetic = TRUE){\n  n <- length(x)\n  ifelse(arithmetic, sum(x)/n, prod(x)^(1/n))\n}\n\nWe will learn more about how to create functions through experience as we face more complex tasks."
  },
  {
    "objectID": "assignment/02-assignment.html#namespaces",
    "href": "assignment/02-assignment.html#namespaces",
    "title": "2: More Programming in R",
    "section": "Namespaces",
    "text": "Namespaces\nOnce you start becoming more of an R expert user, you will likely need to load several add-on packages for some of your analysis. Once you start doing this, it is likely that two packages use the same name for two different functions. And often these functions do completely different things. In fact, you have already encountered this because both dplyr and the R-base stats package define a filter function. There are five other examples in dplyr. We know this because when we first load dplyr we see the following message:\nThe following objects are masked from ‘package:stats’:\n\n    filter, lag\n\nThe following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\nSo what does R do when we type filter? Does it use the dplyr function or the stats function? From our previous work we know it uses the dplyr one. But what if we want to use the stats version?\nThese functions live in different namespaces. R will follow a certain order when searching for a function in these namespaces. You can see the order by typing:\n\nsearch()\n\nThe first entry in this list is the global environment which includes all the objects you define.\nSo what if we want to use the stats filter instead of the dplyr filter but dplyr appears first in the search list? You can force the use of a specific namespace by using double colons (::) like this:\n\nstats::filter\n\nIf we want to be absolutely sure that we use the dplyr filter, we can use\n\ndplyr::filter\n\nAlso note that if we want to use a function in a package without loading the entire package, we can use the double colon as well.\nFor more on this more advanced topic we recommend the R packages book1."
  },
  {
    "objectID": "assignment/02-assignment.html#for-loops",
    "href": "assignment/02-assignment.html#for-loops",
    "title": "2: More Programming in R",
    "section": "For-loops",
    "text": "For-loops\nIf we had to write this section in a single sentence, it would be: Don’t use for-loops. Looping is intuitive, but R is designed to provide more computationally efficient solutions. For-loops should be considered a quick-and-dirty way to get an answer. But, hey, you live your own life. Below we provide a brief overview to for-looping.\nThe formula for the sum of the series \\(1+2+\\dots+n\\) is \\(n(n+1)/2\\). What if we weren’t sure that was the right function? How could we check? Using what we learned about functions we can create one that computes the \\(S_n\\):\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  sum(x)\n}\n\nHow can we compute \\(S_n\\) for various values of \\(n\\), say \\(n=1,\\dots,25\\)? Do we write 25 lines of code calling compute_s_n? No, that is what for-loops are for in programming. In this case, we are performing exactly the same task over and over, and the only thing that is changing is the value of \\(n\\). For-loops let us define the range that our variable takes (in our example \\(n=1,\\dots,10\\)), then change the value and evaluate expression as you loop.\nPerhaps the simplest example of a for-loop is this useless piece of code:\n\nfor(i in 1:5){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nHere is the for-loop we would write for our \\(S_n\\) example:\n\nm <- 25\ns_n <- vector(length = m) # create an empty vector\nfor(n in 1:m){\n  s_n[n] <- compute_s_n(n)\n}\n\nIn each iteration \\(n=1\\), \\(n=2\\), etc…, we compute \\(S_n\\) and store it in the \\(n\\)th entry of s_n.\nNow we can create a plot to search for a pattern:\n\nn <- 1:m\nplot(n, s_n)\n\n\n\n\n\n\nIf you noticed that it appears to be a quadratic, you are on the right track because the formula is \\(n(n+1)/2\\)."
  },
  {
    "objectID": "assignment/02-assignment.html#vectorization",
    "href": "assignment/02-assignment.html#vectorization",
    "title": "2: More Programming in R",
    "section": "Vectorization and functionals",
    "text": "Vectorization and functionals\nAlthough for-loops are an important concept to understand, in R we rarely use them. As you learn more R, you will realize that vectorization is preferred over for-loops since it results in shorter and clearer code. (It’s also vastly more efficient computationally, which can matter as your data grows.) A vectorized function is a function that will apply the same operation on each of the vectors.\n\nx <- 1:10\nsqrt(x)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\ny <- 1:10\nx*y\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\nTo make this calculation, there is no need for for-loops. However, not all functions work this way. For instance, the function we just wrote, compute_s_n, does not work element-wise since it is expecting a scalar. This piece of code does not run the function on each entry of n:\n\nn <- 1:25\ncompute_s_n(n)\n\nFunctionals are functions that help us apply the same function to each entry in a vector, matrix, data frame, or list. Here we cover the functional that operates on numeric, logical, and character vectors: sapply.\nThe function sapply permits us to perform element-wise operations on any function. Here is how it works:\n\nx <- 1:10\nsapply(x, sqrt)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\n\nEach element of x is passed on to the function sqrt and the result is returned. These results are concatenated. In this case, the result is a vector of the same length as the original x. This implies that the for-loop above can be written as follows:\n\nn <- 1:25\ns_n <- sapply(n, compute_s_n)\n\nOther functionals are apply, lapply, tapply, mapply, vapply, and replicate. We mostly use sapply, apply, and replicate in this book, but we recommend familiarizing yourselves with the others as they can be very useful."
  },
  {
    "objectID": "assignment/02-assignment.html#exercises",
    "href": "assignment/02-assignment.html#exercises",
    "title": "2: More Programming in R",
    "section": "Exercises",
    "text": "Exercises\nThis is your first weekly lab assignment. Each lab assignment will need to be done in Rmarkdown using the lab template, just right-click and Save As…Start a new folder on your drive for this course, and inside that a new folder for lab assignments, and inside that a new folder for Lab No. 0. Rmarkdown will place some intermediate files in that folder, so leaving .Rmd files on your desktop will make things messy, fast.\nOnce you’ve saved the file, open it up in Rstudio.\n\nChange the title to “Lab 0”\nPut your name on it\nLeave the date alone. That little `r Sys.time(...)` will ask R to return the date (with M-D-Y formatting), which Rmarkdown will put in as if you had typed in the actual date. Note - if your system time is rendered in unicode characters (which may be the case in some languages), you will get an error. If this happens, you’ll have to manually change the date in the header along with your name.\nWhen you type ## 1. Text of..., Markdown will recognize “1. Text of” as a header and will automatically make it big.\n\nSo please copy the number and text of the question you are answering here.\n\nNext will be the ```{r q1} text that will be in gray. R will recognize this as code and will treat it as such. Anything run in that block will have an output.\n\nIf you want to see what the code will do, copy the code and paste it into the gray area. Then, click the green right arrow in the top-right corner of the gray code chunk. It should show you the results.\nUse the results (plus your understanding of the code) to answer the questions below.\n\nWith each completed question, click the “Knit” button up above the script window. Rmarkdown will create a .pdf for you of your work (as long as it doesn’t hit any R errors). Knit often to make sure you haven’t hit an error!\nThe \\newpage line is a Latex command (the program that makes the typesetting look nice). It will start a new pdf page.\nOn the next page, copy question #2 to a new header using ##.\nKeep going until done. Render one last .pdf, proofread it, and turn it in on D2L!\n\nFor more on using R Markdown, see the R Markdown page of our course resources. You can also learn about the general Markdown language on our Markdown page of our course resources\n\nEXERCISES\n\nIn your first code chunk, load the package library tidyverse, which you will need for Question 8. Always load all your package libraries at the top, in the first code chunk!\nWhat will this conditional expression return and why?\n\n\nx <- c(1,2,-3,4)\n\nif(all(x>0)){\n  print(\"All Postives\")\n} else{\n  print(\"Not all positives\")\n}\n\n\nWhich of the following expressions is always FALSE when at least one entry of a logical vector x is TRUE?\n\n\nall(x)\nany(x)\nany(!x)\nall(!x)\n\n\nThe function nchar tells you how many characters long a character vector is. Write a line of code that assigns a new column in murders called new_names that is equal to the state column. Then, replace new_names with the corresponding state abbreviation when the state name is longer than 8 characters.\nCreate a function sum_n that for any given value, say \\(n\\), computes the sum of the integers from 1 to n (inclusive). Use the function to determine the sum of integers from 1 to 5,000.\nCreate a function altman_plot that takes two arguments, x and y, and plots the difference against the sum. That is, the difference should be on the y-axis and the and the sum should be on the x-axis. Use it to make an altman plot of x <- c(5,7,9) and y <- c(10,11,12). When your function creates the plot, it will output automatically in your Rmarkdown knitted .pdf.\nAfter running the code below, what is the value of x and why?\n\n\nx <- 3\nmy_func <- function(y){\n  x <- 5\n  y+5\n}\n\n\nWrite a function compute_s_n that for any given \\(n\\) computes the sum \\(S_n = 1^2 + 2^2 + 3^2 + \\dots n^2\\). Report the value of the sum when \\(n=10\\).\nDefine an empty numerical vector s_n of size 25 using s_n <- vector(\"numeric\", 25) and store in the results of \\(S_1, S_2, \\dots S_{25}\\) using a for-loop.\nRepeat exercise 8, but this time use sapply.\nRepeat exercise 8, but this time use map_dbl.\nPlot \\(S_n\\) versus \\(n\\). Use points defined by \\(n=1,\\dots,25\\).\nConfirm that the equivalent formula for this sum is \\(S_n= n(n+1)(2n+1)/6\\). To do so, compare whether compute_s_n gives the same result for the vector s_n as the equivalent formula for the sum."
  },
  {
    "objectID": "assignment/03-assignment.html",
    "href": "assignment/03-assignment.html",
    "title": "3: Applying ggplot2 to Real Data",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, September 16th, 11:59pm\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative."
  },
  {
    "objectID": "assignment/03-assignment.html#preliminaries",
    "href": "assignment/03-assignment.html#preliminaries",
    "title": "3: Applying ggplot2 to Real Data",
    "section": "Preliminaries",
    "text": "Preliminaries\nAs always, we will first have to load ggplot2. To do this, we will load the tidyverse by running this code:\nlibrary(tidyverse)"
  },
  {
    "objectID": "assignment/03-assignment.html#background",
    "href": "assignment/03-assignment.html#background",
    "title": "3: Applying ggplot2 to Real Data",
    "section": "Background",
    "text": "Background\nThe New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order. They’ve provided an interactive map here where you can see the different projects. There’s also a link there to download the complete dataset.\nFor this exercise, you’re going to use this data to visualize the amounts or proportions of different types of essential projects in the five boroughs of New York City (Brooklyn, Manhattan, the Bronx, Queens, and Staten Island).\nAs you hopefully figured out by now, you’ll be doing all your R work in R Markdown. You can use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud), but this is optional. If you decide to do so, either create a new project for this exercise only, or make a project for all your work in this class.\nYou’ll need to download one CSV file and put it somewhere on your computer (or upload it to RStudio.cloud if you’ve gone that direction)—preferably in a folder named data in your project folder. You can download the data from the DOB’s map, or use this link to get it directly:\n\n EssentialConstruction.csv\n\n\nR Markdown\nWriting regular text with R Markdown follows the rules of Markdown. You can make lists; different-size headers, etc. This should be relatively straightfoward. We talked about a few Markdown features like bold and italics in class. See this resource for more formatting.\nYou’ll also need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n\n\n\n\n\n\n\nData Prep\nOnce you download the EssentialConstruction.csv file and save it in your project folder, you can open it and start cleaning. Loading in the basic data is straightforward:\n\nlibrary(tidyverse)\nessential = read_csv('https://EC242.netlify.app/data/EssentialConstruction.csv')\n\n\nExercise 1: Essential pandemic construction\nA. Show the count or proportion of approved projects by borough using a bar chart. Make sure all the elements of your plot (axes, legend, etc.) are labeled.\nB. Show the count or proportion of approved projects by category using a lollipop chart. Not sure of what a lollipop chart is? Google R ggplot lollipop. A huge portion of knowing how to code is knowing how to google, find examples, and figure out where to put your variables from your data! Make sure all the elements of your plot (axes, legend, etc.) are labeled.\nYou don’t need to make these super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing fill colors with scale_fill_manual() or with palettes.\nBonus\nOverlay the data from Part 1 above onto a map of NYC. Make sure all the elements of your plot (axes, legend, etc.) are labeled."
  },
  {
    "objectID": "assignment/03-assignment.html#getting-help",
    "href": "assignment/03-assignment.html#getting-help",
    "title": "3: Applying ggplot2 to Real Data",
    "section": "Getting help",
    "text": "Getting help\nUse the SSC442 Slack if you get stuck (click the Slack logo at the top right of this website header)."
  },
  {
    "objectID": "assignment/03-assignment.html#turning-everything-in",
    "href": "assignment/03-assignment.html#turning-everything-in",
    "title": "3: Applying ggplot2 to Real Data",
    "section": "Turning everything in",
    "text": "Turning everything in\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a PDF. Upload the PDF file to D2L."
  },
  {
    "objectID": "assignment/04-assignment.html",
    "href": "assignment/04-assignment.html",
    "title": "4: Visualizing Large(ish) Data",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, September 23st, 11:59pm\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative."
  },
  {
    "objectID": "assignment/04-assignment.html#bonus-exercise",
    "href": "assignment/04-assignment.html#bonus-exercise",
    "title": "4: Visualizing Large(ish) Data",
    "section": "Bonus Exercise",
    "text": "Bonus Exercise\nThis is entirely optional but might be fun. Then again, it might not be fun. I don’t know.\nFor extra fun times, if you feel like it, create a bump chart showing something from the unemployment data (perhaps the top 10 states or bottom 10 states in unemployment?) Adapt the code in the example for today’s session.\nIf you do this, plotting 51 lines is going to be a huge mess. But filtering the data is also a bad idea, because states could drop in and out of the top/bottom 10 over time, and we don’t want to get rid of them. Instead, you can zoom in on a specific range of data in your plot with coord_cartesian(ylim = c(1, 10)), for instance."
  },
  {
    "objectID": "assignment/04-assignment.html#turning-everything-in",
    "href": "assignment/04-assignment.html#turning-everything-in",
    "title": "4: Visualizing Large(ish) Data",
    "section": "Turning everything in",
    "text": "Turning everything in\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a PDF. Upload the PDF file to D2L."
  },
  {
    "objectID": "assignment/04-assignment.html#postscript-how-we-got-this-unemployment-data",
    "href": "assignment/04-assignment.html#postscript-how-we-got-this-unemployment-data",
    "title": "4: Visualizing Large(ish) Data",
    "section": "Postscript: how we got this unemployment data",
    "text": "Postscript: how we got this unemployment data\nFor the curious, here’s the code we used to download the unemployment data from the BLS.\nAnd to pull the curtain back and show how much googling is involved in data visualization (and data analysis and programming in general), here was my process for getting this data:\n\nWe thought “We want to have students show variation in something domestic over time” and then we googled “us data by state”. Nothing really came up (since it was an exceedingly vague search in the first place), but some results mentioned unemployment rates, so we figured that could be cool.\nWe googled “unemployment statistics by state over time” and found that the BLS keeps statistics on this. We clicked on the “Data Tools” link in their main navigation bar, clicked on “Unemployment”, and then clicked on the “Multi-screen data search” button for the Local Area Unemployment Statistics (LAUS).\nWe walked through the multiple screens and got excited that we’d be able to download all unemployment stats for all states for a ton of years, but then the final page had links to 51 individual Excel files, which was dumb.\nSo we went back to Google and searched for “download bls data r” and found a few different packages people have written to do this. The first one we clicked on was blscrapeR at GitHub, and it looked like it had been updated recently, so we went with it.\nWe followed the examples in the blscrapeR package and downloaded data for every state.\n\nAnother day in the life of doing modern data science. This is an example of something you will be able to do by the end of this class. we had no idea people had written R packages to access BLS data, but there are (at least) 3 packages out there. After a few minutes of tinkering, we got it working and it is relatively straightforward."
  },
  {
    "objectID": "assignment/05-assignment.html",
    "href": "assignment/05-assignment.html",
    "title": "5: Statistical Models and Uncertainty",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, September 30th, 11:59pm\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative."
  },
  {
    "objectID": "assignment/05-assignment.html#poll-aggregators",
    "href": "assignment/05-assignment.html#poll-aggregators",
    "title": "5: Statistical Models and Uncertainty",
    "section": "Poll aggregators",
    "text": "Poll aggregators\nA few weeks before the 2012 election Nate Silver was giving Obama a 90% chance of winning. How was Mr. Silver so confident? We will use a Monte Carlo simulation to illustrate the insight Mr. Silver had and others missed. To do this, we generate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls and construct and report 95% confidence intervals for each of the 12 polls. We save the results from this simulation in a data frame and add a poll ID column.\n\nlibrary(tidyverse)\nlibrary(dslabs)\nd <- 0.039 # true spread\nNs <- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)\np <- (d + 1) / 2 # true pr(clinton vote)\n\npolls <- map_df(Ns, function(N) {\n  x <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))\n  x_hat <- mean(x)\n  se_hat <- sqrt(x_hat * (1 - x_hat) / N)\n  list(estimate = 2 * x_hat - 1,\n    low = 2*(x_hat - 1.96*se_hat) - 1,\n    high = 2*(x_hat + 1.96*se_hat) - 1,\n    sample_size = N)\n}) %>% mutate(poll = seq_along(Ns))\n\nHere is a visualization showing the intervals the pollsters would have reported for the difference between Obama and Romney:\n\n\n\n\n\nNot surprisingly, all 12 polls report confidence intervals that include the election night result (dashed line). However, all 12 polls also include 0 (solid black line) as well. Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up. Below we describe a key insight they are missing.\nPoll aggregators, such as Nate Silver, realized that by combining the results of different polls you could greatly improve precision. By doing this, we are effectively conducting a poll with a huge sample size. We can therefore report a smaller 95% confidence interval and a more precise prediction.\nAlthough as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what we would have obtained had we made one large poll with:\n\nsum(polls$sample_size)\n\n[1] 11269\n\n\nparticipants. Basically, we construct an estimate of the spread, let’s call it \\(d\\), with a weighted average in the following way:\n\nd_hat <- polls %>%\n  summarize(avg = sum(estimate*sample_size) / sum(sample_size)) %>%\n  pull(avg)\n\nOnce we have an estimate of \\(d\\), we can construct an estimate for the proportion voting for Obama, which we can then use to estimate the standard error. Once we do this, we see that our margin of error is 0.0184545.\nThus, we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result we eventually observed on election night, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote.\n\n\n\n\n\nOf course, this was just a simulation to illustrate the idea. The actual data science exercise of forecasting elections is much more complicated and it involves modeling. Below we explain how pollsters fit multilevel models to the data and use this to forecast election results. In the 2008 and 2012 US presidential elections, Nate Silver used this approach to make an almost perfect prediction and silence the pundits.\nSince the 2008 elections, other organizations have started their own election forecasting group that, like Nate Silver’s, aggregates polling data and uses statistical models to make predictions. In 2016, forecasters underestimated Trump’s chances of winning greatly. The day before the election the New York Times reported2 the following probabilities for Hillary Clinton winning the presidency:\n\n\n\n\n \n  \n      \n    NYT \n    538 \n    HuffPost \n    PW \n    PEC \n    DK \n    Cook \n    Roth \n  \n \n\n  \n    Win Prob \n    85% \n    71% \n    98% \n    89% \n    >99% \n    92% \n    Lean Dem \n    Lean Dem \n  \n\n\n\n\n\n\nFor example, the Princeton Election Consortium (PEC) gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, higher than tossing two coins and getting two heads. In fact, four days before the election FiveThirtyEight published an article titled Trump Is Just A Normal Polling Error Behind Clinton3. By understanding statistical models and how these forecasters use them, we will start to understand how this happened.\nAlthough not nearly as interesting as predicting the electoral college, for illustrative purposes we will start by looking at predictions for the popular vote. FiveThirtyEight predicted a 3.6% advantage for Clinton4, included the actual result of 2.1% (48.2% to 46.1%) in their interval, and was much more confident about Clinton winning the election, giving her an 81.4% chance. Their prediction was summarized with a chart like this:\n\n\n\n\n\nThe colored areas represent values with an 80% chance of including the actual result, according to the FiveThirtyEight model. \nWe introduce actual data from the 2016 US presidential election to show how models are motivated and built to produce these predictions. To understand the “81.4% chance” statement we need to describe Bayesian statistics, which we don’t cover in this course.\n\nPoll data\nWe use public polling data organized by FiveThirtyEight for the 2016 presidential election. The data is included as part of the dslabs package:\n\ndata(polls_us_election_2016)\n\nThe table includes results for national polls, as well as state polls, taken during the year prior to the election. For this first example, we will filter the data to include national polls conducted during the week before the election. We also remove polls that FiveThirtyEight has determined not to be reliable and graded with a “B” or less. Some polls have not been graded and we include those:\n\npolls <- polls_us_election_2016 %>%\n  filter(state == \"U.S.\" & enddate >= \"2016-10-31\" &\n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade)))\n\nWe add a spread estimate:\n\npolls <- polls %>%\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nFor this example, we will assume that there are only two parties and call \\(p\\) the proportion voting for Clinton and \\(1-p\\) the proportion voting for Trump. We are interested in the spread \\(2p-1\\). Let’s call the spread \\(d\\) (for difference).\nWe have 49 estimates of the spread. The theory we learned tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread \\(d\\) and the standard error is \\(2\\sqrt{p (1 - p) / N}\\). Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is:\n\nd_hat <- polls %>%\n  summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %>%\n  pull(d_hat)\n\nand the standard error is:\n\np_hat <- (d_hat+1)/2\nmoe <- 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(polls$samplesize))\nmoe\n\n[1] 0.006623178\n\n\nSo we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?\nA histogram of the reported spreads shows a problem:\n\npolls %>%\n  ggplot(aes(spread)) +\n  geom_histogram(color=\"black\", binwidth = .01)\n\n\n\n\nThe data does not appear to be normally distributed and the standard error appears to be larger than 0.0066232. The theory is not quite working here.\n\n\nPollster bias\nNotice that various pollsters are involved and some are taking several polls a week:\n\npolls %>% group_by(pollster) %>% summarize(n())\n\n# A tibble: 15 × 2\n   pollster                                                   `n()`\n   <fct>                                                      <int>\n 1 ABC News/Washington Post                                       7\n 2 Angus Reid Global                                              1\n 3 CBS News/New York Times                                        2\n 4 Fox News/Anderson Robbins Research/Shaw & Company Research     2\n 5 IBD/TIPP                                                       8\n 6 Insights West                                                  1\n 7 Ipsos                                                          6\n 8 Marist College                                                 1\n 9 Monmouth University                                            1\n10 Morning Consult                                                1\n11 NBC News/Wall Street Journal                                   1\n12 RKM Research and Communications, Inc.                          1\n13 Selzer & Company                                               1\n14 The Times-Picayune/Lucid                                       8\n15 USC Dornsife/LA Times                                          8\n\n\nLet’s visualize the data for the pollsters that are regularly polling:\n\n\n\n\n\nThis plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll:\n\npolls %>% group_by(pollster) %>%\n  filter(n() >= 6) %>%\n  summarize(se = 2 * sqrt(p_hat * (1-p_hat) / median(samplesize)))\n\n# A tibble: 5 × 2\n  pollster                     se\n  <fct>                     <dbl>\n1 ABC News/Washington Post 0.0265\n2 IBD/TIPP                 0.0333\n3 Ipsos                    0.0225\n4 The Times-Picayune/Lucid 0.0196\n5 USC Dornsife/LA Times    0.0183\n\n\nis between 0.018 and 0.033, which agrees with the within poll variation we see. However, there appears to be differences across the polls. Note, for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump, while Ipsos is predicting a win larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values. All the polls should have the same expected value. FiveThirtyEight refers to these differences as “house effects”. We also call them pollster bias.\nIn the following section, rather than use the urn model theory, we are instead going to develop a data-driven model."
  },
  {
    "objectID": "assignment/05-assignment.html#data-driven-model",
    "href": "assignment/05-assignment.html#data-driven-model",
    "title": "5: Statistical Models and Uncertainty",
    "section": "Data-driven models",
    "text": "Data-driven models\nFor each pollster, let’s collect their last reported result before the election:\n\none_poll_per_pollster <- polls %>% group_by(pollster) %>%\n  filter(enddate == max(enddate)) %>%\n  ungroup()\n\nHere is a histogram of the data for these 15 pollsters:\n\nqplot(spread, data = one_poll_per_pollster, binwidth = 0.01)\n\n\n\n\nIn the previous section, we saw that using the urn model theory to combine these results might not be appropriate due to the pollster effect. Instead, we will model this spread data directly.\nThe new model can also be thought of as an urn model, although the connection is not as direct. Rather than 0s (Republicans) and 1s (Democrats), our urn now contains poll results from all possible pollsters. We assume that the expected value of our urn is the actual spread \\(d=2p-1\\).\nBecause instead of 0s and 1s, our urn contains continuous numbers between -1 and 1, the standard deviation of the urn is no longer \\(\\sqrt{p(1-p)}\\). Rather than voter sampling variability, the standard error now includes the pollster-to-pollster variability. Our new urn also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter. In statistics textbooks, the Greek symbol \\(\\sigma\\) is used to represent this parameter.\nIn summary, we have two unknown parameters: the expected value \\(d\\) and the standard deviation \\(\\sigma\\).\nOur task is to estimate \\(d\\). Because we model the observed values \\(X_1,\\dots X_N\\) as a random sample from the urn, the CLT might still work in this situation because it is an average of independent random variables. For a large enough sample size \\(N\\), the probability distribution of the sample average \\(\\bar{X}\\) is approximately normal with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\). If we are willing to consider \\(N=15\\) large enough, we can use this to construct confidence intervals.\nA problem is that we don’t know \\(\\sigma\\). But theory tells us that we can estimate the urn model \\(\\sigma\\) with the sample standard deviation defined as \\(s = \\sqrt{ \\sum_{i=1}^N (X_i - \\bar{X})^2 / (N-1)}\\).\nUnlike for the population standard deviation definition, we now divide by \\(N-1\\). This makes \\(s\\) a better estimate of \\(\\sigma\\). There is a mathematical explanation for this, which is explained in most statistics textbooks, but we don’t cover it here.\nThe sd function in R computes the sample standard deviation:\n\nsd(one_poll_per_pollster$spread)\n\n[1] 0.02419369\n\n\nWe are now ready to form a new confidence interval based on our new data-driven model:\n\nresults <- one_poll_per_pollster %>%\n  summarize(avg = mean(spread),\n            se = sd(spread) / sqrt(length(spread))) %>%\n  mutate(start = avg - 1.96 * se,\n         end = avg + 1.96 * se)\nround(results * 100, 1)\n\n  avg  se start end\n1 2.9 0.6   1.7 4.1\n\n\nOur confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.\n\nEXERCISES\nNote that using dollar signs $ $ to enclose some text is how you make the fancy math you see below. If you installed tinytex or some other Latex distribution in order to render your PDFs, you should be equipped to insert mathematics directly into your .Rmd file. It only works in the text – inside the code chunks, the dollar sign is still the accessor.\n\nIn this section, we talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election.\n\n\ndata(polls_us_election_2016)\npolls <- polls_us_election_2016 %>%\n  filter(pollster %in% c(\"Rasmussen Reports/Pulse Opinion Research\",\n                         \"The Times-Picayune/Lucid\") &\n           enddate >= \"2016-10-15\" &\n           state == \"U.S.\") %>%\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nWe want to answer the question: is there a poll bias? First, make a plot showing the spreads for each poll.\n\nThe data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance.\n\nThe urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call \\(d\\).\nWe will model the observed data \\(Y_{i,j}\\) in the following way:\n\\[\nY_{i,j} = d + b_i + \\varepsilon_{i,j}\n\\]\nwith \\(i=1,2\\) indexing the two pollsters, \\(b_i\\) the bias for pollster \\(i\\) and \\(\\varepsilon_ij\\) poll to poll chance variability. We assume the \\(\\varepsilon\\) are independent from each other, have expected value \\(0\\) and standard deviation \\(\\sigma_i\\) regardless of \\(j\\).\nWhich of the following best represents our question?\n\nIs \\(\\varepsilon_{i,j}\\) = 0?\nHow close are the \\(Y_{i,j}\\) to \\(d\\)?\nIs \\(b_1 \\neq b_2\\)?\nAre \\(b_1 = 0\\) and \\(b_2 = 0\\) ?\n\n\nSuppose we define \\(\\bar{Y}_1\\) as the average of poll results from the first pollster, \\(Y_{1,1},\\dots,Y_{1,N_1}\\) with \\(N_1\\) the number of polls conducted by the first pollster:\n\n\npolls %>%\n  filter(pollster==\"Rasmussen Reports/Pulse Opinion Research\") %>%\n  summarize(N_1 = n())\n\nWhat is the expected value of \\(\\bar{Y}_1\\)?\n\nWhat is the sample variance \\(s^2_1\\) of the sample \\(Y_1\\)? Using \\(s^2_1\\), what is the standard error of the mean \\(\\bar{Y}_1\\)?\nSuppose we define \\(\\bar{Y}_2\\) as the average of poll results from the second pollster, \\(Y_{2,1},\\dots,Y_{2,N_2}\\) with \\(N_2\\) the number of polls conducted by the second pollster. What is the expected value \\(\\bar{Y}_2\\), the sample variance \\(s^2_2\\), and the standard error of the mean \\(\\bar{Y}_2\\)?\nWhat does the CLT tell us about the distribution of a new random variable that is defined as \\(\\bar{Y}_2 - \\bar{Y}_1\\)?\n\n\nNothing because this is not the average of a sample.\nBecause the \\(Y_{ij}\\) are approximately normal, so are the averages.\nNote that \\(\\bar{Y}_2\\) and \\(\\bar{Y}_1\\) are sample averages, so if we assume \\(N_2\\) and \\(N_1\\) are large enough, each is approximately normal. The difference of normals is also normal.\nThe data are not 0 or 1, so CLT does not apply.\n\n\nThe new random variable defined as \\(\\bar{Y}_2 - \\bar{Y}_1\\) has an expected value of \\(b_2 - b_1\\). To see this, take the equation from problem 2 defined for each pollster, write out the difference, and take expectations. \\(b_2 = b_1\\) is the pollster bias difference we want to learn about statistically. If our model holds, then this random variable has an approximately normal distribution and we know its standard error – by the rules of random variable variance, the standard error is the square root of the variance of the new random variable, and the variance of the new variable is the sum of the variances minus twice the covariance (which is zero by our assumptions). The standard error of our new random variable depends on the standard errors of the \\(Y\\) above, which we already estimated in 3-5.\n\nThe statistic formed by dividing our estimate of \\(b_2-b_1\\) by its estimated standard error:\n\\[\n\\frac{\\bar{Y}_2 - \\bar{Y}_1}{\\sqrt{\\underbrace{s_2^2/N_2}_{\\text{Std error of mean from 4, squared}} + \\underbrace{s_1^2/N_1}_{\\text{Std error of mean from 5, squared}}}}\n\\]\nis called the t-statistic. Now you should be able to do the calculations necessary to answer the question: is \\(b_2 - b_1\\) different from 0? Hint: you can build a 95% confidence interval around your estimate and see if it includes 0."
  },
  {
    "objectID": "assignment/06-assignment.html",
    "href": "assignment/06-assignment.html",
    "title": "6: Correlations and Simple Models",
    "section": "",
    "text": "Due Date\n\n\n\nThis assignment is due on Monday, September 16th, 11:59pm\nAll assignments are due on D2L by 11:59pm on the due date. Late work is not accepted. You do not need to submit your .rmd file - just the properly-knitted PDF. All assignments must be properly rendered to PDF using Latex. Make sure you start your assignment sufficiently early such that you have time to address rendering issues. Come to office hours or use the course Slack if you have issues. Using an Rstudio instance on posit.cloud is always a feasible alternative."
  },
  {
    "objectID": "assignment/06-assignment.html#backstory-and-set-up",
    "href": "assignment/06-assignment.html#backstory-and-set-up",
    "title": "6: Correlations and Simple Models",
    "section": "Backstory and Set Up",
    "text": "Backstory and Set Up\nYou have been recently hired to Zillow’s Zestimate product team as a junior analyst. As a part of their regular hazing, they have given you access to a small subset of their historic sales data. Your job is to present some basic predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.\nFirst, let’s load the data.\n\nameslist  <- read.table('https://raw.githubusercontent.com/ajkirkpatrick/FS20/postS21_rev/classdata/ames.csv', \n                   header = TRUE,\n                   sep = ',') \n\nBefore we proceed, let’s note a few things about the (simple) code above. First, we have specified header = TRUE because—you guessed it—the original dataset has headers. Although simple, this is an incredibly important step because it allows R to do some smart R things. Specifically, once the headers are in, the variables are formatted as int and factor where appropriate. It is absolutely vital that we format the data correctly; otherwise, many R commands will whine at us.\nTry it: Run the above, but instead specifying header = FALSE. What data type are the various columns? Now try ommitting the line altogether. What is the default behavior of the read.table function?1\n\nData Exploration and Processing\nWe are not going to tell you anything about this data. This is intended to replicate a real-world experience that you will all encounter in the (possibly near) future: someone hands you data and you’re expected to make sense of it. Fortunately for us, this data is (somewhat) self-contained. We’ll first check the variable names to try to divine some information. Recall, we have a handy little function for that:\n\nnames(ameslist)\n\nNote that, when doing data exploration, we will sometimes choose to not save our output. This is a judgement call; here we’ve chosen to merely inspect the variables rather than diving in.\nInspection yields some obvious truths. For example:\n\n\n\nVariable\nExplanation\nType\n\n\n\n\nID\nUnique identifier for each row\nint\n\n\nLotArea\nSize of lot (units unknown)\nint\n\n\nSalePrice\nSale price of house ($)\nint\n\n\n\n…but we face some not-so-obvious things as well. For example:\n\n\n\nVariable\nExplanation\nType\n\n\n\n\nLotShape\n? Something about the lot\nfactor\n\n\nMSSubClass\n? No clue at all\nint\n\n\nCondition1\n? Seems like street info\nfactor\n\n\n\nIt will be difficult to learn anything about the data that is of type int without outside documentation. However, we can learn something more about the factor-type variables. In order to understand these a little better, we need to review some of the values that each take on.\nTry it: Go through the variables in the dataset and make a note about your interpretation for each. Many will be obvious, but some require additional thought.\nAlthough there are some variables that would be difficult to clean, there are a few that we can address with relative ease. Consider, for instance, the variable GarageType. This might not be that important, but, remember, the weather in Ames, IA is pretty crummy—a detached garage might be a dealbreaker for some would-be homebuyers. Let’s inspect the values:\n\n> unique(ameslist$GarageType)\n[1] Attchd  Detchd  BuiltIn CarPort <NA> Basment 2Types\n\nWith this, we could make an informed decision and create a new variable. Let’s create OutdoorGarage to indicate, say, homes that have any type of garage that requires the homeowner to walk outdoors after parking their car. (For those who aren’t familiar with different garage types, a car port is not insulated and is therefore considered outdoors. A detached garage presumably requires that the person walks outside after parking. The three other types are inside the main structure, and 2Types we can assume includes at least one attached garage of some sort). This is going to require a bit more coding and we will have to think through each step carefully.\nFirst, let’s create a new object that has indicator variables (that is, a variable whose values are either zero or one) for each of the GarageType values. That is, it has a \\(1\\) if the variable takes on some specific value, and a \\(0\\) otherwise. Do this for all but one of the different values in GarageType, and your descriptive variable is now represented by numbers.\nAs with everything in R, there’s a handy function to do this for us:\n\nGarageTemp = model.matrix( ~ GarageType - 1, data=ameslist )\n\nWe now have two separate objects living in our computer’s memory: ameslist and GarageTemp—so named to indicate that it is a temporary object.2 We now need to stitch it back onto our original data; we’ll use a simple concatenation and write over our old list with the new one:\n\nameslist <- cbind(ameslist, GarageTemp)\n> Error in data.frame(..., check.names = FALSE) :\n  arguments imply differing number of rows: 1460, 1379\n\nHuh. What’s going on?\n\nEXERCISE 1 of 5\n\nFigure out what’s going on above. Figure out where the 1460-1379 = 81 rows of data are going when using model.matrix. Fix this issue so that you have a working version.\n\n\nNow that we’ve got that working (ha!) we can generate a new variable for our outdoor garage:\n\nameslist$GarageOutside <- ifelse(ameslist$GarageTypeDetchd == 1 | ameslist$GarageTypeCarPort == 1, 1, 0)\nunique(ameslist$GarageOutside) \n[1]  0  1 NA\n\nThis seems to have worked. The command above ifelse() does what it says: if some condition is met (here, either of two variables equals one) then it returns a one; else it returns a zero. Such functions are very handy, though as mentioned above, there are other ways of doing this. Also note, that while fixed the issue with NA above, we’ve got new issues: we definitely don’t want NA outputted from this operation. Accordingly, we’re going to need to deal with it somehow.\nTry it: Utilizing a similar approach to what you did above, fix this so that the only outputs are zero and one. This requires taking a stand on what the NA values mean. If you think they correspond to a detached garage (or something functionally equivalent, like “no parking whatsoever”), then change the NA values to zero. If you think they are mistakes, then we should drop all data with NA for the this column. State what you did and why. You can do this just using a subset to state which values you want to replace, or you can use case_when and make sure your last conditional always returns a value. Refresh yourself in Content 01.\nGenerally speaking, this is a persistent issue, and you will spend an extraordinary amount of time dealing with missing data or data that does not encode a variable exactly as you want it. This is expecially true if you deal with real-world data: you will need to learn how to handle NAs. There are a number of fixes (as always, Google is your friend) and anything that works is good. But you should spend some time thinking about this and learning at least one approach.\n\nEXERCISES 2-5\n\nPrune the data to 6-8 of the variables that are type = int about which you have some reasonable intuition for what they mean. Choose those that you believe are likely to be correlated with SalePrice. This must include the variable SalePrice and GrLivArea. Save this new dataset as Ames. Produce documentation for this object in the form of a Markdown table or see further documentation here. This must describe each of the preserved variables, the values it can take (e.g., can it be negative?) and your definition of the variable. Counting the variable name, this means your table should have three columns. Markdown tables are entered in the text body, not code chunks, of your .rmd, so your code creating Ames will be in a code chunk, and your table will be right after it.\nProduce a scatterplot matrix of the chosen variables3\nCompute a matrix of correlations between these variables using the function cor(). Do the correlations match your prior beliefs? Briefly discuss the correlation between the chosen variables and SalePrice and any correlations between these variables.\nProduce a scatterplot between SalePrice and GrLivArea. Run a linear model using lm() to explore the relationship. Finally, use the geom_abline() function to plot the relationship that you’ve found in the simple linear regression. You’ll need to extract the intercept and slope from your lm object. See coef(...) for information on this.4\n\nWhat is the largest outlier that is above the regression line? Produce the other information about this house.\n\n\n(Bonus) Create a visualization that shows the rise of air conditioning over time in homes in Ames."
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Lab Assignments",
    "section": "",
    "text": "This course is the capstone of the Data Analytics Minor in the College of Social Science. Accordingly, you should—fingers crossed—enjoy data analysis. You will get the most of out this class if you:\nAssignments consist of Weekly Writings and Lab Assignments. Each type of assignment in this class helps with one of these strategies. Weekly writings engage with the materials, and lab assignments engage directly with R. The assignments are described below.\nTo get started, download and save the following files (right-click to Save Link As…)"
  },
  {
    "objectID": "assignment/index.html#weekly-writings",
    "href": "assignment/index.html#weekly-writings",
    "title": "Lab Assignments",
    "section": "Weekly Writings",
    "text": "Weekly Writings\nTo encourage you to actively engage with the course content, you will write a ≈150 word memorandum about the reading or lecture each week. That’s fairly short: there are ≈250 words on a typical double-spaced page. You must complete eleven of these in the course. I will drop your one lowest weekly writing score. Your actual prompt will be assigned in class, so you must login each day to ensure you get these assignments. To keep you on your toes, we will vary whether these are assigned on Tuesdays or Thursdays. Each week’s weekly writing will be due on D2L by 11:59pm on Saturday\nYou can do a lot of different things with this memo: discuss something you learned from the course content, write about the best or worst data visualization you saw recently, connect the course content to your own work, etc. These reflections let you explore and answer some of the key questions of this course, including:\n\nWhen is a link correlational vs causal? How can we still make useful statements about non-causal things?\nWhy do we visualize data?\nWhat makes a great data analysis? What makes a bad analysis?\nHow do you choose which kind of analysis method to use?\nWhat is the role of the data structure in choosing an analysis? Can we be flexible?\n\nThe course content for each day will also include a set of questions specific to that topic. You do not have to answer all (or any) of these questions. That would be impossible. They exist to guide your thinking and to make complex reading more digestible. The specific topic for each week will be assigned in class. (We can’t emphasize this enough.)\nThe TA will grade these mini-exercises using a very simple system:\n\n✔+: (9.2 points (115%) in gradebook) Work shows phenomenal thought and engagement with the course content. We will not assign these often.\n✔: (8 points (100%) in gradebook) Work is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance.\n✔−: (4 points (50%) in gradebook) Work is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.\n\n(There is an implicit 0 above for work that is not turned in by Saturday at 11:59pm). Notice that this is essentially a pass/fail or completion-based system. We’re not grading your writing ability; we’re not counting the exact number of words you’re writing; and we’re not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. We are looking for thoughtful engagement. Read the material, engage with the work and you’ll get a ✓.\n\nWeekly Writing Template\nYou will turn these reflections in via D2L. You will write them using R Markdown and this template {{< fa file-arrow-down title=“Download Weekly Writing Template”>}} Weekly writing template. You must knit your work to a PDF document (this will be what you turn in). D2L will have eleven weekly writing assignments available. Upload your first weekly writing assignment to number 1, your second (regardless of which week you are writing on) to number 2, etc."
  },
  {
    "objectID": "assignment/index.html#lab-assignments",
    "href": "assignment/index.html#lab-assignments",
    "title": "Lab Assignments",
    "section": "Lab Assignments",
    "text": "Lab Assignments\nEach week of the course has examples of code that teach and demonstrate how to do specific tasks in R. However, without practicing these principles and making graphics on your own, you won’t remember what you learn.\nPlease do not do labs more than one week ahead of time. I am updating the assignments as the semester proceeds, and you may do an entire assignment that is completely changed.\nFor example, to practice working with ggplot2 and making data-based graphics, you will complete a brief set of exercises over a few class sessions. These exercises will have 1–3 short tasks that are directly related to the topic for the week. You need to show that you made a good faith effort to work each question. There will also be a final question which requires significantly more thought and work. This will be where you get to show some creativity and stretch your abilities. Overall, labs will be graded the same check system:\n\n✔+: (17.5 points (115%) in gradebook) Exercises are complete. Every task was attempted and answered, and most answers are correct. Knitted document is clean and easy to follow. Work on the final problem shows creativity or is otherwise exceptional. We will not assign these often.\n✔: (15 points (100%) in gradebook) Exercises are complete and most answers are correct. This is the expected level of performance.\n✔−: (7.5 points (50%) in gradebook) Exercises are less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. We will hopefully not assign these often, but subpar work can expect a ✔−.\n\nThere is an implicit 0 for any assignment not turned in on time. If you have only partial work, then turn that in for partial credit. As noted in the syllabus, we are not grading your coding ability. We are not checking each line of code to make sure it produces some exact final figure, and we do not expect perfection. Also note that a ✓ does not require 100% success. You will sometimes get stuck with weird errors that you can’t solve, or the demands of pandemic living might occasionally become overwhelming. We are looking for good faith effort. Try hard, engage with the task, and you’ll get a ✓.\nYou may work together on the labs, but you must turn in your own answers.\n\nLab Template\nYou will turn these labs in via D2L. You will write them using R Markdown and this {{< fa file-arrow-down title=“Download Assignment Template”>}} Lab assignment template. You must knit your work to a PDF document (this will be what you turn in). Your output must be rendered in latex. I do not accept rendering to HTML or Word and then converting to PDF and unrendered .rmd files are not allowed."
  },
  {
    "objectID": "assignment/index.html#projects",
    "href": "assignment/index.html#projects",
    "title": "Lab Assignments",
    "section": "Projects",
    "text": "Projects\nTo give you practice with the data and design principles you’ll learn in this class, you will complete two projects en route to the overarching final project of the course. Both these mini projects and the final project must be completed in groups. I will assign groups after the drop deadline passes. Groups will be 2-3 people. You are allowed to form your own groups, but I will assign groups. More details will follow later.\nThe two (mini) projects are checkpoints to ensure you’re working on your project seriously. They will be graded using a check system:\n\n✔+: (55 points (≈115%) in gradebook) Project is phenomenally well-designed and uses advanced R techniques. The project uncovers an important story that is not readily apparent from just looking at the raw data. I will not assign these often.\n✔: (50 points (100%) in gradebook) Project is fine, follows most design principles, answers a question from the data, and uses R correctly. This is the expected level of performance.\n✔−: (25 points (50%) in gradebook) Project is missing large components, is poorly designed, does not answer a relevant question, and/or uses R incorrectly. This indicates that you need to improve next time. I will hopefully not assign these often.\n\nBecause these mini projects give you practice for the final project, we will provide you with substantial feedback on your design and code."
  },
  {
    "objectID": "assignment/index.html#final-project",
    "href": "assignment/index.html#final-project",
    "title": "Lab Assignments",
    "section": "Final project",
    "text": "Final project\nAt the end of the course, you will demonstrate your skills by completing a final project. Complete details for the final project (including past examples of excellent projects) are here. In brief, the final project has the following elements:\n\nYou must find existing data to analyze.1 Aggregating data from multiple sources is encouraged, but is not required.\n\n\nYou must visualize (at least) three interesting features of that data. Visualizations should aid the reader in understanding something about the data that might not be readily aparent.2\n\n\nYou must come up with some analysis—using tools from the course—which relates your data to either a prediction or a policy conclusion. For example, if you collected data from Major League Baseball games, you could try to “predict” whether a left-hander was pitching based solely on the outcomes of the batsmen.3\n\n\nYou must write your analysis as if presenting to a C-suite executive. If you are not familiar with this terminology, the C-suite includes, e.g., the CEO, CFO, and COO of a given company. Generally speaking, such executives are not particularly analytically oriented, and therefore your explanations need to be clear, consise (their time is valuable) and contain actionable (or valuable) information.\n\nThere is no final exam. This project is your final exam.\nThe project will not be graded using a check system, and will be graded by me (the main instructor, not a TA). I will evaluate the following four elements of your project:\n\nTechnical skills: Was the project easy? Does it showcase mastery of data analysis?\nVisual design: Was the information smartly conveyed and usable? Was it beautiful?\nAnalytic design: Was the analysis appropriate? Was it sensible, given the dataset?\nStory: Did we learn something?\n\nIf you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project."
  },
  {
    "objectID": "content/Week_01/01a.html",
    "href": "content/Week_01/01a.html",
    "title": "Welcome Back to R",
    "section": "",
    "text": "As noted in the syllabus, your readings will be assigned each week in this area. For this initial week, please read the course content. Read closely the following:\n\nThe syllabus, content, examples, and labs pages for this class.\nThis page. Yes, the whole thing.\n\n\n\nFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\nHow does this course work?\nDo you remember anything about R?\nWhat are the different data types in R?\nHow do you index specific elements of a vector? Why might you want to do that?"
  },
  {
    "objectID": "content/Week_01/01a.html#starting-point-for-this-course",
    "href": "content/Week_01/01a.html#starting-point-for-this-course",
    "title": "Welcome Back to R",
    "section": "Starting point for this course",
    "text": "Starting point for this course\nBetter utilizing existing data can improve our predictive power whilst providing interpretable outputs for considering new policies.\nWARNING: Causation is tough and we will spend the entire course warning you to avoid making causal claims!\n\nNon-Social Science Approaches to Statistical Learning\nA Brief History\nSuppose you are a researcher and you want to teach a computer to recognize images of a tree.\nNote: this is an ``easy” problem. If you show pictures to a 3-year-old, that child will probably be able to tell you if there is a tree in the picture.\nComputer scientists spent about 20 years on this problem because they thought about the problem like nerds and tried to write down a series of rules.\nRules are difficult to form, and simply writing rules misses the key insight: the data can tell you something.\n\n\nSocial Science Approaches to Statistical Learning\nA Brief History\nSuppose you are a researcher and you want to know whether prisons reduce crime.\nfrom “A Call for a Moratorium on Prison Building” (1976)\n\nBetween 1955 and 1975, fifteen states increased the collective capacity of their adult prison systems by 56% (from, on average, 63,100 to 98,649).\nFifteen other states increased capacity by less than 4% (from 49,575 to 51,440).\nIn “heavy-construction” states the crime rate increased by 167%; in “low-construction” states the crime rate increased by 145%.\n\n\n\n\n\nPrison Capacity\nCrime Rate\n\n\n\n\nHigh construction\n\\(\\uparrow\\)~56%\n\\(\\uparrow\\)~167%\n\n\nLow construction\n\\(\\uparrow\\)~4%\n\\(\\uparrow\\)~145%\n\n\n\n\n\nThe Pros and Cons of Correlation\nPros:\n\nNature gives you correlations for free.\nIn principle, everyone can agree on the facts.\n\nCons:\n\nCorrelations are not very helpful.\nThey show what has happened, but not why.\nFor many things, we care about why. The social science perspective asks “why?”\n\n\nWhy a Correlation Exists Between X and Y\n\n\\(X \\rightarrow Y\\) X causes Y (causality)\n\\(X \\leftarrow Y\\) Y causes X (reverse causality)\n\\(Z \\rightarrow X\\); \\(Z \\rightarrow Y\\) Z causes X and Y (common cause)\n\\(X \\rightarrow Y\\); \\(Y \\rightarrow X\\) X causes Y and Y causes X (simultaneous equations)\n\n\n\nUniting Social Science and Computer Science\nWe will start in this course by examining situations where we do not care about why something has happened, but instead we care about our ability to predict its occurrence from existing data.\n(But of course keep in back of mind that if you are making policy, you must care about why something happened).\nWe will also borrow a few other ideas from CS:\n\nAnything is data\n\nSatellite data\nUnstructured text or audio\nFacial expressions or vocal intonations\n\nSubtle improvements on existing techniques\nAn eye towards practical implementability over “cleanliness”\n\n\n\n\nA Case Study in Prediction\nExample: a firm wishes to predict user behavior based on previous purchases or interactions.\nSmall margins \\(\\rightarrow\\) huge payoffs when scaled up.\n\\(.01\\% \\rightarrow\\) $10 million.\nNot obvious why this was true for Netflix; quite obvious why this is true in financial markets.\nFrom a computer science perspective, it only matters that you get that improvement ($$). From a social science perspective, we would want to use the predictions to learn more about why.\n\n\nMore Recent Examples of Prediction\n\nIdentify the risk factors for prostate cancer.\nClassify a tissue sample into one of several cancer classes, based on a gene expression profile.\nClassify a recorded phoneme based on a log-periodogram.\nPredict whether someone will have a heart attack on the basis of demographic, diet and clinical measurements.\nCustomize an email spam detection system.\nIdentify a hand-drawn object.\nDetermine which oscillations of stellar luminosity are likely due to exoplanets.\nIdentify food combinations that cause spikes in blood glucose level for an individual.\nEstablish the relationship between salary and demographic variables in population survey data.\n\n\n\nAn Aside: Nomenclature\nMachine learning arose as a subfield of Artificial Intelligence.\nStatistical learning arose as a subfield of Statistics.\nThere is much overlap; however, a few points of distinction:\n\nMachine learning has a greater emphasis on large scale applications and prediction accuracy.\nStatistical learning emphasizes models and their interpretability, and precision and uncertainty.\n\nBut the distinction has become more and more blurred, and there is a great deal of “cross-fertilization”.\n\n\nObviously true: machine learning has the upper hand in marketing.\n\n\nLearning from Data\nThe following are the basic requirements for statistical learning:\n\nA pattern exists.\nThis pattern is not easily expressed in a closed mathematical form.\nYou have data."
  },
  {
    "objectID": "content/Week_01/01a.html#case-study-1-global-renewable-energy-production",
    "href": "content/Week_01/01a.html#case-study-1-global-renewable-energy-production",
    "title": "Welcome Back to R",
    "section": "Case study 1: Global Renewable Energy Production",
    "text": "Case study 1: Global Renewable Energy Production\nImagine you are evaluating countries for a potential investment in renewable energy. Headlines like “Renewable Energy Capacity Growth Worldwide” have piqued your interest. Reports from various sources show diverse graphs and charts and you’re curious about the underlying data. You want to know which countries are leading the way in renewable energy production and which are lagging behind. You want to know which countries are growing their renewable energy production the fastest. In short: you want to know which countries are the best bets for investment. You might see something like the following:\n\n\n\n\n\nYou might want to look into the underlying data (in this case, fabricated) and think about what to do next. In this sense, you have learned from data."
  },
  {
    "objectID": "content/Week_01/01a.html#case-study-2-us-homicides-by-firearm",
    "href": "content/Week_01/01a.html#case-study-2-us-homicides-by-firearm",
    "title": "Welcome Back to R",
    "section": "Case study 2: US homicides by firearm",
    "text": "Case study 2: US homicides by firearm\nImagine you live in Europe (if only!) and are offered a job in a US company with many locations in every state. It is a great job, but headlines such as US Gun Homicide Rate Higher Than Other Developed Countries1 have you worried. Fox News runs a scary looking graphic, and charts like the one below only add to you anxiety:\n\n\n\n\n\n\nOr even worse, this version from everytown.org:\n\n\n\n\n\n\nBut then you remember that (1) this is a hypothetical exercise; (2) you’ll take literally any job at this point; and (3) Geographic diversity matters – the United States is a large and diverse country with 50 very different states (plus the District of Columbia and some lovely territories).2\n\n\n\n\n\nCalifornia, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial.\n\n\n\nThis is a relatively simple and straightforward problem in social science: you have options of where to live, and want to determine the safety of the various states. Your “research” is clearly policy-relevant: you will eventually have to live somewhere. In this course, we will begin to tackle the problem by examining data related to gun homicides in the US during 2010 using R as a motivating example along the way.\nBefore we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Ideally, this is a refresher. However, we are aware that your preparation in previously courses varies greatly from student to student. Moreover, we want you to be aware that the usefulness of some of these early building blocks may not be immediately obvious. Later in the class you will appreciate having these skills. Mastery will be rewarded both in this class and (of course) in life.\n\nThe Pre-Basics\nWe’ve now covered a short bit of material. The remainder of this first lecture will be covering setting up R and describing some common errors."
  },
  {
    "objectID": "content/Week_01/01b.html",
    "href": "content/Week_01/01b.html",
    "title": "Working with R and RStudio",
    "section": "",
    "text": "I like to use this spot to publish course announcements. Not so much for y’all, but more so I remember. If you see any announcements that don’t say “Spring 2024” there’s a good chance it’s leftover from earlier course offerings.\n\n\nA careful read of our syllabus under “class participation” will show that I do give extra credit for answering questions and (mainly) sharing completed R coding tasks. That is, we’ll walk through some examples, and when we hit a “try it”, I’ll ask you to give it a go. Then, after a few minutes, I’ll ask if anyone wants to share their answer. You get one point of participation extra credit. Five points is worth 1% of a grade boost, so these aren’t negligible points.\n\n\n\nThe Assignments page has all of our weekly lab assignments (including Week 1, due on Monday at 11:59pm). The assignments often have a preamble and some code that has to be used to set you up for the questions. The questions to be completed and turned in are under “Exercises” at the very end.\n\n\n\nI don’t like to use the DM feature of Slack. Not because I don’t like getting DMs, but because the point of Slack is so we can all learn together. If you have a question, even if you’re worried it’s a silly question, then others will, I promise, have the same question. Asking in the public channel will answer everyone’s question at once. Related to this: you should check the Slack to see if your question has already been asked.\nAnother reason is that the TA will often be faster at responding than I will be, depending on day of week and time of day (I start my day before some of you even go to bed). If you DM me, the TA can’t see it and can’t reply."
  },
  {
    "objectID": "content/Week_01/01b.html#installing-r-and-r-studio-posit-and-review-resources",
    "href": "content/Week_01/01b.html#installing-r-and-r-studio-posit-and-review-resources",
    "title": "Working with R and RStudio",
    "section": "Installing R and R Studio Posit and Review Resources",
    "text": "Installing R and R Studio Posit and Review Resources\nBoth R and RStudio Posit are free and available online. If you have not yet done so, you’ll need to install both R and RStudio Posit. See the Installing page of our course resources for instructions. This will be part of your assignment for this week.\nProfessor Kirkpatrick (the other instructor for this course) has created a video walkthrough for the basics of using R for another course, but it is useful here. You can see part A here (labeled “Part 2a”) here ] and part B here (labeled “Part 2b”) . You should already be at this level of familiarity with R, but if you need a review, this is a good place to start."
  },
  {
    "objectID": "content/Week_01/01b.html#the-very-basics-of-r",
    "href": "content/Week_01/01b.html#the-very-basics-of-r",
    "title": "Working with R and RStudio",
    "section": "The (very) basics of R",
    "text": "The (very) basics of R\nBefore we get started with the motivating dataset, we need to cover the very basics of R.\n\nObjects\nSuppose a relatively math unsavvy student asks us for help solving several quadratic equations of the form \\(ax^2+bx+c = 0\\). You—a savvy student—recall that the quadratic formula gives us the solutions:\n\\[\n  \\frac{-b + \\sqrt{b^2 - 4ac}}{2a}\\,\\, \\mbox{ and } \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\n\\]\nwhich of course depend on the values of \\(a\\), \\(b\\), and \\(c\\). That is, the quadratic equation represents a function with three arguments.\nOne advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve \\(x^2 + x -1 = 0\\), then we define:\n\na <- 1\nb <- 1\nc <- -1\n\nwhich stores the values for later use. We use <- to assign values to the variables.\nWe can also assign values using = instead of <-, but some recommend against using = to avoid confusion.4\n\n\n\n\n\n\nTRY IT\n\n\n\nCopy and paste the code above into your console to define the three variables. Note that R does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message. Throughout these written notes, you’ll have the most success if you continue to copy code into your own console.\n\n\nTo see the value stored in a variable, we simply ask R to evaluate a and it shows the stored value:\n\na\n\n[1] 1\n\n\nA more explicit way to ask R to show us the value stored in a is using print like this:\n\nprint(a)\n\n[1] 1\n\n\nWe use the term object to describe stuff that is stored in R. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.\n\n\nThe workspace\nAs we define objects in the console, we are actually changing the workspace. You can see all the variables saved in your workspace by typing:\n\nls()\n\n[1] \"a\"      \"b\"      \"c\"      \"filter\"\n\n\n(Note that one of my variables listed above comes from generating the graphs above). In RStudio Posit, the Environment tab shows the values:\n\nWe should see a, b, and c. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type x you will receive the following message: Error: object 'x' not found.\nNow since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:\n\n(-b + sqrt(b^2 - 4*a*c) ) / ( 2*a )\n\n[1] 0.618034\n\n(-b - sqrt(b^2 - 4*a*c) ) / ( 2*a )\n\n[1] -1.618034\n\n\n\n\nFunctions\nOnce you define variables, the data analysis process can usually be described as a series of functions applied to the data. R includes several zillion predefined functions and most of the analysis pipelines we construct make extensive use of the built-in functions. But R’s power comes from its scalability. We have access to (nearly) infinite functions via install.packages and library. As we go through the course, we will carefully note new functions we bring to each problem. For now, though, we will stick to the basics.\nNote that you’ve used a function already: you used the function sqrt to solve the quadratic equation above. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.\nIn general, we need to use parentheses to evaluate a function. If you type ls, the function is not evaluated and instead R shows you the code that defines the function. If you type ls() the function is evaluated and, as seen above, we see objects in the workspace.\nUnlike ls, most functions require one or more arguments. Below is an example of how we assign an object to the argument of the function log. Remember that we earlier defined a to be 1:\n\nlog(8)\n\n[1] 2.079442\n\nlog(a)\n\n[1] 0\n\n\nYou can find out what the function expects and what it does by reviewing the very useful manuals included in R. You can get help by using the help function like this:\n\nhelp(\"log\")\n\nFor most functions, we can also use this shorthand:\n\n?log\n\nThe help page will show you what arguments the function is expecting. For example, log needs x and base to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with =. Defining these is optional.5 For example, the base of the function log defaults to base = exp(1)—that is, log evaluates the natural log by default.\nIf you want a quick look at the arguments without opening the help system, you can type:\n\nargs(log)\n\nfunction (x, base = exp(1)) \nNULL\n\n\nYou can change the default values by simply assigning another object:\n\nlog(8, base = 2)\n\n[1] 3\n\n\nNote that we have not been specifying the argument x as such:\n\nlog(x = 8, base = 2)\n\n[1] 3\n\n\nThe above code works, but we can save ourselves some typing: if no argument name is used, R assumes you are entering arguments in the order shown in the help file or by args. So by not using the names, it assumes the arguments are x followed by base:\n\nlog(8,2)\n\n[1] 3\n\n\nIf using the arguments’ names, then we can include them in whatever order we want:\n\nlog(base = 2, x = 8)\n\n[1] 3\n\n\nTo specify arguments, we must use =, and cannot use <-.\nThere are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:\n\n2 ^ 3\n\n[1] 8\n\n\nYou can see the arithmetic operators by typing:\n\nhelp(\"+\")\n\nor\n\n?\"+\"\n\nand the relational operators by typing:\n\nhelp(\">\")\n\nor\n\n?\">\"\n\n\n\nOther prebuilt objects\nThere are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:\n\ndata()\n\nThis shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:\n\nco2\n\nR will show you Mauna Loa atmospheric \\(CO^2\\) concentration data.\nOther prebuilt objects are mathematical quantities, such as the constant \\(\\pi\\) and \\(\\infty\\):\n\npi\n\n[1] 3.141593\n\nInf+1\n\n[1] Inf\n\n\n\n\nVariable names\nWe have used the letters a, b, and c as variable names, but variable names can be almost anything. Some basic rules in R are that variable names have to start with a letter, can’t contain spaces, and should not be variables that are predefined in R. For example, don’t name one of your variables install.packages by typing something like install.packages <- 2. Usually, R is smart enough to prevent you from doing such nonsense, but it’s important to develop good habits.\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:\n\nsolution_1 <- (-b + sqrt(b^2 - 4*a*c)) / (2*a)\nsolution_2 <- (-b - sqrt(b^2 - 4*a*c)) / (2*a)\n\nFor more advice, we highly recommend studying (Hadley Wickham’s style guide)[http://adv-r.had.co.nz/Style.html].\n\n\nSaving your workspace\nValues remain in the workspace until you end your session or erase them with the function rm. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.\nWe actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function save or save.image. To load, use the function load. When saving a workspace, we recommend the suffix rda or RData. In RStudio, you can also do this by navigating to the Session tab and choosing Save Workspace as. You can later load it using the Load Workspace options in the same tab. You can read the help pages on save, save.image, and load to learn more.\n\n\nMotivating scripts\nTo solve another equation such as \\(3x^2 + 2x -1\\), we can copy and paste the code above and then redefine the variables and recompute the solution:\n\na <- 3\nb <- 2\nc <- -1\n(-b + sqrt(b^2 - 4*a*c)) / (2*a)\n(-b - sqrt(b^2 - 4*a*c)) / (2*a)\n\nBy creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.\nThe answer you get from the 4th and 5th lines will depend on the values of a, b, and c. If you were to type new numbers directly into your console: c = 5.33 then re-run the last two lines, you will get a different answer. Your R “environment” is affected by what is run from a script and by what you type in the console. It is good (and necessary) practice to write all your code in a script (or in your Rmarkdown document), and run from the script. Always. Periodically running a script fresh from the start (clearing everything out of the environment first) is a good idea as well.\n\n\nCommenting your code\nIf a line of R code starts with the symbol #, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:\n\n## Code to compute solution to quadratic equation of the form ax^2 + bx + c\n## define the variables\na <- 3\nb <- 2\nc <- -1\n\n## now compute the solution\n(-b + sqrt(b^2 - 4*a*c)) / (2*a)\n(-b - sqrt(b^2 - 4*a*c)) / (2*a)\n\n\nTRY IT\n\nWhat is the sum of the first 100 positive integers? The formula for the sum of integers \\(1\\) through \\(n\\) is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum?\nNow use the same formula to compute the sum of the integers from 1 through 1,000.\nLook at the result of typing the following code into R:\n\n\nn <- 1000\nx <- seq(1, n)\nsum(x)\n\nBased on the result, what do you think the functions seq and sum do? You can use help.\n\nsum creates a list of numbers and seq adds them up.\nseq creates a list of numbers and sum adds them up.\nseq creates a random list and sum computes the sum of 1 through 1,000.\nsum always returns the same number.\n\n\nIn math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.\nWhich of the following will always return the numeric value stored in x? You can try out examples and use the help system if you want.\n\n\nlog(10^x)\nlog10(x^10)\nlog(exp(x))\nexp(log(x, base = 2))\n\n\n## Data types\nVariables in R can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function class helps us determine what type of object we have:\n\na <- 2\nclass(a)\n\n[1] \"numeric\"\n\n\nTo work efficiently in R, it is important to learn the different types of variables and what we can do with these.\n\n\nData frames\nUp to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in R is in a data frame. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.\nA large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the dslabs library and loading the murders dataset using the data function:\n\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.3.3\n\n\n\nAttaching package: 'dslabs'\n\n\nThe following object is masked from 'package:gapminder':\n\n    gapminder\n\ndata(murders)\n\nTo see that this is in fact a data frame, we type:\n\nclass(murders)\n\n[1] \"data.frame\"\n\n\n\n\nExamining an object\nThe function str is useful for finding out more about the structure of an object:\n\n\n\n\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n$ state : chr \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n$ abb : chr \"AL\" \"AK\" \"AZ\" \"AR\" ...\n$ region : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n$ population: num 4779736 710231 6392017 2915918 37253956 ...\n$ total : num 135 19 232 93 1257 ...\n\n\n\n\n\nThis tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function head:\n\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nIn this dataset, each state is considered an observation and five variables are reported for each state.\nBefore we go any further in answering our original question about different states, let’s learn more about the components of this object.\n\n\nThe accessor: $\nFor our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator $ in the following way:\n\nmurders$population\n\n [1]  4779736   710231  6392017  2915918 37253956  5029196  3574097   897934\n [9]   601723 19687653  9920000  1360301  1567582 12830632  6483802  3046355\n[17]  2853118  4339367  4533372  1328361  5773552  6547629  9883640  5303925\n[25]  2967297  5988927   989415  1826341  2700551  1316470  8791894  2059179\n[33] 19378102  9535483   672591 11536504  3751351  3831074 12702379  1052567\n[41]  4625364   814180  6346105 25145561  2763885   625741  8001024  6724540\n[49]  1852994  5686986   563626\n\n\nBut how did we know to use population? Previously, by applying the function str to the object murders, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:\n\nnames(murders)\n\n[1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"     \n\n\nIt is important to know that the order of the entries in murders$population preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.\nTip: R comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing murders$p then hitting the tab key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.\n\n\nVectors: numerics, characters, and logical\nThe object murders$population is not one number but several. We call these types of objects vectors. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function length tells you how many entries are in the vector:\n\npop <- murders$population\nlength(pop)\n\n[1] 51\n\n\nThis particular vector is numeric since population sizes are numbers:\n\nclass(pop)\n\n[1] \"numeric\"\n\n\nIn a numeric vector, every entry must be a number.\nTo store character strings, vectors can also be of class character. For example, the state names are characters:\n\nclass(murders$state)\n\n[1] \"character\"\n\n\nAs with numeric vectors, all entries in a character vector need to be a character.\nAnother important type of vectors are logical vectors. These must be either TRUE or FALSE.\n\nz <- 3 == 2\nz\n\n[1] FALSE\n\nclass(z)\n\n[1] \"logical\"\n\n\nHere the == is a relational operator asking if 3 is equal to 2. In R, if you just use one =, you actually assign a variable, but if you use two == you test for equality. Yet another reason to avoid assigning via =… it can get confusing and typos can really mess things up.\nYou can see the other relational operators by typing:\n\n?Comparison\n\nIn future sections, you will see how useful relational operators can be.\nWe discuss more important features of vectors after the next set of exercises.\nAdvanced: Mathematically, the values in pop are integers and there is an integer class in R. However, by default, numbers are assigned class numeric even when they are round integers. For example, class(1) returns numeric. You can turn them into class integer with the as.integer() function or by adding an L like this: 1L. Note the class by typing: class(1L)\n\n\nFactors\nIn the murders dataset, we might expect the region to also be a character vector. However, it is not:\n\nclass(murders$region)\n\n[1] \"factor\"\n\n\nIt is a factor. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the levels function:\n\nlevels(murders$region)\n\n[1] \"Northeast\"     \"South\"         \"North Central\" \"West\"         \n\n\nIn the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters. It is also useful for computational reasons we’ll explore later.\nNote that the levels have an order that is different from the order of appearance in the factor object. The default in R is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the levels argument when creating the factor with the factor function. For example, in the murders dataset regions are ordered from east to west. The function reorder lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.\nSuppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the reorder and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.\n\nregion <- murders$region\nvalue <- murders$total\nregion <- reorder(region, value, FUN = sum)\nlevels(region)\n\n[1] \"Northeast\"     \"North Central\" \"West\"          \"South\"        \n\n\nThe new order is in agreement with the fact that the Northeast has the least murders and the South has the most.\nWarning: Factors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.\n\n\nLists\nData frames are a special case of lists. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you:\n\n\n\n\nrecord\n\n$name\n[1] \"John Doe\"\n\n$student_id\n[1] 1234\n\n$grades\n[1] 95 82 91 97 93\n\n$final_grade\n[1] \"A\"\n\nclass(record)\n\n[1] \"list\"\n\n\nAs with data frames, you can extract the components of a list with the accessor $. In fact, data frames are a type of list.\n\nrecord$student_id\n\n[1] 1234\n\n\nWe can also use double square brackets ([[) like this:\n\nrecord[[\"student_id\"]]\n\n[1] 1234\n\n\nYou should get used to the fact that in R there are often several ways to do the same thing. such as accessing entries.6\nYou might also encounter lists without variable names.\n\n\n\n\nrecord2\n\n[[1]]\n[1] \"John Doe\"\n\n[[2]]\n[1] 1234\n\n\nIf a list does not have names, you cannot extract the elements with $, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:\n\nrecord2[[1]]\n\n[1] \"John Doe\"\n\n\nWe won’t be using lists until later, but you might encounter one in your own exploration of R. For this reason, we show you some basics here.\n\n\nMatrices\nMatrices are another type of object that are common in R. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.\nYet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this class, but much of what happens in the background when you perform a data analysis involves matrices. We describe them briefly here since some of the functions we will learn return matrices.\nWe can define a matrix using the matrix function. We need to specify the number of rows and columns.\n\nmat <- matrix(1:12, 4, 3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\nYou can access specific entries in a matrix using square brackets ([). If you want the second row, third column, you use:\n\nmat[2, 3]\n\n[1] 10\n\n\nIf you want the entire second row, you leave the column spot empty:\n\nmat[2, ]\n\n[1]  2  6 10\n\n\nNotice that this returns a vector, not a matrix.\nSimilarly, if you want the entire third column, you leave the row spot empty:\n\nmat[, 3]\n\n[1]  9 10 11 12\n\n\nThis is also a vector, not a matrix.\nYou can access more than one column or more than one row if you like. This will give you a new matrix.\n\nmat[, 2:3]\n\n     [,1] [,2]\n[1,]    5    9\n[2,]    6   10\n[3,]    7   11\n[4,]    8   12\n\n\nYou can subset both rows and columns:\n\nmat[1:2, 2:3]\n\n     [,1] [,2]\n[1,]    5    9\n[2,]    6   10\n\n\nWe can convert matrices into data frames using the function as.data.frame:\n\nas.data.frame(mat)\n\n  V1 V2 V3\n1  1  5  9\n2  2  6 10\n3  3  7 11\n4  4  8 12\n\n\nYou can also use single square brackets ([) to access rows and columns of a data frame:\n\ndata(\"murders\")\nmurders[25, 1]\n\n[1] \"Mississippi\"\n\nmurders[2:3, ]\n\n    state abb region population total\n2  Alaska  AK   West     710231    19\n3 Arizona  AZ   West    6392017   232\n\n\n\nTRY IT\n\nLoad the US murders dataset.\n\n\nlibrary(dslabs)\ndata(murders)\n\nUse the function str to examine the structure of the murders object. Which of the following best describes the variables represented in this data frame?\n\nThe 51 states.\nThe murder rates for all 50 states and DC.\nThe state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010.\nstr shows no relevant information.\n\n\nWhat are the column names used by the data frame for these five variables?\nUse the accessor $ to extract the state abbreviations and assign them to the object a. What is the class of this object?\nNow use the square brackets to extract the state abbreviations and assign them to the object b. Use the identical function to determine if a and b are the same.\nWe saw that the region column stores a factor. You can corroborate this by typing:\n\n\nclass(murders$region)\n\nWith one line of code, use the function levels and length to determine the number of regions defined by this dataset.\n\nThe function table takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region."
  },
  {
    "objectID": "content/Week_01/01b.html#vectors",
    "href": "content/Week_01/01b.html#vectors",
    "title": "Working with R and RStudio",
    "section": "Vectors",
    "text": "Vectors\nIn R, the most basic objects available to store data are vectors. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.\n\nCreating vectors\nWe can create vectors using the function c, which stands for concatenate. We use c to concatenate entries in the following way:\n\ncodes <- c(380, 124, 818)\ncodes\n\n[1] 380 124 818\n\n\nWe can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.\n\ncountry <- c(\"italy\", \"canada\", \"egypt\")\n\nIn R you can also use single quotes:\n\ncountry <- c('italy', 'canada', 'egypt')\n\nBut be careful not to confuse the single quote ’ with the back quote, which shares a keyboard key with ~.\nBy now you should know that if you type:\n\ncountry <- c(italy, canada, egypt)\n\nyou receive an error because the variables italy, canada, and egypt are not defined. If we do not use the quotes, R looks for variables with those names and returns an error.\n\n\nNames\nSometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:\n\ncodes <- c(italy = 380, canada = 124, egypt = 818)\ncodes\n\n italy canada  egypt \n   380    124    818 \n\n\nThe object codes continues to be a numeric vector:\n\nclass(codes)\n\n[1] \"numeric\"\n\n\nbut with names:\n\nnames(codes)\n\n[1] \"italy\"  \"canada\" \"egypt\" \n\n\nIf the use of strings without quotes looks confusing, know that you can use the quotes as well:\n\ncodes <- c(\"italy\" = 380, \"canada\" = 124, \"egypt\" = 818)\ncodes\n\n italy canada  egypt \n   380    124    818 \n\n\nThere is no difference between this function call and the previous one. This is one of the many ways in which R is quirky compared to other languages.\n\n\nSequences\nAnother useful function for creating vectors generates sequences:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nThe first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:\n\nseq(1, 10, 2)\n\n[1] 1 3 5 7 9\n\n\nIf we want consecutive integers, we can use the following shorthand:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWhen we use these functions, R produces integers, not numerics, because they are typically used to index something:\n\nclass(1:10)\n\n[1] \"integer\"\n\n\nHowever, if we create a sequence including non-integers, the class changes:\n\nclass(seq(1, 10, 0.5))\n\n[1] \"numeric\"\n\n\n\n\nSubsetting\nWe use square brackets to access specific elements of a vector. For the vector codes we defined above, we can access the second element using:\n\ncodes[2]\n\ncanada \n   124 \n\n\nYou can get more than one entry by using a multi-entry vector as an index:\n\ncodes[c(1,3)]\n\nitaly egypt \n  380   818 \n\n\nThe sequences defined above are particularly useful if we want to access, say, the first two elements:\n\ncodes[1:2]\n\n italy canada \n   380    124 \n\n\nIf the elements have names, we can also access the entries using these names. Below are two examples.\n\ncodes[\"canada\"]\n\ncanada \n   124 \n\ncodes[c(\"egypt\",\"italy\")]\n\negypt italy \n  818   380"
  },
  {
    "objectID": "content/Week_01/01b.html#coercion",
    "href": "content/Week_01/01b.html#coercion",
    "title": "Working with R and RStudio",
    "section": "Coercion",
    "text": "Coercion\nIn general, coercion is an attempt by R to be flexible with data types. When an entry does not match the expected, some of the prebuilt R functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand coercion can drive programmers crazy when attempting to code in R since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples.\nWe said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:\n\nx <- c(1, \"canada\", 3)\n\nBut we don’t get one, not even a warning! What happened? Look at x and its class:\n\nx\n\n[1] \"1\"      \"canada\" \"3\"     \n\nclass(x)\n\n[1] \"character\"\n\n\nR coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings \"1\" and “3”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in R.\nR also offers functions to change from one type to another. For example, you can turn numbers into characters with:\n\nx <- 1:5\ny <- as.character(x)\ny\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\nYou can turn it back with as.numeric:\n\nas.numeric(y)\n\n[1] 1 2 3 4 5\n\n\nThis function is actually quite useful since datasets that include numbers as character strings are common.\n\nNot availables (NA)\nThis “topic” seems to be wholly unappreciated and it has been our experience that students often panic when encountering an NA. This often happens when a function tries to coerce one type to another and encounters an impossible case. In such circumstances, R usually gives us a warning and turns the entry into a special value called an NA (for “not available”). For example:\n\nx <- c(\"1\", \"b\", \"3\")\nas.numeric(x)\n\nWarning: NAs introduced by coercion\n\n\n[1]  1 NA  3\n\n\nR does not have any guesses for what number you want when you type b, so it does not try.\nWhile coercion is a common case leading to NAs, you’ll see them in nearly every real-world dataset. Most often, you will encounter the NAs as a stand-in for missing data. Again, this a common problem in real-world datasets and you need to be aware that it will come up."
  },
  {
    "objectID": "content/Week_01/01b.html#sorting",
    "href": "content/Week_01/01b.html#sorting",
    "title": "Working with R and RStudio",
    "section": "Sorting",
    "text": "Sorting\nNow that we have mastered some basic R knowledge (ha!), let’s try to gain some insights into the safety of different states in the context of gun murders.\n\nsort\nSay we want to rank the states from least to most gun murders. The function sort sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:\n\nlibrary(dslabs)\ndata(murders)\nsort(murders$total)\n\n [1]    2    4    5    5    7    8   11   12   12   16   19   21   22   27   32\n[16]   36   38   53   63   65   67   84   93   93   97   97   99  111  116  118\n[31]  120  135  142  207  219  232  246  250  286  293  310  321  351  364  376\n[46]  413  457  517  669  805 1257\n\n\nHowever, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257.\n\n\norder\nThe function order is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it:\n\nx <- c(31, 4, 15, 92, 65)\nsort(x)\n\n[1]  4 15 31 65 92\n\n\nRather than sort the input vector, the function order returns the index that sorts input vector:\n\nindex <- order(x)\nx[index]\n\n[1]  4 15 31 65 92\n\n\nThis is the same output as that returned by sort(x). If we look at this index, we see why it works:\n\nx\n\n[1] 31  4 15 92 65\n\norder(x)\n\n[1] 2 3 1 5 4\n\n\nThe second entry of x is the smallest, so order(x) starts with 2. The next smallest is the third entry, so the second entry is 3 and so on.\nHow does this help us order the states by murders? First, remember that the entries of vectors you access with $ follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:\n\nmurders$state[1:6]\n\n[1] \"Alabama\"    \"Alaska\"     \"Arizona\"    \"Arkansas\"   \"California\"\n[6] \"Colorado\"  \n\nmurders$abb[1:6]\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\"\n\n\nThis means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:\n\nind <- order(murders$total)\nmurders$abb[ind]\n\n [1] \"VT\" \"ND\" \"NH\" \"WY\" \"HI\" \"SD\" \"ME\" \"ID\" \"MT\" \"RI\" \"AK\" \"IA\" \"UT\" \"WV\" \"NE\"\n[16] \"OR\" \"DE\" \"MN\" \"KS\" \"CO\" \"NM\" \"NV\" \"AR\" \"WA\" \"CT\" \"WI\" \"DC\" \"OK\" \"KY\" \"MA\"\n[31] \"MS\" \"AL\" \"IN\" \"SC\" \"TN\" \"AZ\" \"NJ\" \"VA\" \"NC\" \"MD\" \"OH\" \"MO\" \"LA\" \"IL\" \"GA\"\n[46] \"MI\" \"PA\" \"NY\" \"FL\" \"TX\" \"CA\"\n\n\nAccording to the above, California had the most murders.\n\n\nmax and which.max\nIf we are only interested in the entry with the largest value, we can use max for the value:\n\nmax(murders$total)\n\n[1] 1257\n\n\nand which.max for the index of the largest value:\n\ni_max <- which.max(murders$total)\nmurders$state[i_max]\n\n[1] \"California\"\n\n\nFor the minimum, we can use min and which.min in the same way.\nDoes this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: rank.\n\n\nrank\nAlthough not as frequently used as order and sort, the function rank is also related to order and can be useful. For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:\n\nx <- c(31, 4, 15, 92, 65)\nrank(x)\n\n[1] 3 1 2 5 4\n\n\nTo summarize, let’s look at the results of the three functions we have introduced:\n\n\n\n\n \n  \n    original \n    sort \n    order \n    rank \n  \n \n\n  \n    31 \n    4 \n    2 \n    3 \n  \n  \n    4 \n    15 \n    3 \n    1 \n  \n  \n    15 \n    31 \n    1 \n    2 \n  \n  \n    92 \n    65 \n    5 \n    5 \n  \n  \n    65 \n    92 \n    4 \n    4 \n  \n\n\n\n\n\n\n\nBeware of recycling\nAnother common source of unnoticed errors in R is the use of recycling. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens:\n\nx <- c(1,2,3)\ny <- c(10, 20, 30, 40, 50, 60, 70)\nx+y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 11 22 33 41 52 63 71\n\n\nWe do get a warning, but no error. For the output, R has recycled the numbers in x. Notice the last digit of numbers in the output.\n\nTRY IT\nFor these exercises we will use the US murders dataset. Make sure you load it prior to starting.\n\nlibrary(dslabs)\ndata(\"murders\")\n\n\nUse the $ operator to access the population size data and store it as the object pop. Then use the sort function to redefine pop so that it is sorted. Finally, use the [ operator to report the smallest population size.\nNow instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use order instead of sort.\nWe can actually perform the same operation as in the previous exercise using the function which.min. Write one line of code that does this.\nNow we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable states to be the state names from the murders data frame. Report the name of the state with the smallest population.\nYou can create a data frame using the data.frame function. Here is a quick example:\n\n\ntemp <- c(35, 88, 42, 84, 81, 30)\ncity <- c(\"Beijing\", \"Lagos\", \"Paris\", \"Rio de Janeiro\",\n          \"San Juan\", \"Toronto\")\ncity_temps <- data.frame(name = city, temperature = temp)\n\nUse the rank function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called ranks, then create a data frame with the state name and its rank. Call the data frame my_df.\n\nRepeat the previous exercise, but this time order my_df so that the states are ordered from least populous to most populous. Hint: create an object ind that stores the indexes needed to order the population values. Then use the bracket operator [ to re-order each column in the data frame.\nThe na_example vector represents a series of counts. You can quickly examine the object using:\n\n\ndata(\"na_example\")\nstr(na_example)\n\n int [1:1000] 2 1 3 2 1 3 1 4 3 2 ...\n\n\nHowever, when we compute the average with the function mean, we obtain an NA:\n\nmean(na_example)\n\n[1] NA\n\n\nThe is.na function returns a logical vector that tells us which entries are NA. Assign this logical vector to an object called ind and determine how many NAs does na_example have.\n\nNow compute the average again, but only for the entries that are not NA. Hint: remember the ! operator."
  },
  {
    "objectID": "content/Week_01/01b.html#vector-arithmetics",
    "href": "content/Week_01/01b.html#vector-arithmetics",
    "title": "Working with R and RStudio",
    "section": "Vector arithmetics",
    "text": "Vector arithmetics\nCalifornia had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:\n\nlibrary(dslabs)\ndata(\"murders\")\nmurders$state[which.max(murders$population)]\n\n[1] \"California\"\n\n\nwith over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of R come in handy.\n\nRescaling a vector\nIn R, arithmetic operations on vectors occur element-wise. For a quick example, suppose we have height in inches:\n\ninches <- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\n\nand want to convert to centimeters. Notice what happens when we multiply inches by 2.54:\n\ninches * 2.54\n\n [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18 177.80\n\n\nIn the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:\n\ninches - 69\n\n [1]  0 -7 -3  1  1  4 -2  4 -2  1\n\n\n\n\nTwo vectors\nIf we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:\n\\[\n  \\begin{pmatrix}\na\\\\\nb\\\\\nc\\\\\nd\n\\end{pmatrix}\n+\n  \\begin{pmatrix}\ne\\\\\nf\\\\\ng\\\\\nh\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\na +e\\\\\nb + f\\\\\nc + g\\\\\nd + h\n\\end{pmatrix}\n\\]\nThe same holds for other mathematical operations, such as -, * and /.\nThis implies that to compute the murder rates we can simply type:\n\nmurder_rate <- murders$total / murders$population * 100000\n\nOnce we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:\n\nmurders$abb[order(murder_rate)]\n\n [1] \"VT\" \"NH\" \"HI\" \"ND\" \"IA\" \"ID\" \"UT\" \"ME\" \"WY\" \"OR\" \"SD\" \"MN\" \"MT\" \"CO\" \"WA\"\n[16] \"WV\" \"RI\" \"WI\" \"NE\" \"MA\" \"IN\" \"KS\" \"NY\" \"KY\" \"AK\" \"OH\" \"CT\" \"NJ\" \"AL\" \"IL\"\n[31] \"OK\" \"NC\" \"NV\" \"VA\" \"AR\" \"TX\" \"NM\" \"CA\" \"FL\" \"TN\" \"PA\" \"AZ\" \"GA\" \"MS\" \"MI\"\n[46] \"DE\" \"SC\" \"MD\" \"MO\" \"LA\" \"DC\"\n\n\n\nTRY IT\n\nPreviously we created this data frame:\n\n\ntemp <- c(35, 88, 42, 84, 81, 30)\ncity <- c(\"Beijing\", \"Lagos\", \"Paris\", \"Rio de Janeiro\",\n          \"San Juan\", \"Toronto\")\ncity_temps <- data.frame(name = city, temperature = temp)\n\nRemake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is \\(C = \\frac{5}{9} \\times (F - 32)\\).\n\nWrite code to compute the following sum \\(1+1/2^2 + 1/3^2 + \\dots 1/100^2\\)? Hint: thanks to Euler, we know it should be close to \\(\\pi^2/6\\).\nCompute the per 100,000 murder rate for each state and store it in the object murder_rate. Then compute the average murder rate for the US using the function mean. What is the average?"
  },
  {
    "objectID": "content/Week_01/01b.html#indexing",
    "href": "content/Week_01/01b.html#indexing",
    "title": "Working with R and RStudio",
    "section": "Indexing",
    "text": "Indexing\nIndexing is a boring name for an important tool. R provides a powerful and convenient way of referencing specific elements of vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this:\n\nlibrary(dslabs)\ndata(\"murders\")\n\n\nSubsetting with logicals\nWe have now calculated the murder rate using:\n\nmurder_rate <- murders$total / murders$population * 100000\n\nImagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of R is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:\n\nind <- murder_rate < 0.71\n\nIf we instead want to know if a value is less or equal, we can use:\n\nind <- murder_rate <= 0.71\n\nNote that we get back a logical vector with TRUE for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.\n\nmurders$state[ind]\n\n[1] \"Hawaii\"        \"Iowa\"          \"New Hampshire\" \"North Dakota\" \n[5] \"Vermont\"      \n\n\nIn order to count how many are TRUE, the function sum returns the sum of the entries of a vector and logical vectors get coerced to numeric with TRUE coded as 1 and FALSE as 0. Thus we can count the states using:\n\nsum(ind)\n\n[1] 5\n\n\n\n\nLogical operators\nSuppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator and, which in R is represented with &. This operation results in TRUE only when both logicals are TRUE. To see this, consider this example:\n\nTRUE & TRUE\n\n[1] TRUE\n\nTRUE & FALSE\n\n[1] FALSE\n\nFALSE & FALSE\n\n[1] FALSE\n\n\nFor our example, we can form two logicals:\n\nwest <- murders$region == \"West\"\nsafe <- murder_rate <= 1\n\nand we can use the & to get a vector of logicals that tells us which states satisfy both conditions:\n\nind <- safe & west\nmurders$state[ind]\n\n[1] \"Hawaii\"  \"Idaho\"   \"Oregon\"  \"Utah\"    \"Wyoming\"\n\n\n\n\nwhich\nSuppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type:\n\nind <- which(murders$state == \"California\")\nmurder_rate[ind]\n\n[1] 3.374138\n\n\n\n\n%in%\nIf rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let’s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:\n\nc(\"Boston\", \"Dakota\", \"Washington\") %in% murders$state\n\n[1] FALSE FALSE  TRUE\n\n\nNote that we will be using %in% often throughout the course\n\nTRY IT\nStart by loading the library and data.\n\nlibrary(dslabs)\ndata(murders)\n\n\nCompute the per 100,000 murder rate for each state and store it in an object called murder_rate. Then use logical operators to create a logical vector named low that tells us which entries of murder_rate are lower than 1.\nNow use the results from the previous exercise and the function which to determine the indices of murder_rate associated with values lower than 1.\nUse the results from the previous exercise to report the names of the states with murder rates lower than 1.\nNow extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector low and the logical operator &.\nIn a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?\nUse the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of murders$abb that match the three abbreviations, then use the [ operator to extract the states.\nUse the %in% operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\nExtend the code you used in exercise 7 to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index."
  },
  {
    "objectID": "content/Week_01/01b.html#further-help-with-r",
    "href": "content/Week_01/01b.html#further-help-with-r",
    "title": "Working with R and RStudio",
    "section": "Further help with R",
    "text": "Further help with R\nIf you are not comfortable with R, the earlier you seek out help, the better. Quietly letting the course pass by you because you don’t know how to fix an error will do nobody any good. Attend TA office hours or attend TA or Prof. K’s office hours see Syllabus for times and Zoom links. Also, join the course Slack (see the front page of our course website for a link) and post questions there.\nFinally, there are also primers on Rstudio.cloud that can be useful. There are many ways we can help you get used to R, but only if you reach out."
  },
  {
    "objectID": "content/Week_02/02a.html",
    "href": "content/Week_02/02a.html",
    "title": "Introduction to the tidyverse",
    "section": "",
    "text": "This page.\nChapter 1 of Introduction to Statistical Learning, available here.\nOptional: The “Tidy Your Data” tutorial on Rstudio Cloud Primers"
  },
  {
    "objectID": "content/Week_02/02a.html#some-reminders",
    "href": "content/Week_02/02a.html#some-reminders",
    "title": "Introduction to the tidyverse",
    "section": "Some Reminders:",
    "text": "Some Reminders:\n\nStart labs early!\n\nThey are not trivial.\nThey are not short.\nThey are not easy.\nThey are not optional.\n\nYou install.packages(\"packageName\") once on your computer.\n\nAnd never ever ever in your code.\n\nYou load an already-installed package using library(packageName) in a code chunk\n\nNever in your console\nWhen RMarkdown knits, it starts a whole new, empty session that has no knowledge of what you typed into the console\n\nSlack\n\nUse it.\nI would very much prefer posting in the class-visible channels. Others can learn from your issues.\n\nWe have a channel just for labs and R. Please use that one.\n\n\n\n\nGroup Projects\nYour final is a group project. You will also have two “mini” projects. They comprise a large part of your grade. As mentioned last week, this mean that you need to start planning soon.\nTo aid in your planning, here are the required elements of your final project.\n\nYou must find existing data to analyze. Aggregating and merging data from multiple sources is encouraged.\nYou must visualize 3 interesting features of that data.\nYou must come up with some analysis—using tools from this course—which relates your data to either a prediction or a policy conclusion.\nYou must think critically about your analysis and be able to identify potential issues.\nYou must present your analysis as if presenting to a C-suite executive.\n\nYour mini-projects along the way will be more structured, but will serve to guide you towards the final project.\n\n\nTeams\nPlease form teams of 3 people. Once all agree to be on a team, have ONE PERSON email our TA Allen scovelpa@msu.edu and cc all of the members of the team so that nobody is surprised to be included on a team. Title the email [SSC442] - Group Formation. Tell us your team name (be creative), and list in the email the names of all of the team members and their email address (in addition to cc-ing those team members on the email).\nIf you opt to not form a team, you will be automatically added to the “willing to be randomly assigned” pool and will be paired with two others from the “willing to be randomly assigned” pool.\nSend this email by January 20th and we will assign un-teamed folks at the beginning of the following week. Project 1 is due in no time. See schedule for all the important project dates.\n\n\nGuiding Question\nFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\nWhy do we want tidy data?\nWhat are the challenges associated with shaping things into a tidy format?"
  },
  {
    "objectID": "content/Week_02/02a.html#tidy-data",
    "href": "content/Week_02/02a.html#tidy-data",
    "title": "Introduction to the tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\n\nWe say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\n\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nEach row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.\nTo see how the same information can be provided in different formats, consider the following example:\n\nlibrary(dslabs)\ndata(\"gapminder\") # gapminder will now be a data.frame in your \"environment\" (memory)\ntidy_data <- gapminder %>%\n  filter(country %in% c(\"South Korea\", \"Germany\") & !is.na(fertility)) %>%\n  select(country, year, fertility)\nhead(tidy_data, 6)\n\n      country year fertility\n1     Germany 1960      2.41\n2 South Korea 1960      6.16\n3     Germany 1961      2.44\n4 South Korea 1961      5.99\n5     Germany 1962      2.47\n6 South Korea 1962      5.79\n\n\nThis tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the dslabs package. Originally, the data was in the following format:\n\n\n      country 1960 1961 1962\n1     Germany 2.41 2.44 2.47\n2 South Korea 6.16 5.99 5.79\n\n\nThe same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables’ values, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into tidy format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.\nAlthough not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.\n\nTRY IT\n\nExamine the built-in dataset co2. Which of the following is true:\n\n\nco2 is tidy data: it has one year for each row.\nco2 is not tidy: we need at least one column with a character vector.\nco2 is not tidy: it is a matrix instead of a data frame.\nco2 is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.\n\n\nExamine the built-in dataset ChickWeight. Which of the following is true:\n\n\nChickWeight is not tidy: each chick has more than one row.\nChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.\nChickWeight is not tidy: we are missing the year column.\nChickWeight is tidy: it is stored in a data frame.\n\n\nExamine the built-in dataset BOD. Which of the following is true:\n\n\nBOD is not tidy: it only has six rows.\nBOD is not tidy: the first column is just an index.\nBOD is tidy: each row is an observation with two values (time and demand)\nBOD is tidy: all small datasets are tidy by definition.\n\n\nWhich of the following built-in datasets is tidy (you can pick more than one):\n\n\nBJsales\nEuStockMarkets\nDNase\nFormaldehyde\nOrange\nUCBAdmissions"
  },
  {
    "objectID": "content/Week_02/02a.html#manipulating-data-frames",
    "href": "content/Week_02/02a.html#manipulating-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Manipulating data frames",
    "text": "Manipulating data frames\nThe dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.\n\nAdding a column with mutate\nWe want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values. So, to add murder rates, we use:\n\nlibrary(dslabs)\ndata(\"murders\")\nmurders <- mutate(murders, rate = total / population * 100000)\n\nNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error?\nThis is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable.\nWe can see that the new column is added:\n\nhead(murders)\n\n       state abb region population total     rate\n1    Alabama  AL  South    4779736   135 2.824424\n2     Alaska  AK   West     710231    19 2.675186\n3    Arizona  AZ   West    6392017   232 3.629527\n4   Arkansas  AR  South    2915918    93 3.189390\n5 California  CA   West   37253956  1257 3.374138\n6   Colorado  CO   West    5029196    65 1.292453\n\n\nNote: Although we have overwritten the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will overwrite our mutated version.\n\n\nSubsetting with filter\nNow suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the filter function, which takes the data table as the first argument and then the conditional statement as the second. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace.\n\nfilter(murders, rate <= 0.71)\n\n          state abb        region population total      rate\n1        Hawaii  HI          West    1360301     7 0.5145920\n2          Iowa  IA North Central    3046355    21 0.6893484\n3 New Hampshire  NH     Northeast    1316470     5 0.3798036\n4  North Dakota  ND North Central     672591     4 0.5947151\n5       Vermont  VT     Northeast     625741     2 0.3196211\n\n\n\n\nSelecting columns with select\nAlthough our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object:\n\nnew_table <- select(murders, state, region, rate)\nfilter(new_table, rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nIn the call to select, the first argument murders is an object, but state, region, and rate are variable names.\n\nTRY IT\n\nLoad the dplyr package and the murders dataset.\n\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\n\nYou can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted:\n\nmurders <- mutate(murders, population_in_millions = population / 10^6)\n\nWe can write population rather than murders$population because mutate is part of dplyr. The function mutate knows we are grabbing columns from murders.\nUse the function mutate to add a murders column named rate with the per 100,000 murder rate as in the example code above. Make sure you redefine murders as done in the example code above ( murders <- [your code]) so we can keep using this variable.\n\nIf rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\nWith dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes:\n\n\nselect(murders, state, population) %>% head()\n\nUse select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\n\nThe dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this:\n\n\nfilter(murders, state == \"New York\")\n\nYou can use other logical vectors to filter rows.\nUse filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\n\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:\n\n\nno_florida <- filter(murders, state != \"Florida\")\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n\nWe can also use %in% to filter with dplyr. You can therefore see the data from New York and Texas like this:\n\n\nfilter(murders, state %in% c(\"New York\", \"Texas\"))\n\nCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\n\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\n\n\nfilter(murders, population < 5000000 & region == \"Northeast\")\n\nMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate, and the rank."
  },
  {
    "objectID": "content/Week_02/02a.html#the-pipe",
    "href": "content/Week_02/02a.html#the-pipe",
    "title": "Introduction to the tidyverse",
    "section": "The pipe: %>%",
    "text": "The pipe: %>%\nWith dplyr we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %>%. Some details are included below.\nWe wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects:\n\\[ \\mbox{original data }\n\\rightarrow \\mbox{ select }\n\\rightarrow \\mbox{ filter } \\]\nFor such an operation, we can use the pipe %>%. The code looks like this:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nThis line of code is equivalent to the two lines of code above. What is going on here?\nIn general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:\n\n16 %>% sqrt()\n\n[1] 4\n\n\nWe can continue to pipe values along:\n\n16 %>% sqrt() %>% log2()\n\n[1] 2\n\n\nThe above statement is equivalent to log2(sqrt(16)).\nRemember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:\n\n16 %>% sqrt() %>% log(base = 2)\n\n[1] 2\n\n\nTherefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\nmurders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function.\nNote that the pipe works well with functions where the first argument is the input data. Functions in tidyverse packages like dplyr have this format and can be used easily with the pipe. It’s worth noting that as of R 4.1, there is a base-R version of the pipe |>, though this has its disadvantages. We’ll stick with %>% for now.\n\nTRY IT\n\nThe pipe %>% can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.\n\n\nmurders <- mutate(murders, rate =  total / population * 100000,\n                  rank = rank(-rate))\n\nIn the solution to the previous exercise, we did the following:\n\nmy_states <- filter(murders, region %in% c(\"Northeast\", \"West\") &\n                      rate < 1)\n\nselect(my_states, state, rate, rank)\n\nThe pipe %>% permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this:\n\nmutate(murders, rate =  total / population * 100000,\n       rank = rank(-rate)) %>%\n  select(state, rate, rank)\n\nNotice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %>%.\nRepeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %>% to do this in just one line.\n\nReset murders to the original table by using data(murders). Use a pipe to create a new data frame called my_states that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three %>%. The code should look something like this:\n\n\nmy_states <- murders %>%\n  mutate SOMETHING %>%\n  filter SOMETHING %>%\n  select SOMETHING"
  },
  {
    "objectID": "content/Week_02/02a.html#summarizing-data",
    "href": "content/Week_02/02a.html#summarizing-data",
    "title": "Introduction to the tidyverse",
    "section": "Summarizing data",
    "text": "Summarizing data\nAn important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using the pull function.\n\n\n\n\nsummarize\nThe summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The heights dataset includes heights and sex reported by students in an in-class survey.\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(heights)\nhead(heights)\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65\n\n\nThe following code computes the average and standard deviation for females:\n\ns <- heights %>%\n  filter(sex == \"Female\") %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\ns\n\n   average standard_deviation\n1 64.93942           3.760656\n\n\nThis takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same.\nBecause the resulting table stored in s is a data frame, we can access the components with the accessor $:\n\ns$average\n\n[1] 64.93942\n\ns$standard_deviation\n\n[1] 3.760656\n\n\nAs with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(median = median(height), minimum = min(height),\n            maximum = max(height))\n\n    median minimum maximum\n1 64.98031      51      79\n\n\nWe can obtain these three values with just one line using the quantile function: for example, quantile(x, c(0,0.5,1)) returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector x. However, if we attempt to use a function like this that returns two or more values inside summarize:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nwe will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In later sections, we will learn how to deal with functions that return more than one value.\nFor another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column:\n\nmurders <- murders %>% mutate(rate = total/population*100000)\n\nRemember that the US murder rate is not the average of the state murder rates:\n\nsummarize(murders, mean(rate))\n\n  mean(rate)\n1   2.779125\n\n\nThis is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000)\nus_murder_rate\n\n      rate\n1 3.034555\n\n\nThis computation counts larger states proportionally to their size which results in a larger value.\n\n\npull\nThe us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame:\n\nclass(us_murder_rate)\n\n[1] \"data.frame\"\n\n\nsince, as most dplyr functions, summarize always returns a data frame.\nThis might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. To understand what we mean take a look at this line of code:\n\nus_murder_rate %>% pull(rate)\n\n[1] 3.034555\n\n\nThis returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate.\nTo get a number from the original data table with one line of code we can type:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000) %>%\n  pull(rate)\n\nus_murder_rate\n\n[1] 3.034555\n\n\nwhich is now a numeric:\n\nclass(us_murder_rate)\n\n[1] \"numeric\"\n\n\n\n\nGroup then summarize with group_by\nA common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this.\nIf we type this:\n\nheights %>% group_by(sex)\n\n# A tibble: 1,050 × 2\n# Groups:   sex [2]\n   sex    height\n   <fct>   <dbl>\n 1 Male       75\n 2 Male       70\n 3 Male       68\n 4 Male       74\n 5 Male       61\n 6 Female     65\n 7 Female     66\n 8 Female     62\n 9 Female     66\n10 Male       67\n# ℹ 1,040 more rows\n\n\nThe result does not look very different from heights, except we see Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:\n\nheights %>%\n  group_by(sex) %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\n\n# A tibble: 2 × 3\n  sex    average standard_deviation\n  <fct>    <dbl>              <dbl>\n1 Female    64.9               3.76\n2 Male      69.3               3.61\n\n\nThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median murder rate in the four regions of the country:\n\nmurders %>%\n  group_by(region) %>%\n  summarize(median_rate = median(rate))\n\n# A tibble: 4 × 2\n  region        median_rate\n  <fct>               <dbl>\n1 Northeast            1.80\n2 South                3.40\n3 North Central        1.97\n4 West                 1.29"
  },
  {
    "objectID": "content/Week_02/02a.html#sorting-data-frames",
    "href": "content/Week_02/02a.html#sorting-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Sorting data frames",
    "text": "Sorting data frames\nWhen examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size:\n\nmurders %>%\n  arrange(population) %>%\n  head()\n\n                 state abb        region population total       rate\n1              Wyoming  WY          West     563626     5  0.8871131\n2 District of Columbia  DC         South     601723    99 16.4527532\n3              Vermont  VT     Northeast     625741     2  0.3196211\n4         North Dakota  ND North Central     672591     4  0.5947151\n5               Alaska  AK          West     710231    19  2.6751860\n6         South Dakota  SD North Central     814180     8  0.9825837\n\n\nWith arrange we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by rate instead:\n\nmurders %>%\n  arrange(rate) %>%\n  head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102\n\n\nNote that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type:\n\nmurders %>%\n  arrange(desc(rate))\n\n\nNested sorting\nIf we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region, then within region we order by murder rate:\n\nmurders %>%\n  arrange(region, rate) %>%\n  head()\n\n          state abb    region population total      rate\n1       Vermont  VT Northeast     625741     2 0.3196211\n2 New Hampshire  NH Northeast    1316470     5 0.3798036\n3         Maine  ME Northeast    1328361    11 0.8280881\n4  Rhode Island  RI Northeast    1052567    16 1.5200933\n5 Massachusetts  MA Northeast    6547629   118 1.8021791\n6      New York  NY Northeast   19378102   517 2.6679599\n\n\n\n\nThe top \\(n\\)\nIn the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:\n\nmurders %>% top_n(5, rate)\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Maryland  MD         South    5773552   293  5.074866\n4             Missouri  MO North Central    5988927   321  5.359892\n5       South Carolina  SC         South    4625364   207  4.475323\n\n\nNote that rows are not sorted by rate, only filtered. If we want to sort, we need to use arrange. Note that if the third argument is left blank, top_n filters by the last column.\n\nTRY IT\nFor these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\n\nlibrary(NHANES)\n\nWarning: package 'NHANES' was built under R version 4.3.3\n\ndata(NHANES)\n\nThe NHANES data has many missing values. The mean and sd functions in R will return NA if any of the entries of the input vector is an NA. Here is an example:\n\nlibrary(dslabs)\ndata(na_example)\nmean(na_example)\n\n[1] NA\n\nsd(na_example)\n\n[1] NA\n\n\nTo ignore the NAs we can use the na.rm argument:\n\nmean(na_example, na.rm = TRUE)\n\n[1] 2.301754\n\nsd(na_example, na.rm = TRUE)\n\n[1] 1.22338\n\n\nLet’s now explore the NHANES data.\n\nWe will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like ” 20-29”, with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref.\n\nHint: Use filter and summarize and use the na.rm = TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter.\n\nUsing a pipe, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then pull.\nNow report the min and max values for the same group.\nCompute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age and gender, filter by Gender and then use group_by.\nRepeat exercise 4 for males.\nWe can actually combine both summaries for exercises 4 and 5 into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender).\nFor males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure."
  },
  {
    "objectID": "content/Week_02/02a.html#tibbles",
    "href": "content/Week_02/02a.html#tibbles",
    "title": "Introduction to the tidyverse",
    "section": "Tibbles",
    "text": "Tibbles\nTidy data must be stored in data frames. We have been using the murders data frame throughout the unit. In an earlier section we introduced the group_by function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?\n\nmurders %>% group_by(region)\n\n# A tibble: 51 × 6\n# Groups:   region [4]\n   state                abb   region    population total  rate\n   <chr>                <chr> <fct>          <dbl> <dbl> <dbl>\n 1 Alabama              AL    South        4779736   135  2.82\n 2 Alaska               AK    West          710231    19  2.68\n 3 Arizona              AZ    West         6392017   232  3.63\n 4 Arkansas             AR    South        2915918    93  3.19\n 5 California           CA    West        37253956  1257  3.37\n 6 Colorado             CO    West         5029196    65  1.29\n 7 Connecticut          CT    Northeast    3574097    97  2.71\n 8 Delaware             DE    South         897934    38  4.23\n 9 District of Columbia DC    South         601723    99 16.5 \n10 Florida              FL    South       19687653   669  3.40\n# ℹ 41 more rows\n\n\nNotice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble followed by dimensions. We can learn the class of the returned object using:\n\nmurders %>% group_by(region) %>% class()\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe tbl, pronounced tibble, is a special kind of data frame. The functions group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. For consistency, the dplyr manipulation verbs (select, filter, mutate, and arrange) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble.\nTibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.\n\nTibbles display better\nThe print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing murders and the output of murders if we convert it to a tibble. We can do this using as_tibble(murders). If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.\n\n\nSubsets of tibbles are tibbles\nIf you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:\n\nclass(murders[,4])\n\n[1] \"numeric\"\n\n\nis not a data frame. With tibbles this does not happen:\n\nclass(as_tibble(murders)[,4])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThis is useful in the tidyverse since functions require data frames as input.\nWith tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $:\n\nclass(as_tibble(murders)$population)\n\n[1] \"numeric\"\n\n\nA related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write Population instead of population this:\n\nmurders$Population\n\nNULL\n\n\nreturns a NULL with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:\n\nas_tibble(murders)$Population\n\nWarning: Unknown or uninitialised column: `Population`.\n\n\nNULL\n\n\n\n\nTibbles can have complex entries\nWhile data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:\n\ntibble(id = c(1, 2, 3), func = c(mean, median, sd))\n\n# A tibble: 3 × 2\n     id func  \n  <dbl> <list>\n1     1 <fn>  \n2     2 <fn>  \n3     3 <fn>  \n\n\n\n\nTibbles can be grouped\nThe function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information.\n\n\nCreate a tibble using tibble instead of data.frame\nIt is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the tibble function.\n\ngrades <- tibble(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\n\nNote that base R (without packages loaded) has a function with a very similar name, data.frame, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default data.frame coerces characters into factors without providing a warning or message:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo avoid this, we use the rather cumbersome argument stringsAsFactors:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90),\n                     stringsAsFactors = FALSE)\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo convert a regular data frame to a tibble, you can use the as_tibble function.\n\nas_tibble(grades) %>% class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\""
  },
  {
    "objectID": "content/Week_02/02a.html#the-dot-operator",
    "href": "content/Week_02/02a.html#the-dot-operator",
    "title": "Introduction to the tidyverse",
    "section": "The dot operator",
    "text": "The dot operator\nOne of the advantages of using the pipe %>% is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:\n\ntab_1 <- filter(murders, region == \"South\")\ntab_2 <- mutate(tab_1, rate = total / population * 10^5)\nrates <- tab_2$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nWe can avoid defining any new intermediate objects by instead typing:\n\nfilter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  summarize(median = median(rate)) %>%\n  pull(median)\n\n[1] 3.398069\n\n\nWe can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the pull function was not available and we wanted to access tab_2$rate? What data frame name would we use? The answer is the dot operator.\nFor example to access the rate vector without the pull function we could use\n\nrates <-   filter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  .$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nIn the next section, we will see other instances in which using the . is useful."
  },
  {
    "objectID": "content/Week_02/02a.html#do",
    "href": "content/Week_02/02a.html#do",
    "title": "Introduction to the tidyverse",
    "section": "do",
    "text": "do\nThe tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %>%, tidyverse functions consistently take data frames and return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The quantile function is an example we described earlier. The do function serves as a bridge between R functions such as quantile and the tidyverse. The do function understands grouped tibbles and always returns a data frame.\nIn the summarize section (above), we noted that if we attempt to use quantile to obtain the min, median and max in one call, we will receive something unexpected. Prior to R 4.1, we would receive an error. After R 4.1, we actually get:\n\ndata(heights)\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nWe probably wanted three columns: min, median, and max. We can use the do function to fix this.\nFirst we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame. Note that it returns a single-row data frame.\n\nmy_summary <- function(dat){\n  x <- quantile(dat$height, c(0, 0.5, 1))\n  tibble(min = x[1], median = x[2], max = x[3])\n}\n\nWe can now apply the function to the heights dataset to obtain the summaries:\n\nheights %>%\n  group_by(sex) %>%\n  my_summary\n\n# A tibble: 1 × 3\n    min median   max\n  <dbl>  <dbl> <dbl>\n1    50   68.5  82.7\n\n\nBut this is not what we want. We want a summary for each sex and the code returned just one summary. This is because my_summary is not part of the tidyverse and does not know how to handled grouped tibbles. do makes this connection:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary(.))\n\n# A tibble: 2 × 4\n# Groups:   sex [2]\n  sex      min median   max\n  <fct>  <dbl>  <dbl> <dbl>\n1 Female    51   65.0  79  \n2 Male      50   69    82.7\n\n\nNote that here we need to use the dot operator. The tibble created by group_by is piped to do. Within the call to do, the name of this tibble is . and we want to send it to my_summary. If you do not use the dot, then my_summary has no argument and returns an error telling us that argument \"dat\" is missing. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary())\n\nIf you do not use the parenthesis, then the function is not executed and instead do tries to return the function. This gives an error because do must always return a data frame. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary)\n\nSo do serves as a bridge between non-tidyverse functions and the tidyverse."
  },
  {
    "objectID": "content/Week_02/02a.html#the-purrr-package",
    "href": "content/Week_02/02a.html#the-purrr-package",
    "title": "Introduction to the tidyverse",
    "section": "The purrr package",
    "text": "The purrr package\nIn previous sections (and labs) we learned about the sapply function, which permitted us to apply the same function to each element of a vector. We constructed a function and used sapply to compute the sum of the first n integers for several values of n like this:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  sum(x)\n}\nn <- 1:25\ns_n <- sapply(n, compute_s_n)\ns_n\n\n [1]   1   3   6  10  15  21  28  36  45  55  66  78  91 105 120 136 153 171 190\n[20] 210 231 253 276 300 325\n\n\nThis type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, sapply can return several different object types; for example, we might expect a numeric result from a line of code, but sapply might convert our result to character under some circumstances. purrr functions will never do this: they will return objects of a specified type or return an error if this is not possible.\nThe first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list:\n\nlibrary(purrr) # or library(tidyverse)\nn <- 1:25\ns_n <- map(n, compute_s_n)\nclass(s_n)\n\n[1] \"list\"\n\n\nIf we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values.\n\ns_n <- map_dbl(n, compute_s_n)\nclass(s_n)\n\n[1] \"numeric\"\n\n\nThis produces the same results as the sapply call shown above.\nA particularly useful purrr function for interacting with the rest of the tidyverse is map_df, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a Argument 1 must have names error:\n\ns_n <- map_df(n, compute_s_n)\n\nWe need to change the function to make this work:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x))\n}\ns_n <- map_df(n, compute_s_n)\nhead(s_n)\n\n# A tibble: 6 × 1\n    sum\n  <int>\n1     1\n2     3\n3     6\n4    10\n5    15\n6    21\n\n\nBecause map_df returns a tibble, we can have more columns defined in our function and returned.\n\ncompute_s_n2 <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x), sumSquared = sum(x^2))\n}\ns_n <- map_df(n, compute_s_n2)\nhead(s_n)\n\n# A tibble: 6 × 2\n    sum sumSquared\n  <int>      <dbl>\n1     1          1\n2     3          5\n3     6         14\n4    10         30\n5    15         55\n6    21         91\n\n\nThe purrr package provides much more functionality not covered here. For more details you can consult this online resource."
  },
  {
    "objectID": "content/Week_02/02a.html#tidyverse-conditionals",
    "href": "content/Week_02/02a.html#tidyverse-conditionals",
    "title": "Introduction to the tidyverse",
    "section": "Tidyverse conditionals",
    "text": "Tidyverse conditionals\nA typical data analysis will often involve one or more conditional operations. In the section on Conditionals, we described the ifelse function, which we will use extensively in this book. In this section we present two dplyr functions that provide further functionality for performing conditional operations.\n\ncase_when\nThe case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0:\n\nx <- c(-2, -1, 0, 1, 2)\ncase_when(x < 0 ~ \"Negative\",\n          x > 0 ~ \"Positive\",\n          x == 0  ~ \"Zero\")\n\n[1] \"Negative\" \"Negative\" \"Zero\"     \"Positive\" \"Positive\"\n\n\nA common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this:\n\nmurders %>%\n  mutate(group = case_when(\n    abb %in% c(\"ME\", \"NH\", \"VT\", \"MA\", \"RI\", \"CT\") ~ \"New England\",\n    abb %in% c(\"WA\", \"OR\", \"CA\") ~ \"West Coast\",\n    region == \"South\" ~ \"South\",\n    TRUE ~ \"Other\")) %>%\n  group_by(group) %>%\n  summarize(rate = sum(total) / sum(population) * 10^5)\n\n# A tibble: 4 × 2\n  group        rate\n  <chr>       <dbl>\n1 New England  1.72\n2 Other        2.71\n3 South        3.63\n4 West Coast   2.90\n\n\nThat TRUE on the fourth line of case_when serves as a catch-all. As case_when steps through the conditions, if none of them are true, it comes to the last line. Since TRUE is always true, the function will return “Other”. Leaving out the last line of case_when would result in NA values for any observation that fails the first three conditionals. This may or may not be what you want.\n\n\nbetween\nA common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type\n\nx >= a & x <= b\n\nHowever, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation.\n\nbetween(x, a, b)\n\n\nTRY IT\n\nLoad the murders dataset. Which of the following is true?\n\n\nmurders is in tidy format and is stored in a tibble.\nmurders is in tidy format and is stored in a data frame.\nmurders is not in tidy format and is stored in a tibble.\nmurders is not in tidy format and is stored in a data frame.\n\n\nUse as_tibble to convert the murders data table into a tibble and save it in an object called murders_tibble.\nUse the group_by function to convert murders into a tibble that is grouped by region.\nWrite tidyverse code that is equivalent to this code:\n\n\nexp(mean(log(murders$population)))\n\nWrite it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with murders %>%.\n\nUse the map_df to create a data frame with three columns named n, s_n, and s_n_2. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through \\(n\\) with \\(n\\) the row number."
  },
  {
    "objectID": "content/Week_02/02b.html",
    "href": "content/Week_02/02b.html",
    "title": "Introduction to Visualization",
    "section": "",
    "text": "Looking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table:\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nWhat do you learn from staring at this table? Even though it is a relatively straightforward table, we can’t learn anything. For starters, it is grossly abbreviated, though you could scroll through. In doing so, how quickly might you be able to determine which states have the largest populations? Which states have the smallest? How populous is a typical state? Is there a relationship between population size and total murders? How do murder rates vary across regions of the country? For most folks, it is quite difficult to extract this information just by looking at the numbers. In contrast, the answer to the questions above are readily available from examining this plot:\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(ggrepel)\n\nr <- murders %>%\n  summarize(pop=sum(population), tot=sum(total)) %>%\n  mutate(rate = tot/pop*10^6) %>% pull(rate)\n\nmurders %>% ggplot(aes(x = population/10^6, y = total, label = abb)) +\n  geom_abline(intercept = log10(r), lty=2, col=\"darkgrey\") +\n  geom_point(aes(color=region), size = 3) +\n  geom_text_repel() +\n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") +\n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\") +\n  scale_color_discrete(name=\"Region\") +\n  theme_economist_white()\n\n\n\n\nWe are reminded of the saying: “A picture is worth a thousand words”. Data visualization provides a powerful way to communicate a data-driven finding. In some cases, the visualization is so convincing that no follow-up analysis is required. You should consider visualization the most potent tool in your data analytics arsenal.\nThe growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many industries, academia, and government. A salient example is news organizations, which are increasingly embracing data journalism and including effective infographics as part of their reporting.\nA particularly salient example—given the current state of the world—is a Wall Street Journal article1 showing data related to the impact of vaccines on battling infectious diseases. One of the graphs shows measles cases by US state through the years with a vertical line demonstrating when the vaccine was introduced.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n(Source: Wall Street Journal)\nAnother striking example comes from a New York Times chart2, which summarizes scores from the NYC Regents Exams. As described in the article3, these scores are collected for several reasons, including to determine if a student graduates from high school. In New York City you need a 65 to pass. The distribution of the test scores forces us to notice something somewhat problematic:\n\n\n\n\n\n(Source: New York Times via Amanda Cox)\nThe most common test score is the minimum passing grade, with very few scores just below the threshold. This unexpected result is consistent with students close to passing having their scores bumped up.\nThis is an example of how data visualization can lead to discoveries which would otherwise be missed if we simply subjected the data to a battery of data analysis tools or procedures. Data visualization is the strongest tool of what we call exploratory data analysis (EDA). John W. Tukey4, considered the father of EDA, once said,\n\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.”\n\n\nMany widely used data analysis tools were initiated by discoveries made via EDA. EDA is perhaps the most important part of data analysis, yet it is one that is often overlooked.\nData visualization is also now pervasive in philanthropic and educational organizations. In the talks New Insights on Poverty5 and The Best Stats You’ve Ever Seen6, Hans Rosling forces us to notice the unexpected with a series of plots related to world health and economics. In his videos, he uses animated graphs to show us how the world is changing and how old narratives are no longer true.\n\n\n\n\n\nIt is also important to note that mistakes, biases, systematic errors and other unexpected problems often lead to data that should be handled with care. Failure to discover these problems can give rise to flawed analyses and false discoveries. As an example, consider that measurement devices sometimes fail and that most data analysis procedures are not designed to detect these. Yet these data analysis procedures will still give you an answer. The fact that it can be difficult or impossible to notice an error just from the reported results makes data visualization particularly important.\nToday, we will discuss the basics of data visualization and exploratory data analysis. We will use the ggplot2 package to code. To learn the very basics, we will start with a somewhat artificial example: heights reported by students. Then we will cover the two examples mentioned above: 1) world health and economics and 2) infectious disease trends in the United States.\nOf course, there is much more to data visualization than what we cover here. The following are references for those who wish to learn more:\n\nER Tufte (1983) The visual display of quantitative information. Graphics Press.\nER Tufte (1990) Envisioning information. Graphics Press.\nER Tufte (1997) Visual explanations. Graphics Press.\nWS Cleveland (1993) Visualizing data. Hobart Press.\nWS Cleveland (1994) The elements of graphing data. CRC Press.\nA Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach: Turning tables into graphs. The American Statistician 56:121-130.\nNB Robbins (2004) Creating more effective graphs. Wiley.\nA Cairo (2013) The functional art: An introduction to information graphics and visualization. New Riders.\nN Yau (2013) Data points: Visualization that means something. Wiley.\n\nWe also do not cover interactive graphics, a topic that is both too advanced for this course and too unweildy. Some useful resources for those interested in learning more can be found below, and you are encouraged to draw inspiration from those websites in your projects:\n\nhttps://shiny.rstudio.com/\nhttps://d3js.org/"
  },
  {
    "objectID": "content/Week_02/02b.html#the-components-of-a-graph",
    "href": "content/Week_02/02b.html#the-components-of-a-graph",
    "title": "Introduction to Visualization",
    "section": "The components of a graph",
    "text": "The components of a graph\nWe will eventually construct a graph that summarizes the US murders dataset that looks like this:\n\n\n\n\n\nWe can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.\nThis data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.\nThe first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main five components to note are:\n\nData: The US murders data table is being summarized. We refer to this as the data component.\nGeometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, boxplot, pie (ew!), and many, many more. We will learn about these later.\nAesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using.\nAnnotations: These are things like axis labels, axis ticks (the lines along the axis at regular intervals or specific points of interest), axis scales (e.g. log-scale), titles, legends, etc.\nStyle: An overall appearance of the graph determined by fonts, color palattes, layout, blank spaces, and more.\n\nWe also note that:\n\nThe points are labeled with the state abbreviations.\nThe range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.\nThere are labels, a title, a legend, and we use the style of The Economist magazine.\n\nAll of the flexibility and visualization power of ggplot is contained in these four elements (plus your data)"
  },
  {
    "objectID": "content/Week_02/02b.html#ggplot-objects",
    "href": "content/Week_02/02b.html#ggplot-objects",
    "title": "Introduction to Visualization",
    "section": "ggplot objects",
    "text": "ggplot objects\nWe will now construct the plot piece by piece.\nWe start by loading the dataset:\n\nlibrary(dslabs)\ndata(murders)\n\n\n\n\nThe first step in creating a ggplot2 graph is to define a ggplot object. We do this with the function ggplot, which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object:\n\nggplot(data = murders)\n\nWe can also pipe the data in as the first argument. So this line of code is equivalent to the one above:\n\nmurders %>% ggplot()\n\n\n\n\nIt renders a plot, in this case a blank slate since no geometry has been defined. The only style choice we see is a grey background.\nWhat has happened above is that the object was created and, because it was not assigned, it was automatically evaluated. But we can assign our plot to an object, for example like this:\n\np <- ggplot(data = murders)\nclass(p)\n\n[1] \"gg\"     \"ggplot\"\n\n\nTo render the plot associated with this object, we simply print the object p. The following two lines of code each produce the same plot we see above:\n\nprint(p)\np"
  },
  {
    "objectID": "content/Week_02/02b.html#geometries-briefly",
    "href": "content/Week_02/02b.html#geometries-briefly",
    "title": "Introduction to Visualization",
    "section": "Geometries (briefly)",
    "text": "Geometries (briefly)\nIn ggplot2 we create graphs by adding geometry layers. Layers can define geometries, compute summary statistics, define what scales to use, create annotations, or even change styles. To add layers, we use the symbol +. In general, a line of code will look like this:\n\nDATA %>% ggplot() + LAYER 1 + LAYER 2 + ... + LAYER N\n\nUsually, the first added layer after ggplot() + defines the geometry. After that, we may add additional geometries, we may rescale an axis, we may add annotations and labels, or we may change the style. For now, we want to make a scatterplot like the one you all created in Lab 0. What geometry do we use?\n\n\n\n\n\nTaking a quick look at the cheat sheet, we see that the ggplot2 function used to create plots with this geometry is geom_point.\nSee Here\n(Image courtesy of RStudio9. CC-BY-4.0 license10.)\n\nGeometry function names follow the pattern: geom_X where X is the name of some specific geometry. Some examples include geom_point, geom_bar, and geom_histogram. You’ve already seen a few of these. We will start with a scatterplot created using geom_point() for now, then circle back to more geometries after we cover aesthetic mappings, layers, and annotations.\nFor geom_point to run properly we need to provide data and an aesthetic mapping. The simplest mapping for a scatter plot is to say we want one variable on the X-axis, and a different one on the Y-axis, so each point is an {X,Y} pair. That is an aesthetic mapping because X and Y are aesthetics in a geom_point scatterplot.\nWe have already connected the object p with the murders data table, and if we add the layer geom_point it defaults to using this data. To find out what mappings are expected, we read the Aesthetics section of the help file ?geom_point help file:\n> Aesthetics\n>\n> geom_point understands the following aesthetics (required aesthetics are in bold):\n>\n> **x**\n>\n> **y**\n>\n> alpha\n>\n> colour\n>\n> fill\n>\n> group\n>\n> shape\n>\n> size\n>\n> stroke\nand—although it does not show in bold above—we see that at least two arguments are required: x and y. You can’t have a geom_point scatterplot unless you state what you want on the X and Y axes."
  },
  {
    "objectID": "content/Week_02/02b.html#aesthetic-mappings",
    "href": "content/Week_02/02b.html#aesthetic-mappings",
    "title": "Introduction to Visualization",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\nAesthetic mappings describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The aes function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the aes function is often used as the argument of a geometry function. This example produces a scatterplot of population in millions (x-axis) versus total murders (y-axis):\n\nmurders %>% ggplot() +\n  geom_point(aes(x = population/10^6, y = total))\n\nInstead of defining our plot from scratch, we can also add a layer to the p object that was defined above as p <- ggplot(data = murders):\n\np + geom_point(aes(x = population/10^6, y = total))\n\n\n\n\nThe scales and annotations like axis labels are defined by default when adding this layer (note the x-axis label is exactly what we wrote in the function call). Like dplyr functions, aes also uses the variable names from the object component: we can use population and total without having to call them as murders$population and murders$total. The behavior of recognizing the variables from the data component is quite specific to aes. With most functions, if you try to access the values of population or total outside of aes you receive an error.\nNote that we did some rescaling within the aes() call - we can do simple things like multiplication or division on the variable names in the ggplot call. The axis labels reflect this. We will change the axis labels later.\nThe aesthetic mappings are very powerful - changing the variable in x= or y= changes the meaning of the plot entirely. We’ll come back to additional aesthetic mappings once we talk about aesthetics in general.\n\nAesthetics in general\nEven without mappings, a plots aesthetics can be useful. Things like color, fill, alpha, and size are aesthetics that can be changed.\nLet’s say we want larger points in our scatterplot. The size aesthetic can be used to set the size. The scale of size is “multiples of the defaults” (so size = 1 is the default)\n\np + geom_point(aes(x = population/10^6, y = total), size = 3)\n\n\n\n\nsize is not a mapping so it is not in the aes() part: whereas mappings use data from specific observations and need to be inside aes(), operations we want to affect all the points the same way do not need to be included inside aes. We’ll see what happens if size is inside aes(size = xxx) in a second.\nWe can change the shape to one of the many different base-R options found here:\n\np + geom_point(aes(x = population/10^6, y = total), size = 3, shape = 17)\n\n\n\n\nWe can also change the fill and the color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 4, shape = 23, fill = '#18453B')\n\n\n\n\nfill can take a common name like 'green', or can take a hex color like '#18453B', which is MSU Green according to MSU’s branding site. You can also find UM Maize and OSU Scarlet on respective branding pages, or google “XXX color hex.” We’ll learn how to build a color palatte later on.\ncolor (or colour, same thing because ggplot creators allow both spellings) is a little tricky with points - it changes the outline of the geometry rather than the fill color, but in geom_point() most shapes are only the outline, including the default. This is more useful with, say, a barplot where the outline and the fill might be different colors. Still, shapes 21-25 have both fill and color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 5, shape = 23, fill = '#18453B', color = 'white')\n\n\n\n\nThe color = 'white' makes the outline of the shape white, which you can see if you look closely in the areas where the shapes overlap. This only works with shapes 21-25, or any other geometry that has both an outline and a fill.\n\n\nNow, back to aesthetic mappings\nNow that we’ve seen a few aesthetics (and know we can find more by looking at which aesthetics work with our geometry in the help file), let’s return to the power of aesthetic mappings.\nAn aesthetic mapping means we can vary an aesthetic (like fill or shape or size) according to some variable in our data. This opens up a world of possibilities! Let’s try adding to our x and y aesthetics with a color aesthetic (since points respond to color better than fill) that varies by region, which is a column in our data:\n\np + geom_point(aes(x = population/10^6, y = total, color = region), size = 3)\n\n\n\n\nWe include color=region inside the aes call, which tells R to find a variable called region and change color based on that. R will choose a somewhat ghastly color palatte, and every unique value in the data for region will get a different color if the variable is discrete. If the variable is a continuous value, then ggplot will automatically make a color ramp. Thus, discrete and continuous values for aesthetic mappings work differently.\nLet’s see a useful example of a continuous aesthetic mapping to color. In our data, we are making a scatterplot of population and total murders, which really just shows that states with higher populations have higher murders. What we really want is murders per capita (I think COVID taught us a lot about rates vs. levels like “cases” and “cases per 100,000 people”). We can create a variable of “murders per capita” on the fly. Since “murders per capita” is a very small number and hard to read, we’ll multiply by 100 so that we get “percent of population murdered per year”:\n\np + geom_point(aes(x = population/10^5, y = total, color = 100*total/population), size = 3)\n\n\n\n\nWhile the clear pattern of “more population means more murders” is still there, look at the outlier in light blue in the bottom left. With the color ramp, see how easy it is to see here that there is one location where murders per capita is quite high?\nNote that size is outside of aes and is set to an explicit value, not to a variable. What if we set size to a variable in the data?\n\np + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6))\n\n\n\n\n\n\nLegends for aesthetics\nHere we see yet another useful default behavior: ggplot2 automatically adds a legend that maps color to region, and size to population (which we scaled by 1,000,000). To avoid adding this legend we set the geom_point argument show.legend = FALSE. This removes both the size and the color legend.\n\np + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6), show.legend = FALSE)\n\n\n\n\nLater on, when we get to annotation layers, we’ll talk about controlling the legend text and layout. For now, we just need to know how to turn them off."
  },
  {
    "objectID": "content/Week_02/02b.html#annotation-layers",
    "href": "content/Week_02/02b.html#annotation-layers",
    "title": "Introduction to Visualization",
    "section": "Annotation Layers",
    "text": "Annotation Layers\nA second layer in the plot we wish to make involves adding a label to each point to identify the state. The geom_label and geom_text functions permit us to add text to the plot with and without a rectangle behind the text, respectively.\nBecause each point (each state in this case) has a label, we need an aesthetic mapping to make the connection between points and labels. By reading the help file ?geom_text, we learn that we supply the mapping between point and label through the label argument of aes. That is, label is an aesthetic that we can map. So the code looks like this:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\n\n\n\nWe have successfully added a second layer to the plot.\nAs an example of the unique behavior of aes mentioned above, note that this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total, label = abb))\n\nis fine, whereas this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total), label = abb)\n\nwill give you an error since abb is not found because it is outside of the aes function. The layer geom_text does not know where to find abb since it is a column name and not a global variable, and ggplot does not look for column names for non-mapped aesthetics. For a trivial example:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(population/10^6, total), label = 'abb')\n\n\n\n\n\nGlobal versus local aesthetic mappings\nIn the previous line of code, we define the mapping aes(population/10^6, total) twice, once in each geometry. We can avoid this by using a global aesthetic mapping. We can do this when we define the blank slate ggplot object. Remember that the function ggplot contains an argument that permits us to define aesthetic mappings:\n\nargs(ggplot)\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \nNULL\n\n\nIf we define a mapping in ggplot, all the geometries that are added as layers will default to this mapping. We redefine p:\n\np <- murders %>% ggplot(aes(x = population/10^6, y = total, label = abb))\n\nand then we can simply write the following code to produce the previous plot:\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 1.5) # offsets the label\n\nWe keep the size and nudge_x arguments in geom_point and geom_text, respectively, because we want to only increase the size of points and only nudge the labels. If we put those arguments in aes then they would apply to both plots. Also note that the geom_point function does not need a label argument and therefore ignores that aesthetic.\nIf necessary, we can override the global mapping by defining a new mapping within each layer. These local definitions override the global. Here is an example:\n\np + geom_point(size = 3) +\n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))\n\n\n\n\nClearly, the second call to geom_text does not use x = population and y = total."
  },
  {
    "objectID": "content/Week_02/02b.html#try-it",
    "href": "content/Week_02/02b.html#try-it",
    "title": "Introduction to Visualization",
    "section": "Try it!",
    "text": "Try it!\n\nLet’s break in to smaller groups and try playing with some of the aesthetics and aesthetic mappings. If we’re in person (woohoo!), we’ll form the same number of groups in class.\nIn each group, one person should be the main coder - someone who has the packages like dslabs installed and has successfully run the plots above. Each set of tasks ask you to learn about an aesthetic and put it into action with the murder data. We’ll leave about 5 minutes to do the task, then have you come back and share your results with the class.\nFor each group, we’ll start with the following code:\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\nThe alpha aesthetic mapping.\n\nThe alpha aesthetic can only take a number between 0 and 1. So first, in murders, create a murders_per_capita column by dividing total by population. Second, find the max(murders$murders_per_capita) and then create another new column called murders_per_capita_rescaled which divides murders_per_capita by the max value. murders_per_capita_rescaled will be between 0 and 1, with the value of 1 for the state with the max murder rate. This is a little hard to do on the fly in ggplot.\nSet the alpha aesthetic mapping to murders_per_capita_rescaled for geom_point.\nTurn off the legend using show.legend=FALSE\nInclude the geom_text labels, but make sure the aesthetic mapping does not apply to the labels.\nUse nudge_x = 1.5 as before to offset the labels.\nBe able to explain the plot.\n\nDoes the alpha aesthetic help present the data here? It’s OK if it doesn’t!\n\n\nThe stroke aesthetic mapping.\n\nThe stroke aesthetic works a bit like the size aesthetic. It must be used with a plot that has both a border and a fill, like shapes 21-25, so use one of those.\nUse the stroke aesthetic mapping (meaning the stroke will change according to a value in the data) to set a different stroke size based on murders per capita. You can create a murders per capita variable on the fly, or add it to your murders data.\n\nInclude the text labels as before and use nudge_x = 1.5.\nMake sure you’re only setting the aesthetic for the points on the scatterplot!\n\n\nThe angle aesthetic\n\nUsing the ?geom_text help, note that geom_text takes an aesthetic of angle.\nUse the angle aesthetic (not aesthetic mapping) in the appropriate place (e.g. on geom_text and not on other geometries) to adjust the labels on our plot.\nNow, try using the angle aesthetic mapping by using the total field as both the y value and the angle value in the geom_text layer.\nDoes using angle as an aesthetic help? What about as an aesthetic mapping?\n\nThe color aesthetic mapping\n\nSet the color aesthetic mapping in geom_text to total/population.\n\nUse the nudge_x = 1.5 aesthetic in geom_text still\n\nTry it with and without the legend using show.legend.\nBe able to explain the plot.\n\nDoes the color aesthetic mapping help present the data here?\n\n\ngeom_label and the fill aesthetic\n\nLooking at ?geom_label (which is the same help as geom_text), we note that “The fill aesthetic controls the backgreound colour of the label”.\nSet the fill aesthetic mapping to total/population in geom_label (replacing geom_text but still using nudge_x=1.5)\nSet the fill aesthetic (not mapping) to the color of your choice.\nBe able to explain the plots.\n\n\nDoes the fill aesthetic mapping help present the data here?\nWhat color did you choose for the non-mapped fill aesthetic?"
  },
  {
    "objectID": "content/Week_03/03a.html",
    "href": "content/Week_03/03a.html",
    "title": "Effective Visualizations",
    "section": "",
    "text": "This page."
  },
  {
    "objectID": "content/Week_03/03a.html#a-starting-list-from-tufte",
    "href": "content/Week_03/03a.html#a-starting-list-from-tufte",
    "title": "Effective Visualizations",
    "section": "A Starting List (from Tufte)",
    "text": "A Starting List (from Tufte)\n\nThe representation of numbers, as physically measured on the surface of the graph itself, should be directly proportional to the numerical quantities represented.\nClear, detailed and thorough labeling should be used to defeat graphical distortion and ambiguity. Write out explanations of the data on the graph itself. Label important events in the data.\nShow data variation, not design variation.\nIn time-series displays of money, deflated and standardized units of monetary measurement are nearly always better than nominal units.\nThe number of information carrying (variable) dimensions depicted should not exceed the number of dimensions in the data. Graphics must not quote data out of context.\n\n\nExamples (Dos and Don’ts)\nAs with the discussion above, we will be using these libraries—note the addition of gridExtra:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(gridExtra)\n\n\n\nEncoding data using visual cues\nVisual cues are any element of design that gives the viewer clues as to how to use the object. For instance, we can look at door handles like these\n\nand know exactly what to do with them. We don’t even need the “PUSH” and “PULL” – approaching just the pull handle in the wild gives sufficient visual cues that you know the door is a “pull” door. Encountering a metal plate with no handle is going to imply “push”. If the door on the right were a “push” door, you’d be momentarily confused! It’s poor design. Your plots should use visual cues to help readers understand how to use them with no confusion.\nWe start by describing some principles for encoding data. There are several approaches at our disposal including position, aligned lengths, angles, area, brightness, and color hue.\n\n\n\nTo illustrate how some of these strategies compare, let’s suppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015. For each year, we are simply comparing five quantities – the five percentages. A widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart:\n\n\n\n\n\n\n\n\n\nHere we are representing quantities with both areas and angles, since both the angle and area of each pie slice are proportional to the quantity the slice represents. This turns out to be a sub-optimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when area is the only available visual cue. The donut chart is an example of a plot that uses only area:\n\n\n\n\n\n\n\n\n\nTo see how hard it is to quantify angles and area, note that the rankings and all the percentages in the plots above changed from 2000 to 2015. Can you determine the actual percentages and rank the browsers’ popularity? Can you see how the percentages changed from 2000 to 2015? It is not easy to tell from the plot. In fact, the pie R function help file states that:\n\nPie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data.\n\nIn this case, simply showing the numbers is not only clearer, but would also save on printing costs if printing a paper copy:\n\n\n\n\n \n  \n    Browser \n    2000 \n    2015 \n  \n \n\n  \n    Opera \n    3 \n    2 \n  \n  \n    Safari \n    21 \n    22 \n  \n  \n    Firefox \n    23 \n    21 \n  \n  \n    Chrome \n    26 \n    29 \n  \n  \n    IE \n    28 \n    27 \n  \n\n\n\n\n\nThe preferred way to plot these quantities is to use length and position as visual cues, since humans are much better at judging linear measures. The barplot uses this approach by using bars of length proportional to the quantities of interest. By adding horizontal lines at strategically chosen values, in this case at every multiple of 10, we ease the visual burden of quantifying through the position of the top of the bars. Compare and contrast the information we can extract from the two figures.\n\n\n\n\n\n\n\n\n\nNotice how much easier it is to see the differences in the barplot. In fact, we can now determine the actual percentages by following a horizontal line to the x-axis.\nIf for some reason you need to make a pie chart, label each pie slice with its respective percentage so viewers do not have to infer them from the angles or area:\n\n\n\n\n\n\n\n\n\nIn general, when displaying quantities, position and length are preferred over angles and/or area. Brightness and color are even harder to quantify than angles. But, as we will see later, they are sometimes useful when more than two dimensions must be displayed at once.\n\n\nAvoid pseudo-three-dimensional plots\nThe figure below, taken from the scientific literature5, shows three variables: dose, drug type and survival. Although your screen/book page is flat and two-dimensional, the plot tries to imitate three dimensions and assigned a dimension to each variable.\n (Image courtesy of Karl Broman)\nHumans are not good at seeing in three dimensions (which explains why it is hard to parallel park) and our limitation is even worse with regard to pseudo-three-dimensions. To see this, try to determine the values of the survival variable in the plot above. Can you tell when the purple ribbon intersects the red one? This is an example in which we can easily use color to represent the categorical variable instead of using a pseudo-3D:\n\n##First read data\nurl <- \"https://github.com/kbroman/Talk_Graphs/raw/master/R/fig8dat.csv\"\ndat <- read.csv(url)\n\n##Now make alternative plot\ndat %>% gather(drug, survival, -log.dose) %>%\n  mutate(drug = gsub(\"Drug.\",\"\",drug)) %>%\n  ggplot(aes(log.dose, survival, color = drug)) +\n  geom_line()\n\n\n\n\n\n\n\n\nNotice how much easier it is to determine the survival values.\nPseudo-3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. Here are two examples:\n  (Images courtesy of Karl Broman)\n\n\nAvoid too many significant digits\nBy default, statistical software like R returns many significant digits. The default behavior in R is to show 7 significant digits. That many digits often adds no information and the added visual clutter can make it hard for the viewer to understand the message. As an example, here are the per 10,000 disease rates, computed from totals and population in R, for California across the five decades:\n\n\n\n\n \n  \n    state \n    year \n    Measles \n    Pertussis \n    Polio \n  \n \n\n  \n    California \n    1940 \n    37.8826320 \n    18.3397861 \n    0.8266512 \n  \n  \n    California \n    1950 \n    13.9124205 \n    4.7467350 \n    1.9742639 \n  \n  \n    California \n    1960 \n    14.1386471 \n    NA \n    0.2640419 \n  \n  \n    California \n    1970 \n    0.9767889 \n    NA \n    NA \n  \n  \n    California \n    1980 \n    0.3743467 \n    0.0515466 \n    NA \n  \n\n\n\n\n\nWe are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates. In this case, two significant figures is more than enough and clearly makes the point that rates are decreasing:\n\n\n\n\n \n  \n    state \n    year \n    Measles \n    Pertussis \n    Polio \n  \n \n\n  \n    California \n    1940 \n    37.9 \n    18.3 \n    0.8 \n  \n  \n    California \n    1950 \n    13.9 \n    4.7 \n    2.0 \n  \n  \n    California \n    1960 \n    14.1 \n    NA \n    0.3 \n  \n  \n    California \n    1970 \n    1.0 \n    NA \n    NA \n  \n  \n    California \n    1980 \n    0.4 \n    0.1 \n    NA \n  \n\n\n\n\n\nUseful ways to change the number of significant digits or to round numbers are signif and round. You can define the number of significant digits globally by setting options like this: options(digits = 3).\nAnother principle related to displaying tables is to place values being compared on columns rather than rows. Note that our table above is easier to read than this one:\n\n\n\n\n \n  \n    state \n    disease \n    1940 \n    1950 \n    1960 \n    1970 \n    1980 \n  \n \n\n  \n    California \n    Measles \n    37.9 \n    13.9 \n    14.1 \n    1 \n    0.4 \n  \n  \n    California \n    Pertussis \n    18.3 \n    4.7 \n    NA \n    NA \n    0.1 \n  \n  \n    California \n    Polio \n    0.8 \n    2.0 \n    0.3 \n    NA \n    NA \n  \n\n\n\n\n\n\n\nKnow your audience\nGraphs can be used for 1) our own exploratory data analysis, 2) to convey a message to experts, or 3) to help tell a story to a general audience. Make sure that the intended audience understands each element of the plot.\nAs a simple example, consider that for your own exploration it may be more useful to log-transform data and then plot it. However, for a general audience that is unfamiliar with converting logged values back to the original measurements, using a log-scale for the axis instead of log-transformed values will be much easier to digest.\n\n\nKnow when to include 0\nWhen using barplots, it is misinformative not to start the bars at 0. This is because, by using a barplot, we are implying the length is proportional to the quantities being displayed. By avoiding 0, relatively small differences can be made to look much bigger than they actually are. This approach is often used by politicians or media organizations trying to exaggerate a difference. Below is an illustrative example used by Peter Aldhous in this lecture: http://paldhous.github.io/ucb/2016/dataviz/week2.html.\n (Source: Fox News, via Media Matters6.)\nFrom the plot above, it appears that apprehensions have almost tripled when, in fact, they have only increased by about 16%. Starting the graph at 0 illustrates this clearly:\n\n\n\n\n\n\n\n\n\nHere is another example, described in detail in a Flowing Data blog post: \n(Source: Fox News, via Flowing Data7.)\nThis plot makes a 13% increase look like a five fold change. Here is the appropriate plot:\n\n\n\n\n\n\n\n\n\nFinally, here is an extreme example that makes a very small difference of under 2% look like a 10-100 fold change: \n(Source: Venezolana de Televisión via Pakistan Today8 and Diego Mariano.)\nHere is the appropriate plot:\n\n\n\n\n\n\n\n\n\nWhen using position rather than length, it is then not necessary to include 0. This is particularly the case when we want to compare differences between groups relative to the within-group variability. Here is an illustrative example showing country average life expectancy stratified across continents in 2012:\n\n\n\n\n\n\n\n\n\nNote that in the plot on the left, which includes 0, the space between 0 and 43 adds no information and makes it harder to compare the between and within group variability.\n\n\nDo not distort quantities\nDuring President Barack Obama’s 2011 State of the Union Address, the following chart was used to compare the US GDP to the GDP of four competing nations: \n(Source: The 2011 State of the Union Address9)\nJudging by the area of the circles, the US appears to have an economy over five times larger than China’s and over 30 times larger than France’s. However, if we look at the actual numbers, we see that this is not the case. The actual ratios are 2.6 and 5.8 times bigger than China and France, respectively. The reason for this distortion is that the radius, rather than the area, was made to be proportional to the quantity, which implies that the proportion between the areas is squared: 2.6 turns into 6.5 and 5.8 turns into 34.1. Here is a comparison of the circles we get if we make the value proportional to the radius and to the area:\n\ngdp <- c(14.6, 5.7, 5.3, 3.3, 2.5)\ngdp_data <- data.frame(Country = rep(c(\"United States\", \"China\", \"Japan\", \"Germany\", \"France\"),2),\n           y = factor(rep(c(\"Radius\",\"Area\"),each=5), levels = c(\"Radius\", \"Area\")),\n           GDP= c(gdp^2/min(gdp^2), gdp/min(gdp))) %>%\n   mutate(Country = reorder(Country, GDP))\ngdp_data %>%\n  ggplot(aes(Country, y, size = GDP)) +\n  geom_point(show.legend = FALSE, color = \"blue\") +\n  scale_size(range = c(2,25)) +\n  coord_flip() + \n  ylab(\"\") + xlab(\"\") # identical to labs(y = \"\", x = \"\")\n\n\n\n\n\n\n\n\nNot surprisingly, ggplot2 defaults to using area rather than radius. Of course, in this case, we really should not be using area at all since we can use position and length:\n\ngdp_data %>%\n  filter(y == \"Area\") %>%\n  ggplot(aes(Country, GDP)) +\n  geom_bar(stat = \"identity\", width = 0.5) +\n  labs(y = \"GDP in trillions of US dollars\")\n\n\n\n\n\n\n\n\n\n\nOrder categories by a meaningful value\nWhen one of the axes is used to show categories, as is done in barplots, the default ggplot2 behavior is to order the categories alphabetically when they are defined by character strings. If they are defined by factors, they are ordered by the factor levels. We rarely want to use alphabetical order. Instead, we should order by a meaningful quantity. In all the cases above, the barplots were ordered by the values being displayed. The exception was the graph showing barplots comparing browsers. In this case, we kept the order the same across the barplots to ease the comparison. Specifically, instead of ordering the browsers separately in the two years, we ordered both years by the average value of 2000 and 2015.\nWe previously learned how to use the reorder function, which helps us achieve this goal. To appreciate how the right order can help convey a message, suppose we want to create a plot to compare the murder rate across states. We are particularly interested in the most dangerous and safest states. Note the difference when we order alphabetically (the default) versus when we order by the actual rate:\n\ndata(murders)\np1 <- murders %>% mutate(murder_rate = total / population * 100000) %>%\n  ggplot(aes(x = state, y = murder_rate)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  theme(axis.text.y = element_text(size = 8))  +\n  xlab(\"\")\n\np2 <- murders %>% mutate(murder_rate = total / population * 100000) %>%\n  mutate(state = reorder(state, murder_rate)) %>% # here's the magic!\n  ggplot(aes(x = state, y = murder_rate)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  theme(axis.text.y = element_text(size = 8))  +\n  xlab(\"\")\n\ngrid.arrange(p1, p2, ncol = 2) # we'll cover this later\n\n\n\n\n\n\n\n\nWe can make the second plot like this:\n\n\n\nThe reorder function lets us reorder groups as well. Earlier we saw an example related to income distributions across regions. Here are the two versions plotted against each other:\n\n\n\n\n\n\n\n\n\nThe first orders the regions alphabetically, while the second orders them by the group’s median."
  },
  {
    "objectID": "content/Week_03/03a.html#show-the-data",
    "href": "content/Week_03/03a.html#show-the-data",
    "title": "Effective Visualizations",
    "section": "Show the data",
    "text": "Show the data\nWe have focused on displaying single quantities across categories. We now shift our attention to displaying data, with a focus on comparing groups.\nTo motivate our first principle, “show the data”, we go back to our artificial example of describing heights to a person who is unaware of some basic facts about the population of interest (and is otherwise unsophisticated). This time let’s assume that this person is interested in the difference in heights between males and females. A commonly seen plot used for comparisons between groups, popularized by software such as Microsoft Excel, is the dynamite plot, which shows the average and standard errors.10 The plot looks like this:\n\n\n\n\n\n\n\n\n\nThe average of each group is represented by the top of each bar and the antennae extend out from the average to the average plus two standard errors. If all ET receives is this plot, he will have little information on what to expect if he meets a group of human males and females. The bars go to 0: does this mean there are tiny humans measuring less than one foot? Are all males taller than the tallest females? Is there a range of heights? ET can’t answer these questions since we have provided almost no information on the height distribution.\nThis brings us to our first principle: show the data. This simple ggplot2 code already generates a more informative plot than the barplot by simply showing all the data points:\n\n\n\n\n\n\n\n\n\nFor example, this plot gives us an idea of the range of the data. However, this plot has limitations as well, since we can’t really see all the 238 and 812 points plotted for females and males, respectively, and many points are plotted on top of each other. As we have previously described, visualizing the distribution is much more informative. But before doing this, we point out two ways we can improve a plot showing all the points.\nThe first is to add jitter, which adds a small random shift to each point. In this case, adding horizontal jitter does not alter the interpretation, since the point heights do not change, but we minimize the number of points that fall on top of each other and, therefore, get a better visual sense of how the data is distributed. A second improvement comes from using alpha blending: making the points somewhat transparent. The more points fall on top of each other, the darker the plot, which also helps us get a sense of how the points are distributed. Here is the same plot with jitter and alpha blending:\n\nheights %>%\n  ggplot(aes(sex, height)) +\n  geom_jitter(width = 0.1, alpha = 0.2)\n\n\n\n\n\n\n\n\nNow we start getting a sense that, on average, males are taller than females. We also note dark horizontal bands of points, demonstrating that many report values that are rounded to the nearest integer."
  },
  {
    "objectID": "content/Week_03/03a.html#faceting",
    "href": "content/Week_03/03a.html#faceting",
    "title": "Effective Visualizations",
    "section": "Faceting",
    "text": "Faceting\nLooking at the previous plot, it’s easy to tell that males tend to be taller than females. Before, we showed how we can plot two distributions over each other using an aesthetic mapping. Something like this:\n\nheights %>%\n  ggplot(aes(x = height, fill = sex)) +\n  geom_histogram(alpha = .5, show.legend = TRUE) +\n  labs(fill = 'Sex')\n\n\n\n\n\n\n\n\nSometimes, putting the plots on top of each other, even with a well-chosen alpha, does not clearly communicate the differences in the distribution. When we want to compare side-by-side, we will often use facets. Facets are a bit like supercharged aesthetic mapping because they let us separate plots based on categorical variables, but instead of putting them together, we can have side-by-side plots.\nTwo functions in ggplot give facets: facet_wrap and facet_grid. We’ll use facet_grid as this is a little more powerful.\nFacets are added as an additional layer like this: + facet_grid(. ~ sex). Inside the function, we have a “formula” that is written without quotes (which is unusual for R). Since facet_grid takes a “formula”, all we have to do to facet is decide how we want to lay out our plots. If we want each of the faceting groups to lie along the vertical axis, we put the variable on which we want to facet before the “~”, and after the “~” we simply put a period. If we want the groups to lie along the horizontal axis, we put the variable after the “~” and the period before. In the example, we’ll separate the histogram by drawing them side by side along the horizontal axis.\n\nheights %>%\n  ggplot(aes(x = height)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(.~sex)\n\n\n\n\n\n\n\n\nThis would be the result if we took the females, plotted the histogram, then took the males, made another histogram, and then put them side by side. But we do it in one command by adding +facet_grid(...)\n\nUse common axes with facets\nSince we have plots side-by-side, they can have different scales along the x-axis (or along the y-axis if we were stacking with sex ~ .). We want to be careful here - if we don’t have matching scales on these axes, then it’ll be really hard to visually see differences in the distribution.\nAs an example of what not to do, and to show that we can use the scales argument in facet_grid, we can allow the x-axis to freely scale between the plots. This makes it hard to tell that males are, on average, taller because the average male height, despite being larger than the average female height (70 vs. 65 or so) falls in the same location within the plot box. Note that 80 is the extreme edge for the left plot, but not in the right plot.\n\nheights %>%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(. ~ sex, scales = \"free_x\")\n\n\n\n\n\n\n\n\n\n\nAlign plots vertically to see horizontal changes and horizontally to see vertical changes\nIn these histograms, the visual cue related to decreases or increases in height are shifts to the left or right, respectively: horizontal changes. Aligning the plots vertically helps us see this change when the axes are fixed:\n\nheights %>%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(. ~ sex)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis plot makes it much easier to notice that men’s heights are, on average, higher.\nThe sample size of females is smaller than of males – that is, we have more males in the data. Try table(heights$sex) to see this. It’s also clear from the above plot because the height of the bars on the y-axis (count) are smaller for females. If we are interested in the distribution within our sample, this is useful. If we’re interested in the distribution of females vs. the distribution of males, we might want to re-scale the y-axis. Here we use scales = 'free_y' to allow each of the y-axes to have their own scale. Pay close attention to the axis labels now!\n\np2 <- heights %>%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(sex~., scales = 'free_y')\np2\n\n\n\n\n\n\n\n\nWe still have count on the y-axis, so we didn’t switch to density (though it would look the same). Instead, we rescaled the y-axis, which gives us a different perspective but still contains the count information.\nIf we want the more compact summary provided by boxplots, we then align them horizontally since, by default, boxplots move up and down with changes in height. Following our show the data principle, we then overlay all the data points:\n\np3=heights %>%\n  ggplot(aes(sex, height)) +\n  geom_boxplot(coef=3) +\n  geom_jitter(width = 0.1, alpha = 0.2) +\n  ylab(\"Height in inches\")\n\np3\n\n\n\n\n\n\n\n\nNow contrast and compare these three plots, based on exactly the same data:\n\n\n\n\n\n\n\n\n\nNotice how much more we learn from the two plots on the right. Barplots are useful for showing one number, but not very useful when we want to describe distributions.\n\n\nFacet grids\nAs the name implies, facet_grid can make more than just side-by-plots. If we specify variables on boths sides of the “~”, we get a grid of plots.\n\ngapminder::gapminder %>%\n  filter(year %in% c(1952,1972, 1992, 2002)) %>%\n  filter(continent != 'Oceania') %>%\n  ggplot(aes(x = lifeExp)) + \n  geom_density() +\n  facet_grid(continent ~ year)\n\n\n\n\n\n\n\n\nThis makes it easy to read the life expectancy distribution over time (left-to-right) and across continents (up-and-down). It makes it easy to see that Africa has spread it’s life expectancy distribution (some improved, some didn’t), while Europe has become more clustered at the top end over time. Faceting in a grid is very helpful when you have a time dimension.\n\n\nVisual cues to be compared should be adjacent, continued\nFor each continent, let’s compare income in 1970 versus 2010. When comparing income data across regions between 1970 and 2010, we made a figure similar to the one below, but this time we investigate continents rather than regions.\nNote that there are two gapminder datasets, one in dslabs and one in the gapminder package. The dslabs version has more data, so I will switch to that here by using dslabs::gapminder as our data.\n\ndslabs::gapminder %>%\n  filter(year %in% c(1970, 2010) & !is.na(gdp)) %>%\n  mutate(dollars_per_day = gdp/population/365) %>%\n  mutate(labels = paste(year, continent)) %>%  # creating text labels\n  ggplot(aes(x = labels, y = dollars_per_day)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +\n  scale_y_continuous(trans = \"log2\") +\n  ylab(\"Income in dollars per day\")\n\n\n\n\n\n\n\n\nThe default in ggplot2 is to order labels alphabetically so the labels with 1970 come before the labels with 2010, making the comparisons challenging because a continent’s distribution in 1970 is visually far from its distribution in 2010. It is much easier to make the comparison between 1970 and 2010 for each continent when the boxplots for that continent are next to each other:\n\ndslabs::gapminder %>%\n  filter(year %in% c(1970, 2010) & !is.na(gdp)) %>%\n  mutate(dollars_per_day = gdp/population/365) %>%\n  mutate(labels = paste(continent, year)) %>%\n  ggplot(aes(labels, dollars_per_day)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25)) +\n  scale_y_continuous(trans = \"log2\") +\n  ylab(\"Income in dollars per day\") + xlab('Continent and Year') \n\n\n\n\n\n\n\n\n\n\nLeave some space\nThe design maven Edward Tufte emphasizes th eneed for clarifying and empty space. Resist the urge to pack everything into a small layout. Especially in digital formats, space can be inexpensive, and can help attract the eye to your work.\nWe can control the space around our plots using the theme() function. We’ll cover more details of theme() on Thursday. To add some space around a plot, we use theme(plot.margin = margin(t=2, r = 2, b = 2, l = 2, unit = 'cm')), where the arguments correspond to top, right, bottom, left (in that order). I’m also going to add a black border to the outside so that we can see the boundary of the frame.\n\ndslabs::gapminder %>%\n  filter(year %in% c(1970, 2010) & !is.na(gdp)) %>%\n  mutate(dollars_per_day = gdp/population/365) %>%\n  mutate(labels = paste(continent, year)) %>%\n  ggplot(aes(labels, dollars_per_day)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25)) +\n  scale_y_continuous(trans = \"log2\") +\n  ylab(\"Income in dollars per day\") + xlab('Continent and Year') +\n  theme(plot.margin = margin(t=2, r = 2, b = 2, l = 2, unit = 'cm'),\n        plot.background = element_rect(color = 'black', size = 1)) # Adds black border\n\n\n\n\n\n\n\n\n\n\nUse color\nThe comparison becomes even easier to make if we use color to denote the two things we want to compare. Now we do not have to make the labels column and can just use continent on the x-axis:"
  },
  {
    "objectID": "content/Week_03/03a.html#think-of-the-color-blind",
    "href": "content/Week_03/03a.html#think-of-the-color-blind",
    "title": "Effective Visualizations",
    "section": "Think of the color blind",
    "text": "Think of the color blind\nAbout 10% of the population is color blind. Unfortunately, the default colors used in ggplot2 are not optimal for this group. However, ggplot2 does make it easy to change the color palette used in the plots. An example of how we can use a color blind friendly palette is described here: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette:\n\ncolor_blind_friendly_cols <-\n  c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n    \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nHere are the colors\n\n\n\n\n\n\n\n\n\nFrom Seafood Prices Reveal Impacts of a Major Ecological Disturbance: \nThere are several resources that can help you select colors, for example this one: http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/.\n\nUsing a discrete color palette\nIf you’re simply trying to differentiate between groups by using color, there are many ways of changing your color palette in ggplot. Most use scale_fill_discrete or scale_color_discrete (depending on the aesthetic for which you’re setting the color).\nThe easiest way of getting good-looking (e.g. non-default) colors is the scale_fill_viridis_d function, which “inherits” (takes the place of and has the properties of) scale_fill_discrete. Viridis has four color palettes and each is designed to be used to maximize the differentiation between colors.\nWe will subset our dslabs::gapminder dataset to five different years and take a look at what Viridis colors can do across those five:\n\ngp = dslabs::gapminder %>% \nfilter(year == 1990 | year == 1995 | year==2000 |  year == 2005 | year==2010 ) %>%\nggplot(aes(x = continent, y = gdp/population, fill = as.factor(year)))  + coord_flip()\n\ngp + geom_boxplot()  + labs(title = 'Default')\n\n\n\n\n\n\n\n\nThe default uses five different colors plucked seemingly at random. They are actually drawn from a palette of default ggplot colors.\nLet’s try Viridis\n\ngp = dslabs::gapminder %>% \nfilter(year == 1990 | year == 1995 | year==2000 |  year == 2005 | year==2010 ) %>%\nggplot(aes(x = continent, y = gdp/population, fill = as.factor(year)))  + coord_flip() + labs(fill = 'Year')\n\nviridis_a = gp + geom_boxplot()  + labs(title = 'Viridis A') + scale_fill_viridis_d(option = 'A')\nviridis_b = gp + geom_boxplot()  + labs(title = 'Viridis B') + scale_fill_viridis_d(option = 'B')\nviridis_c = gp + geom_boxplot()  + labs(title = 'Viridis C') + scale_fill_viridis_d(option = 'C')\nviridis_d = gp + geom_boxplot()  + labs(title = 'Viridis D') + scale_fill_viridis_d(option = 'D')\n\ngrid.arrange(viridis_a, viridis_b, viridis_c, viridis_d)\n\n\n\n\n\n\n\n\nViridis uses a better palette of colors that, though distinct, have some cohesiveness to them.\nWe can also use a custom palette, like the colorblind palette from before (see above where we defined color_blind_friendly_cols). We just need to use the type= argument and give ggplot our color-blind friendly palette of colors. If the palette has more entries than we have (N) distinct categories, R reverts to the default.\n\ngp = dslabs::gapminder %>% \nfilter(year == 1990 | year == 1995 | year==2000 |  year == 2005 | year==2010 ) %>%\nggplot(aes(x = continent, y = gdp/population, fill = as.factor(year)))  + coord_flip() + labs(fill = 'Year') \n\ncustom_a = gp + geom_boxplot()  + labs(title = 'Custom palette') + scale_fill_discrete(type = color_blind_friendly_cols)\ncustom_b = gp + geom_boxplot()  + labs(title = 'Custom palette 1-3') + scale_fill_discrete(type = color_blind_friendly_cols[1:3])\n\ngrid.arrange(custom_a, custom_b)\n\n\n\n\n\n\n\n\nIn the lower plot, we only give it a length-3 vector of colors, and it needs 5, so it returns to default. Unfortunately, it doesn’t warn you as to this behavior. Bad R!\n\n\nUsing a continuous color palette\nWe may often want to use the color to indicate a numeric value instead of simply using it to delineate groupings. When this is the case, the fill or color aesthetic is set to a continuous value. For instance, if one were to plot election results by precinct, we may represent precincts with heavy Republican support as dark red, swing districts as purple or white, and Democratic districts as blue. The intensity of red/blue indicates how heavily slanted votes in that precinct were in the election. This is known as a color ramp.\nLets plot one country’s GDP by year, but have the color indicate the life expectancy. Whenever you map a continuous variable to a color or fill aesthetic, you get the default color ramp – dark-to-light blue:\n\ndslabs::gapminder %>%\n  filter(country=='Romania' & year>1980) %>%\n  ggplot(aes(x = year, y = gdp/population, color = life_expectancy)) + \n  geom_point(size = 5) +\n  labs(x = 'Year', y = 'GDP Per Capita', fill = 'Life Expectancy')\n\n\n\n\n\n\n\n\nWe can see that GDP per capita went up, then down in 1989 (fall of the Soviet Union), then up after that. The color ramp tells us that life expectancy reached 75 years near the end, and it certainly improved in the post-2000 era.\nWe can set some of the points on the ramp manually - here, the ramp starts at dark blue and ends at light blue, but what if we wanted to start at red, and at blue, and cross white in the middle? Easy! We use scale_color_gradient2 and specify the colors for low, mid, and high, and specify the midpoint at 72.5 years.\n\ndslabs::gapminder %>%\n  filter(country=='Romania' & year>1980) %>%\n  ggplot(aes(x = year, y = gdp/population, color = life_expectancy)) + \n  scale_color_gradient2(low = 'red', mid = 'white', high = 'blue', midpoint = 72.5) + \n  geom_point(size = 5) +\n  labs(x = 'Year', y = 'GDP Per Capita', color = 'Life Expectancy')\n\n\n\n\n\n\n\n\nThe midpoint specification is extra useful when there is a threshold (like 50% of the vote) that indicates a different qualitative outcome.\nThe gradient2 method does not always work with the colorblind discrete palette - the colors interpolated may be in the range in which colorblindness tends to be a problem:\n\ndslabs::gapminder %>%\n  filter(country=='Romania' & year>1980) %>%\n  ggplot(aes(x = year, y = gdp/population, color = life_expectancy)) + \n  scale_color_gradient2(low = color_blind_friendly_cols[3], mid = color_blind_friendly_cols[4], high = color_blind_friendly_cols[5], midpoint = 72.5) + \n  geom_point(size = 5) +\n  labs(x = 'Year', y = 'GDP Per Capita', color = 'Life Expectancy')"
  },
  {
    "objectID": "content/Week_03/03a.html#gridextra-and-grid.arrange",
    "href": "content/Week_03/03a.html#gridextra-and-grid.arrange",
    "title": "Effective Visualizations",
    "section": "gridExtra and grid.arrange",
    "text": "gridExtra and grid.arrange\nThe gridExtra package has been used a few times in this lesson to combine plots using the grid.arrange function. The use is pretty intuitive - you save your plots as objects plot1 <- ggplot(data, aes(x = var1)) and plot2 <- ggplot(data, aes(x = var2)), and then use grid.arrange(plot1, plot2) to combine. The function will align as best it can, and there are more advanced grob-based functions that can adjust and align axes between plots, but we won’t get into them. If we want to set the layout, we can specify nrow and ncol to set the rows and columns.\nThe very-useful patchwork package is quickly replacing grid.arrange and provides more flexibility."
  },
  {
    "objectID": "content/Week_03/03b.html",
    "href": "content/Week_03/03b.html",
    "title": "ggplot2: Everything you ever wanted to know",
    "section": "",
    "text": "Throughout your education, you may have noticed that numerical data is often summarized with the average value. For example, the quality of a high school is sometimes summarized with one number: the average score on a standardized test. Occasionally, a second number is reported: the standard deviation. For example, you might read a report stating that scores were 680 plus or minus 50 (the standard deviation). The report has summarized an entire vector of scores with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list?\nOur first data visualization building block is learning to summarize lists of factors or numeric vectors—the two primary data types that we encounter in data analytics. More often than not, the best way to share or explore this summary is through data visualization. The most basic statistical summary of a list of objects or numbers is its distribution. Once a vector has been summarized as a distribution, there are several data visualization techniques to effectively relay this information.\nIn this section, we first discuss properties of a variety of distributions and how to visualize distributions using a motivating example of student heights. We then discuss some principles of data visualizations more broadly, and introduce new ggplot geometries to help us along the way.\n\n\nWe will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.\nWhen each entry in a vector comes from one of a small number of groups, we refer to the data as categorical data. Two simple examples are sex (male or female) and regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers per se, such as spiciness (mild, medium, hot). In statistics textbooks, ordered categorical data are referred to as ordinal data. In psychology, a number of different terms are used for this same idea.\nExamples of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches, respectively. Counts, such as population sizes, are discrete because they have to be integers—that’s how we count.1\n\n\n\nHere we consider an artificial problem to help us illustrate the underlying concepts.\nPretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the heights data frame:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(heights)\n\nOne way to convey the heights to ET is to simply send him this list of 1050 heights. But there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights.\n\n\nIt turns out that, in some cases, the average and the standard deviation are pretty much all we need to understand the data. We will learn data visualization techniques that will help us determine when this two number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.\nThe most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for readers of this book. For example, with categorical data, the distribution simply describes the proportion of each unique category. The sex represented in the heights dataset is:\n\n\n\n   Female      Male \n0.2266667 0.7733333 \n\n\nThis two-category frequency table is the simplest form of a distribution. We don’t really need to visualize it since one number describes everything we need to know: 23% are females and the rest are males. When there are more categories, then a simple barplot describes the distribution. Here is an example with US state regions:\n\n\n\n\n\nThis particular plot simply shows us four numbers, one for each category. We usually use barplots to display a few numbers. Although this particular plot does not provide much more insight than a frequency table itself, it is a first example of how we convert a vector into a plot that succinctly summarizes all the information in the vector. When the data is numerical, the task of displaying distributions is more challenging.\n\n\n\nNumerical data that are not categorical also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of 68.503937007874 inches and only one student reported a height 68.8976377952756 inches. We assume that they converted from 174 and 175 centimeters, respectively.\nStatistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below \\(a\\) for all possible values of \\(a\\). This function is called the cumulative distribution function (CDF). In statistics, the following notation is used:\n\\[ F(a) = \\mbox{Pr}(x \\leq a) \\]\nHere is a plot of \\(F\\) for the male height data:\n\n\n\n\n\nSimilar to what the frequency table does for categorical data, the CDF defines the distribution for numerical data. From the plot, we can see that 16% of the values are below 65, since \\(F(66)=\\) 0.1637931, or that 84% of the values are below 72, since \\(F(72)=\\) 0.841133, and so on. In fact, we can report the proportion of values between any two heights, say \\(a\\) and \\(b\\), by computing \\(F(b) - F(a)\\). This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousand words”, in this case, a picture is as informative as 812 numbers.\nA final note: because CDFs can be defined mathematically—and absent any data—the word empirical is added to make the distinction when data is used. We therefore use the term empirical CDF (eCDF).\n\n\n\n\nNow, we’ll introduce ggplot geometries useful for describing distributions (or for many other things).\n\n\nAlthough the CDF concept is widely discussed in statistics textbooks, the plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? I doubt you can figure these out from glancing at the plot above. Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce plots that are much easier to interpret.\nThe simplest way to make a histogram is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: \\((49.5, 50.5],(50.5, 51.5],(51.5,52.5],(52.5,53.5],...,(82.5,83.5]\\)\n\nheights %>%\n  filter(sex==\"Male\") %>%\n  ggplot(aes(x = height)) +\n  geom_histogram(binwidth = 1, color = \"black\")\n\n\n\n\nIf we send this histogram plot to some uninformed reader, she will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, this reader could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 heights with about 30 bin counts.\nWhat information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers.\nThe geom_histogram layer only requires one aesthetic mapping - the x-axis. This is because the y-axis is computed from counts of the x-axis. Giving an aesthetic mapping to an additional variable for y will result in an error. Using an aesthetic mapping like fill will work - it’ll give you two histograms on top of each other. Try it! Try setting the alpha aesthetic to .5 (not an aesthetic mapping) so you can see both layers when they overlap.\n\n\n\nSmooth density plots are aesthetically more appealing than histograms. geom_density is the geometry that gives a smoothed density. Here is what a smooth density plot looks like for our heights data:\n\nheights %>%\n  filter(sex==\"Male\") %>%\n  ggplot(aes(height)) +\n  geom_density(alpha = .2, fill= \"#00BFC4\", color = 'gray50')  \n\n\n\n\nIn this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to density. That is, the area under the curve will add up to 1, so we can read it like a probability density.\nTo understand the smooth densities, we have to understand estimates, a topic we don’t cover until later. However, we provide a heuristic explanation to help you understand the basics so you can use this useful data visualization tool.\nThe main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of 812 male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let’s say there are 1,000,000 of these measurements. This list of values has a distribution, like any list of values, and this larger distribution is really what we want to report to ET since it is much more general. Unfortunately, we don’t get to see it.\nHowever, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps in the heights of consecutive bins. Below we have a hypothetical histogram with bins of size 1:\n\n\n\n\n\nThe smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5, and 0.1:\n\n\n\n\n\nThe smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts. We do this by using the double-dot object ..density... Objects surrounded by .. are objects that are calculated by ggplot. If we look at ?geom_histogram, and go down to “Computed variables”, we see that we could use ..count.. to get “number of points in a bin”; ..ncount.. for the count scaled to a max of 1; or ..ndensity.. which scales the density to a max of 1 (which is a strange one). We can manually set the y aesthetic mapping, which defaults to ..count.., to ..density..:\n\nx %>% ggplot(aes(x = height)) +\n  geom_histogram(aes(y=..density..), binwidth = 0.1, color = \"black\") \n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nNow, back to reality. We don’t have millions of measurements. In this concrete example, we have 812 and we can’t make a histogram with very small bins.\nWe therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars. The following plots (loosely) demonstrate the steps that the computer goes through to ultimately create a smooth density:\n\n\n\n\n\n\n\n\nNote that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:\n\n\n\n\n\nThe proportion of this area is about 0.3, meaning that about 30% of male heights are between 65 and 68 inches.\nBy understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:\n\nheights %>%\n  filter(sex==\"Male\") %>%\n  ggplot(aes(x = height)) +\n  geom_density(alpha=.2, fill= \"#00BFC4\", color = 'black') \n\n\n\n\nNote that the only aesthetic mapping is x = height, while the fill and color are set as un-mapped aesthetics.\n\n\n\nAs a final note, we point out that an advantage of smooth densities over histograms for visualization purposes is that densities make it easier to compare two distributions. This is in large part because the jagged edges of the histogram add clutter. Here is an example comparing male and female heights:\n\nheights %>%\n  ggplot(aes(height, fill=sex)) +\n  geom_density(alpha = 0.2, color = 'black')\n\n\n\n\nWith the right argument, ggplot automatically shades the intersecting region with a different color.\n\n\n\n\nHistograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.\nThe normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these later. Here we focus on how the normal distribution helps us summarize data.\nRather than using data, the normal distribution is defined with a mathematical formula. For any interval \\((a,b)\\), the proportion of values in that interval can be computed using this formula:\n\\[\\mbox{Pr}(a < x < b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}s} e^{-\\frac{1}{2}\\left( \\frac{x-m}{s} \\right)^2} \\, dx\\]\nYou don’t need to memorize or understand the details of the formula. But note that it is completely defined by just two parameters: \\(m\\) and \\(s\\). The rest of the symbols in the formula represent the interval ends that we determine, \\(a\\) and \\(b\\), and known mathematical constants \\(\\pi\\) and \\(e\\). These two parameters, \\(m\\) and \\(s\\), are referred to as the average (also called the mean) and the standard deviation (SD) of the distribution, respectively.\nThe distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average. Here is what the normal distribution looks like when the average is 0 and the SD is 1:\n\n\n\n\n\nThe fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the average and the standard deviation. We now define these values for an arbitrary list of numbers.\nFor a list of numbers contained in a vector x, the average is defined as:\n\nm <- sum(x) / length(x)\n\nand the SD is defined as:\n\ns <- sqrt(sum((x-mu)^2) / length(x))\n\nwhich can be interpreted as the average distance between values and their average.\nLet’s compute the values for the height for males which we will store in the object \\(x\\):\n\nindex <- heights$sex == \"Male\"\nx <- heights$height[index]\n\nThe pre-built functions mean and sd (note that for reasons explained later, sd divides by length(x)-1 rather than length(x)) can be used here:\n\nm <- mean(x)\ns <- sd(x)\nc(average = m, sd = s)\n\n  average        sd \n69.314755  3.611024 \n\n\nHere is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:\n\n\n\n\n\nNow, we can ask the question “is our height data approximately normally distributed?”. The normal distribution does appear to be quite a good approximation here. We now will see how well this approximation works at predicting the proportion of values within intervals.\n\n\nFor data that is approximately normally distributed, it is convenient to think in terms of standard units. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value x from a vector X, we define the value of x in standard units as z = (x - m)/s with m and s the average and standard deviation of X, respectively. Why is this convenient?\nFirst look back at the formula for the normal distribution and note that what is being exponentiated is \\(-z^2/2\\) with \\(z\\) equivalent to \\(x\\) in standard units. Because the maximum of \\(e^{-z^2/2}\\) is when \\(z=0\\), this explains why the maximum of the distribution occurs at the average. It also explains the symmetry since \\(- z^2/2\\) is symmetric around 0. Second, note that if we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average (\\(z=0\\)), one of the largest (\\(z \\approx 2\\)), one of the smallest (\\(z \\approx -2\\)), or an extremely rare occurrence (\\(z > 3\\) or \\(z < -3\\)). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.\nIn R, we can obtain standard units using the function scale:\n\nz <- scale(x)\n\nNow to see how many men are within 2 SDs from the average, we simply type:\n\nmean(abs(z) < 2)\n\n[1] 0.9495074\n\n\nThe proportion is about 95%, which is what the normal distribution predicts! To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots.\n\n\n\nA systematic way to assess how well the normal distribution fits the data is to check if the observed and predicted proportions match. In general, this is the approach of the quantile-quantile plot (QQ-plot). If our heights distribution is really normal, then the 10th percentile of our heights data should be the same as the 10th percentile of a theoretical normal, as should the 20th, 30th, 33rd, 37.5th, etc. percentiles.\nFirst let’s define the theoretical quantiles (percentiles) for the normal distribution. In statistics books we use the symbol \\(\\Phi(x)\\) to define the function that gives us the probability of a standard normal distribution being smaller than \\(x\\). So, for example, \\(\\Phi(-1.96) = 0.025\\) and \\(\\Phi(1.96) = 0.975\\). In R, we can evaluate \\(\\Phi\\) using the pnorm function:\n\npnorm(-1.96)\n\n[1] 0.0249979\n\n\nThe inverse function \\(\\Phi^{-1}(x)\\) gives us the theoretical quantiles for the normal distribution. So, for example, \\(\\Phi^{-1}(0.975) = 1.96\\). In R, we can evaluate the inverse of \\(\\Phi\\) using the qnorm function.\n\nqnorm(0.975)\n\n[1] 1.959964\n\n\nNote that these calculations are for the standard normal distribution by default (mean = 0, standard deviation = 1), but we can also define these for any normal distribution. We can do this using the mean and sd arguments in the pnorm and qnorm function. For example, we can use qnorm to determine quantiles of a distribution with a specific average and standard deviation\n\nqnorm(0.975, mean = 5, sd = 2)\n\n[1] 8.919928\n\n\nFor the normal distribution, all the calculations related to quantiles are done without data, thus the name theoretical quantiles. But quantiles can be defined for any distribution, including an empirical one. So if we have data in a vector \\(x\\), we can define the quantile associated with any proportion \\(p\\) as the \\(q\\) for which the proportion of values below \\(q\\) is \\(p\\). Using R code, we can define q as the value for which mean(x <= q) = p. Notice that not all \\(p\\) have a \\(q\\) for which the proportion is exactly \\(p\\). There are several ways of defining the best \\(q\\) as discussed in the help for the quantile function.\nTo give a quick example, for the male heights data, we have that:\n\nmean(x <= 69.5)\n\n[1] 0.5147783\n\n\nSo about 50% are shorter or equal to 69 inches. This implies that if \\(p=0.50\\) then \\(q=69.5\\).\nThe idea of a QQ-plot is that if your data is well approximated by normal distribution then the quantiles of your data should be similar to the quantiles of a normal distribution. To construct a QQ-plot, we do the following:\n\nDefine a vector of \\(m\\) proportions \\(p_1, p_2, \\dots, p_m\\).\nDefine a vector of quantiles \\(q_1, \\dots, q_m\\) for your data for the proportions \\(p_1, \\dots, p_m\\). We refer to these as the sample quantiles.\nDefine a vector of theoretical quantiles for the proportions \\(p_1, \\dots, p_m\\) for a normal distribution with the same average and standard deviation as the data.\nPlot the sample quantiles versus the theoretical quantiles.\n\nLet’s construct a QQ-plot using R code. Start by defining the vector of proportions.\n\np <- seq(0.005, 0.995, 0.01)\n\nTo obtain the quantiles from the data, we can use the quantile function like this:\n\nsample_quantiles <- quantile(x, p)\n\nTo obtain the theoretical normal distribution quantiles with the corresponding average and SD, we use the qnorm function:\n\ntheoretical_quantiles <- qnorm(p, mean = mean(x), sd = sd(x))\n\ndf = data.frame(sample_quantiles, theoretical_quantiles)\n\nTo see if they match or not, we plot them against each other and draw the identity line:\n\nggplot(data = df, aes(x = theoretical_quantiles, y = sample_quantiles)) + \n  geom_point() + \n  geom_abline() # a 45-degree line \n\n\n\n\nNotice that this code becomes much cleaner if we use standard units:\n\nsample_quantiles <- quantile(z, p)\ntheoretical_quantiles <- qnorm(p)\ndf2 =  data.frame(sample_quantiles, theoretical_quantiles)\n\nggplot(data = df2, aes(x = theoretical_quantiles, y = sample_quantiles)) + \n  geom_point() + \n  geom_abline()\n\n\n\n\nThe above code is included to help describe QQ-plots. However, in practice it is easier to use the ggplot geometry geom_qq:\n\nheights %>% filter(sex == \"Male\") %>%\n  ggplot(aes(sample = scale(height))) +\n  geom_qq() +\n  geom_abline()\n\n\n\n\nWhile for the illustration above we used 100 quantiles, the default from the geom_qq function is to use as many quantiles as data points.\n\n\n\nBefore we move on, let’s define some terms that are commonly used in exploratory data analysis.\nPercentiles are special cases of quantiles that are commonly used. The percentiles are the quantiles you obtain when setting the \\(p\\) at \\(0.01, 0.02, ..., 0.99\\). We call, for example, the case of \\(p=0.25\\) the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the median.\nFor the normal distribution the median and average are the same, but this is generally not the case.\nAnother special case that receives a name are the quartiles, which are obtained when setting \\(p=0.25,0.50\\), and \\(0.75\\).\n\n\n\n\nAlhough we haven’t gone into detain about the ggplot2 package for data visualization, we now will briefly discuss some of the geometries involved in the plots above. We will discuss ggplot2 in (excruciating) detail later this week. For now, we will briefly demonstrate how to generate plots related to distributions.\n\n\nTo generate a barplot we can use the geom_bar geometry. The default is to count the number of each category and draw a bar. Here is the plot for the regions of the US.\n\nmurders %>% ggplot(aes(region)) + geom_bar()\n\n\n\n\nWe often already have a table with a distribution that we want to present as a barplot. Here is an example of such a table:\n\ndata(murders)\ntab <- murders %>%\n  count(region) %>%\n  mutate(proportion = n/sum(n))\ntab\n\n         region  n proportion\n1     Northeast  9  0.1764706\n2         South 17  0.3333333\n3 North Central 12  0.2352941\n4          West 13  0.2549020\n\n\nWe no longer want geom_bar to count, but rather just plot a bar to the height provided by the proportion variable. For this we need to provide x (the categories) and y (the values) and use the stat=\"identity\" option. This tells R to just use the actual value in proportion for the y aesthetic. This is only necessary when you’re telling R that you have your own field (proportion) that you want to use instead of just the count.\n\ntab %>% ggplot(aes(x = region, y = proportion)) + geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\nTo generate histograms we use geom_histogram. By looking at the help file for this function, we learn that the only required argument is x, the variable for which we will construct a histogram. We dropped the x because we know it is the first argument. The code looks like this:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  ggplot(aes(height)) +\n  geom_histogram()\n\nIf we run the code above, it gives us a message:\n\nstat_bin() using bins = 30. Pick better value with binwidth.\n\nWe previously used a bin size of 1 inch (of observed height), so the code looks like this:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1)\n\nFinally, if for aesthetic reasons we want to add color, we use the arguments described in the help file. We also add labels and a title:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", col = \"black\") +\n  labs(x = \"Male heights in inches\", title = \"Histogram\")\n\n\n\n\n\n\n\nTo create a smooth density, we use the geom_density. To make a smooth density plot with the data previously shown as a histogram we can use this code:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  ggplot(aes(x = height)) +\n  geom_density()\n\nTo fill in with color, we can use the fill argument.\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  ggplot(aes(x = height)) +\n  geom_density(fill=\"blue\")\n\n\n\n\nTo change the smoothness of the density, we use the adjust argument to multiply the default value by that adjust. For example, if we want the bandwidth to be twice as big we use:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  ggplot(aes(x = height)) + \n  geom_density(fill=\"blue\", adjust = 2)\n\n\n\n\nThe geometry for boxplot is geom_boxplot. As discussed, boxplots are useful for comparing distributions. For example, below are the previously shown heights for women, but compared to men. For this geometry, we need arguments x as the categories, and y as the values.\n\n\n\n\n\nNote that our x-axis is a categorical variable. The order is determined by either the factor variable levels in heights or, if no levels are set, in the order in which the sex variable first encounters them. Later on, we’ll learn how to change the ordering.\nWe can do much more with boxplots when we have more data. Right now, our heights data has only two variables - sex and height. Let’s say we took the measurements over two different years - 2010 and 2020. That’s not in our data, so purely for exposition, we’ll add it by randomly drawing a year for each observation. We’ll do this with sample\n\nheights = heights %>%\n  dplyr::mutate(year = sample(x = c(2010, 2020), size = n(), replace = TRUE, prob = c(.5, .5)))\n\nhead(heights)\n\n     sex height year\n1   Male     75 2020\n2   Male     70 2010\n3   Male     68 2020\n4   Male     74 2010\n5   Male     61 2020\n6 Female     65 2010\n\n\nNow, let’s look at the boxplot of heights by sex, but broken out by year. We can do this by adding year as an aesthetic mapping. Because our year variable is an integer, R will start by thinking it’s a continuous numeric, but we want to treat it as a discrete variable. So, we wrap it in as.factor() to force R to recognize it as a discrete variable.\n\nheights %>% ggplot(aes(x = sex, y = height, fill = as.factor(year))) +\n  geom_boxplot() +\n  labs(fill = 'Year')\n\n\n\n\nNow we have each sex broken out by year! Since we randomly assigned year to our data (and didn’t actually take samples in two different decades), the distribution between years and within sex is nearly identical.\nWhat if we wanted to have year on the x-axis, but then put the sex boxplots next to each other. This would let us compare the difference in heights by sex over the two sample years.\n\nheights %>% ggplot(aes(x = year, y = height, fill = sex)) + \n  geom_boxplot() +\n  labs(fill = 'Sex')\n\n\n\n\nWoah. Wait. What? Remember, in our data, class(heights$year) is numeric, so when we ask R to put year on the x-axis, it thinks it’s plotting a number. It gives us a nonsense x-axis. How do we fix this? We force as.factor(year) to tell R that yes, year is a categorical variable. Note that we didn’t have to use as.factor(sex) - that’s because sex is already a categorical variable.\n\nheights %>% ggplot(aes(x = as.factor(year), y = height, fill = sex)) + \n  geom_boxplot() +\n  labs(fill = 'Sex')\n\n\n\n\nNow we can see the height difference by sex, by year.\nWe will explore more with boxplots and colors in our next lecture.\n\n\n\n\n\n\n\nTry it!\nTRY IT\nStart by loading the dplyr and ggplot2 library as well as the murders and heights data.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(dslabs)\ndata(heights)\ndata(murders)\n\n\nFirst, create a new variable in murders that has murders_per_capita.\n\n\nmurders = murders %>%\n  mutate(........)\n\n\nMake a histogram of murders per capita. Use the default values for color and fill, but make sure you label the x-axis with a meaningful label.\nMake the same histogram, but set the fill aesthetic to MSU Green and the color to black.\nDo the same, but make it a smooth density plot\nFinally, plot the smooth density but use a fill aesthetic mapping so that each region’s density is shown. Set a meaningful title on the legend, and make sure you make the density transparent so we can see all of the region’s densities (see alpha aesthetic).\nNow, try making a boxplot to show the same data - the distribution across states of murders per capita by region. What is the average Northeastern state’s murder rate? What about the average Southern state?"
  },
  {
    "objectID": "content/Week_04/04a.html",
    "href": "content/Week_04/04a.html",
    "title": "Visualizations in Practice",
    "section": "",
    "text": "This page.\n\n\n\n\nToday we’re mostly learning some technical aspects of ggplot.\nWhy are we covering the material this way? (Good question. There’s an answer!)\n\n\n\nIn this activity, we will use R to visualize and compare the actual electoral votes from the 2008 presidential election with the polling data prior to the election. Specifically, we will explore how accurately the polls predicted the election results. Yes, I know this is an older election… that’s kind of the point. This isn’t a politics class.\nSetup:\nYou’ll need to download two CSV files and put them somewhere on your computer (or upload it to RStudio.cloud if you’ve gone that direction)—preferably in a folder named data in your project folder. You can download the data from the link below:\n\n pres08.csv\n polls08.csv\n\nSteps Prior to Analysis:\n\nLoad the Data\n\n\n# Load the necessary libraries\nlibrary(tidyverse)\nelectoral_votes <- read.csv(\"path_to/pres08.csv\")\npolling_data <- read.csv(\"path_to/polls08.csv\")\n\nNote that you’ll have to replace path_to with the actual path to the files on your computer.\n\nData Preparation\n\nWe need to aggregate the polling data to get an average poll result for each state. We will then merge this with the electoral votes data.\n\n# Aggregating polling data\navg_polls <- polling_data %>%\n  group_by(state) %>%\n  summarise(Obama_avg = mean(Obama), McCain_avg = mean(McCain))\n\n# Merging with electoral votes data\ncombined_data <- merge(electoral_votes, avg_polls, by = \"state\")\n\n\nComplete Your Analysis\n\nYou are tasked with three things:\nA: Explore the discrepancies between polls and actual results.\nB: Visualize the relationship between the actual results and the poll results. Does this relationship change over time? Or, put another way, were early polls or later polls more accurate?\nYou might find this pseudo-code helpful:\n\n# Define the regions\nregions <- tibble(\n  region = c('Northeast', 'Midwest', 'South', 'West'),\n  states = list(\n    c('CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA'),\n    c('IL', 'IN', 'MI', 'OH', 'WI', 'IA', 'KS', 'MN', 'MO', 'NE', 'ND', 'SD'),\n    c('DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'DC', 'WV', 'AL', 'KY', 'MS', 'TN', 'AR', 'LA', 'OK', 'TX'),\n    c('AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AK', 'CA', 'HI', 'OR', 'WA')\n  )\n)\n\n# Unnest the states to create a mapping dataframe\nstate_region_mapping <- regions %>%\n  unnest(states) %>%\n  rename(state = states)\n\n# Assuming 'polling_data' is your original dataframe\npolling_data_with_regions <- left_join(polling_data, state_region_mapping, by = \"state\")\n\nC: Write up your conclusions.\nHappy visualizing!\n\n\n\n\nLoad up our murders data\n\nlibrary(dslabs)\nlibrary(ggplot2)\nlibrary(dplyr)\ndata(murders)\n\np <- ggplot(data = murders, aes(x = population, y = total, label = abb))"
  },
  {
    "objectID": "content/Week_04/04a.html#scales-and-transformations",
    "href": "content/Week_04/04a.html#scales-and-transformations",
    "title": "Visualizations in Practice",
    "section": "Scales and transformations",
    "text": "Scales and transformations\n\nLog transformations\nLast lecture, we re-scaled our population by 10^6 (millions), but still had a lot of variation because some states are tiny and some are huge. Sometimes, we want to have one (or both) of our axes scaled non-linearly. For instance, if we wanted to have our x-axis be in log base 10, then each major tick would represent a factor of 10 over the last. This is not the default, so this change needs to be added through a scales layer. A quick look at the cheat sheet reveals the scale_x_continuous function lets us control the behavior of scales. We use them like this:\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 0.05) +\n  scale_x_continuous(trans = \"log10\") +\n  scale_y_continuous(trans = \"log10\")\n\n\n\n\nA couple of things here: adding things like scale_x_continuous(...) operates on the whole plot. In some cases, order matters, but it doesn’t here, so we can throw scale_x_continuous anywhere. Because we have altered the whole plot’s scale to be in the log-scale now, the nudge must be made smaller. It is in log-base-10 units. Using ?scale_x_continuous brings us to the help for both scale_x_continuous and scale_y_continuous, which shows us the options for transformations trans = ...\nThis particular transformation is so common that ggplot2 provides the specialized functions scale_x_log10 and scale_y_log10 which “inherit” (take the place of) the scale_x_continuous functions but have log base 10 as default. We can use these to rewrite the code like this:\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 0.05) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\nThis can make a plot much easier to read, though one has to be sure to pay attention to the values on the axes. Plotting anything with very large outliers will almost always be better if done in log-scale. Adding the scale layer is an easy way to fix this.\nWe can also use one of many built-in transformations. Of note: reverse just inverts the scale, which can be helpful, log uses the natural log, sqrt takes the square root (dropping anything with a negative value), reciprocal takes 1/x. If your x-axis is in a date format, you can also scale to hms (hour-minute-second) or date.\n\n\nTransforming data vs. transforming using scale_...\nWe could simply take the log of population and log of total in the call and we’d get something very similar. Note that we had to override the aesthetic mapping set in p in each of the geometries:\n\np + geom_point(aes(x = log(population, base=10), y = log(total, base=10)), size = 3) +\n  geom_text(aes(x = log(population, base=10), y = log(total, base=10)), nudge_x = 0.05) \n\n\n\n\nThis avoids using scale_x_continuous or it’s child function scale_x_log10. One advantage to using scale_x... is that the axes are correctly labeled. When we transform the data directly, the axis labels only show the transformed values, so 7,000,000 becomes 7.0. This could be confusing! We could update the axis labels to say “total murders (log base 10)” and “total population (log base 10)”, but that’s cumbersome. Using scale_x... is a lot more refined and easy."
  },
  {
    "objectID": "content/Week_04/04a.html#axis-labels-legends-and-titles",
    "href": "content/Week_04/04a.html#axis-labels-legends-and-titles",
    "title": "Visualizations in Practice",
    "section": "Axis labels, legends, and titles",
    "text": "Axis labels, legends, and titles\nBut let’s say we did want to re-name our x-axis label. Or maybe we don’t like that the variable column name is lower-case “p”.\nAs with many things in ggplot, there are many ways to get the same result. We’ll go over one way of changing titles and labels, but know that there are many more.\n\nChanging axis titles\nWe’ll use the labs(...) annotation layer to do this, which is pretty straightforward. ?labs shows us what we can change, and while it looks pretty basic, the real meat is in the ... argument, which the help says is “A list of new name-value pairs”. This means we can re-define the label on anything that is an aesthetic mapping. X and Y are aesthetic mappings, so…\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 0.05) +\n  scale_x_log10() +\n  scale_y_log10() + \n  labs(x = 'Population', y = 'Total murders')\n\n\n\n\nNow, let’s use an aesthetic mapping that generates a legend, like color, and see what labs renames:\n\np + geom_point(aes(color = region), size = 3) +\n  geom_text(nudge_x = 0.05) +\n  scale_x_log10() +\n  scale_y_log10() + \n  labs(x = 'Population', y = 'Total murders', color = 'US Region')\n\n\n\n\nWe can rename the aesthetic mapping-relevant label using labs. Even if there are multiple mapped aesthetics:\n\np + geom_point(aes(color = region, size = total/population)) +\n  geom_text(nudge_x = 0.05) +\n  scale_x_log10() +\n  scale_y_log10() + \n  labs(x = 'Population', y = 'Total murders', color = 'US Region', size = 'Murder rate')\n\n\n\n\n\n\nTitles\nIn ?labs, we also see some things that look like titles and captions. We can include those:\n\np + geom_point(aes(color = region, size = total/population)) +\n  geom_text(nudge_x = 0.05) +\n  scale_x_log10() +\n  scale_y_log10() + \n  labs(x = 'Population', y = 'Total murders', color = 'US Region', size = 'Murder rate',\n       title = 'This is a title', subtitle = 'This is a subtitle', caption = 'This is a caption', tag = 'This is a tag')\n\n\n\n\nNow that you know how, always label your plots with at least a title and have meaningful axis and legend labels."
  },
  {
    "objectID": "content/Week_04/04a.html#axis-ticks",
    "href": "content/Week_04/04a.html#axis-ticks",
    "title": "Visualizations in Practice",
    "section": "Axis ticks",
    "text": "Axis ticks\nIn addition to the axis labels, we may want to format or change the axis tick labels (like “1e+06” above) or even where the tick marks and lines are drawn. If we don’t specify anything, the axis labels and tick marks are drawn as best as ggplot can do, but we can change this. This might be especially useful if our data has some meaningful cutoffs that aren’t found by the default, or we just don’t like where the marks fall or how they are labeled. This is easy to fix with ggplot.\nTo change the tick mark labels, we have to set the tick mark locations. Then we can set a label for each tick mark. Let’s go back to our murders data and, for simplicity, take the log transformation off the Y axis. We’ll use scale_y_continuous to tell R where to put the breaks (breaks =) and what to label the breaks. We have to give it one label for every break. Let’s say we just want a line at the 500’s and let’s say we want to (absurdly) use written numerics for each of the Y-axis lines. Since scale_y_log10 inherits from scale_y_continuous, we can just use that and add the breaks and labels:\n\np + geom_point(aes(color = region), size = 3) +\n  geom_text(nudge_x = .05) +\n  scale_x_log10() +\n  scale_y_log10(breaks = c(0,50, 100, 500,1000,1500), \n                     labels = c('Zero','Fifty','One hundred','Five hundred','One thousand','Fifteen hundred')) +\n  labs(x = 'Population', y = 'Total murders', color = 'US Region')\n\n\n\n\nWe have manually set both the location and the label for the y-axis. Note that R filled in the in-between “minor” tick lines, but we can take those out. Since we are setting the location of the lines, we can do anything we want:\n\np + geom_point(aes(color = region), size = 3) +\n  geom_text(nudge_x = .05) +\n  scale_x_log10() +\n  scale_y_log10(breaks = c(0,50, 100, 721, 1000,1500), \n                     labels = c('Zero','Fifty','One hundred','Seven hundred twenty one','One thousand','Fifteen hundred'),\n                     minor_breaks = NULL) +\n  labs(x = 'Population', y = 'Total murders', color = 'US Region')\n\n\n\n\nSo we can now define where axis tick lines should lie and how they should be labeled."
  },
  {
    "objectID": "content/Week_04/04a.html#additional-geometries",
    "href": "content/Week_04/04a.html#additional-geometries",
    "title": "Visualizations in Practice",
    "section": "Additional geometries",
    "text": "Additional geometries\nLet’s say we are happy with our axis tick locations, but we want to add a single additional line. Maybe we want to divide at 1,000,000 population (a vertical line at 1,000,000) becuase we think those over 1,000,000 are somehow different, and we want to call attention to the data around that point. As a more general example, if we were to plot, say, car accidents by age, we would maybe want to label age 21, when people can legally purchase alcohol (and subsequently cause car accidents).\nThis brings us to our first additional geometry beyond geom_point (OK, we used geom_text, but that’s more of an annotation). geom_vline lets us add a single vertical line (without aesthetic mappings). If we look at ?geom_vline we see that it requires ones aesthetic:xintercept. It also takes aesthetics like color and size, and introduces the linetype aesthetic:\n\np + geom_point(aes(color = region), size = 3) +\n  geom_text(nudge_x = .05) +\n  geom_vline(aes(xintercept = 1000000), col = 'red', size = 2, linetype = 2) +\n  scale_x_log10() +\n  scale_y_log10(breaks = c(0,50, 100, 721, 1000,1500), \n                     labels = c('Zero','Fifty','One hundred','Seven hundred twenty one','One thousand','Fifteen hundred'),\n                     minor_breaks = NULL) +\n  labs(x = 'Population', y = 'Total murders', color = 'US Region')\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nCombining geometries is as easy as adding the layers with +.\n\ngeom_line\nFor a good old line plot, we use the line geometry at geom_line. The help for ?geom_line tells us that we need an x and a y aesthetic (much like geom_points). Since our murders data isn’t really suited to a line graph, we’ll use a daily stock price. We’ll get this using tidyquant, which pulls stock prices from Yahoo Finance and maintains the “tidy” format. You’ll need to install.packages('tidyquant') before you run this the first time.\n\nlibrary(tidyquant)\nAAPL = tq_get(\"AAPL\", from = '2009-01-01', to = '2021-08-01', get = 'stock.prices')\nhead(AAPL)\n\n# A tibble: 6 × 8\n  symbol date        open  high   low close     volume adjusted\n  <chr>  <date>     <dbl> <dbl> <dbl> <dbl>      <dbl>    <dbl>\n1 AAPL   2009-01-02  3.07  3.25  3.04  3.24  746015200     2.74\n2 AAPL   2009-01-05  3.33  3.43  3.31  3.38 1181608400     2.85\n3 AAPL   2009-01-06  3.43  3.47  3.30  3.32 1289310400     2.81\n4 AAPL   2009-01-07  3.28  3.30  3.22  3.25  753048800     2.74\n5 AAPL   2009-01-08  3.23  3.33  3.22  3.31  673500800     2.80\n6 AAPL   2009-01-09  3.33  3.34  3.22  3.23  546845600     2.73\n\n\nNow, we can plot a line graph of the Apple closing stock price over the requested date range. We want this to be a time series, so the x-axis will be the date and the y-axis will be the closing price.\n\nggplot(AAPL, aes(x = date, y = close)) +\n  geom_line() +\n  labs(x = 'Date', y = 'Closing price', title = 'Apple stock price')\n\n\n\n\nIn geom_line, R will automatically sort on the x-variable. If you don’t want this, then geom_path will use whatever order the data is in. Either way, if you have multiple observations for the same value on the x-axis, then you’ll get something pretty messy because R will try to connect, in some order, all the points. Let’s see an example with two stocks:\n\nAAPLNFLX = tq_get(c(\"AAPL\",\"NFLX\"), from = '2021-01-01', to = '2021-08-01', get = 'stock.prices')\nggplot(AAPLNFLX, aes(x = date, y = close)) +\n  geom_line() +\n  labs(x = 'Date', y = 'Closing price', title = 'Apple and Netflix stock price')\n\n\n\n\nThat looks kinda strange. That’s because, for every date, we have two values - the NFLX and the AAPL value, so each day has a vertical line drawn between the two prices. This is nonsense, especially since what we want to see is the history of NFLX and AAPL over time.\nAesthetics to the rescue! Remember, when we use an aesthetic mapping, we are able to separate out data by things like color or linetype. Let’s use color as the aesthetic here, and map it to the stock ticker:\n\nAAPLNFLX = tq_get(c(\"AAPL\",\"NFLX\"), from = '2021-01-01', to = '2021-08-01', get = 'stock.prices')\nggplot(AAPLNFLX, aes(x = date, y = close, color = symbol)) +\n  geom_line() +\n  labs(x = 'Date', y = 'Closing price', title = 'Apple and Netflix stock price')\n\n\n\n\nWell there we go! We can now see each stock price over time, with a convenient legend. Later on, we’ll learn how to change the color palatte. If we don’t necessarily want a different color but we do want to separate the lines, we can use the group aesthetic.\n\nAAPLNFLX = tq_get(c(\"AAPL\",\"NFLX\"), from = '2021-01-01', to = '2021-08-01', get = 'stock.prices')\nggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +\n  geom_line() + \n  labs(x = 'Date', y = 'Closing price', title = 'Apple and Netflix stock price')\n\n\n\n\nSimilar result as geom_line, but without the color difference (which makes it rather hard to tell what you’re looking at). But if we add labels using geom_label, we’ll get one label for every point, which will be overwhelming. The solution? Use some filtered data so that there is only one point for each label. But that means replacing the data in ggplot. Here’s how.\n\n\nUsing different data with different geometries\nJust as we can use different aesthetic mappings on each geometry, we can use different data entirely. This is useful when we want one geometry to have one set of data (like the stock prices above), but another geometry to only have a subset of the data. Why would we want that? Well, we’d like to label just one part of each of the lines in our plot, right? That means we want to label a subset of the stock data.\nTo replace data in a geometry, we just need to specify the data = argument separately:\n\nggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +\n  geom_line() +\n  geom_label(data = AAPLNFLX %>% group_by(symbol) %>% slice(100),\n             aes(label = symbol),\n             nudge_y = 20) + \n  labs(x = 'Date', y = 'Closing price', title = 'Apple and Netflix stock price')\n\n\n\n\nIn geom_label, we specified we wanted the 100th observation from each symbol to be the label location. Then, we nudged it up along y by 20 so that it’s clear of the line.\nR also has a very useful ggrepel package that gives us geom_label_repel which takes care of the nudging for us, even in complicated situations (lots of points, lines, etc.). It does a decent job here of moving the label to a point where it doesn’t cover a lot of data.\n\nlibrary(ggrepel)\nggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +\n  geom_line() +\n  geom_label_repel(data = AAPLNFLX %>% group_by(symbol) %>% slice(100),\n             aes(label = symbol)) + \n  labs(x = 'Date', y = 'Closing price', title = 'Apple and Netflix stock price')\n\n\n\n\nNow, we don’t lose a lot of space to a legend, and we haven’t had to use color to separate the stock symbols.\n\n\nMultiple geometries\nSince this section is about adding geometries, we can combine points and lines. Since lines connect points, it will look like a giant connect-the-dots.\n\nlibrary(ggrepel)\nggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +\n  geom_line() +\n  geom_point() + \n  geom_label_repel(data = AAPLNFLX %>% group_by(symbol) %>% slice(100),\n             aes(label = symbol)) + \n  labs(x = 'Date', y = 'Closing price', title = 'Apple and Netflix stock price')"
  },
  {
    "objectID": "content/Week_04/04a.html#try-it",
    "href": "content/Week_04/04a.html#try-it",
    "title": "Visualizations in Practice",
    "section": "Try it!",
    "text": "Try it!\n\nTRY IT\nStart by loading the dplyr and ggplot2 library as well as the murders and heights data.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(dslabs)\ndata(heights)\ndata(murders)\n\n\nWith ggplot2 plots can be saved as objects. For example we can associate a dataset with a plot object like this\n\n\np <- ggplot(data = murders)\n\nBecause data is the first argument we don’t need to spell it out\n\np <- ggplot(murders)\n\nand we can also use the pipe:\n\np <- murders %>% ggplot()\n\nWhat is class of the object p?\n\nRemember that to print an object you can use the command print or simply type the object. Print the object p defined in exercise one and describe what you see.\n\n\nNothing happens.\nA blank slate plot.\nA scatterplot.\nA histogram.\n\n\nUsing the pipe %>%, create an object p but this time associated with the heights dataset instead of the murders dataset.\nWhat is the class of the object p you have just created?\nNow we are going to add a layer and the corresponding aesthetic mappings. For the murders data we plotted total murders versus population sizes. Explore the murders data frame to remind yourself what are the names for these two variables and select the correct answer. Hint: Look at ?murders.\n\n\nstate and abb.\ntotal_murders and population_size.\ntotal and population.\nmurders and size.\n\n\nTo create the scatterplot we add a layer with geom_point. The aesthetic mappings require us to define the x-axis and y-axis variables, respectively. So the code looks like this:\n\n\nmurders %>% ggplot(aes(x = , y = )) +\n  geom_point()\n\nexcept we have to define the two variables x and y. Fill this out with the correct variable names.\n\nNote that if we don’t use argument names, we can obtain the same plot by making sure we enter the variable names in the right order like this:\n\n\nmurders %>% ggplot(aes(population, total)) +\n  geom_point()\n\nRemake the plot but now with total in the x-axis and population in the y-axis.\n\nIf instead of points we want to add text, we can use the geom_text() or geom_label() geometries. The following code\n\n\nmurders %>% ggplot(aes(population, total)) + geom_label()\n\nwill give us the error message: Error: geom_label requires the following missing aesthetics: label\nWhy is this?\n\nWe need to map a character to each point through the label argument in aes.\nWe need to let geom_label know what character to use in the plot.\nThe geom_label geometry does not require x-axis and y-axis values.\ngeom_label is not a ggplot2 command.\n\n\nRewrite the code above to use abbreviation as the label through aes\nChange the color of the labels to blue. How will we do this?\n\n\nAdding a column called blue to murders.\nBecause each label needs a different color we map the colors through aes.\nUse the color argument in ggplot.\nBecause we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.\n\n\nRewrite the code above to make the labels blue.\nNow suppose we want to use color to represent the different regions. In this case which of the following is most appropriate:\n\n\nAdding a column called color to murders with the color we want to use.\nBecause each label needs a different color we map the colors through the color argument of aes .\nUse the color argument in ggplot.\nBecause we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.\n\n\nRewrite the code above to make the labels’ color be determined by the state’s region.\nNow we are going to change the x-axis to a log scale to account for the fact the distribution of population is skewed. Let’s start by defining an object p holding the plot we have made up to now\n\n\np <- murders %>%\n  ggplot(aes(population, total, label = abb, color = region)) +\n  geom_label()\n\nTo change the y-axis to a log scale we learned about the scale_x_log10() function. Add this layer to the object p to change the scale and render the plot.\n\nRepeat the previous exercise but now change both axes to be in the log scale.\nNow edit the code above to add the title “Gun murder data” to the plot. Hint: use the labs function or the ggtitle function."
  },
  {
    "objectID": "content/Week_04/04b.html#the-ecological-fallacy-and-importance-of-showing-the-data",
    "href": "content/Week_04/04b.html#the-ecological-fallacy-and-importance-of-showing-the-data",
    "title": "Visualizations",
    "section": "The ecological fallacy and importance of showing the data",
    "text": "The ecological fallacy and importance of showing the data\nThroughout this section, we have been comparing regions of the world. We have seen that, on average, some regions do better than others. In this section, we focus on describing the importance of variability within the groups when examining the relationship between a country’s infant mortality rates and average income.\nWe define a few more regions and compare the averages across regions:\n\n\n\n\n\nThe relationship between these two variables is almost perfectly linear and the graph shows a dramatic difference. While in the West less than 0.5% of infants die, in Sub-Saharan Africa the rate is higher than 6%!\nNote that the plot uses a new transformation, the logit transformation.\n\nLogit transformation\nThe logit transformation for a proportion or rate \\(p\\) is defined as:\n\\[f(p) = \\log \\left( \\frac{p}{1-p} \\right)\\]\nWhen \\(p\\) is a proportion or probability, the quantity that is being logged, \\(p/(1-p)\\), is called the odds. In this case \\(p\\) is the proportion of infants that survived. The odds tell us how many more infants are expected to survive than to die. The log transformation makes this symmetric. If the rates are the same, then the log odds is 0. Fold increases or decreases turn into positive and negative increments, respectively.\nThis scale is useful when we want to highlight differences near 0 or 1. For survival rates this is important because a survival rate of 90% is unacceptable, while a survival of 99% is relatively good. We would much prefer a survival rate closer to 99.9%. We want our scale to highlight these difference and the logit does this. Note that 99.9/0.1 is about 10 times bigger than 99/1 which is about 10 times larger than 90/10. By using the log, these fold changes turn into constant increases.\n\n\nShow the data\nNow, back to our plot. Based on the plot above, do we conclude that a country with a low income is destined to have low survival rate? Do we conclude that survival rates in Sub-Saharan Africa are all lower than in Southern Asia, which in turn are lower than in the Pacific Islands, and so on?\nJumping to this conclusion based on a plot showing averages is referred to as the ecological fallacy. The almost perfect relationship between survival rates and income is only observed for the averages at the region level. Once we show all the data, we see a somewhat more complicated story:\n\n\n\n\n\nSpecifically, we see that there is a large amount of variability. We see that countries from the same regions can be quite different and that countries with the same income can have different survival rates. For example, while on average Sub-Saharan Africa had the worse health and economic outcomes, there is wide variability within that group. Mauritius and Botswana are doing better than Angola and Sierra Leone, with Mauritius comparable to Western countries."
  },
  {
    "objectID": "content/Week_04/04b.html#vaccines",
    "href": "content/Week_04/04b.html#vaccines",
    "title": "Visualizations",
    "section": "Case study: vaccines and infectious diseases",
    "text": "Case study: vaccines and infectious diseases\nVaccines have helped save millions of lives. In the 19th century, before herd immunization was achieved through vaccination programs, deaths from infectious diseases, such as smallpox and polio, were common. However, today vaccination programs have become somewhat controversial despite all the scientific evidence for their importance.\nThe controversy started with a paper1 published in 1988 and led by Andrew Wakefield claiming there was a link between the administration of the measles, mumps, and rubella (MMR) vaccine and the appearance of autism and bowel disease. Despite much scientific evidence contradicting this finding, sensationalist media reports and fear-mongering from conspiracy theorists led parts of the public into believing that vaccines were harmful. As a result, many parents ceased to vaccinate their children. This dangerous practice can be potentially disastrous given that the Centers for Disease Control (CDC) estimates that vaccinations will prevent more than 21 million hospitalizations and 732,000 deaths among children born in the last 20 years (see Benefits from Immunization during the Vaccines for Children Program Era — United States, 1994-2013, MMWR2). The 1988 paper has since been retracted and Andrew Wakefield was eventually “struck off the UK medical register, with a statement identifying deliberate falsification in the research published in The Lancet, and was thereby barred from practicing medicine in the UK.” (source: Wikipedia3). Yet misconceptions persist, in part due to self-proclaimed activists who continue to disseminate misinformation about vaccines.\nEffective communication of data is a strong antidote to misinformation and fear-mongering. Earlier we used an example provided by a Wall Street Journal article4 showing data related to the impact of vaccines on battling infectious diseases. Here we reconstruct that example.\nThe data used for these plots were collected, organized, and distributed by the Tycho Project5. They include weekly reported counts for seven diseases from 1928 to 2011, from all fifty states. The yearly totals are helpfully included in the dslabs package:\n\nlibrary(RColorBrewer)\ndata(us_contagious_diseases)\nnames(us_contagious_diseases)\n\n[1] \"disease\"         \"state\"           \"year\"            \"weeks_reporting\"\n[5] \"count\"           \"population\"     \n\n\nWe create a temporary object dat that stores only the measles data, includes a per 100,000 rate, orders states by average value of disease and removes Alaska and Hawaii since they only became states in the late 1950s. Note that there is a weeks_reporting column that tells us for how many weeks of the year data was reported. We have to adjust for that value when computing the rate.\n\nthe_disease <- \"Measles\"\ndat <- us_contagious_diseases %>%\n  filter(!state%in%c(\"Hawaii\",\"Alaska\") & disease == the_disease) %>%\n  mutate(rate = count / population * 10000 * 52 / weeks_reporting) %>%\n  mutate(state = reorder(state, rate))\n\nWe can now easily plot disease rates per year. Here are the measles data from California:\n\ndat %>% filter(state == \"California\" & !is.na(rate)) %>%\n  ggplot(aes(year, rate)) +\n  geom_line() +\n  ylab(\"Cases per 10,000\")  +\n  geom_vline(xintercept=1963, col = \"blue\")\n\n\n\n\nWe add a vertical line at 1963 since this is when the vaccine was introduced [Control, Centers for Disease; Prevention (2014). CDC health information for international travel 2014 (the yellow book). p. 250. ISBN 9780199948505].\nNow can we show data for all states in one plot? We have three variables to show: year, state, and rate. In the WSJ figure, they use the x-axis for year, the y-axis for state, and color hue to represent rates. However, the color scale they use, which goes from yellow to blue to green to orange to red, can be improved.\nIn our example, we want to use a sequential palette since there is no meaningful center, just low and high rates.\nWe use the geometry geom_tile to tile the region with colors representing disease rates. We use a square root transformation to avoid having the really high counts dominate the plot. Notice that missing values are shown in grey. Note that once a disease was pretty much eradicated, some states stopped reporting cases all together. This is why we see so much grey after 1980.\n\ndat %>% ggplot(aes(year, state, fill = rate)) +\n  geom_tile(color = \"grey50\") +\n  scale_x_continuous(expand=c(0,0)) +\n  scale_fill_gradientn(colors = brewer.pal(9, \"Reds\"), trans = \"sqrt\") +\n  geom_vline(xintercept=1963, col = \"blue\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        legend.position=\"bottom\",\n        text = element_text(size = 8)) +\n  ggtitle(the_disease) +\n  ylab(\"\") + xlab(\"\")\n\n\n\n\nThis plot makes a very striking argument for the contribution of vaccines. However, one limitation of this plot is that it uses color to represent quantity, which we earlier explained makes it harder to know exactly how high values are going. Position and lengths are better cues. If we are willing to lose state information, we can make a version of the plot that shows the values with position. We can also show the average for the US, which we compute like this:\n\navg <- us_contagious_diseases %>%\n  filter(disease==the_disease) %>% group_by(year) %>%\n  summarize(us_rate = sum(count, na.rm = TRUE) /\n              sum(population, na.rm = TRUE) * 10000)\n\nNow to make the plot we simply use the geom_line geometry: ::: {.cell}\ndat %>%\n  filter(!is.na(rate)) %>%\n    ggplot() +\n  geom_line(aes(year, rate, group = state),  color = \"grey50\",\n            show.legend = FALSE, alpha = 0.2, size = 1) +\n  geom_line(mapping = aes(year, us_rate),  data = avg, size = 1) +\n  scale_y_continuous(trans = \"sqrt\", breaks = c(5, 25, 125, 300)) +\n  ggtitle(\"Cases per 10,000 by state\") +\n  xlab(\"\") + ylab(\"\") +\n  geom_text(data = data.frame(x = 1955, y = 50),\n            mapping = aes(x, y, label=\"US average\"),\n            color=\"black\") +\n  geom_vline(xintercept=1963, col = \"blue\")\n\n\n\n:::\nIn theory, we could use color to represent the categorical value state, but it is hard to pick 50 distinct colors."
  },
  {
    "objectID": "content/Week_04/04b.html#try-it",
    "href": "content/Week_04/04b.html#try-it",
    "title": "Visualizations",
    "section": "TRY IT",
    "text": "TRY IT\n\nTRY IT\n\nReproduce the heatmap plot we previously made but for smallpox. For this plot, do not include years in which cases were not reported in 10 or more weeks.\nNow reproduce the time series plot we previously made, but this time following the instructions of the previous question for smallpox.\nFor the state of California, make a time series plot showing rates for all diseases. Include only years with 10 or more weeks reporting. Use a different color for each disease.\nNow do the same for the rates for the US. Hint: compute the US rate by using summarize: the total divided by total population."
  },
  {
    "objectID": "content/Week_05/05a.html",
    "href": "content/Week_05/05a.html",
    "title": "Probability and Statistics",
    "section": "",
    "text": "This page.\n\n\n\n\n Why It’s So Hard for Us to Visualize Uncertainty\n Amanda Cox’s keynote address at the 2017 OpenVis Conf\n Communicating Uncertainty When Lives Are on the Line\n Showing uncertainty during the live election forecast & Trolling the uncertainty dial\n\n\n\n\n\nWhy is uncertainty inherently a major part of data analytics?\nHow have past attempts to visualize uncertainty failed?\nWhat is the right way to visualize election uncertainty?"
  },
  {
    "objectID": "content/Week_05/05a.html#monte-carlo-simulations-for-categorical-data",
    "href": "content/Week_05/05a.html#monte-carlo-simulations-for-categorical-data",
    "title": "Probability and Statistics",
    "section": "Monte Carlo simulations for categorical data",
    "text": "Monte Carlo simulations for categorical data\nComputers provide a way to actually perform the simple random experiment described above: pick a bead at random from a bag that contains three blue beads and two red ones. Random number generators permit us to mimic the process of picking at random.\nAn example is the sample function in R. We demonstrate its use in the code below. First, we use the function rep to generate the urn:\n\nbeads <- rep(c(\"red\", \"blue\"), times = c(2,3))\nbeads\n\n[1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n\n\nand then use sample to pick a bead at random:\n\nsample(beads, 1)\n\n[1] \"red\"\n\n\nThis line of code produces one random outcome. We want to repeat this experiment an infinite number of times, but it is impossible to repeat forever. Instead, we repeat the experiment a large enough number of times to make the results practically equivalent to repeating forever. This is an example of a Monte Carlo simulation.\nMuch of what mathematical and theoretical statisticians study, which we do not cover in this class, relates to providing rigorous definitions of “practically equivalent” as well as studying how close a large number of experiments gets us to what happens in the limit. Later in this lecture, we provide a practical approach to deciding what is “large enough”.\nTo perform our first Monte Carlo simulation, we use the replicate function, which permits us to repeat the same task any number of times. Here, we repeat the random event \\(B =\\) 10,000 times:\n\nB <- 10000\nevents <- replicate(B, sample(beads, 1))\n\nWe can now see if our definition actually is in agreement with this Monte Carlo simulation approximation. We can use table to see the distribution:\n\ntab <- table(events)\ntab\n\nevents\nblue  red \n5999 4001 \n\n\nand prop.table gives us the proportions:\n\nprop.table(tab)\n\nevents\n  blue    red \n0.5999 0.4001 \n\n\nThe numbers above are the estimated probabilities provided by this Monte Carlo simulation. Statistical theory, not covered here, tells us that as \\(B\\) gets larger, the estimates get closer to 3/5=.6 and 2/5=.4.\nAlthough this is a simple and not very useful example, we will use Monte Carlo simulations to estimate probabilities in cases in which it is harder to compute the exact ones. Before delving into more complex examples, we use simple ones to demonstrate the computing tools available in R.\n\nSetting the random seed\nBefore we continue, we will briefly explain the following important line of code:\n\nset.seed(1986)\n\nThroughout this class, we use random number generators. This implies that many of the results presented can actually change by chance, which then suggests that a frozen version of the class may show a different result than what you obtain when you try to code as shown in the class. This is actually fine since the results are random and change from time to time. However, if you want to ensure that results are exactly the same every time you run them, you can set R’s random number generation seed to a specific number. Above we set it to 1986. We want to avoid using the same seed everytime. A popular way to pick the seed is the year - month - day. For example, we picked 1986 on December 20, 2018: \\(2018 - 12 - 20 = 1986\\).\nYou can learn more about setting the seed by looking at the documentation:\n\n?set.seed\n\nIn the exercises, we may ask you to set the seed to assure that the results you obtain are exactly what we expect them to be.\n\n\nWith and without replacement\nThe function sample has an argument that permits us to pick more than one element from the urn. However, by default, this selection occurs without replacement: after a bead is selected, it is not put back in the bag. Notice what happens when we ask to randomly select five beads:\n\nsample(beads, 5)\n\n[1] \"red\"  \"blue\" \"blue\" \"blue\" \"red\" \n\nsample(beads, 5)\n\n[1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n\nsample(beads, 5)\n\n[1] \"blue\" \"red\"  \"blue\" \"red\"  \"blue\"\n\n\nThis results in rearrangements that always have three blue and two red beads. If we ask that six beads be selected, we get an error:\n\nsample(beads, 6)\n\nError in sample.int(length(x), size, replace, prob) :   cannot take a sample larger than the population when 'replace = FALSE'\nHowever, the sample function can be used directly, without the use of replicate, to repeat the same experiment of picking 1 out of the 5 beads, continually, under the same conditions. To do this, we sample with replacement: return the bead back to the urn after selecting it. We can tell sample to do this by changing the replace argument, which defaults to FALSE, to replace = TRUE:\n\nevents <- sample(beads, B, replace = TRUE)\nprop.table(table(events))\n\nevents\n  blue    red \n0.6017 0.3983 \n\n\nNot surprisingly, we get results very similar to those previously obtained with replicate."
  },
  {
    "objectID": "content/Week_05/05a.html#independence",
    "href": "content/Week_05/05a.html#independence",
    "title": "Probability and Statistics",
    "section": "Independence",
    "text": "Independence\nWe say two events are independent if the outcome of one does not affect the other. The classic example is coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement. In the example above, the probability of red is 0.40 regardless of previous draws.\nMany examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities: Ace, Deuce, Three, \\(\\dots\\), Ten, Jack, Queen, King, and Ace. Now if we deal a King for the first card, and don’t replace it into the deck, the probabilities of a second card being a King is less because there are only three Kings left: the probability is 3 out of 51. These events are therefore not independent: the first outcome affected the next one.\nTo see an extreme case of non-independent events, consider our example of drawing five beads at random without replacement:\n\n\n\n\nx <- sample(beads, 5)\n\nIf you have to guess the color of the first bead, you will predict blue since blue has a 60% chance. But if I show you the result of the last four outcomes:\n\nx[2:5]\n\n[1] \"blue\" \"blue\" \"blue\" \"red\" \n\n\nwould you still guess blue? Of course not. Now you know that the probability of red is 1 since the only bead left is red. The events are not independent, so the probabilities change."
  },
  {
    "objectID": "content/Week_05/05a.html#conditional-probabilities",
    "href": "content/Week_05/05a.html#conditional-probabilities",
    "title": "Probability and Statistics",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\nWhen events are not independent, conditional probabilities are useful. We already saw an example of a conditional probability: we computed the probability that a second dealt card is a King given that the first was a King. In probability, we use the following notation:\n\\[\n\\mbox{Pr}(\\mbox{Card 2 is a king} \\mid \\mbox{Card 1 is a king}) = 3/51\n\\]\nWe use the \\(\\mid\\) as shorthand for “given that” or “conditional on”.\nWhen two events, say \\(A\\) and \\(B\\), are independent, we have:\n\\[\n\\mbox{Pr}(A \\mid B) = \\mbox{Pr}(A)\n\\]\nThis is the mathematical way of saying: the fact that \\(B\\) happened does not affect the probability of \\(A\\) happening. In fact, this can be considered the mathematical definition of independence."
  },
  {
    "objectID": "content/Week_05/05a.html#addition-and-multiplication-rules",
    "href": "content/Week_05/05a.html#addition-and-multiplication-rules",
    "title": "Probability and Statistics",
    "section": "Addition and multiplication rules",
    "text": "Addition and multiplication rules\n\nMultiplication rule\nIf we want to know the probability of two events, say \\(A\\) and \\(B\\), occurring, we can use the multiplication rule:\n\\[\n\\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\n\\] Let’s use Blackjack as an example. In Blackjack, you are assigned two random cards. After you see what you have, you can ask for more. The goal is to get closer to 21 than the dealer, without going over. Face cards are worth 10 points and Aces are worth 11 or 1 (you choose).\nSo, in a Blackjack game, to calculate the chances of getting a 21 by drawing an Ace and then a face card, we compute the probability of the first being an Ace and multiply by the probability of drawing a face card or a 10 given that the first was an Ace: \\(1/13 \\times 16/51 \\approx 0.025\\)\nThe multiplication rule also applies to more than two events. We can use induction to expand for more events:\n\\[\n\\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\\mbox{Pr}(C \\mid A \\mbox{ and } B)\n\\]\n\n\nMultiplication rule under independence\nWhen we have independent events, then the multiplication rule becomes simpler:\n\\[\n\\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B)\\mbox{Pr}(C)\n\\]\nBut we have to be very careful before using this since assuming independence can result in very different and incorrect probability calculations when we don’t actually have independence.\nAs an example, imagine a court case in which the suspect was described as having a mustache and a beard. The defendant has a mustache and a beard and the prosecution brings in an “expert” to testify that 1/10 men have beards and 1/5 have mustaches, so using the multiplication rule we conclude that only \\(1/10 \\times 1/5\\) or 0.02 have both.\nBut to multiply like this we need to assume independence! Say the conditional probability of a man having a mustache conditional on him having a beard is .95. So the correct calculation probability is much higher: \\(1/10 \\times 95/100 = 0.095\\).\nThe multiplication rule also gives us a general formula for computing conditional probabilities:\n\\[\n\\mbox{Pr}(B \\mid A) = \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)}\n\\]\nTo illustrate how we use these formulas and concepts in practice, we will use several examples related to card games.\n\n\nAddition rule\nThe addition rule tells us that:\n\\[\n\\mbox{Pr}(A \\mbox{ or } B) = \\mbox{Pr}(A) + \\mbox{Pr}(B) - \\mbox{Pr}(A \\mbox{ and } B)\n\\]\nThis rule is intuitive: think of a Venn diagram. If we simply add the probabilities, we count the intersection twice so we need to substract one instance."
  },
  {
    "objectID": "content/Week_05/05a.html#combinations-and-permutations",
    "href": "content/Week_05/05a.html#combinations-and-permutations",
    "title": "Probability and Statistics",
    "section": "Combinations and permutations",
    "text": "Combinations and permutations\nIn our very first example, we imagined an urn with five beads. As a reminder, to compute the probability distribution of one draw, we simply listed out all the possibilities. There were 5 and so then, for each event, we counted how many of these possibilities were associated with the event. The resulting probability of choosing a blue bead is 3/5 because out of the five possible outcomes, three were blue.\nFor more complicated cases, the computations are not as straightforward. For instance, what is the probability that if I draw five cards without replacement, I get all cards of the same suit, what is known as a “flush” in poker? In a discrete probability course you learn theory on how to make these computations. Here we focus on how to use R code to compute the answers.\nFirst, let’s construct a deck of cards. For this, we will use the expand.grid and paste functions. We use paste to create strings by joining smaller strings. To do this, we take the number and suit of a card and create the card name like this:\n\nnumber <- \"Three\"\nsuit <- \"Hearts\"\npaste(number, suit)\n\n[1] \"Three Hearts\"\n\n\npaste also works on pairs of vectors performing the operation element-wise:\n\npaste(letters[1:5], as.character(1:5))\n\n[1] \"a 1\" \"b 2\" \"c 3\" \"d 4\" \"e 5\"\n\n\nThe function expand.grid gives us all the combinations of entries of two vectors. For example, if you have blue and black pants and white, grey, and plaid shirts, all your combinations are:\n\nexpand.grid(pants = c(\"blue\", \"black\"), shirt = c(\"white\", \"grey\", \"plaid\"))\n\n  pants shirt\n1  blue white\n2 black white\n3  blue  grey\n4 black  grey\n5  blue plaid\n6 black plaid\n\n\nHere is how we generate a deck of cards:\n\nsuits <- c(\"Diamonds\", \"Clubs\", \"Hearts\", \"Spades\")\nnumbers <- c(\"Ace\", \"Deuce\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\",\n             \"Eight\", \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\")\ndeck <- expand.grid(number=numbers, suit=suits)\ndeck <- paste(deck$number, deck$suit)\n\nWith the deck constructed, we can double check that the probability of a King in the first card is 1/13 by computing the proportion of possible outcomes that satisfy our condition:\n\nkings <- paste(\"King\", suits)\nmean(deck %in% kings)\n\n[1] 0.07692308\n\n\nNow, how about the conditional probability of the second card being a King given that the first was a King? Earlier, we deduced that if one King is already out of the deck and there are 51 left, then this probability is 3/51. Let’s confirm by listing out all possible outcomes.\nTo do this, we can use the permutations function from the gtools package. For any list of size n, this function computes all the different combinations we can get when we select r items. Here are all the ways we can choose two numbers from a list consisting of 1,2,3:\n\nlibrary(gtools)\npermutations(3, 2)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    1\n[4,]    2    3\n[5,]    3    1\n[6,]    3    2\n\n\nNotice that the order matters here: 3,1 is different than 1,3. Also, note that (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again.\nOptionally, we can add a vector. If you want to see five random seven digit phone numbers out of all possible phone numbers (without repeats), you can type:\n\nall_phone_numbers <- permutations(10, 7, v = 0:9)\nn <- nrow(all_phone_numbers)\nindex <- sample(n, 5)\nall_phone_numbers[index,]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]    1    3    8    0    6    7    5\n[2,]    2    9    1    6    4    8    0\n[3,]    5    1    6    0    9    8    2\n[4,]    7    4    6    0    2    8    1\n[5,]    4    6    5    9    2    8    0\n\n\nInstead of using the numbers 1 through 10, the default, it uses what we provided through v: the digits 0 through 9.\nTo compute all possible ways we can choose two cards when the order matters, we type:\n\nhands <- permutations(52, 2, v = deck)\n\nThis is a matrix with two columns and 2652 rows. Here’s what it looks like:\n\nhead(hands)\n\n     [,1]        [,2]            \n[1,] \"Ace Clubs\" \"Ace Diamonds\"  \n[2,] \"Ace Clubs\" \"Ace Hearts\"    \n[3,] \"Ace Clubs\" \"Ace Spades\"    \n[4,] \"Ace Clubs\" \"Deuce Clubs\"   \n[5,] \"Ace Clubs\" \"Deuce Diamonds\"\n[6,] \"Ace Clubs\" \"Deuce Hearts\"  \n\n\nWith a matrix we can get the first and second cards like this:\n\nfirst_card <- hands[,1]\nsecond_card <- hands[,2]\n\nNow the cases for which the first hand was a King can be computed like this:\n\nkings <- paste(\"King\", suits)\nsum(first_card %in% kings)\n\n[1] 204\n\n\nTo get the conditional probability, we compute what fraction of these have a King in the second card:\n\nsum(first_card%in%kings & second_card%in%kings) / sum(first_card%in%kings)\n\n[1] 0.05882353\n\n\nwhich is exactly 3/51, as we had already deduced. Notice that the code above is equivalent to:\n\nmean(first_card%in%kings & second_card%in%kings) / mean(first_card%in%kings)\n\n[1] 0.05882353\n\n\nwhich uses mean instead of sum and is an R version of:\n\\[\n\\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)}\n\\]\nHow about if the order doesn’t matter? For example, in Blackjack if you get an Ace and a face card in the first draw, it is called a Natural 21 and you win automatically. If we wanted to compute the probability of this happening, we would enumerate the combinations, not the permutations, since the order does not matter.\n\ncombinations(3,2)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    3\n\n\nIn the second line, the outcome does not include (2,1) because (1,2) already was enumerated. The same applies to (3,1) and (3,2).\nSo to compute the probability of a Natural 21 in Blackjack, we can do this:\n\naces <- paste(\"Ace\", suits)\n\nfacecard <- c(\"King\", \"Queen\", \"Jack\", \"Ten\")\nfacecard <- expand.grid(number = facecard, suit = suits)\nfacecard <- paste(facecard$number, facecard$suit)\n\nhands <- combinations(52, 2, v = deck)\nmean(hands[,1] %in% aces & hands[,2] %in% facecard)\n\n[1] 0.04826546\n\n\nIn the last line, we assume the Ace comes first. This is only because we know the way combination enumerates possibilities and it will list this case first. But to be safe, we could have written this and produced the same answer:\n\nmean((hands[,1] %in% aces & hands[,2] %in% facecard) |\n       (hands[,2] %in% aces & hands[,1] %in% facecard))\n\n[1] 0.04826546\n\n\n\nMonte Carlo example\nInstead of using combinations to deduce the exact probability of a Natural 21, we can use a Monte Carlo to estimate this probability. In this case, we draw two cards over and over and keep track of how many 21s we get. We can use the function sample to draw two cards without replacements:\n\nhand <- sample(deck, 2)\nhand\n\n[1] \"Queen Clubs\"  \"Seven Spades\"\n\n\nAnd then check if one card is an Ace and the other a face card or a 10. Going forward, we include 10 when we say face card. Now we need to check both possibilities:\n\n(hand[1] %in% aces & hand[2] %in% facecard) |\n  (hand[2] %in% aces & hand[1] %in% facecard)\n\n[1] FALSE\n\n\nIf we repeat this 10,000 times, we get a very good approximation of the probability of a Natural 21.\nLet’s start by writing a function that draws a hand and returns TRUE if we get a 21. The function does not need any arguments because it uses objects defined in the global environment.\n\nblackjack <- function(){\n   hand <- sample(deck, 2)\n  (hand[1] %in% aces & hand[2] %in% facecard) |\n    (hand[2] %in% aces & hand[1] %in% facecard)\n}\n\nHere we do have to check both possibilities: Ace first or Ace second because we are not using the combinations function. The function returns TRUE if we get a 21 and FALSE otherwise:\n\nblackjack()\n\n[1] FALSE\n\n\nNow we can play this game, say, 10,000 times:\n\nB <- 10000\nresults <- replicate(B, blackjack())\nmean(results)\n\n[1] 0.0475"
  },
  {
    "objectID": "content/Week_05/05a.html#examples",
    "href": "content/Week_05/05a.html#examples",
    "title": "Probability and Statistics",
    "section": "Examples",
    "text": "Examples\nIn this section, we describe two discrete probability popular examples: the Monty Hall problem and the birthday problem. We’ll use R to help illustrate the mathematical concepts and to demostrate an idea that we’ll return to: simulation.\n\nMonty Hall problem\nIn the 1970s, there was a game show called “Let’s Make a Deal” and Monty Hall was the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door there was a prize. The other doors had a goat behind them to show the contestant they had lost. After the contestant picked a door, before revealing whether the chosen door contained a prize, Monty Hall would open one of the two remaining doors and show the contestant there was no prize behind that door. Then he would ask “Do you want to switch doors?” What would you do?\nWe can use probability to show that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3! This seems counterintuitive. Many people incorrectly think both chances are 1 in 2 since you are choosing between 2 options. You can watch a detailed mathematical explanation on Khan Academy2 or read one on Wikipedia3. Below we use a Monte Carlo simulation to see which strategy is better. Note that this code is written longer than it should be for pedagogical purposes.\nLet’s start with the stick strategy:\n\nB <- 10000\nmonty_hall <- function(strategy){\n  doors <- as.character(1:3) \n  prize <- sample(c(\"car\", \"goat\", \"goat\")) # no replacement\n  prize_door <- doors[prize == \"car\"] # which door has the prize?\n  my_pick  <- sample(doors, 1) # we pick a door at random\n  show <- sample(doors[!doors %in% c(my_pick, prize_door)],1)\n  stick <- my_pick\n  switch <- doors[!doors%in%c(my_pick, show)]\n  choice <- ifelse(strategy == \"stick\", stick, switch) # apply the strategy\n  choice == prize_door # did we win?\n}\nstick <- replicate(B, monty_hall(\"stick\"))\nmean(stick)\n\n[1] 0.3416\n\nswitch <- replicate(B, monty_hall(\"switch\"))\nmean(switch)\n\n[1] 0.6682\n\n\nAs we write the code, we note that the lines starting with my_pick and show have no influence on the last logical operation when we stick to our original choice anyway. From this we should realize that the chance is 1 in 3, what we began with. When we switch, the Monte Carlo estimate confirms the 2/3 calculation. This helps us gain some insight by showing that we are removing a door, show, that is definitely not a winner from our choices. We also see that unless we get it right when we first pick, you win: 1 - 1/3 = 2/3.\n\n\nBirthday problem\nSuppose you are in a classroom with 50 people. If we assume this is a randomly selected group of 50 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do this later. Here we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29. This actually doesn’t change the answer much.\nFirst, note that birthdays can be represented as numbers between 1 and 365, so a sample of 50 birthdays can be obtained like this:\n\n\n\n\nn <- 50\nbdays <- sample(1:365, n, replace = TRUE)\n\nTo check if in this particular set of 50 people we have at least two with the same birthday, we can use the function duplicated, which returns TRUE whenever an element of a vector is a duplicate. Here is an example:\n\nduplicated(c(1,2,3,1,4,3,5))\n\n[1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n\nThe second time 1 and 3 appear, we get a TRUE. So to check if two birthdays were the same, we simply use the any and duplicated functions like this:\n\nany(duplicated(bdays))\n\n[1] TRUE\n\n\nIn this case, we see that it did happen. At least two people had the same birthday.\nTo estimate the probability of a shared birthday in the group, we repeat this experiment by sampling sets of 50 birthdays over and over:\n\nB <- 10000\nsame_birthday <- function(n){\n  bdays <- sample(1:365, n, replace=TRUE)\n  any(duplicated(bdays))\n}\nresults <- replicate(B, same_birthday(50))\nmean(results)\n\n[1] 0.9691\n\n\nWere you expecting the probability to be this high?\nPeople tend to underestimate these probabilities. To get an intuition as to why it is so high, think about what happens when the group size is close to 365. At this stage, we run out of days and the probability is one.\nSay we want to use this knowledge to bet with friends about two people having the same birthday in a group of people. When are the chances larger than 50%? Larger than 75%?\nLet’s create a look-up table. We can quickly create a function to compute this for any group size:\n\ncompute_prob <- function(n, B=10000){\n  results <- replicate(B, same_birthday(n))\n  mean(results)\n}\n\nUsing the function sapply, we can perform element-wise operations on any function:\n\nn <- seq(1,60)\nprob <- sapply(n, compute_prob)\n\nWe can now make a plot of the estimated probabilities of two people having the same birthday in a group of size \\(n\\):\n\nlibrary(tidyverse)\nprob <- sapply(n, compute_prob)\nqplot(n, prob)\n\n\n\n\nNow let’s compute the exact probabilities rather than use Monte Carlo approximations. Not only do we get the exact answer using math, but the computations are much faster since we don’t have to generate experiments.\nTo make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening. For this, we use the multiplication rule.\nLet’s start with the first person. The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one, is 364/365. Then, given that the first two people have unique birthdays, person 3 is left with 363 days to choose from. We continue this way and find the chances of all 50 people having a unique birthday is:\n\\[\n1 \\times \\frac{364}{365}\\times\\frac{363}{365} \\dots \\frac{365-n + 1}{365}\n\\]\nWe can write a function that does this for any number:\n\nexact_prob <- function(n){\n  prob_unique <- seq(365,365-n+1)/365\n  1 - prod( prob_unique)\n}\neprob <- sapply(n, exact_prob)\nqplot(n, prob) + geom_line(aes(n, eprob), col = \"red\")\n\n\n\n\nThis plot shows that the Monte Carlo simulation provided a very good estimate of the exact probability. Had it not been possible to compute the exact probabilities, we would have still been able to accurately estimate the probabilities."
  },
  {
    "objectID": "content/Week_05/05a.html#infinity-in-practice",
    "href": "content/Week_05/05a.html#infinity-in-practice",
    "title": "Probability and Statistics",
    "section": "Infinity in practice",
    "text": "Infinity in practice\nThe theory described here requires repeating experiments over and over forever. In practice we can’t do this. In the examples above, we used \\(B=10,000\\) Monte Carlo experiments and it turned out that this provided accurate estimates. The larger this number, the more accurate the estimate becomes until the approximaton is so good that your computer can’t tell the difference. But in more complex calculations, 10,000 may not be nearly enough. Also, for some calculations, 10,000 experiments might not be computationally feasible. In practice, we won’t know what the answer is, so we won’t know if our Monte Carlo estimate is accurate. We know that the larger \\(B\\), the better the approximation. But how big do we need it to be? This is actually a challenging question and answering it often requires advanced theoretical statistics training.\nOne practical approach we will describe here is to check for the stability of the estimate. The following is an example with the birthday problem for a group of 25 people.\n\nB <- 10^seq(1, 5, len = 100)\ncompute_prob <- function(B, n=25){\n  same_day <- replicate(B, same_birthday(n))\n  mean(same_day)\n}\nprob <- sapply(B, compute_prob)\nqplot(log10(B), prob, geom = \"line\")\n\n\n\n\nIn this plot, we can see that the values start to stabilize (that is, they vary less than .01) around 1000. Note that the exact probability, which we know in this case, is 0.5686997.\n\nTRY IT\n\nOne ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?\nWhat is the probability that the ball will not be cyan?\nInstead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\nNow repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\nTwo events \\(A\\) and \\(B\\) are independent if \\(\\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A) P(B)\\). Under which situation are the draws independent?\n\n\nYou don’t replace the draw.\nYou replace the draw.\nNeither\nBoth\n\n\nSay you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow?\nIf you roll a 6-sided die six times, what is the probability of not seeing a 6?\nTwo teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics win at least one game?\nCreate a Monte Carlo simulation to confirm your answer to the previous problem. Use B <- 10000 simulations. Hint: use the following code to generate the results of the first four games:\n\n\nceltic_wins <- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4))\n\nThe Celtics must win one of these 4 games.\n\nTwo teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series?\nConfirm the results of the previous question with a Monte Carlo simulation.\nTwo teams, \\(A\\) and \\(B\\), are playing a seven game series. Team \\(A\\) is better than team \\(B\\) and has a \\(p>0.5\\) chance of winning each game. Given a value \\(p\\), the probability of winning the series for the underdog team \\(B\\) can be computed with the following function based on a Monte Carlo simulation:\n\n\nprob_win <- function(p){\n  B <- 10000\n  result <- replicate(B, {\n    b_win <- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p))\n    sum(b_win)>=4\n  })\n  mean(result)\n}\n\nUse the function sapply to compute the probability, call it Pr, of winning for p <- seq(0.5, 0.95, 0.025). Then plot the result.\n\nRepeat the exercise above, but now keep the probability fixed at p <- 0.75 and compute the probability for different series lengths: best of 1 game, 3 games, 5 games,… Specifically, N <- seq(1, 25, 2). Hint: use this function:\n\n\nprob_win <- function(N, p=0.75){\n  B <- 10000\n  result <- replicate(B, {\n    b_win <- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p))\n    sum(b_win)>=(N+1)/2\n  })\n  mean(result)\n}\n\n\nIn previous lectures, we explained why when summarizing a list of numeric values, such as heights, it is not useful to construct a distribution that defines a proportion to each possible outcome. For example, if we measure every single person in a very large population of size \\(n\\) with extremely high precision, since no two people are exactly the same height, we need to assign the proportion \\(1/n\\) to each observed value and attain no useful summary at all. Similarly, when defining probability distributions, it is not useful to assign a very small probability to every single height.\nJust as when using distributions to summarize numeric data, it is much more practical to define a function that operates on intervals rather than single values. The standard way of doing this is using the cumulative distribution function (CDF).\nWe described empirical cumulative distribution function (eCDF) as a basic summary of a list of numeric values. As an example, we earlier defined the height distribution for adult male students. Here, we define the vector \\(x\\) to contain these heights:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(heights)\nx <- heights %>% filter(sex==\"Male\") %>% pull(height)\n\nWe defined the empirical distribution function as:\n\nF <- function(a) mean(x<=a)\n\nwhich, for any value a, gives the proportion of values in the list x that are smaller or equal than a.\nKeep in mind that we have not yet introduced probability in the context of CDFs. Let’s do this by asking the following: if I pick one of the male students at random, what is the chance that he is taller than 70.5 inches? Because every student has the same chance of being picked, the answer to this is equivalent to the proportion of students that are taller than 70.5 inches. Using the CDF we obtain an answer by typing:\n\n1 - F(70)\n\n[1] 0.3768473\n\n\nOnce a CDF is defined, we can use this to compute the probability of any subset. For instance, the probability of a student being between height a and height b is:\n\nF(b)-F(a)\n\nBecause we can compute the probability for any possible event this way, the cumulative probability function defines the probability distribution for picking a height at random from our vector of heights x."
  },
  {
    "objectID": "content/Week_05/05a.html#theoretical-continuous-distributions",
    "href": "content/Week_05/05a.html#theoretical-continuous-distributions",
    "title": "Probability and Statistics",
    "section": "Theoretical continuous distributions",
    "text": "Theoretical continuous distributions\nThe normal distribution is a useful approximation to many naturally occurring distributions, including that of height. The cumulative distribution for the normal distribution is defined by a mathematical formula which in R can be obtained with the function pnorm. We say that a random quantity is normally distributed with average m and standard deviation s if its probability distribution is defined by:\n\nF(a) = pnorm(a, m, s)\n\nThis is useful because if we are willing to use the normal approximation for, say, height, we don’t need the entire dataset to answer questions such as: what is the probability that a randomly selected student is taller then 70 inches? We just need the average height and standard deviation:\n\nm <- mean(x)\ns <- sd(x)\n1 - pnorm(70.5, m, s)\n\n[1] 0.371369\n\n\n\nTheoretical distributions as approximations\nThe normal distribution is derived mathematically: we do not need data to define it. For practicing data scientists, almost everything we do involves data. Data is always, technically speaking, discrete. For example, we could consider our height data categorical with each specific height a unique category. The probability distribution is defined by the proportion of students reporting each height. Here is a plot of that probability distribution:\n\n\n\n\n\nWhile most students rounded up their heights to the nearest inch, others reported values with more precision. One student reported his height to be 69.6850393700787, which is 177 centimeters. The probability assigned to this height is 0.0012315 or 1 in 812. The probability for 70 inches is much higher at 0.1059113, but does it really make sense to think of the probability of being exactly 70 inches as being different than 69.6850393700787? Clearly it is much more useful for data analytic purposes to treat this outcome as a continuous numeric variable, keeping in mind that very few people, or perhaps none, are exactly 70 inches, and that the reason we get more values at 70 is because people round to the nearest inch.\nWith continuous distributions, the probability of a singular value is not even defined. For example, it does not make sense to ask what is the probability that a normally distributed value is 70. Instead, we define probabilities for intervals. We thus could ask what is the probability that someone is between 69.5 and 70.5.\nIn cases like height, in which the data is rounded, the normal approximation is particularly useful if we deal with intervals that include exactly one round number. For example, the normal distribution is useful for approximating the proportion of students reporting values in intervals like the following three:\n\nmean(x <= 68.5) - mean(x <= 67.5)\n\n[1] 0.114532\n\nmean(x <= 69.5) - mean(x <= 68.5)\n\n[1] 0.1194581\n\nmean(x <= 70.5) - mean(x <= 69.5)\n\n[1] 0.1219212\n\n\nNote how close we get with the normal approximation:\n\npnorm(68.5, m, s) - pnorm(67.5, m, s)\n\n[1] 0.1031077\n\npnorm(69.5, m, s) - pnorm(68.5, m, s)\n\n[1] 0.1097121\n\npnorm(70.5, m, s) - pnorm(69.5, m, s)\n\n[1] 0.1081743\n\n\nHowever, the approximation is not as useful for other intervals. For instance, notice how the approximation breaks down when we try to estimate:\n\nmean(x <= 70.9) - mean(x<=70.1)\n\n[1] 0.02216749\n\n\nwith\n\npnorm(70.9, m, s) - pnorm(70.1, m, s)\n\n[1] 0.08359562\n\n\nIn general, we call this situation discretization. Although the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding. As long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool.\n\n\nThe probability density\nFor categorical distributions, we can define the probability of a category. For example, a roll of a die, let’s call it \\(X\\), can be 1,2,3,4,5 or 6. The probability of 4 is defined as:\n\\[\n\\mbox{Pr}(X=4) = 1/6\n\\]\nThe CDF can then easily be defined: \\[\nF(4) = \\mbox{Pr}(X\\leq 4) =  \\mbox{Pr}(X = 4) +  \\mbox{Pr}(X = 3) +  \\mbox{Pr}(X = 2) +  \\mbox{Pr}(X = 1)\n\\]\nAlthough for continuous distributions the probability of a single value \\(\\mbox{Pr}(X=x)\\) is not defined, there is a theoretical definition that has a similar interpretation. The probability density at \\(x\\) is defined as the function \\(f(a)\\) such that:\n\\[\nF(a) = \\mbox{Pr}(X\\leq a) = \\int_{-\\infty}^a f(x)\\, dx\n\\]\nFor those that know calculus, remember that the integral is related to a sum: it is the sum of bars with widths approximating 0. If you don’t know calculus, you can think of \\(f(x)\\) as a curve for which the area under that curve up to the value \\(a\\), gives you the probability \\(\\mbox{Pr}(X\\leq a)\\).\nFor example, to use the normal approximation to estimate the probability of someone being taller than 76 inches, we use:\n\n1 - pnorm(76, m, s)\n\n[1] 0.03206008\n\n\nwhich mathematically is the grey area below:\n\n\n\n\n\nThe curve you see is the probability density for the normal distribution. In R, we get this using the function dnorm.\nAlthough it may not be immediately obvious why knowing about probability densities is useful, understanding this concept will be essential to those wanting to fit models to data for which predefined functions are not available."
  },
  {
    "objectID": "content/Week_05/05a.html#monte-carlo-simulations-for-continuous-variables",
    "href": "content/Week_05/05a.html#monte-carlo-simulations-for-continuous-variables",
    "title": "Probability and Statistics",
    "section": "Monte Carlo simulations for continuous variables",
    "text": "Monte Carlo simulations for continuous variables\nR provides functions to generate normally distributed outcomes. Specifically, the rnorm function takes three arguments: size, average (defaults to 0), and standard deviation (defaults to 1) and produces random numbers. Here is an example of how we could generate data that looks like our reported heights:\n\nn <- length(x)\nm <- mean(x)\ns <- sd(x)\nsimulated_heights <- rnorm(n, m, s)\n\nNot surprisingly, the distribution looks normal:\n\n\n\n\n\nThis is one of the most useful functions in R as it will permit us to generate data that mimics natural events and answers questions related to what could happen by chance by running Monte Carlo simulations.\nIf, for example, we pick 800 males at random, what is the distribution of the tallest person? How rare is a seven footer in a group of 800 males? The following Monte Carlo simulation helps us answer that question:\n\nB <- 10000\ntallest <- replicate(B, {\n  simulated_data <- rnorm(800, m, s)\n  max(simulated_data)\n})\n\nHaving a seven footer is quite rare:\n\nmean(tallest >= 7*12)\n\n[1] 0.0172\n\n\nHere is the resulting distribution:\n\n\n\n\n\nNote that it does not look normal."
  },
  {
    "objectID": "content/Week_05/05a.html#continuous-distributions",
    "href": "content/Week_05/05a.html#continuous-distributions",
    "title": "Probability and Statistics",
    "section": "Continuous distributions",
    "text": "Continuous distributions\nThe normal distribution is not the only useful theoretical distribution. Other continuous distributions that we may encounter are the student-t, Chi-square, exponential, gamma, beta, and beta-binomial. R provides functions to compute the density, the quantiles, the cumulative distribution functions and to generate Monte Carlo simulations. R uses a convention that lets us remember the names, namely using the letters d, q, p, and r in front of a shorthand for the distribution. We have already seen the functions dnorm, pnorm, and rnorm for the normal distribution. Remember that dnorm gives us the PDF, pnorm gives us the CDF, rnorm gives us random draws from the normal, and the function qnorm gives us the quantiles (the input to qnorm, then, must be between 0 and 1).\n\nqnorm(.25, mean = 0, sd = 1)\n\n[1] -0.6744898\n\nqnorm(.25, mean = -10, sd = 5)\n\n[1] -13.37245\n\nqnorm(.975, mean = 0, sd = 1)\n\n[1] 1.959964\n\n\n\nTRY IT\n\nAssume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?\nAssume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?\nAssume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?\nRepeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?\nNotice that the answer to the question does not change when you change units. This makes sense since the answer to the question should not be affected by what units we use. In fact, if you look closely, you notice that 61 and 64 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average.\nTo see the math that explains why the answers to questions 3, 4, and 5 are the same, suppose we have a random variable with average \\(m\\) and standard error \\(s\\). Suppose we ask the probability of \\(X\\) being smaller or equal to \\(a\\). Remember that, by definition, \\(a\\) is \\((a - m)/s\\) standard deviations \\(s\\) away from the average \\(m\\). The probability is:\n\n\\[\n\\mbox{Pr}(X \\leq a)\n\\]\nNow we subtract \\(\\mu\\) to both sides and then divide both sides by \\(\\sigma\\):\n\\[\n\\mbox{Pr}\\left(\\frac{X-m}{s} \\leq \\frac{a-m}{s} \\right)\n\\]\nThe quantity on the left is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it \\(Z\\):\n\\[\n\\mbox{Pr}\\left(Z \\leq \\frac{a-m}{s} \\right)\n\\]\nSo, no matter the units, the probability of \\(X\\leq a\\) is the same as the probability of a standard normal variable being less than \\((a - m)/s\\). If mu is the average and sigma the standard error, which of the following R code would give us the right answer in every situation:\n\nmean(X<=a)\npnorm((a - m)/s)\npnorm((a - m)/s, m, s)\npnorm(a)\n\n\nImagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use qnorm.\nThe distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. Suppose you want to know the distribution of the highest IQ across all graduating classes if 10,000 people are born each in your school district. Run a Monte Carlo simulation with B=1000 generating 10,000 IQ scores and keeping the highest. Make a histogram."
  },
  {
    "objectID": "content/Week_05/05a.html#definition-of-random-variables",
    "href": "content/Week_05/05a.html#definition-of-random-variables",
    "title": "Probability and Statistics",
    "section": "Definition of Random variables",
    "text": "Definition of Random variables\nRandom variables are numeric outcomes resulting from random processes. We can easily generate random variables using some of the simple examples we have shown. For example, define X to be 1 if a bead is blue and red otherwise:\n\n\n\n\nbeads <- rep( c(\"red\", \"blue\"), times = c(2,3))\nX <- ifelse(sample(beads, 1) == \"blue\", 1, 0)\n\nHere X is a random variable: every time we select a new bead the outcome changes randomly. See below:\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n\n[1] 1\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n\n[1] 0\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n\n[1] 0\n\n\nSometimes it’s 1 and sometimes it’s 0."
  },
  {
    "objectID": "content/Week_05/05a.html#sampling-models",
    "href": "content/Week_05/05a.html#sampling-models",
    "title": "Probability and Statistics",
    "section": "Sampling models",
    "text": "Sampling models\nMany data generation procedures, those that produce the data we study, can be modeled quite well as draws from an urn. For instance, we can model the process of polling likely voters as drawing 0s (Republicans) and 1s (Democrats) from an urn containing the 0 and 1 code for all likely voters. In epidemiological studies, we often assume that the subjects in our study are a random sample from the population of interest. The data related to a specific outcome can be modeled as a random sample from an urn containing the outcome for the entire population of interest. Similarly, in experimental research, we often assume that the individual organisms we are studying, for example worms, flies, or mice, are a random sample from a larger population. Randomized experiments can also be modeled by draws from an urn given the way individuals are assigned into groups: when getting assigned, you draw your group at random. Sampling models are therefore ubiquitous in data science. Casino games offer a plethora of examples of real-world situations in which sampling models are used to answer specific questions. We will therefore start with such examples.\nSuppose a very small casino hires you to consult on whether they should set up roulette wheels. To keep the example simple, we will assume that 1,000 people will play and that the only game you can play on the roulette wheel is to bet on red or black. The casino wants you to predict how much money they will make or lose. They want a range of values and, in particular, they want to know what’s the chance of losing money. If this probability is too high, they will pass on installing roulette wheels.\nWe are going to define a random variable \\(S\\) that will represent the casino’s total winnings. Let’s start by constructing the urn. A roulette wheel has 18 red pockets, 18 black pockets and 2 green ones. So playing a color in one game of roulette is equivalent to drawing from this urn:\n\ncolor <- rep(c(\"Black\", \"Red\", \"Green\"), c(18, 18, 2))\n\nThe 1,000 outcomes from 1,000 people playing are independent draws from this urn. If red comes up, the gambler wins and the casino loses a dollar, so we draw a -1. Otherwise, the casino wins a dollar and we draw a 1. To construct our random variable \\(S\\), we can use this code:\n\nn <- 1000\nX <- sample(ifelse(color == \"Red\", -1, 1),  n, replace = TRUE)\nX[1:10]\n\n [1] -1  1  1 -1 -1 -1  1  1  1  1\n\n\nBecause we know the proportions of 1s and -1s, we can generate the draws with one line of code, without defining color:\n\nX <- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\n\nWe call this a sampling model since we are modeling the random behavior of roulette with the sampling of draws from an urn. The total winnings \\(S\\) is simply the sum of these 1,000 independent draws:\n\nX <- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\nS <- sum(X)\nS\n\n[1] 22"
  },
  {
    "objectID": "content/Week_05/05a.html#the-probability-distribution-of-a-random-variable",
    "href": "content/Week_05/05a.html#the-probability-distribution-of-a-random-variable",
    "title": "Probability and Statistics",
    "section": "The probability distribution of a random variable",
    "text": "The probability distribution of a random variable\nIf you run the code above, you see that \\(S\\) changes every time. This is, of course, because \\(S\\) is a random variable. The probability distribution of a random variable tells us the probability of the observed value falling at any given interval. So, for example, if we want to know the probability that we lose money, we are asking the probability that \\(S\\) is in the interval \\(S<0\\).\nNote that if we can define a cumulative distribution function \\(F(a) = \\mbox{Pr}(S\\leq a)\\), then we will be able to answer any question related to the probability of events defined by our random variable \\(S\\), including the event \\(S<0\\). We call this \\(F\\) the random variable’s distribution function.\nWe can estimate the distribution function for the random variable \\(S\\) by using a Monte Carlo simulation to generate many realizations of the random variable. With this code, we run the experiment of having 1,000 people play roulette, over and over, specifically \\(B = 10,000\\) times:\n\nn <- 1000\nB <- 10000\nroulette_winnings <- function(n){\n  X <- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\n  sum(X)\n}\nS <- replicate(B, roulette_winnings(n))\n\nNow we can ask the following: in our simulations, how often did we get sums less than or equal to a?\n\nmean(S <= a)\n\nThis will be a very good approximation of \\(F(a)\\) and we can easily answer the casino’s question: how likely is it that we will lose money? We can see it is quite low:\n\nmean(S<0)\n\n[1] 0.0456\n\n\nWe can visualize the distribution of \\(S\\) by creating a histogram showing the probability \\(F(b)-F(a)\\) for several intervals \\((a,b]\\):\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nWe see that the distribution appears to be approximately normal. A qq-plot will confirm that the normal approximation is close to a perfect approximation for this distribution. If, in fact, the distribution is normal, then all we need to define the distribution is the average and the standard deviation. Because we have the original values from which the distribution is created, we can easily compute these with mean(S) and sd(S). The blue curve you see added to the histogram above is a normal density with this average and standard deviation.\nThis average and this standard deviation have special names. They are referred to as the expected value and standard error of the random variable \\(S\\). We will say more about these in the next section where we will discuss an incredibly useful approximation provided by mathematical theory that applies generally to sums and averages of draws from any urn: the Central Limit Theorem (CLT)."
  },
  {
    "objectID": "content/Week_05/05a.html#distributions-versus-probability-distributions",
    "href": "content/Week_05/05a.html#distributions-versus-probability-distributions",
    "title": "Probability and Statistics",
    "section": "Distributions versus probability distributions",
    "text": "Distributions versus probability distributions\nBefore we continue, let’s make an important distinction and connection between the distribution of a list of numbers and a probability distribution. In the visualization lectures, we described how any list of numbers \\(x_1,\\dots,x_n\\) has a distribution. The definition is quite straightforward. We define \\(F(a)\\) as the function that tells us what proportion of the list is less than or equal to \\(a\\). Because they are useful summaries when the distribution is approximately normal, we define the average and standard deviation. These are defined with a straightforward operation of the vector containing the list of numbers x:\n\nm <- sum(x)/length(x)\ns <- sqrt(sum((x - m)^2) / length(x))\n\nA random variable \\(X\\) has a distribution function. To define this, we do not need a list of numbers. It is a theoretical concept. In this case, we define the distribution as the \\(F(a)\\) that answers the question: what is the probability that \\(X\\) is less than or equal to \\(a\\)? There is no list of numbers.\nHowever, if \\(X\\) is defined by drawing from an urn with numbers in it, then there is a list: the list of numbers inside the urn. In this case, the distribution of that list is the probability distribution of \\(X\\) and the average and standard deviation of that list are the expected value and standard error of the random variable.\nAnother way to think about it that does not involve an urn is to run a Monte Carlo simulation and generate a very large list of outcomes of \\(X\\). These outcomes are a list of numbers. The distribution of this list will be a very good approximation of the probability distribution of \\(X\\). The longer the list, the better the approximation. The average and standard deviation of this list will approximate the expected value and standard error of the random variable."
  },
  {
    "objectID": "content/Week_05/05a.html#notation-for-random-variables",
    "href": "content/Week_05/05a.html#notation-for-random-variables",
    "title": "Probability and Statistics",
    "section": "Notation for random variables",
    "text": "Notation for random variables\nIn statistical textbooks, upper case letters are used to denote random variables and we follow this convention here. Lower case letters are used for observed values. You will see some notation that includes both. For example, you will see events defined as \\(X \\leq x\\). Here \\(X\\) is a random variable, making it a random event, and \\(x\\) is an arbitrary value and not random. So, for example, \\(X\\) might represent the number on a die roll and \\(x\\) will represent an actual value we see 1, 2, 3, 4, 5, or 6. So in this case, the probability of \\(X=x\\) is 1/6 regardless of the observed value \\(x\\). This notation is a bit strange because, when we ask questions about probability, \\(X\\) is not an observed quantity. Instead, it’s a random quantity that we will see in the future. We can talk about what we expect it to be, what values are probable, but not what it is. But once we have data, we do see a realization of \\(X\\). So data scientists talk of what could have been after we see what actually happened."
  },
  {
    "objectID": "content/Week_05/05a.html#the-expected-value-and-standard-error",
    "href": "content/Week_05/05a.html#the-expected-value-and-standard-error",
    "title": "Probability and Statistics",
    "section": "The expected value and standard error",
    "text": "The expected value and standard error\nWe have described sampling models for draws. We will now go over the mathematical theory that lets us approximate the probability distributions for the sum of draws. Once we do this, we will be able to help the casino predict how much money they will make. The same approach we use for the sum of draws will be useful for describing the distribution of averages and proportion which we will need to understand how polls work.\nThe first important concept to learn is the expected value. In statistics books, it is common to use letter \\(\\mbox{E}\\) like this:\n\\[\\mbox{E}[X]\\]\nto denote the expected value of the random variable \\(X\\).\nA random variable will vary around its expected value in a way that if you take the average of many, many draws, the average of the draws will approximate the expected value, getting closer and closer the more draws you take.\nTheoretical statistics provides techniques that facilitate the calculation of expected values in different circumstances. For example, a useful formula tells us that the expected value of a random variable defined by one draw is the average of the numbers in the urn. In the urn used to model betting on red in roulette, we have 20 one dollars and 18 negative one dollars. The expected value is thus:\n\\[\n\\mbox{E}[X] = (20 + -18)/38\n\\]\nwhich is about 5 cents. It is a bit counterintuitive to say that \\(X\\) varies around 0.05, when the only values it takes is 1 and -1. One way to make sense of the expected value in this context is by realizing that if we play the game over and over, the casino wins, on average, 5 cents per game. A Monte Carlo simulation confirms this:\n\nB <- 10^6\nx <- sample(c(-1,1), B, replace = TRUE, prob=c(9/19, 10/19))\nmean(x)\n\n[1] 0.05169\n\n\nIn general, if the urn has two possible outcomes, say \\(a\\) and \\(b\\), with proportions \\(p\\) and \\(1-p\\) respectively, the average is:\n\\[\\mbox{E}[X] = ap + b(1-p)\\]\nTo see this, notice that if there are \\(n\\) beads in the urn, then we have \\(np\\) \\(a\\)s and \\(n(1-p)\\) \\(b\\)s and because the average is the sum, \\(n\\times a \\times p + n\\times b \\times (1-p)\\), divided by the total \\(n\\), we get that the average is \\(ap + b(1-p)\\).\nNow the reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distributions of sum, which then is useful for describing the distribution of averages and proportions. The first useful fact is that the expected value of the sum of the draws is:\n\\[\n\\mbox{}\\mbox{number of draws } \\times \\mbox{ average of the numbers in the urn}\n\\]\nSo if 1,000 people play roulette, the casino expects to win, on average, about 1,000 \\(\\times\\) $0.05 = $50. But this is an expected value. How different can one observation be from the expected value? The casino really needs to know this. What is the range of possibilities? If negative numbers are too likely, they will not install roulette wheels. Statistical theory once again answers this question. The standard error (SE) gives us an idea of the size of the variation around the expected value. In statistics books, it’s common to use:\n\\[\\mbox{SE}[X]\\]\nto denote the standard error of a random variable.\nIf our draws are independent, then the standard error of the sum is given by the equation:\n\\[\n\\sqrt{\\mbox{number of draws }} \\times \\mbox{ standard deviation of the numbers in the urn}\n\\]\nUsing the definition of standard deviation, we can derive, with a bit of math, that if an urn contains two values \\(a\\) and \\(b\\) with proportions \\(p\\) and \\((1-p)\\), respectively, the standard deviation is:\n\\[\\mid b - a \\mid \\sqrt{p(1-p)}.\\]\nSo in our roulette example, the standard deviation of the values inside the urn is: \\(\\mid 1 - (-1) \\mid \\sqrt{10/19 \\times 9/19}\\) or:\n\n2 * sqrt(90)/19\n\n[1] 0.998614\n\n\nThe standard error tells us the typical difference between a random variable and its expectation. Since one draw is obviously the sum of just one draw, we can use the formula above to calculate that the random variable defined by one draw has an expected value of 0.05 and a standard error of about 1. This makes sense since we either get 1 or -1, with 1 slightly favored over -1.\nUsing the formula above, the sum of 1,000 people playing has standard error of about $32:\n\nn <- 1000\nsqrt(n) * 2 * sqrt(90)/19\n\n[1] 31.57895\n\n\nAs a result, when 1,000 people bet on red, the casino is expected to win $50 with a standard error of $32. It therefore seems like a safe bet. But we still haven’t answered the question: how likely is it to lose money? Here the CLT will help.\nAdvanced note: Before continuing we should point out that exact probability calculations for the casino winnings can be performed with the binomial distribution. However, here we focus on the CLT, which can be generally applied to sums of random variables in a way that the binomial distribution can’t.\n\nPopulation SD versus the sample SD\nThe standard deviation of a list x (below we use heights as an example) is defined as the square root of the average of the squared differences:\n\nlibrary(dslabs)\nx <- heights$height\nm <- mean(x)\ns <- sqrt(mean((x-m)^2))\n\nThe SD is the the square root of the sample variance, and the sample variance is the square of the sample SD. Using mathematical notation we write:\n\\[\n\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i \\\\\n\\] and \\[\n\\sigma =  \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2}\n\\]\nHowever, be aware that the sd function returns a slightly different result:\n\nidentical(s, sd(x))\n\n[1] FALSE\n\ns-sd(x)\n\n[1] -0.001942661\n\n\nThis is because the sd function R does not return the sd of the list, but rather uses a formula that estimates standard deviations of a population from a random sample \\(X_1, \\dots, X_N\\) which, for reasons not discussed here, divide the sum of squares by the \\(N-1\\).\n\\[\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i, \\,\\,\\,\\,\ns =  \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2}\n\\]\nYou can see that this is the case by typing:\n\nn <- length(x)\ns-sd(x)*sqrt((n-1) / n)\n\n[1] 0\n\n\nFor all the theory discussed here, you need to compute the actual standard deviation as defined:\n\nsqrt(mean((x-m)^2))\n\nSo be careful when using the sd function in R. However, keep in mind that throughout the book we sometimes use the sd function when we really want the actual SD. This is because when the list size is big, these two are practically equivalent since \\(\\sqrt{(N-1)/N} \\approx 1\\)."
  },
  {
    "objectID": "content/Week_05/05a.html#central-limit-theorem",
    "href": "content/Week_05/05a.html#central-limit-theorem",
    "title": "Probability and Statistics",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe Central Limit Theorem (CLT) tells us that when the number of draws, also called the sample size, is large, the probability distribution of the sum of the independent draws is approximately normal. Because sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history.\nPreviously, we discussed that if we know that the distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and standard deviation. We also know that the same applies to probability distributions. If a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe the probability distribution are the average and standard deviation, referred to as the expected value and standard error.\nWe previously ran this Monte Carlo simulation:\n\nn <- 1000\nB <- 10000\nroulette_winnings <- function(n){\n  X <- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\n  sum(X)\n}\nS <- replicate(B, roulette_winnings(n))\n\nThe Central Limit Theorem (CLT) tells us that the sum \\(S\\) is approximated by a normal distribution. Using the formulas above, we know that the expected value and standard error are:\n\nn * (20-18)/38\n\n[1] 52.63158\n\nsqrt(n) * 2 * sqrt(90)/19\n\n[1] 31.57895\n\n\nThe theoretical values above match those obtained with the Monte Carlo simulation:\n\nmean(S)\n\n[1] 52.2242\n\nsd(S)\n\n[1] 31.65508\n\n\nUsing the CLT, we can skip the Monte Carlo simulation and instead compute the probability of the casino losing money using this approximation:\n\nmu <- n * (20-18)/38\nse <-  sqrt(n) * 2 * sqrt(90)/19\npnorm(0, mu, se)\n\n[1] 0.04779035\n\n\nwhich is also in very good agreement with our Monte Carlo result:\n\nmean(S < 0)\n\n[1] 0.0458\n\n\n\nHow large is large in the Central Limit Theorem?\nThe CLT works when the number of draws is large. But large is a relative term. In many circumstances as few as 30 draws is enough to make the CLT useful. In some specific instances, as few as 10 is enough. However, these should not be considered general rules. Note, for example, that when the probability of success is very small, we need much larger sample sizes.\nBy way of illustration, let’s consider the lottery. In the lottery, the chances of winning are less than 1 in a million. Thousands of people play so the number of draws is very large. Yet the number of winners, the sum of the draws, range between 0 and 4. This sum is certainly not well approximated by a normal distribution, so the CLT does not apply, even with the very large sample size. This is generally true when the probability of a success is very low. In these cases, the Poisson distribution is more appropriate.\nYou can examine the properties of the Poisson distribution using dpois and ppois. You can generate random variables following this distribution with rpois. However, we do not cover the theory here. You can learn about the Poisson distribution in any probability textbook and even Wikipedia6"
  },
  {
    "objectID": "content/Week_05/05a.html#statistical-properties-of-averages",
    "href": "content/Week_05/05a.html#statistical-properties-of-averages",
    "title": "Probability and Statistics",
    "section": "Statistical properties of averages",
    "text": "Statistical properties of averages\nThere are several useful mathematical results that we used above and often employ when working with data. We list them below.\n1. The expected value of the sum of random variables is the sum of each random variable’s expected value. We can write it like this:\n\\[\n\\mbox{E}[X_1+X_2+\\dots+X_n] =  \\mbox{E}[X_1] + \\mbox{E}[X_2]+\\dots+\\mbox{E}[X_n]\n\\]\nIf the \\(X\\) are independent draws from the urn, then they all have the same expected value. Let’s call it \\(\\mu\\) and thus:\n\\[\n\\mbox{E}[X_1+X_2+\\dots+X_n]=  n\\mu\n\\]\nwhich is another way of writing the result we show above for the sum of draws.\n2. The expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable. This is easier to explain with symbols:\n\\[\n\\mbox{E}[aX] =  a\\times\\mbox{E}[X]\n\\]\nTo see why this is intuitive, consider change of units. If we change the units of a random variable, say from dollars to cents, the expectation should change in the same way. A consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, call it \\(\\mu\\) again:\n\\[\n\\mbox{E}[(X_1+X_2+\\dots+X_n) / n]=   \\mbox{E}[X_1+X_2+\\dots+X_n] / n = n\\mu/n = \\mu\n\\]\n3. The square of the standard error of the sum of independent random variables is the sum of the square of the standard error of each random variable. This one is easier to understand in math form:\n\\[\n\\mbox{SE}[X_1+X_2+\\dots+X_n] = \\sqrt{\\mbox{SE}[X_1]^2 + \\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2  }\n\\]\nThe square of the standard error is referred to as the variance in statistical textbooks. Note that this particular property is not as intuitive as the previous three and more in depth explanations can be found in statistics textbooks.\n4. The standard error of a non-random constant times a random variable is the non-random constant times the random variable’s standard error. As with the expectation: \\[\n\\mbox{SE}[aX] =  a \\times \\mbox{SE}[X]\n\\]\nTo see why this is intuitive, again think of units.\nA consequence of 3 and 4 is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of \\(n\\) (the number of draws), call it \\(\\sigma\\):\n\\[\n\\begin{aligned}\n\\mbox{SE}[(X_1+X_2+\\dots+X_n) / n] &=   \\mbox{SE}[X_1+X_2+\\dots+X_n]/n \\\\\n&= \\sqrt{\\mbox{SE}[X_1]^2+\\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2}/n \\\\\n&= \\sqrt{\\sigma^2+\\sigma^2+\\dots+\\sigma^2}/n\\\\\n&= \\sqrt{n\\sigma^2}/n\\\\\n&= \\sigma / \\sqrt{n}\n\\end{aligned}\n\\]\n5. If \\(X\\) is a normally distributed random variable, then if \\(a\\) and \\(b\\) are non-random constants, \\(aX + b\\) is also a normally distributed random variable. All we are doing is changing the units of the random variable by multiplying by \\(a\\), then shifting the center by \\(b\\).\nNote that statistical textbooks use the Greek letters \\(\\mu\\) and \\(\\sigma\\) to denote the expected value and standard error, respectively. This is because \\(\\mu\\) is the Greek letter for \\(m\\), the first letter of mean, which is another term used for expected value. Similarly, \\(\\sigma\\) is the Greek letter for \\(s\\), the first letter of standard error."
  },
  {
    "objectID": "content/Week_05/05a.html#law-of-large-numbers",
    "href": "content/Week_05/05a.html#law-of-large-numbers",
    "title": "Probability and Statistics",
    "section": "Law of large numbers",
    "text": "Law of large numbers\nAn important implication of the final result is that the standard error of the average becomes smaller and smaller as \\(n\\) grows larger. When \\(n\\) is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn. This is known in statistical textbooks as the law of large numbers or the law of averages.\n\nMisinterpreting law of averages\nThe law of averages is sometimes misinterpreted. For example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50% heads and 50% tails. A similar argument would be to say that red “is due” on the roulette wheel after seeing black come up five times in a row. These events are independent so the chance of a coin landing heads is 50% regardless of the previous 5. This is also the case for the roulette outcome. The law of averages applies only when the number of draws is very large and not in small samples. After a million tosses, you will definitely see about 50% heads regardless of the outcome of the first five tosses.\nAnother funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row."
  },
  {
    "objectID": "content/Week_05/05b.html",
    "href": "content/Week_05/05b.html",
    "title": "Visualizing Uncertainty",
    "section": "",
    "text": "Setup:\nYou’ll need to download one CSV files and put them somewhere on your computer (or upload it to RStudio.cloud if you’ve gone that direction)—preferably in a folder named data in your project folder. You can download the data from the link below:\n\n Dataset\n About this Dataset\n\nTask:\nExplore the relationship between the vote margin in a given district and whether the congressperson voted with or against President Biden’s position."
  },
  {
    "objectID": "content/Week_05/05b.html#polls",
    "href": "content/Week_05/05b.html#polls",
    "title": "Visualizing Uncertainty",
    "section": "Polls",
    "text": "Polls\nOpinion polling has been conducted since the 19th century. The general goal is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as inference and it is the main topic of this chapter.\nPerhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know in which geographical locations to focus their “get out the vote” efforts.\nElections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results.\nAlthough typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at such data.\nReal Clear Politics1 is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the 2016 presidential election2:\n\n\n\n\nAlthough in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC and we will go into some detail on this later.\nLet’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different spread: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled MoE which stands for margin of error.\nIn this example, we will show how the probability concepts we learned in the previous content can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define estimates and margins of errors, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: confidence intervals and p-values. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modeling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election.\nWe start by connecting probability theory to the task of using polls to learn about a population.\n\nThe sampling model for polls\nTo help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this hypothetical urn.\nBefore making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you 10 cents per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay \\$25 to collect your \\$25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.\nThe dslabs package includes a function that shows a random draw from this urn:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ntake_poll(25)\n\n\n\n\n\n\nThink about how you would construct your interval based on the data shown above.\nWe have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors. That is, that there are just two parties: Republican and Democratic."
  },
  {
    "objectID": "content/Week_05/05b.html#populations-samples-parameters-and-estimates",
    "href": "content/Week_05/05b.html#populations-samples-parameters-and-estimates",
    "title": "Visualizing Uncertainty",
    "section": "Populations, samples, parameters, and estimates",
    "text": "Populations, samples, parameters, and estimates\nWe want to predict the proportion of blue beads in the urn. Let’s call this quantity \\(p\\), which then tells us the proportion of red beads \\(1-p\\), and the spread \\(p - (1-p)\\), which simplifies to \\(2p - 1\\).\nIn statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the population \\(p\\) is called a parameter. The 25 beads we see in the previous plot are called a sample. The task of statistical inference is to predict the parameter \\(p\\) using the observed data in the sample.\nCan we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that \\(p\\) > .9 or \\(p\\) < .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?\nWe want to construct an estimate of \\(p\\) using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample \\(0.48\\) must be at least related to the actual proportion \\(p\\). But do we simply predict \\(p\\) to be 0.48? First, remember that the sample proportion is a random variable. If we run the command take_poll(25) four times, we get a different answer each time, since the sample proportion is a random variable.\n\n\n\n\n\nNote that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better.\n\nThe sample average\nConducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter \\(p\\). Once we have this estimate, we can easily report an estimate for the spread \\(2p-1\\), but for simplicity we will illustrate the concepts for estimating \\(p\\). We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion \\(p\\).\nWe start by defining the random variable \\(X\\) as: \\(X=1\\) if we pick a blue bead at random and \\(X=0\\) if it is red. This implies that the population is a list of 0s and 1s. If we sample \\(N\\) beads, then the average of the draws \\(X_1, \\dots, X_N\\) is equivalent to the proportion of blue beads in our sample. This is because adding the \\(X\\)s is equivalent to counting the blue beads and dividing this count by the total \\(N\\) is equivalent to computing a proportion. We use the symbol \\(\\bar{X}\\) to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant \\(1/N\\):\n\\[\\bar{X} = 1/N \\times \\sum_{i=1}^N X_i\\]\nFor simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is \\(N\\) times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be \\(p\\), the proportion of blue beads.\nHere we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to estimate \\(p\\).\n\n\nParameters\nJust like we use variables to define unknowns in systems of equations, in statistical inference we define parameters to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters \\(p\\) to represent this quantity. \\(p\\) is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is \\(p\\), we are going to estimate this parameter.\nThe ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask, what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample.\n\n\nPolling versus forecasting\nBefore we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the \\(p\\) for that moment and not for election day. The \\(p\\) for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section.\n\n\nProperties of our estimate: expected value and standard error\nTo understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion \\(\\bar{X}\\). Remember that \\(\\bar{X}\\) is the sum of independent draws so the rules we covered in the probability chapter apply.\nUsing what we have learned, the expected value of the sum \\(N\\bar{X}\\) is \\(N \\times\\) the average of the urn, \\(p\\). So dividing by the non-random constant \\(N\\) gives us that the expected value of the average \\(\\bar{X}\\) is \\(p\\). We can write it using our mathematical notation:\n\\[\n\\mbox{E}(\\bar{X}) = p\n\\]\nWe can also use what we learned to figure out the standard error: the standard error of the sum is \\(\\sqrt{N} \\times\\) the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is \\((1-0) \\sqrt{p (1-p)}\\) = \\(\\sqrt{p (1-p)}\\). Because we are dividing the sum by \\(N\\), we arrive at the following formula for the standard error of the average:\n\\[\n\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\n\\]\nThis result reveals the power of polls. The expected value of the sample proportion \\(\\bar{X}\\) is the parameter of interest \\(p\\) and we can make the standard error as small as we want by increasing \\(N\\). The law of large numbers tells us that with a large enough poll, our estimate converges to \\(p\\).\nIf we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?\nOne problem is that we do not know \\(p\\), so we can’t compute the standard error. However, for illustrative purposes, let’s assume that \\(p=0.51\\) and make a plot of the standard error versus the sample size \\(N\\):\n\n\n\n\n\nFrom the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and \\(p=0.51\\), the standard error is:\n\nsqrt(p*(1-p))/sqrt(1000)\n\n[1] 0.01580823\n\n\nor 1.5 percentage points. So even with large polls, for close elections, \\(\\bar{X}\\) can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the \\(p\\)."
  },
  {
    "objectID": "content/Week_05/05b.html#clt",
    "href": "content/Week_05/05b.html#clt",
    "title": "Visualizing Uncertainty",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe Central Limit Theorem (CLT) tells us that the distribution function for a sum of draws is approximately normal. You also may recall that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of \\(\\bar{X}\\) is approximately normal.\nIn summary, we have that \\(\\bar{X}\\) has an approximately normal distribution with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\nNow how does this help us? Suppose we want to know what is the probability that we are within 1% from \\(p\\). We are basically asking what is\n\\[\n\\mbox{Pr}(| \\bar{X} - p| \\leq .01)\n\\] which is the same as:\n\\[\n\\mbox{Pr}(\\bar{X}\\leq p + .01) - \\mbox{Pr}(\\bar{X} \\leq p - .01)\n\\]\nCan we answer this question? We can use the mathematical trick we learned in the previous chapter. Subtract the expected value and divide by the standard error to get a standard normal random variable, call it \\(Z\\), on the left. Since \\(p\\) is the expected value and \\(\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\\) is the standard error we get:\n\\[\n\\mbox{Pr}\\left(Z \\leq \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right) -\n\\mbox{Pr}\\left(Z \\leq - \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right)\n\\]\nOne problem we have is that since we don’t know \\(p\\), we don’t know \\(\\mbox{SE}(\\bar{X})\\). But it turns out that the CLT still works if we estimate the standard error by using \\(\\bar{X}\\) in place of \\(p\\). We say that we plug-in the estimate. Our estimate of the standard error is therefore:\n\\[\n\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N}\n\\] In statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and \\(N\\).\nNow we continue with our calculation, but dividing by \\(\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N})\\) instead. In our first sample we had 12 blue and 13 red so \\(\\bar{X} = 0.48\\) and our estimate of standard error is:\n\nx_hat <- 0.48\nse <- sqrt(x_hat*(1-x_hat)/25)\nse\n\n[1] 0.09991997\n\n\nAnd now we can answer the question of the probability of being close to \\(p\\). The answer is:\n\npnorm(0.01/se) - pnorm(-0.01/se)\n\n[1] 0.07971926\n\n\nTherefore, there is a small chance that we will be close. A poll of only \\(N=25\\) people is not really very useful, at least not for a close election.\n\nMargin of Error\nEarlier we mentioned the margin of error. Now we can define it because it is simply 1.96 times the standard error, which we can now estimate. In our case it is:\n\n1.96*se\n\n[1] 0.1958431\n\n\nWhy do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from \\(p\\), we get:\n\\[\n\\mbox{Pr}\\left(Z \\leq \\, 1.96\\,\\mbox{SE}(\\bar{X})  / \\mbox{SE}(\\bar{X}) \\right) -\n\\mbox{Pr}\\left(Z \\leq - 1.96\\, \\mbox{SE}(\\bar{X}) / \\mbox{SE}(\\bar{X}) \\right)\n\\] which is:\n\\[\n\\mbox{Pr}\\left(Z \\leq 1.96 \\right) -\n\\mbox{Pr}\\left(Z \\leq - 1.96\\right)\n\\]\nwhich we know is about 95%:\n\npnorm(1.96)-pnorm(-1.96)\n\n[1] 0.9500042\n\n\nHence, there is a 95% probability that \\(\\bar{X}\\) will be within \\(1.96\\times \\hat{SE}(\\bar{X})\\), in our case within about 0.2, of \\(p\\). Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. We often round 1.96 up to 2 for simplicity of presentation.\nIn summary, the CLT tells us that our poll based on a sample size of \\(25\\) is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.\nFrom the table above, we see that typical sample sizes range from 700 to 3500. To see how this gives us a much more practical result, notice that if we had obtained a \\(\\bar{X}\\)=0.48 with a sample size of 2,000, our standard error \\(\\hat{\\mbox{SE}}(\\bar{X})\\) would have been 0.0111714. So our result is an estimate of 48% with a margin of error of 2%. In this case, the result is much more informative and would make us think that there are more red balls than blue. Keep in mind, however, that this is hypothetical. We did not take a poll of 2,000 since we don’t want to ruin the competition.\n\n\nA Monte Carlo simulation\n(Optional) Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:\n\nB <- 10000\nN <- 1000\nx_hat <- replicate(B, {\n  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\n  mean(x)\n})\n\nThe problem is, of course, we don’t know p. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function take_poll(n=1000) instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.\nOne thing we therefore do to corroborate theoretical results is to pick one or several values of p and run the simulations. Let’s set p=0.45. We can then simulate a poll:\n\np <- 0.45\nN <- 1000\n\nx <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\nx_hat <- mean(x)\n\nIn this particular sample, our estimate is x_hat. We can use that code to do a Monte Carlo simulation:\n\nB <- 10000\nx_hat <- replicate(B, {\n  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\n  mean(x)\n})\n\nTo review, the theory tells us that \\(\\bar{X}\\) is approximately normally distributed, has expected value \\(p=\\) 0.45 and standard error \\(\\sqrt{p(1-p)/N}\\) = 0.0157321. The simulation confirms this:\n\nmean(x_hat)\n\n[1] 0.4500761\n\nsd(x_hat)\n\n[1] 0.01579523\n\n\nA histogram and qq-plot confirm that the normal approximation is accurate as well:\n\n\n\n\n\nOf course, in real life we would never be able to run such an experiment because we don’t know \\(p\\). But we could run it for various values of \\(p\\) and \\(N\\) and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing p and N.\n\n\nThe spread\nThe competition is to predict the spread, not the proportion \\(p\\). However, because we are assuming there are only two parties, we know that the spread is \\(p - (1-p) = 2p - 1\\). As a result, everything we have done can easily be adapted to an estimate of \\(2p - 1\\). Once we have our estimate \\(\\bar{X}\\) and \\(\\hat{\\mbox{SE}}(\\bar{X})\\), we estimate the spread with \\(2\\bar{X} - 1\\) and, since we are multiplying by 2, the standard error is \\(2\\hat{\\mbox{SE}}(\\bar{X})\\). Note that subtracting 1 does not add any variability so it does not affect the standard error.\nFor our 25 item sample above, our estimate \\(p\\) is .48 with margin of error .20 and our estimate of the spread is 0.04 with margin of error .40. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for \\(p\\), we have it for the spread \\(2p-1\\).\n\n\nBias: why not run a very large poll?\nFor realistic values of \\(p\\), say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%:\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nOne reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is \\(p\\). We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this in our Assignment for this week."
  },
  {
    "objectID": "content/Week_05/05b.html#code",
    "href": "content/Week_05/05b.html#code",
    "title": "Visualizing Uncertainty",
    "section": "Code",
    "text": "Code\n\nLoad and clean data\nFirst, we load the libraries we’ll be using:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggridges)\nlibrary(gghalves)\n\nThen we load the data with read_csv(). Here we assume that the CSV file lives in a subfolder in my project named data. Naturally, you’ll need to point this to wherever you stashed the data.\n\nweather_atl_raw <- read_csv(\"data/atl-weather-2019.csv\")\n\nWe’ll add a couple columns that we can use for faceting and filling using the month() and wday() functions from lubridate for extracting parts of the date:\n\nweather_atl <- weather_atl_raw %>%\n  mutate(Month = month(time, label = TRUE, abbr = FALSE),\n         Day = wday(time, label = TRUE, abbr = FALSE))\n\nNow we’re ready to go!\n\n\nHistograms\nWe can first make a histogram of wind speed. We’ll use a bin width of 1 and color the edges of the bars white:\n\nggplot(weather_atl, aes(x = windSpeed)) +\n  geom_histogram(binwidth = 1, color = \"white\")\n\n\n\n\n\n\n\n\nThis is fine enough, but we can improve it by forcing the buckets/bins to start at whole numbers instead of containing ranges like 2.5–3.5. We’ll use the boundary argument for that. We also add scale_x_continuous() to add our own x-axis breaks instead of having things like 2.5, 5, and 7.5:\n\nggplot(weather_atl, aes(x = windSpeed)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 1) +\n  scale_x_continuous(breaks = seq(0, 12, by = 1))\n\n\n\n\n\n\n\n\nWe can show the distribution of wind speed by month if we map the Month column we made onto the fill aesthetic:\n\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 1) +\n  scale_x_continuous(breaks = seq(0, 12, by = 1))\n\n\n\n\n\n\n\n\nThis is colorful, but it’s impossible to actually interpret. Instead of only filling, we’ll also facet by month to see separate graphs for each month. We can turn off the fill legend because it’s now redundant.\n\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 1) +\n  scale_x_continuous(breaks = seq(0, 12, by = 1)) +\n  guides(fill = FALSE) +\n  facet_wrap(vars(Month))\n\nWarning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\n\n\n\n\nNeat! January, March, and April appear to have the most variation in windy days, with a few wind-less days and a few very-windy days, while August was very wind-less.\n\n\nDensity plots\nThe code to create a density plot is nearly identical to what we used for the histogram—the only thing we change is the geom layer:\n\nggplot(weather_atl, aes(x = windSpeed)) +\n  geom_density(color = \"grey20\", fill = \"grey50\")\n\n\n\n\n\n\n\n\nIf we want, we can mess with some of the calculus options like the kernel and bandwidth:\n\nggplot(weather_atl, aes(x = windSpeed)) +\n  geom_density(color = \"grey20\", fill = \"grey50\",\n               bw = 0.1, kernel = \"epanechnikov\")\n\n\n\n\n\n\n\n\nWe can also fill by month. We’ll make the different layers 50% transparent so we can kind of see through the whole stack:\n\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\nEven with the transparency, this is really hard to interpret. We can fix this by faceting, like we did with the histograms:\n\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\n  geom_density(alpha = 0.5) +\n  guides(fill = FALSE) +\n  facet_wrap(vars(Month))\n\n\n\n\n\n\n\n\nOr we can stack the density plots behind each other with ggridges. For that to work, we also need to map Month to the y-axis. We can reverse the y-axis so that January is at the top if we use the fct_rev() function:\n\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +\n  geom_density_ridges() +\n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\nWe can add some extra information to geom_density_ridges() with some other arguments like quantile_lines. We can use the quantiles argument to tell the plow how many parts to be cut into. Since we just want to show the median, we’ll set that to 2 so each density plot is divided in half:\n\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +\n  geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +\n  guides(fill = FALSE)\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\nNow that we have good working code, we can easily substitute in other variables by changing the x mapping:\n\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = Month)) +\n  geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +\n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\nWe can get extra fancy if we fill by temperature instead of filling by month. To get this to work, we need to use geom_density_ridges_gradient(), and we need to change the fill mapping to the strange looking ..x.., which is a weird ggplot trick that tells it to use the variable we mapped to the x-axis. For whatever reason, fill = temperatureHigh doesn’t work:\n\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = ..x..)) +\n  geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(x = \"High temperature\", y = NULL, color = \"Temp\")\n\nWarning: The dot-dot notation (`..x..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(x)` instead.\n\n\n\n\n\n\n\n\n\nAnd finally, we can get extra fancy and show the distributions for both the high and low temperatures each month. To make this work, we need to manipulate the data a little. Right now there are two columns for high and low temperature: temperatureLow and temperatureHigh. To be able to map temperature to the x-axis and high vs. low to another aesthetic (like linetype), we need a column with the temperature and a column with an indicator variable for whether it is high or low. This data needs to be tidied (since right now we have a variable (high/low) encoded in the column name). We can tidy this data using pivot_longer() from tidyr, which was already loaded with library(tidyverse). In the RStudio primers, you did this same thing with gather()—pivot_longer() is the newer version of gather():\n\nweather_atl_long <- weather_atl %>%\n  pivot_longer(cols = c(temperatureLow, temperatureHigh),\n               names_to = \"temp_type\",\n               values_to = \"temp\") %>%\n  # Clean up the new temp_type column so that \"temperatureHigh\" becomes \"High\", etc.\n  mutate(temp_type = recode(temp_type,\n                            temperatureHigh = \"High\",\n                            temperatureLow = \"Low\")) %>%\n  # This is optional—just select a handful of columns\n  select(time, temp_type, temp, Month)\n\n# Show the first few rows\nhead(weather_atl_long)\n\n# A tibble: 6 × 4\n  time                temp_type  temp Month  \n  <dttm>              <chr>     <dbl> <ord>  \n1 2019-01-01 05:00:00 Low        50.6 January\n2 2019-01-01 05:00:00 High       63.9 January\n3 2019-01-02 05:00:00 Low        49.0 January\n4 2019-01-02 05:00:00 High       57.4 January\n5 2019-01-03 05:00:00 Low        53.1 January\n6 2019-01-03 05:00:00 High       55.3 January\n\n\nNow we have a column for the temperature (temp) and a column indicating if it is high or low (temp_type). The dataset is also twice as long (730 rows) because each day has two rows (high and low). Let’s plot it and map high/low to the linetype aesthetic to show high/low in the border of the plots:\n\nggplot(weather_atl_long, aes(x = temp, y = fct_rev(Month),\n                             fill = ..x.., linetype = temp_type)) +\n  geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(x = \"High temperature\", y = NULL, color = \"Temp\")\n\n\n\n\n\n\n\n\nWe can see much wider temperature disparities during the summer, with large gaps between high and low, and relatively equal high/low temperatures during the winter.\n\n\nBox, violin, and rain cloud plots\nFinally, we can look at the distribution of variables with box plots, violin plots, and other similar graphs. First, we’ll make a box plot of windspeed, filled by the Day variable we made indicating weekday:\n\nggplot(weather_atl,\n       aes(y = windSpeed, fill = Day)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nWe can switch this to a violin plot by just changing the geom layer and mapping Day to the x-axis:\n\nggplot(weather_atl,\n       aes(y = windSpeed, x = Day, fill = Day)) +\n  geom_violin()\n\n\n\n\n\n\n\n\nWith violin plots it’s typically good to overlay other geoms. We can add some jittered points for a strip plot:\n\nggplot(weather_atl,\n       aes(y = windSpeed, x = Day, fill = Day)) +\n  geom_violin() +\n  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +\n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\nWe can also add larger points for the daily averages. We’ll use a special layer for this: stat_summary(). It has a slightly different syntax, since we’re not actually mapping a column from the dataset. Instead, we’re feeding a column from a dataset into a function (here \"mean\") and then plotting that result:\n\nggplot(weather_atl,\n       aes(y = windSpeed, x = Day, fill = Day)) +\n  geom_violin() +\n  stat_summary(geom = \"point\", fun = \"mean\", size = 5, color = \"white\") +\n  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +\n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\nWe can also show the mean and confidence interval at the same time by changing the summary function:\n\nggplot(weather_atl,\n       aes(y = windSpeed, x = Day, fill = Day)) +\n  geom_violin() +\n  stat_summary(geom = \"pointrange\", fun.data = \"mean_se\", size = 1, color = \"white\") +\n  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +\n  guides(fill = FALSE)\n\n\n\n\n\n\n\n\nOverlaying the points directly on top of the violins shows extra information, but it’s also really crowded and hard to read. If we use the gghalves package, we can use special halved versions of some of these geoms like so:\n\nggplot(weather_atl,\n       aes(x = fct_rev(Day), y = temperatureHigh)) +\n  geom_half_point(aes(color = Day), side = \"l\", size = 0.5) +\n  geom_half_boxplot(aes(fill = Day), side = \"r\") +\n  guides(color = FALSE, fill = FALSE)\n\n\n\n\n\n\n\n\nNote the side argument for specifying which half of the column the geom goes. We can also use geom_half_violin():\n\nggplot(weather_atl,\n       aes(x = fct_rev(Day), y = temperatureHigh)) +\n  geom_half_point(aes(color = Day), side = \"l\", size = 0.5) +\n  geom_half_violin(aes(fill = Day), side = \"r\") +\n  guides(color = FALSE, fill = FALSE)\n\n\n\n\n\n\n\n\nIf we flip the plot, we can make a rain cloud plot:\n\nggplot(weather_atl,\n       aes(x = fct_rev(Day), y = temperatureHigh)) +\n  geom_half_boxplot(aes(fill = Day), side = \"l\", width = 0.5, nudge = 0.1) +\n  geom_half_point(aes(color = Day), side = \"l\", size = 0.5) +\n  geom_half_violin(aes(fill = Day), side = \"r\") +\n  guides(color = FALSE, fill = FALSE) +\n  coord_flip()"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Course Content",
    "section": "",
    "text": "Each week has two sets of required readings (pages in the sidebar) that you should complete before coming to lecture. Read the first before our first meeting of the week, and read the second before the second meetings. That is, you should complete the first reading, attend Tuesday class, then do the associated exercises contained within the reading, and read the second reading before Thursday. You will be working each week’s lab between Thursday afternoon and Monday at 11:59 PM (when the labs are due). Don’t forget your weekly writing in between, due Saturday at 11:59pm.\nThe course content is structured as follows. For each topic, we begin with a set of questions that might guide your reading and help frame your thoughts. These questions can serve as helpful starting places for your thinking; they are not representative of the totality of the content and are not intended to be limiting. You should not try to respond to all of these (or any of them if you don’t want to)—they’ll just help you know what to look for and think about as you read. The first reading is generally a “principles” reading, discussing the concepts for the week. The second is generally an “applications” reading, meant to give concrete examples and code. This isn’t always the case, but holds in general."
  },
  {
    "objectID": "content/index.html#content-navigation",
    "href": "content/index.html#content-navigation",
    "title": "Course Content",
    "section": "Content navigation",
    "text": "Content navigation\nUse the links on the sidebar to navigate between required reading. Readings are ordered by week, and within each week, by order of the class meetings."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EC242 - Social Science Data Analytics",
    "section": "",
    "text": "Date: Date and time\nRoom: Location\nInstructors: Instructor A, B, C\n\n\n\n\n\n\n\nThis workshop template\n\n\n\nThis workshop template contains 4 pages:\n\nHome: index.qmd (this page)\nAbout: about.qmd\n\nTwo content pages\n\nPage without code: part_1_prep.qmd\nPage with R code: part_2_eda.qmd\nAnd another: content/Week 00/00a.qmd\n\nIt is straightforward to add more content pages: you need to create a new .qmd file (or copy/paste the existing ones), then link the new page inside _quarto.yml.\n\n\n\n\n\n\n\n\nHomepage of your workshop\n\n\n\nThis is the homepage index.qmd for your workshop, so ideally it should contain some key information such as time and place, instructors and information of the course/workshop.\nIt should be easy to navigate.\n\n\n\nWelcome!\n\nThe goal of the workshop is to … (insert your message)\nFor example, introduce kep concepts in machine learning, such as regularisation.\nWorkshop material can be found in the workshop github repository.\n\n\nLearning Objectives\nAt the end of the tutorial, participants will be able to\n\nunderstand key concepts for … (insert your message)\nFor example, training machine learning models such as regularisation.\nanother objective\n\n\n\nPre-requisites\n\nBasic familiarity with R\nSome other knowledge\n\n\n\n\nSchedule\n\n\n\n\n\n\nTabular schedule\n\n\n\nIt can be useful to include a tabular schedule with links.\n\n\n\n\n\nTime\nTopic\nPresenter\n\n\n\n\n9:00 - 10:30\nSession 1: Preparation\nInstructor A\n\n\n10:45 - 12:00\nSession 2: Exploratory Analysis\nInstructor B, C"
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Helpful resources",
    "section": "",
    "text": "In these reference pages, I’ve included some useful resources to help you in the course."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This course meets Tu/Th 1:20 - 2:40pm in NatSci 116. This class is in-person and will not have an online component.\n\n\n\n\nProf. K office hours: Thursdays 2:45 (after class) till 4:00pm\nProf. K office hours Zoom link: zoom.\n\nMy office hours start immediately following class on Thursday, so I will stay in the classroom and answer questions, or we can walk-and-talk back to my office if you have a longer question. If you prefer to meet at my office located at Old Botany 105 (off W. Circle Ave), allow a little time for me to return from class. I’ll log onto Zoom when I arrive at the office for anyone attending remotely.\nI also hold dedicated Sunday evening office hours via Slack between 6:30 PM until about 8:00 PM. In addition, I will check Slack throughout the week and use it as an always-on avenue of communication and help.\nIt would be remarkable if you didn’t need some assistance with the material, and I am here to help. One of the benefits of open office hours is to accommodate many students at once; if fellow students are in my office, please join in and feel very free to show up in groups. As a general rule, please first seek course-related help from the course website. However, if my scheduled office hours do not work for you please let me know. I may encourage you to make appointments with me. I ask that you schedule your studying so that you are prepared to ask questions during office hours – office hours are not a lecture and if you’re not prepared with questions we will end up awkwardly staring at each other for an hour until you leave.\nSome gentle requests regarding office hours and on contacting me. First, my office hours end sharply at the end, so don’t arrive 10 minutes before the scheduled end and expect a full session. Please arrive early if you have lengthy questions, or if you don’t want to risk not having time due to others’ questions. You are free to ask me some stuff by e-mail, (e.g. a typo or something on a handout), but please know e-mail sucks for answering many types of questions. “How do I do this lab?” or “What’s wrong with my R setup?” are short questions with long answers. Come to office hours or ask on Slack.\n\n\n\n\nTA Placeholder: TBA@msu.edu\nTA Office Hours via Zoom: TA Office Hours TBA\nTA Office Hours Zoom link: zoomlink\n\n\n\n\nWe will use Slack as a forum for asking questions about the course, including course policies and, primarily, help questions with R. Students are encouraged to help answer each others’ questions, and to use the forum as a first-step for seeking help. Myself and the TA will monitor slack and answer questions regularly. You can join our Slack channel with this link: Join EC242 Slack. Once you have joined, bookmark our Slack.\nMy DM’s are closed. Please ask your question in one of the coursewide channels so that our TA can answer when I am unavailable. You will receive a faster reply that way.\nJoining Slack counts towards course participation. I highly recommend you join."
  },
  {
    "objectID": "syllabus.html#what-is-this-course",
    "href": "syllabus.html#what-is-this-course",
    "title": "Syllabus",
    "section": "What is This Course?",
    "text": "What is This Course?\n\nWhat it is:\nInnovations in statistical learning have created many engineering breakthroughs. From real time voice recognition to automatic categorization (and in some cases production) of news stories, machine learning is transforming the way we live our lives. These techniques are, at their heart, novel ways to work with data, and therefore they should have implications for social science. This course explores the intersection of statistical learning (or machine learning) and social science and aims to answer two primary questions about these new techniques:\n\nHow does statistical learning work and what kinds of statistical guarantees can be made about the performance of statistical-learning algorithms?\nHow can statistical learning be used to answer questions that interest social science researchers, such as testing theories or improving social policy?\n\nIn order to address these questions, we will cover so-called “standard” techniques such as supervised and unsupervised learning, statistical learning theory and nonparametric and Bayesian approaches. If it were up to me, this course would be titled “Statistical Learning for Social Scientists”—I believe this provides a more appropriate guide to the content of this course. And while this class will cover these novel statistical methodologies in some detail, it is not a substitute for the appropriate class in Computer Science or Statistics. Nor is this a class that teaches specific skills for the job market. Rather, this class will teach you to think about data analytics broadly. We will spend a great deal of time learning how to interpret the output of statistical learning algorithms and approaches, and will also spend a great deal of time on better understanding the basic ideas in statistical learning. This, of course, comes at some cost in terms of time spent on learning computational and/or programming skills.\nEnrollment for credit in this course is simply not suitable for those unprepared in or uninterested in elementary statistical theory no matter the intensity of interest in machine learning or “Big Data”. Really.\nYou will be required to understand elementary mathematics in this course and should have at least some exposure to statistical theory. The class is front-loaded technically: early lectures are more mathematically oriented, while later lectures are more applied.\nThe topics covered in this course are listed later in this document. I will assign readings sparingly from  Introduction to Statistical Learning, henceforth referred to as ISL. This text is available for free online and, for those who like physical books, can be purchased for about $25. Importantly, the lectures deviate a fair bit from the reading, and thus you will rely on your course notes much more than you might in other classes.\nIf—after you have read this document and preferably after attending the first lecture—you have any questions about whether this course is appropriate for you, please come talk to me.\n\n\nWhat it is Not:\nThe focus of this course is conceptual. The goal is to create a working understanding of when and how tools from computer science and statistics can be profitably applied to problems in social science. Though students will be required to apply some of these techniques themselves, this course is not…\n…a replacement for EC420, EC422, or a course in causal inference.\nAs social scientists, we are most often concerned with causal inference in order to anaOpelyze and write policies. Statistical learning and the other methods we will discuss in this course are generally not well-suited to these problems, and while I’ll give a short overview of standard methods, this is only to build intuitions. Ultimately, this course has a different focus and you should still pursue standard methodological insights from your home departments.\n…a course on the computational aspects of the underlying methods.\nThere are many important innovations that have made machine learning techniques computationally feasible. We will not discuss these, as there are computer science courses better equipped to cover them. When appropriate, we will discuss whether something is computable, and we will even give rough approximations of the amount of time required (e.g. P vs NP). But we will not discuss how optimizers work or best practices in programming.\n…a primer on the nitty-gritty of how to use these tools or a way to pad your resume.\nThe mechanics of implementation, whether it be programming languages or learning to use APIs, will not be covered in any satisfying level of depth. Students will be expected to learn most of the programming skills on their own. Specifically, while there will be some material to remind you of basic R commands, this is not a good course for people who are simply looking to learn the mechanics of programming. This course is designed to get you to use both traditional analytics and, eventually, machine learning tools. We will do some review of basic programming, and you will have the opportunity to explore topics that interest you through a final project, but ultimately this is a course that largely focuses on the theoretical and practical aspects of statistical learning as applied to social science and not a class on programming.\nPerhaps most importantly, this course is an attempt to push undergraduate education toward the frontiers in social science. Accordingly, please allow some messiness. Some topics may be underdeveloped for a given person’s passions, but given the wide variety of technical skills and overall interests, this is a near certainty. Both the challenge and opportunity of this area comes from the fact that there is no fully developed, wholly unifying framework. Our collective struggle—me from teaching, you from learning—will ultimately bear fruit.\n\n\nSuccess in this Course\nI promise, you are equipped to succeed in this course.\nLearning R can be difficult at first. Like learning a new language—Spanish, French, or Chinese—it takes dedication and perseverance. Hadley Wickham (the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like) ggplot2—made this wise observation:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\nEven experienced programmers (like me) find themselves bashing their heads against seemingly intractable errors.1 If you’re finding yourself bashing your head against a wall and not making progress, try the following. First, take a break. Sometimes you just need space to see an error. Next, talk to classmates. Finally, if you genuinely cannot see the solution, e-mail the TA. But, honestly, it’s probably just a typo."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course materials",
    "text": "Course materials\nThe course website can be found at https://ec242.netlify.app (but you know that. You’re on it right now.)\nAll of the readings and software in this class are free. There are free online version of all the texts including  Introduction to Statistical Learning (2nd Ed) and R / RStudio are free (don’t pay for RStudio). We will reference outside readings and there exist paper versions of some “books” but you won’t need to buy anything2\n\nR and RStudio/Posit\nYou will do all of your analysis with the open source (and free!) programming language R. You will use RStudio (which is undergoing a slow-mo rebrand to “Posit” while the functionality remains the same) as the main program to access R. Think of R as an engine and RStudio as a car—R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code.\nR is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free Posit.cloud (formerly Rstudio.cloud) service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R. We recommend this for those who may be switching between computers and are trying to get some work done. That said, while Posit.cloud is convenient, it can be slow and it is not designed to be able to handle larger datasets or more complicated analysis and graphics. You also can’t use your own custom fonts with RStudio.cloud.3 And, generally speaking, you should have (from the prerequisite course) sufficient experience to make your R work. If not, over the course of the semester, you’ll probably want to get around to installing R, RStudio, and other R packages on your computer and wean yourself off of RStudio.cloud. If you plan on making a career out of data science, you should consider this a necessary step.\nYou can find instructions for installing R, RStudio/Posit, and all the tidyverse packages here. And you may find some other goodies.\n\n\nOnline help\nData science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Likewise, whenever using a specific package, try searching for that package name instead of the letter “r” (e.g. “ggplot scatterplot”). Good, concise searches are generally more effective.\nHelp with Using R: There are some excellent additional tutorials on R available through Rstudio/Posit Clould Primers."
  },
  {
    "objectID": "syllabus.html#evaluations-and-grades",
    "href": "syllabus.html#evaluations-and-grades",
    "title": "Syllabus",
    "section": "Evaluations and Grades",
    "text": "Evaluations and Grades\nYour grade in this course will be based on attendance/participation, labs, weekly writings, and a final project.\nThe general breakdown will be approximately 55% for labs, participation, and weekly writings, and 45% for projects (see below for specific details). The primary focus of the course is a final project; this requires two “mini-projects” to ensure you’re making satisfactory progress. Assignment of numeric grades will follow the standard, where ties (e.g., 91.5%) are rounded to favor the student. Evaluations (read: grades) are designed not to deter anyone from taking this course who might otherwise be interested, but will be taken seriously.\nWeekly writings are intended to be an easy way to get some points. Labs will be short homework assignments that require you to do something practical using R. You must have access to computing resources and the ability to program basic statistical analyses. If you are unprepared to implement basic statistical coding, please take (or retake) PLS202. I highly encourage seeking coding advice from those who instruct computer science courses – it’s their job and they are better at it than I am. I’ll try to provide a good service, but I’m really not an expert in computer science.\nMore in-depth descriptions for all the assignments are on the assignments page. As the course progresses, the assignments themselves will be posted within that page.\n\nDropping your lowest scores\nI will automatically drop your two lowest weekly writings and your one lowest lab assignment score. This allowance absorbs any personal issues, travel problems, computing issues, or non-excused medical issues that may preclude you from completing your assignment on time. If you request an extension or exemption, I will politely point you here.\n\n\nGrade Rubric\n\n\n\n\n\n\n\n\n\nAssignment\nPoints\nPercent\n\n\n\n\nClass Participation\n25\n5%\n\n\nWeekly Writings (14-2 x 8 ea), drop two lowest\n96\n18%\n\n\nLabs (13-1 x 15 ea), drop one lowest\n180\n34%\n\n\nMini project 1\n50\n9%\n\n\nMini project 2\n50\n9%\n\n\nFinal project\n135\n25%\n\n\nTotal\n536\n—\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\n4.0\n92-100%\n2.0\n72-76%\n\n\n3.5\n87-91%\n1.5\n67-72%\n\n\n3.0\n82-87%\n1.0\n62-67%\n\n\n2.5\n77-81%\n0.0\nbad-66%\n\n\n\n\n\n\nClass Participation\n\nParticipation can take many forms. Most preferred is active participation during class – asking clarifying questions or responding to lecture questions. In the latter half of the semester, we will work in groups in class on small coding exercises and will share results at the end. Sharing your group’s results and code will always count towards participation. Finally, I will often bribe give extra credit points for answering specific questions in class, which I will clearly state as extra credit. Wrong answers get the same credit as right answers. We are here to learn . If you knew everything already, you wouldn’t be in the class.\n\n\nAcademic honesty\nViolation of MSU’s Spartan Code of Honor will result in a grade of 0.0 in the course. Moreover, I am required by MSU policy to report suspected cases of academic dishonesty for possible disciplinary action.4\n\n\nGenerative AI\nGenerative AI is both a computing resource and potential avenue for cheating in violation of the Spartan Code of Honor (see Academic Integrity, below). For this course, some use of generative AI is permitted or forbidden as follows:\n\nFor the purposes of learning R coding, data cleaning and processing, visualization, and other coding applications of R, you are fully permitted to use ChatGPT or other generative AI models provided you indicate such a use at the top of your problem set, and include a comment #Used Generative AI in your code. When “stuck” on coding, I have always encouraged students to use Stack Overflow to find similar problems and then translate the found solutions into the problem at hand. It is one of the most important skills in coding. Generative AI facilitates this process, and thus is a tool. Since R itself is a tool to help you learn and apply the material, I view using generative AI for help in specific coding tasks as a suitable use. You must, in all cases, understand what your code is doing, and be able to justify its use. If you cannot tell me, on review, what a line of code is used for, then you have not used generative AI properly.\n\n\n\nGrading\nAll grades are considered final. Any request for a re-grade beyond simple point-tallying mistakes will require that the entire assignment be re-graded. Any points previously awarded may be changed in either direction in the re-grade."
  },
  {
    "objectID": "syllabus.html#resources",
    "href": "syllabus.html#resources",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nMental health concerns or stressful events may lead to diminished academic performance or reduce a student’s ability to participate in daily activities. Services are available to assist you with addressing these and other concerns you may be experiencing. You can learn more about the broad range of confidential mental health services available on campus via the Counseling & Psychiatric Services (CAPS) website at www.caps.msu.edu."
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\nIf you need a special accommodation for a disability, religious observance, or have any other concerns about your ability to perform well in this course, please contact me immediately so that we can discuss the issue and make appropriate arrangements. MSU has a specific policy for religious observance available here.\nMichigan State University is committed to providing equal opportunity for participation in all programs, services and activities. Requests for accommodations by persons with disabilities may be made by contacting the Resource Center for Persons with Disabilities at 517-884-RCPD or on the web at rcpd.msu.edu. Once your eligibility for an accommodation has been determined, you will be issued a verified individual services accommodation (“VISA”) form. Please present this form to me at the start of the term and/or two weeks prior to the accommodation date (test, project, etc). Requests received after this date will be honored whenever possible."
  },
  {
    "objectID": "syllabus.html#mandated-reporting",
    "href": "syllabus.html#mandated-reporting",
    "title": "Syllabus",
    "section": "Mandated Reporting",
    "text": "Mandated Reporting\nEssays, journals, and other materials submitted for this class are generally considered confidential pursuant to the University’s student record policies. However, students should be aware that University employees, including instructors, may not be able to maintain confidentiality when it conflicts with their responsibility to report certain issues to protect the health and safety of MSU community members and others. As the instructor, I must report the following information to other University offices (including the Department of Police and Public Safety) if you share it with me: • Suspected child abuse/neglect, even if this maltreatment happened when you were a child; • Allegations of sexual assault, relationship violence, stalking, or sexual harassment; and • Credible threats of harm to oneself or to others. These reports may trigger contact from a campus official who will want to talk with you about the incident that you have shared. In almost all cases, it will be your decision whether you wish to speak with that individual. If you would like to talk about these events in a more confidential setting, you are encouraged to make an appointment with the MSU Counseling and Psychiatric Services."
  },
  {
    "objectID": "syllabus.html#acknowledgements",
    "href": "syllabus.html#acknowledgements",
    "title": "Syllabus",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course structure and content has been improved greatly by Professor Kirkpatrick. All remaining errors are my own."
  },
  {
    "objectID": "syllabus.html#miscellanea",
    "href": "syllabus.html#miscellanea",
    "title": "Syllabus",
    "section": "Miscellanea",
    "text": "Miscellanea\nAll class material will be posted on https://ec242.netlify.app. D2L will be used sparingly for submission of weekly writings and assignments and distribution of grades.\n\nContacting Me\nEmail is a blessing and a curse. Instant communication is wonderful, but often email is the wrong medium to have a productive conversation about course material. Moreover, I get a lot of emails. This means that I am frequently triaging emails into two piles: “my house is burning down” and “everything else”. Your email is unlikely to make the former pile. So… asking questions about course material is always best done in-class or in office hours. Students always roll their eyes when professors say things like that, but it’s true that if you have a question, it’s very likely someone else has the same question.\nThat said, email is still useful. If you’re going to use it, you should at least use if effectively. There’s a running joke in academia that professors only read an email until they find a question. They then respond to that question and ignore the rest of the email. I won’t do this, but I do think it is helpful to assume that the person on the receiving end of an email will operate this way. By keeping this in mind, you will write a much more concise and easy to understand email.\nSome general tips:\n\nAlways include [EC242] in your subject line (brackets included).\nUse a short but informative subject line. For example: [EC242] Final Project Grading\nUse your University-supplied email for University business. This helps me know who you are.\nOne topic, one email. If you have multiple things to discuss, and you anticipate followup replies, it is best to split them into two emails so that the threads do not get cluttered.\nAsk direct questions. If you’re asking multiple questions in one email, use a bulleted list.\nDon’t ask questions that are answered by reading the syllabus! This drives me nuts.\nI’ve also found that students are overly polite in emails. I suppose it may be intimidating to email a professor, and you should try to match the style that the professor prefers, but I view email for a course as a casual form of communication. Said another way: get to the point. Students often send an entire paragraph introducing themselves, but if you use your University email address, and add the course name in the subject, I will already know who you are. Here’s an example of a perfectly reasonable email:\n\n\nSubject: [EC242] Lab, Question 2, Typo\nHi Prof. K,\nThere seems to be a typo in the Lab on Question 2. The problem says to use a column of data that doesn’t seem to exist. Can you correct this or which should we use?\nThanks, Student McStudentFace\n\n\n\nLetters of Recommendation / References\nAt this time, I am essentially not writing letters of recommendation. If you want to convince me to do so, you’ll likely need to work exceptionally hard. Consider this a warning."
  }
]