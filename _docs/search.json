[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About page\n\n\n\nThis page contains some elaborated background information about your workshop, or the instructors.\n\n\nFor example: A central problem in machine learning is how to make an algorithm perform well not just on the training data, but also on new inputs. Many strategies in machine learning are explicitly designed to reduce this test error, possibly at the expense of increased training error. These strategies are collectively known as regularisation and they are instrumental for good performance of any kind of prediction or classification model, especially in the context of small data (many features, few samples).\nIn the hands-on tutorial we will use R to perform an integrated analysis of multi-omics data with penalised regression.\n\nContact\nInstructor A: contact\nInstructor B: contact\nInstructor C: contact"
  },
  {
    "objectID": "content/Week_00/00a.html",
    "href": "content/Week_00/00a.html",
    "title": "0A - Testing",
    "section": "",
    "text": "Page with R code\n\n\n\nThis page contains an example template for a lab session, where R code and results are displayed here.\nYou can find more information on how to include code in Quarto website here.\nYou can experiment with code-fold and code-tools in the yaml header above to change how the code cells look like."
  },
  {
    "objectID": "content/Week_00/00a.html#a-cancer-modeling-example",
    "href": "content/Week_00/00a.html#a-cancer-modeling-example",
    "title": "0A - Testing",
    "section": "A Cancer Modeling Example",
    "text": "A Cancer Modeling Example\nExercise on analysis of miRNA, mRNA and protein data from the paper Aure et al, Integrated analysis reveals microRNA networks coordinately expressed with key proteins in breast cancer, Genome Medicine, 2015.\nPlease run the code provided to replicate some of the analyses. Make sure you can explain what all the analysis steps do and that you understand all the results.\nIn addition, there are some extra tasks (Task 1), where no R code is provided. Please do these tasks when you have time available at the end of the lab.\n\nLoad the data\nRead the data, and convert to matrix format.\n\nmrna <- read.table(here(\"data/data_example.txt\"), header=T, sep=\"\\t\", dec=\".\")\n\n# Convert to matrix format\n\nmrna <- as.matrix(mrna)\n\nPrint the data\n\nmrna[1:4, 1:4]\n\n      OSL2R.3002T4 OSL2R.3005T1 OSL2R.3013T1 OSL2R.3030T2\nACACA      1.60034     -0.49087     -0.26553     -0.27857\nANXA1     -2.42501     -0.05416     -0.46478     -2.18393\nAR         0.39615     -0.43348     -0.10232      0.58299\nBAK1       0.78627      0.39897      0.22598     -1.31202\n\n\nVisualise the overall distribution of expression levels by histogram\n\nhist(mrna, nclass=40, xlim=c(-5,5), col=\"lightblue\")\n\n\n\n\n\n\n\n\n\n\nTask 1\n\n\n\nThis is a callout-note, and it can be quite useful for exercises. You can find more about callout here.\nExample: Extend the above analysis to cover all genes."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html",
    "title": "Introduction to the tidyverse",
    "section": "",
    "text": "This page.\nChapter 1 of Introduction to Statistical Learning, available here.\nOptional: The “Tidy Your Data” tutorial on Rstudio Cloud Primers"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#some-reminders",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#some-reminders",
    "title": "Introduction to the tidyverse",
    "section": "Some Reminders:",
    "text": "Some Reminders:\n\nStart labs early!\n\nThey are not trivial.\nThey are not short.\nThey are not easy.\nThey are not optional.\n\nYou install.packages(\"packageName\") once on your computer.\n\nAnd never ever ever in your code.\n\nYou load an already-installed package using library(packageName) in a code chunk\n\nNever in your console\nWhen RMarkdown knits, it starts a whole new, empty session that has no knowledge of what you typed into the console\n\nSlack\n\nUse it.\nI would very much prefer posting in the class-visible channels. Others can learn from your issues.\n\nWe have a channel just for labs and R. Please use that one.\n\n\n\n\nGroup Projects\nYour final is a group project. You will also have two “mini” projects. They comprise a large part of your grade. As mentioned last week, this mean that you need to start planning soon.\nTo aid in your planning, here are the required elements of your final project.\n\nYou must find existing data to analyze. Aggregating and merging data from multiple sources is encouraged.\nYou must visualize 3 interesting features of that data.\nYou must come up with some analysis—using tools from this course—which relates your data to either a prediction or a policy conclusion.\nYou must think critically about your analysis and be able to identify potential issues.\nYou must present your analysis as if presenting to a C-suite executive.\n\nYour mini-projects along the way will be more structured, but will serve to guide you towards the final project.\n\n\nTeams\nPlease form teams of 3 people. Once all agree to be on a team, have ONE PERSON email our TA Allen scovelpa@msu.edu and cc all of the members of the team so that nobody is surprised to be included on a team. Title the email [SSC442] - Group Formation. Tell us your team name (be creative), and list in the email the names of all of the team members and their email address (in addition to cc-ing those team members on the email).\nIf you opt to not form a team, you will be automatically added to the “willing to be randomly assigned” pool and will be paired with two others from the “willing to be randomly assigned” pool.\nSend this email by January 20th and we will assign un-teamed folks at the beginning of the following week. Project 1 is due in no time. See schedule for all the important project dates.\n\n\nGuiding Question\nFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\nWhy do we want tidy data?\nWhat are the challenges associated with shaping things into a tidy format?"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#tidy-data",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#tidy-data",
    "title": "Introduction to the tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\n\nWe say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\n\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nEach row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.\nTo see how the same information can be provided in different formats, consider the following example:\n\nlibrary(dslabs)\ndata(\"gapminder\") # gapminder will now be a data.frame in your \"environment\" (memory)\ntidy_data <- gapminder %>%\n  filter(country %in% c(\"South Korea\", \"Germany\") & !is.na(fertility)) %>%\n  select(country, year, fertility)\nhead(tidy_data, 6)\n\n      country year fertility\n1     Germany 1960      2.41\n2 South Korea 1960      6.16\n3     Germany 1961      2.44\n4 South Korea 1961      5.99\n5     Germany 1962      2.47\n6 South Korea 1962      5.79\n\n\nThis tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the dslabs package. Originally, the data was in the following format:\n\n\n      country 1960 1961 1962\n1     Germany 2.41 2.44 2.47\n2 South Korea 6.16 5.99 5.79\n\n\nThe same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables’ values, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into tidy format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.\nAlthough not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.\n\nTRY IT\n\nExamine the built-in dataset co2. Which of the following is true:\n\n\nco2 is tidy data: it has one year for each row.\nco2 is not tidy: we need at least one column with a character vector.\nco2 is not tidy: it is a matrix instead of a data frame.\nco2 is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.\n\n\nExamine the built-in dataset ChickWeight. Which of the following is true:\n\n\nChickWeight is not tidy: each chick has more than one row.\nChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.\nChickWeight is not tidy: we are missing the year column.\nChickWeight is tidy: it is stored in a data frame.\n\n\nExamine the built-in dataset BOD. Which of the following is true:\n\n\nBOD is not tidy: it only has six rows.\nBOD is not tidy: the first column is just an index.\nBOD is tidy: each row is an observation with two values (time and demand)\nBOD is tidy: all small datasets are tidy by definition.\n\n\nWhich of the following built-in datasets is tidy (you can pick more than one):\n\n\nBJsales\nEuStockMarkets\nDNase\nFormaldehyde\nOrange\nUCBAdmissions"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#manipulating-data-frames",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#manipulating-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Manipulating data frames",
    "text": "Manipulating data frames\nThe dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.\n\nAdding a column with mutate\nWe want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values. So, to add murder rates, we use:\n\nlibrary(dslabs)\ndata(\"murders\")\nmurders <- mutate(murders, rate = total / population * 100000)\n\nNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error?\nThis is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable.\nWe can see that the new column is added:\n\nhead(murders)\n\n       state abb region population total     rate\n1    Alabama  AL  South    4779736   135 2.824424\n2     Alaska  AK   West     710231    19 2.675186\n3    Arizona  AZ   West    6392017   232 3.629527\n4   Arkansas  AR  South    2915918    93 3.189390\n5 California  CA   West   37253956  1257 3.374138\n6   Colorado  CO   West    5029196    65 1.292453\n\n\nNote: Although we have overwritten the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will overwrite our mutated version.\n\n\nSubsetting with filter\nNow suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the filter function, which takes the data table as the first argument and then the conditional statement as the second. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace.\n\nfilter(murders, rate <= 0.71)\n\n          state abb        region population total      rate\n1        Hawaii  HI          West    1360301     7 0.5145920\n2          Iowa  IA North Central    3046355    21 0.6893484\n3 New Hampshire  NH     Northeast    1316470     5 0.3798036\n4  North Dakota  ND North Central     672591     4 0.5947151\n5       Vermont  VT     Northeast     625741     2 0.3196211\n\n\n\n\nSelecting columns with select\nAlthough our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object:\n\nnew_table <- select(murders, state, region, rate)\nfilter(new_table, rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nIn the call to select, the first argument murders is an object, but state, region, and rate are variable names.\n\nTRY IT\n\nLoad the dplyr package and the murders dataset.\n\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\n\nYou can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted:\n\nmurders <- mutate(murders, population_in_millions = population / 10^6)\n\nWe can write population rather than murders$population because mutate is part of dplyr. The function mutate knows we are grabbing columns from murders.\nUse the function mutate to add a murders column named rate with the per 100,000 murder rate as in the example code above. Make sure you redefine murders as done in the example code above ( murders <- [your code]) so we can keep using this variable.\n\nIf rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\nWith dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes:\n\n\nselect(murders, state, population) %>% head()\n\nUse select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\n\nThe dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this:\n\n\nfilter(murders, state == \"New York\")\n\nYou can use other logical vectors to filter rows.\nUse filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\n\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:\n\n\nno_florida <- filter(murders, state != \"Florida\")\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n\nWe can also use %in% to filter with dplyr. You can therefore see the data from New York and Texas like this:\n\n\nfilter(murders, state %in% c(\"New York\", \"Texas\"))\n\nCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\n\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\n\n\nfilter(murders, population < 5000000 & region == \"Northeast\")\n\nMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate, and the rank."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#the-pipe",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#the-pipe",
    "title": "Introduction to the tidyverse",
    "section": "The pipe: %>%",
    "text": "The pipe: %>%\nWith dplyr we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %>%. Some details are included below.\nWe wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects:\n\\[ \\mbox{original data }\n\\rightarrow \\mbox{ select }\n\\rightarrow \\mbox{ filter } \\]\nFor such an operation, we can use the pipe %>%. The code looks like this:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nThis line of code is equivalent to the two lines of code above. What is going on here?\nIn general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:\n\n16 %>% sqrt()\n\n[1] 4\n\n\nWe can continue to pipe values along:\n\n16 %>% sqrt() %>% log2()\n\n[1] 2\n\n\nThe above statement is equivalent to log2(sqrt(16)).\nRemember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:\n\n16 %>% sqrt() %>% log(base = 2)\n\n[1] 2\n\n\nTherefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\nmurders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function.\nNote that the pipe works well with functions where the first argument is the input data. Functions in tidyverse packages like dplyr have this format and can be used easily with the pipe. It’s worth noting that as of R 4.1, there is a base-R version of the pipe |>, though this has its disadvantages. We’ll stick with %>% for now.\n\nTRY IT\n\nThe pipe %>% can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.\n\n\nmurders <- mutate(murders, rate =  total / population * 100000,\n                  rank = rank(-rate))\n\nIn the solution to the previous exercise, we did the following:\n\nmy_states <- filter(murders, region %in% c(\"Northeast\", \"West\") &\n                      rate < 1)\n\nselect(my_states, state, rate, rank)\n\nThe pipe %>% permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this:\n\nmutate(murders, rate =  total / population * 100000,\n       rank = rank(-rate)) %>%\n  select(state, rate, rank)\n\nNotice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %>%.\nRepeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %>% to do this in just one line.\n\nReset murders to the original table by using data(murders). Use a pipe to create a new data frame called my_states that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three %>%. The code should look something like this:\n\n\nmy_states <- murders %>%\n  mutate SOMETHING %>%\n  filter SOMETHING %>%\n  select SOMETHING"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#summarizing-data",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#summarizing-data",
    "title": "Introduction to the tidyverse",
    "section": "Summarizing data",
    "text": "Summarizing data\nAn important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using the pull function.\n\n\n\n\nsummarize\nThe summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The heights dataset includes heights and sex reported by students in an in-class survey.\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(heights)\nhead(heights)\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65\n\n\nThe following code computes the average and standard deviation for females:\n\ns <- heights %>%\n  filter(sex == \"Female\") %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\ns\n\n   average standard_deviation\n1 64.93942           3.760656\n\n\nThis takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same.\nBecause the resulting table stored in s is a data frame, we can access the components with the accessor $:\n\ns$average\n\n[1] 64.93942\n\ns$standard_deviation\n\n[1] 3.760656\n\n\nAs with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(median = median(height), minimum = min(height),\n            maximum = max(height))\n\n    median minimum maximum\n1 64.98031      51      79\n\n\nWe can obtain these three values with just one line using the quantile function: for example, quantile(x, c(0,0.5,1)) returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector x. However, if we attempt to use a function like this that returns two or more values inside summarize:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nwe will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In later sections, we will learn how to deal with functions that return more than one value.\nFor another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column:\n\nmurders <- murders %>% mutate(rate = total/population*100000)\n\nRemember that the US murder rate is not the average of the state murder rates:\n\nsummarize(murders, mean(rate))\n\n  mean(rate)\n1   2.779125\n\n\nThis is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000)\nus_murder_rate\n\n      rate\n1 3.034555\n\n\nThis computation counts larger states proportionally to their size which results in a larger value.\n\n\npull\nThe us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame:\n\nclass(us_murder_rate)\n\n[1] \"data.frame\"\n\n\nsince, as most dplyr functions, summarize always returns a data frame.\nThis might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. To understand what we mean take a look at this line of code:\n\nus_murder_rate %>% pull(rate)\n\n[1] 3.034555\n\n\nThis returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate.\nTo get a number from the original data table with one line of code we can type:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000) %>%\n  pull(rate)\n\nus_murder_rate\n\n[1] 3.034555\n\n\nwhich is now a numeric:\n\nclass(us_murder_rate)\n\n[1] \"numeric\"\n\n\n\n\nGroup then summarize with group_by\nA common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this.\nIf we type this:\n\nheights %>% group_by(sex)\n\n# A tibble: 1,050 × 2\n# Groups:   sex [2]\n   sex    height\n   <fct>   <dbl>\n 1 Male       75\n 2 Male       70\n 3 Male       68\n 4 Male       74\n 5 Male       61\n 6 Female     65\n 7 Female     66\n 8 Female     62\n 9 Female     66\n10 Male       67\n# ℹ 1,040 more rows\n\n\nThe result does not look very different from heights, except we see Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:\n\nheights %>%\n  group_by(sex) %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\n\n# A tibble: 2 × 3\n  sex    average standard_deviation\n  <fct>    <dbl>              <dbl>\n1 Female    64.9               3.76\n2 Male      69.3               3.61\n\n\nThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median murder rate in the four regions of the country:\n\nmurders %>%\n  group_by(region) %>%\n  summarize(median_rate = median(rate))\n\n# A tibble: 4 × 2\n  region        median_rate\n  <fct>               <dbl>\n1 Northeast            1.80\n2 South                3.40\n3 North Central        1.97\n4 West                 1.29"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#sorting-data-frames",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#sorting-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Sorting data frames",
    "text": "Sorting data frames\nWhen examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size:\n\nmurders %>%\n  arrange(population) %>%\n  head()\n\n                 state abb        region population total       rate\n1              Wyoming  WY          West     563626     5  0.8871131\n2 District of Columbia  DC         South     601723    99 16.4527532\n3              Vermont  VT     Northeast     625741     2  0.3196211\n4         North Dakota  ND North Central     672591     4  0.5947151\n5               Alaska  AK          West     710231    19  2.6751860\n6         South Dakota  SD North Central     814180     8  0.9825837\n\n\nWith arrange we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by rate instead:\n\nmurders %>%\n  arrange(rate) %>%\n  head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102\n\n\nNote that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type:\n\nmurders %>%\n  arrange(desc(rate))\n\n\nNested sorting\nIf we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region, then within region we order by murder rate:\n\nmurders %>%\n  arrange(region, rate) %>%\n  head()\n\n          state abb    region population total      rate\n1       Vermont  VT Northeast     625741     2 0.3196211\n2 New Hampshire  NH Northeast    1316470     5 0.3798036\n3         Maine  ME Northeast    1328361    11 0.8280881\n4  Rhode Island  RI Northeast    1052567    16 1.5200933\n5 Massachusetts  MA Northeast    6547629   118 1.8021791\n6      New York  NY Northeast   19378102   517 2.6679599\n\n\n\n\nThe top \\(n\\)\nIn the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:\n\nmurders %>% top_n(5, rate)\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Maryland  MD         South    5773552   293  5.074866\n4             Missouri  MO North Central    5988927   321  5.359892\n5       South Carolina  SC         South    4625364   207  4.475323\n\n\nNote that rows are not sorted by rate, only filtered. If we want to sort, we need to use arrange. Note that if the third argument is left blank, top_n filters by the last column.\n\nTRY IT\nFor these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\n\nlibrary(NHANES)\n\nWarning: package 'NHANES' was built under R version 4.3.3\n\ndata(NHANES)\n\nThe NHANES data has many missing values. The mean and sd functions in R will return NA if any of the entries of the input vector is an NA. Here is an example:\n\nlibrary(dslabs)\ndata(na_example)\nmean(na_example)\n\n[1] NA\n\nsd(na_example)\n\n[1] NA\n\n\nTo ignore the NAs we can use the na.rm argument:\n\nmean(na_example, na.rm = TRUE)\n\n[1] 2.301754\n\nsd(na_example, na.rm = TRUE)\n\n[1] 1.22338\n\n\nLet’s now explore the NHANES data.\n\nWe will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like ” 20-29”, with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref.\n\nHint: Use filter and summarize and use the na.rm = TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter.\n\nUsing a pipe, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then pull.\nNow report the min and max values for the same group.\nCompute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age and gender, filter by Gender and then use group_by.\nRepeat exercise 4 for males.\nWe can actually combine both summaries for exercises 4 and 5 into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender).\nFor males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#tibbles",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#tibbles",
    "title": "Introduction to the tidyverse",
    "section": "Tibbles",
    "text": "Tibbles\nTidy data must be stored in data frames. We have been using the murders data frame throughout the unit. In an earlier section we introduced the group_by function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?\n\nmurders %>% group_by(region)\n\n# A tibble: 51 × 6\n# Groups:   region [4]\n   state                abb   region    population total  rate\n   <chr>                <chr> <fct>          <dbl> <dbl> <dbl>\n 1 Alabama              AL    South        4779736   135  2.82\n 2 Alaska               AK    West          710231    19  2.68\n 3 Arizona              AZ    West         6392017   232  3.63\n 4 Arkansas             AR    South        2915918    93  3.19\n 5 California           CA    West        37253956  1257  3.37\n 6 Colorado             CO    West         5029196    65  1.29\n 7 Connecticut          CT    Northeast    3574097    97  2.71\n 8 Delaware             DE    South         897934    38  4.23\n 9 District of Columbia DC    South         601723    99 16.5 \n10 Florida              FL    South       19687653   669  3.40\n# ℹ 41 more rows\n\n\nNotice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble followed by dimensions. We can learn the class of the returned object using:\n\nmurders %>% group_by(region) %>% class()\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe tbl, pronounced tibble, is a special kind of data frame. The functions group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. For consistency, the dplyr manipulation verbs (select, filter, mutate, and arrange) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble.\nTibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.\n\nTibbles display better\nThe print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing murders and the output of murders if we convert it to a tibble. We can do this using as_tibble(murders). If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.\n\n\nSubsets of tibbles are tibbles\nIf you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:\n\nclass(murders[,4])\n\n[1] \"numeric\"\n\n\nis not a data frame. With tibbles this does not happen:\n\nclass(as_tibble(murders)[,4])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThis is useful in the tidyverse since functions require data frames as input.\nWith tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $:\n\nclass(as_tibble(murders)$population)\n\n[1] \"numeric\"\n\n\nA related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write Population instead of population this:\n\nmurders$Population\n\nNULL\n\n\nreturns a NULL with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:\n\nas_tibble(murders)$Population\n\nWarning: Unknown or uninitialised column: `Population`.\n\n\nNULL\n\n\n\n\nTibbles can have complex entries\nWhile data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:\n\ntibble(id = c(1, 2, 3), func = c(mean, median, sd))\n\n# A tibble: 3 × 2\n     id func  \n  <dbl> <list>\n1     1 <fn>  \n2     2 <fn>  \n3     3 <fn>  \n\n\n\n\nTibbles can be grouped\nThe function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information.\n\n\nCreate a tibble using tibble instead of data.frame\nIt is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the tibble function.\n\ngrades <- tibble(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\n\nNote that base R (without packages loaded) has a function with a very similar name, data.frame, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default data.frame coerces characters into factors without providing a warning or message:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo avoid this, we use the rather cumbersome argument stringsAsFactors:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90),\n                     stringsAsFactors = FALSE)\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo convert a regular data frame to a tibble, you can use the as_tibble function.\n\nas_tibble(grades) %>% class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\""
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#the-dot-operator",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#the-dot-operator",
    "title": "Introduction to the tidyverse",
    "section": "The dot operator",
    "text": "The dot operator\nOne of the advantages of using the pipe %>% is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:\n\ntab_1 <- filter(murders, region == \"South\")\ntab_2 <- mutate(tab_1, rate = total / population * 10^5)\nrates <- tab_2$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nWe can avoid defining any new intermediate objects by instead typing:\n\nfilter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  summarize(median = median(rate)) %>%\n  pull(median)\n\n[1] 3.398069\n\n\nWe can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the pull function was not available and we wanted to access tab_2$rate? What data frame name would we use? The answer is the dot operator.\nFor example to access the rate vector without the pull function we could use\n\nrates <-   filter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  .$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nIn the next section, we will see other instances in which using the . is useful."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#do",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#do",
    "title": "Introduction to the tidyverse",
    "section": "do",
    "text": "do\nThe tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %>%, tidyverse functions consistently take data frames and return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The quantile function is an example we described earlier. The do function serves as a bridge between R functions such as quantile and the tidyverse. The do function understands grouped tibbles and always returns a data frame.\nIn the summarize section (above), we noted that if we attempt to use quantile to obtain the min, median and max in one call, we will receive something unexpected. Prior to R 4.1, we would receive an error. After R 4.1, we actually get:\n\ndata(heights)\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nWe probably wanted three columns: min, median, and max. We can use the do function to fix this.\nFirst we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame. Note that it returns a single-row data frame.\n\nmy_summary <- function(dat){\n  x <- quantile(dat$height, c(0, 0.5, 1))\n  tibble(min = x[1], median = x[2], max = x[3])\n}\n\nWe can now apply the function to the heights dataset to obtain the summaries:\n\nheights %>%\n  group_by(sex) %>%\n  my_summary\n\n# A tibble: 1 × 3\n    min median   max\n  <dbl>  <dbl> <dbl>\n1    50   68.5  82.7\n\n\nBut this is not what we want. We want a summary for each sex and the code returned just one summary. This is because my_summary is not part of the tidyverse and does not know how to handled grouped tibbles. do makes this connection:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary(.))\n\n# A tibble: 2 × 4\n# Groups:   sex [2]\n  sex      min median   max\n  <fct>  <dbl>  <dbl> <dbl>\n1 Female    51   65.0  79  \n2 Male      50   69    82.7\n\n\nNote that here we need to use the dot operator. The tibble created by group_by is piped to do. Within the call to do, the name of this tibble is . and we want to send it to my_summary. If you do not use the dot, then my_summary has no argument and returns an error telling us that argument \"dat\" is missing. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary())\n\nIf you do not use the parenthesis, then the function is not executed and instead do tries to return the function. This gives an error because do must always return a data frame. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary)\n\nSo do serves as a bridge between non-tidyverse functions and the tidyverse."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#the-purrr-package",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#the-purrr-package",
    "title": "Introduction to the tidyverse",
    "section": "The purrr package",
    "text": "The purrr package\nIn previous sections (and labs) we learned about the sapply function, which permitted us to apply the same function to each element of a vector. We constructed a function and used sapply to compute the sum of the first n integers for several values of n like this:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  sum(x)\n}\nn <- 1:25\ns_n <- sapply(n, compute_s_n)\ns_n\n\n [1]   1   3   6  10  15  21  28  36  45  55  66  78  91 105 120 136 153 171 190\n[20] 210 231 253 276 300 325\n\n\nThis type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, sapply can return several different object types; for example, we might expect a numeric result from a line of code, but sapply might convert our result to character under some circumstances. purrr functions will never do this: they will return objects of a specified type or return an error if this is not possible.\nThe first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list:\n\nlibrary(purrr) # or library(tidyverse)\nn <- 1:25\ns_n <- map(n, compute_s_n)\nclass(s_n)\n\n[1] \"list\"\n\n\nIf we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values.\n\ns_n <- map_dbl(n, compute_s_n)\nclass(s_n)\n\n[1] \"numeric\"\n\n\nThis produces the same results as the sapply call shown above.\nA particularly useful purrr function for interacting with the rest of the tidyverse is map_df, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a Argument 1 must have names error:\n\ns_n <- map_df(n, compute_s_n)\n\nWe need to change the function to make this work:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x))\n}\ns_n <- map_df(n, compute_s_n)\nhead(s_n)\n\n# A tibble: 6 × 1\n    sum\n  <int>\n1     1\n2     3\n3     6\n4    10\n5    15\n6    21\n\n\nBecause map_df returns a tibble, we can have more columns defined in our function and returned.\n\ncompute_s_n2 <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x), sumSquared = sum(x^2))\n}\ns_n <- map_df(n, compute_s_n2)\nhead(s_n)\n\n# A tibble: 6 × 2\n    sum sumSquared\n  <int>      <dbl>\n1     1          1\n2     3          5\n3     6         14\n4    10         30\n5    15         55\n6    21         91\n\n\nThe purrr package provides much more functionality not covered here. For more details you can consult this online resource."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#tidyverse-conditionals",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#tidyverse-conditionals",
    "title": "Introduction to the tidyverse",
    "section": "Tidyverse conditionals",
    "text": "Tidyverse conditionals\nA typical data analysis will often involve one or more conditional operations. In the section on Conditionals, we described the ifelse function, which we will use extensively in this book. In this section we present two dplyr functions that provide further functionality for performing conditional operations.\n\ncase_when\nThe case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0:\n\nx <- c(-2, -1, 0, 1, 2)\ncase_when(x < 0 ~ \"Negative\",\n          x > 0 ~ \"Positive\",\n          x == 0  ~ \"Zero\")\n\n[1] \"Negative\" \"Negative\" \"Zero\"     \"Positive\" \"Positive\"\n\n\nA common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this:\n\nmurders %>%\n  mutate(group = case_when(\n    abb %in% c(\"ME\", \"NH\", \"VT\", \"MA\", \"RI\", \"CT\") ~ \"New England\",\n    abb %in% c(\"WA\", \"OR\", \"CA\") ~ \"West Coast\",\n    region == \"South\" ~ \"South\",\n    TRUE ~ \"Other\")) %>%\n  group_by(group) %>%\n  summarize(rate = sum(total) / sum(population) * 10^5)\n\n# A tibble: 4 × 2\n  group        rate\n  <chr>       <dbl>\n1 New England  1.72\n2 Other        2.71\n3 South        3.63\n4 West Coast   2.90\n\n\nThat TRUE on the fourth line of case_when serves as a catch-all. As case_when steps through the conditions, if none of them are true, it comes to the last line. Since TRUE is always true, the function will return “Other”. Leaving out the last line of case_when would result in NA values for any observation that fails the first three conditionals. This may or may not be what you want.\n\n\nbetween\nA common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type\n\nx >= a & x <= b\n\nHowever, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation.\n\nbetween(x, a, b)\n\n\nTRY IT\n\nLoad the murders dataset. Which of the following is true?\n\n\nmurders is in tidy format and is stored in a tibble.\nmurders is in tidy format and is stored in a data frame.\nmurders is not in tidy format and is stored in a tibble.\nmurders is not in tidy format and is stored in a data frame.\n\n\nUse as_tibble to convert the murders data table into a tibble and save it in an object called murders_tibble.\nUse the group_by function to convert murders into a tibble that is grouped by region.\nWrite tidyverse code that is equivalent to this code:\n\n\nexp(mean(log(murders$population)))\n\nWrite it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with murders %>%.\n\nUse the map_df to create a data frame with three columns named n, s_n, and s_n_2. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through \\(n\\) with \\(n\\) the row number."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html",
    "title": "Introduction to Visualization",
    "section": "",
    "text": "Looking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table:\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nWhat do you learn from staring at this table? Even though it is a relatively straightforward table, we can’t learn anything. For starters, it is grossly abbreviated, though you could scroll through. In doing so, how quickly might you be able to determine which states have the largest populations? Which states have the smallest? How populous is a typical state? Is there a relationship between population size and total murders? How do murder rates vary across regions of the country? For most folks, it is quite difficult to extract this information just by looking at the numbers. In contrast, the answer to the questions above are readily available from examining this plot:\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(ggrepel)\n\nr <- murders %>%\n  summarize(pop=sum(population), tot=sum(total)) %>%\n  mutate(rate = tot/pop*10^6) %>% pull(rate)\n\nmurders %>% ggplot(aes(x = population/10^6, y = total, label = abb)) +\n  geom_abline(intercept = log10(r), lty=2, col=\"darkgrey\") +\n  geom_point(aes(color=region), size = 3) +\n  geom_text_repel() +\n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") +\n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\") +\n  scale_color_discrete(name=\"Region\") +\n  theme_economist_white()\n\n\n\n\nWe are reminded of the saying: “A picture is worth a thousand words”. Data visualization provides a powerful way to communicate a data-driven finding. In some cases, the visualization is so convincing that no follow-up analysis is required. You should consider visualization the most potent tool in your data analytics arsenal.\nThe growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many industries, academia, and government. A salient example is news organizations, which are increasingly embracing data journalism and including effective infographics as part of their reporting.\nA particularly salient example—given the current state of the world—is a Wall Street Journal article1 showing data related to the impact of vaccines on battling infectious diseases. One of the graphs shows measles cases by US state through the years with a vertical line demonstrating when the vaccine was introduced.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n(Source: Wall Street Journal)\nAnother striking example comes from a New York Times chart2, which summarizes scores from the NYC Regents Exams. As described in the article3, these scores are collected for several reasons, including to determine if a student graduates from high school. In New York City you need a 65 to pass. The distribution of the test scores forces us to notice something somewhat problematic:\n\n\n\n\n\n(Source: New York Times via Amanda Cox)\nThe most common test score is the minimum passing grade, with very few scores just below the threshold. This unexpected result is consistent with students close to passing having their scores bumped up.\nThis is an example of how data visualization can lead to discoveries which would otherwise be missed if we simply subjected the data to a battery of data analysis tools or procedures. Data visualization is the strongest tool of what we call exploratory data analysis (EDA). John W. Tukey4, considered the father of EDA, once said,\n\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.”\n\n\nMany widely used data analysis tools were initiated by discoveries made via EDA. EDA is perhaps the most important part of data analysis, yet it is one that is often overlooked.\nData visualization is also now pervasive in philanthropic and educational organizations. In the talks New Insights on Poverty5 and The Best Stats You’ve Ever Seen6, Hans Rosling forces us to notice the unexpected with a series of plots related to world health and economics. In his videos, he uses animated graphs to show us how the world is changing and how old narratives are no longer true.\n\n\n\n\n\nIt is also important to note that mistakes, biases, systematic errors and other unexpected problems often lead to data that should be handled with care. Failure to discover these problems can give rise to flawed analyses and false discoveries. As an example, consider that measurement devices sometimes fail and that most data analysis procedures are not designed to detect these. Yet these data analysis procedures will still give you an answer. The fact that it can be difficult or impossible to notice an error just from the reported results makes data visualization particularly important.\nToday, we will discuss the basics of data visualization and exploratory data analysis. We will use the ggplot2 package to code. To learn the very basics, we will start with a somewhat artificial example: heights reported by students. Then we will cover the two examples mentioned above: 1) world health and economics and 2) infectious disease trends in the United States.\nOf course, there is much more to data visualization than what we cover here. The following are references for those who wish to learn more:\n\nER Tufte (1983) The visual display of quantitative information. Graphics Press.\nER Tufte (1990) Envisioning information. Graphics Press.\nER Tufte (1997) Visual explanations. Graphics Press.\nWS Cleveland (1993) Visualizing data. Hobart Press.\nWS Cleveland (1994) The elements of graphing data. CRC Press.\nA Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach: Turning tables into graphs. The American Statistician 56:121-130.\nNB Robbins (2004) Creating more effective graphs. Wiley.\nA Cairo (2013) The functional art: An introduction to information graphics and visualization. New Riders.\nN Yau (2013) Data points: Visualization that means something. Wiley.\n\nWe also do not cover interactive graphics, a topic that is both too advanced for this course and too unweildy. Some useful resources for those interested in learning more can be found below, and you are encouraged to draw inspiration from those websites in your projects:\n\nhttps://shiny.rstudio.com/\nhttps://d3js.org/"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#the-components-of-a-graph",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#the-components-of-a-graph",
    "title": "Introduction to Visualization",
    "section": "The components of a graph",
    "text": "The components of a graph\nWe will eventually construct a graph that summarizes the US murders dataset that looks like this:\n\n\n\n\n\nWe can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.\nThis data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.\nThe first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main five components to note are:\n\nData: The US murders data table is being summarized. We refer to this as the data component.\nGeometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, boxplot, pie (ew!), and many, many more. We will learn about these later.\nAesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using.\nAnnotations: These are things like axis labels, axis ticks (the lines along the axis at regular intervals or specific points of interest), axis scales (e.g. log-scale), titles, legends, etc.\nStyle: An overall appearance of the graph determined by fonts, color palattes, layout, blank spaces, and more.\n\nWe also note that:\n\nThe points are labeled with the state abbreviations.\nThe range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.\nThere are labels, a title, a legend, and we use the style of The Economist magazine.\n\nAll of the flexibility and visualization power of ggplot is contained in these four elements (plus your data)"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#ggplot-objects",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#ggplot-objects",
    "title": "Introduction to Visualization",
    "section": "ggplot objects",
    "text": "ggplot objects\nWe will now construct the plot piece by piece.\nWe start by loading the dataset:\n\nlibrary(dslabs)\ndata(murders)\n\n\n\n\nThe first step in creating a ggplot2 graph is to define a ggplot object. We do this with the function ggplot, which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object:\n\nggplot(data = murders)\n\nWe can also pipe the data in as the first argument. So this line of code is equivalent to the one above:\n\nmurders %>% ggplot()\n\n\n\n\nIt renders a plot, in this case a blank slate since no geometry has been defined. The only style choice we see is a grey background.\nWhat has happened above is that the object was created and, because it was not assigned, it was automatically evaluated. But we can assign our plot to an object, for example like this:\n\np <- ggplot(data = murders)\nclass(p)\n\n[1] \"gg\"     \"ggplot\"\n\n\nTo render the plot associated with this object, we simply print the object p. The following two lines of code each produce the same plot we see above:\n\nprint(p)\np"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#geometries-briefly",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#geometries-briefly",
    "title": "Introduction to Visualization",
    "section": "Geometries (briefly)",
    "text": "Geometries (briefly)\nIn ggplot2 we create graphs by adding geometry layers. Layers can define geometries, compute summary statistics, define what scales to use, create annotations, or even change styles. To add layers, we use the symbol +. In general, a line of code will look like this:\n\nDATA %>% ggplot() + LAYER 1 + LAYER 2 + ... + LAYER N\n\nUsually, the first added layer after ggplot() + defines the geometry. After that, we may add additional geometries, we may rescale an axis, we may add annotations and labels, or we may change the style. For now, we want to make a scatterplot like the one you all created in Lab 0. What geometry do we use?\n\n\n\n\n\nTaking a quick look at the cheat sheet, we see that the ggplot2 function used to create plots with this geometry is geom_point.\nSee Here\n(Image courtesy of RStudio9. CC-BY-4.0 license10.)\n\nGeometry function names follow the pattern: geom_X where X is the name of some specific geometry. Some examples include geom_point, geom_bar, and geom_histogram. You’ve already seen a few of these. We will start with a scatterplot created using geom_point() for now, then circle back to more geometries after we cover aesthetic mappings, layers, and annotations.\nFor geom_point to run properly we need to provide data and an aesthetic mapping. The simplest mapping for a scatter plot is to say we want one variable on the X-axis, and a different one on the Y-axis, so each point is an {X,Y} pair. That is an aesthetic mapping because X and Y are aesthetics in a geom_point scatterplot.\nWe have already connected the object p with the murders data table, and if we add the layer geom_point it defaults to using this data. To find out what mappings are expected, we read the Aesthetics section of the help file ?geom_point help file:\n> Aesthetics\n>\n> geom_point understands the following aesthetics (required aesthetics are in bold):\n>\n> **x**\n>\n> **y**\n>\n> alpha\n>\n> colour\n>\n> fill\n>\n> group\n>\n> shape\n>\n> size\n>\n> stroke\nand—although it does not show in bold above—we see that at least two arguments are required: x and y. You can’t have a geom_point scatterplot unless you state what you want on the X and Y axes."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#aesthetic-mappings",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#aesthetic-mappings",
    "title": "Introduction to Visualization",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\nAesthetic mappings describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The aes function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the aes function is often used as the argument of a geometry function. This example produces a scatterplot of population in millions (x-axis) versus total murders (y-axis):\n\nmurders %>% ggplot() +\n  geom_point(aes(x = population/10^6, y = total))\n\nInstead of defining our plot from scratch, we can also add a layer to the p object that was defined above as p <- ggplot(data = murders):\n\np + geom_point(aes(x = population/10^6, y = total))\n\n\n\n\nThe scales and annotations like axis labels are defined by default when adding this layer (note the x-axis label is exactly what we wrote in the function call). Like dplyr functions, aes also uses the variable names from the object component: we can use population and total without having to call them as murders$population and murders$total. The behavior of recognizing the variables from the data component is quite specific to aes. With most functions, if you try to access the values of population or total outside of aes you receive an error.\nNote that we did some rescaling within the aes() call - we can do simple things like multiplication or division on the variable names in the ggplot call. The axis labels reflect this. We will change the axis labels later.\nThe aesthetic mappings are very powerful - changing the variable in x= or y= changes the meaning of the plot entirely. We’ll come back to additional aesthetic mappings once we talk about aesthetics in general.\n\nAesthetics in general\nEven without mappings, a plots aesthetics can be useful. Things like color, fill, alpha, and size are aesthetics that can be changed.\nLet’s say we want larger points in our scatterplot. The size aesthetic can be used to set the size. The scale of size is “multiples of the defaults” (so size = 1 is the default)\n\np + geom_point(aes(x = population/10^6, y = total), size = 3)\n\n\n\n\nsize is not a mapping so it is not in the aes() part: whereas mappings use data from specific observations and need to be inside aes(), operations we want to affect all the points the same way do not need to be included inside aes. We’ll see what happens if size is inside aes(size = xxx) in a second.\nWe can change the shape to one of the many different base-R options found here:\n\np + geom_point(aes(x = population/10^6, y = total), size = 3, shape = 17)\n\n\n\n\nWe can also change the fill and the color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 4, shape = 23, fill = '#18453B')\n\n\n\n\nfill can take a common name like 'green', or can take a hex color like '#18453B', which is MSU Green according to MSU’s branding site. You can also find UM Maize and OSU Scarlet on respective branding pages, or google “XXX color hex.” We’ll learn how to build a color palatte later on.\ncolor (or colour, same thing because ggplot creators allow both spellings) is a little tricky with points - it changes the outline of the geometry rather than the fill color, but in geom_point() most shapes are only the outline, including the default. This is more useful with, say, a barplot where the outline and the fill might be different colors. Still, shapes 21-25 have both fill and color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 5, shape = 23, fill = '#18453B', color = 'white')\n\n\n\n\nThe color = 'white' makes the outline of the shape white, which you can see if you look closely in the areas where the shapes overlap. This only works with shapes 21-25, or any other geometry that has both an outline and a fill.\n\n\nNow, back to aesthetic mappings\nNow that we’ve seen a few aesthetics (and know we can find more by looking at which aesthetics work with our geometry in the help file), let’s return to the power of aesthetic mappings.\nAn aesthetic mapping means we can vary an aesthetic (like fill or shape or size) according to some variable in our data. This opens up a world of possibilities! Let’s try adding to our x and y aesthetics with a color aesthetic (since points respond to color better than fill) that varies by region, which is a column in our data:\n\np + geom_point(aes(x = population/10^6, y = total, color = region), size = 3)\n\n\n\n\nWe include color=region inside the aes call, which tells R to find a variable called region and change color based on that. R will choose a somewhat ghastly color palatte, and every unique value in the data for region will get a different color if the variable is discrete. If the variable is a continuous value, then ggplot will automatically make a color ramp. Thus, discrete and continuous values for aesthetic mappings work differently.\nLet’s see a useful example of a continuous aesthetic mapping to color. In our data, we are making a scatterplot of population and total murders, which really just shows that states with higher populations have higher murders. What we really want is murders per capita (I think COVID taught us a lot about rates vs. levels like “cases” and “cases per 100,000 people”). We can create a variable of “murders per capita” on the fly. Since “murders per capita” is a very small number and hard to read, we’ll multiply by 100 so that we get “percent of population murdered per year”:\n\np + geom_point(aes(x = population/10^5, y = total, color = 100*total/population), size = 3)\n\n\n\n\nWhile the clear pattern of “more population means more murders” is still there, look at the outlier in light blue in the bottom left. With the color ramp, see how easy it is to see here that there is one location where murders per capita is quite high?\nNote that size is outside of aes and is set to an explicit value, not to a variable. What if we set size to a variable in the data?\n\np + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6))\n\n\n\n\n\n\nLegends for aesthetics\nHere we see yet another useful default behavior: ggplot2 automatically adds a legend that maps color to region, and size to population (which we scaled by 1,000,000). To avoid adding this legend we set the geom_point argument show.legend = FALSE. This removes both the size and the color legend.\n\np + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6), show.legend = FALSE)\n\n\n\n\nLater on, when we get to annotation layers, we’ll talk about controlling the legend text and layout. For now, we just need to know how to turn them off."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#annotation-layers",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#annotation-layers",
    "title": "Introduction to Visualization",
    "section": "Annotation Layers",
    "text": "Annotation Layers\nA second layer in the plot we wish to make involves adding a label to each point to identify the state. The geom_label and geom_text functions permit us to add text to the plot with and without a rectangle behind the text, respectively.\nBecause each point (each state in this case) has a label, we need an aesthetic mapping to make the connection between points and labels. By reading the help file ?geom_text, we learn that we supply the mapping between point and label through the label argument of aes. That is, label is an aesthetic that we can map. So the code looks like this:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\n\n\n\nWe have successfully added a second layer to the plot.\nAs an example of the unique behavior of aes mentioned above, note that this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total, label = abb))\n\nis fine, whereas this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total), label = abb)\n\nwill give you an error since abb is not found because it is outside of the aes function. The layer geom_text does not know where to find abb since it is a column name and not a global variable, and ggplot does not look for column names for non-mapped aesthetics. For a trivial example:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(population/10^6, total), label = 'abb')\n\n\n\n\n\nGlobal versus local aesthetic mappings\nIn the previous line of code, we define the mapping aes(population/10^6, total) twice, once in each geometry. We can avoid this by using a global aesthetic mapping. We can do this when we define the blank slate ggplot object. Remember that the function ggplot contains an argument that permits us to define aesthetic mappings:\n\nargs(ggplot)\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \nNULL\n\n\nIf we define a mapping in ggplot, all the geometries that are added as layers will default to this mapping. We redefine p:\n\np <- murders %>% ggplot(aes(x = population/10^6, y = total, label = abb))\n\nand then we can simply write the following code to produce the previous plot:\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 1.5) # offsets the label\n\nWe keep the size and nudge_x arguments in geom_point and geom_text, respectively, because we want to only increase the size of points and only nudge the labels. If we put those arguments in aes then they would apply to both plots. Also note that the geom_point function does not need a label argument and therefore ignores that aesthetic.\nIf necessary, we can override the global mapping by defining a new mapping within each layer. These local definitions override the global. Here is an example:\n\np + geom_point(size = 3) +\n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))\n\n\n\n\nClearly, the second call to geom_text does not use x = population and y = total."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#try-it",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#try-it",
    "title": "Introduction to Visualization",
    "section": "Try it!",
    "text": "Try it!\n\nLet’s break in to smaller groups and try playing with some of the aesthetics and aesthetic mappings. If we’re in person (woohoo!), we’ll form the same number of groups in class.\nIn each group, one person should be the main coder - someone who has the packages like dslabs installed and has successfully run the plots above. Each set of tasks ask you to learn about an aesthetic and put it into action with the murder data. We’ll leave about 5 minutes to do the task, then have you come back and share your results with the class.\nFor each group, we’ll start with the following code:\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\nThe alpha aesthetic mapping.\n\nThe alpha aesthetic can only take a number between 0 and 1. So first, in murders, create a murders_per_capita column by dividing total by population. Second, find the max(murders$murders_per_capita) and then create another new column called murders_per_capita_rescaled which divides murders_per_capita by the max value. murders_per_capita_rescaled will be between 0 and 1, with the value of 1 for the state with the max murder rate. This is a little hard to do on the fly in ggplot.\nSet the alpha aesthetic mapping to murders_per_capita_rescaled for geom_point.\nTurn off the legend using show.legend=FALSE\nInclude the geom_text labels, but make sure the aesthetic mapping does not apply to the labels.\nUse nudge_x = 1.5 as before to offset the labels.\nBe able to explain the plot.\n\nDoes the alpha aesthetic help present the data here? It’s OK if it doesn’t!\n\n\nThe stroke aesthetic mapping.\n\nThe stroke aesthetic works a bit like the size aesthetic. It must be used with a plot that has both a border and a fill, like shapes 21-25, so use one of those.\nUse the stroke aesthetic mapping (meaning the stroke will change according to a value in the data) to set a different stroke size based on murders per capita. You can create a murders per capita variable on the fly, or add it to your murders data.\n\nInclude the text labels as before and use nudge_x = 1.5.\nMake sure you’re only setting the aesthetic for the points on the scatterplot!\n\n\nThe angle aesthetic\n\nUsing the ?geom_text help, note that geom_text takes an aesthetic of angle.\nUse the angle aesthetic (not aesthetic mapping) in the appropriate place (e.g. on geom_text and not on other geometries) to adjust the labels on our plot.\nNow, try using the angle aesthetic mapping by using the total field as both the y value and the angle value in the geom_text layer.\nDoes using angle as an aesthetic help? What about as an aesthetic mapping?\n\nThe color aesthetic mapping\n\nSet the color aesthetic mapping in geom_text to total/population.\n\nUse the nudge_x = 1.5 aesthetic in geom_text still\n\nTry it with and without the legend using show.legend.\nBe able to explain the plot.\n\nDoes the color aesthetic mapping help present the data here?\n\n\ngeom_label and the fill aesthetic\n\nLooking at ?geom_label (which is the same help as geom_text), we note that “The fill aesthetic controls the backgreound colour of the label”.\nSet the fill aesthetic mapping to total/population in geom_label (replacing geom_text but still using nudge_x=1.5)\nSet the fill aesthetic (not mapping) to the color of your choice.\nBe able to explain the plots.\n\n\nDoes the fill aesthetic mapping help present the data here?\nWhat color did you choose for the non-mapped fill aesthetic?"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Course Content",
    "section": "",
    "text": "Each week has two sets of required readings (pages in the sidebar) that you should complete before coming to lecture. Read the first before our first meeting of the week, and read the second before the second meetings. That is, you should complete the reading, attend Tuesday class, then do the associated “exercises” (contained within the reading) before Thursday. You will be working each week’s lab between Thursday afternoon and Monday at 11:59 PM (when the labs are due). Don’t forget your weekly writing in between, due Saturday at 11:59pm.\nThe course content is structured as follows. For each topic, we begin with a set of questions that might guide your reading and help frame your thoughts. These questions can serve as helpful starting places for your thinking; they are not representative of the totality of the content and are not intended to be limiting. You should not try to respond to all of these (or any of them if you don’t want to)—they’ll just help you know what to look for and think about as you read."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EC242 - Social Science Data Analytics",
    "section": "",
    "text": "Date: Date and time\nRoom: Location\nInstructors: Instructor A, B, C\n\n\n\n\n\n\n\nThis workshop template\n\n\n\nThis workshop template contains 4 pages:\n\nHome: index.qmd (this page)\nAbout: about.qmd\n\nTwo content pages\n\nPage without code: part_1_prep.qmd\nPage with R code: part_2_eda.qmd\nAnd another: content/Week 00/00a.qmd\n\nIt is straightforward to add more content pages: you need to create a new .qmd file (or copy/paste the existing ones), then link the new page inside _quarto.yml.\n\n\n\n\n\n\n\n\nHomepage of your workshop\n\n\n\nThis is the homepage index.qmd for your workshop, so ideally it should contain some key information such as time and place, instructors and information of the course/workshop.\nIt should be easy to navigate.\n\n\n\nWelcome!\n\nThe goal of the workshop is to … (insert your message)\nFor example, introduce kep concepts in machine learning, such as regularisation.\nWorkshop material can be found in the workshop github repository.\n\n\nLearning Objectives\nAt the end of the tutorial, participants will be able to\n\nunderstand key concepts for … (insert your message)\nFor example, training machine learning models such as regularisation.\nanother objective\n\n\n\nPre-requisites\n\nBasic familiarity with R\nSome other knowledge\n\n\n\n\nSchedule\n\n\n\n\n\n\nTabular schedule\n\n\n\nIt can be useful to include a tabular schedule with links.\n\n\n\n\n\nTime\nTopic\nPresenter\n\n\n\n\n9:00 - 10:30\nSession 1: Preparation\nInstructor A\n\n\n10:45 - 12:00\nSession 2: Exploratory Analysis\nInstructor B, C"
  },
  {
    "objectID": "part_1_prep.html",
    "href": "part_1_prep.html",
    "title": "Title Preparation",
    "section": "",
    "text": "Page without code\n\n\n\nThis page contains an example for some structured preparation information for a workshop. No code is executed here.\nHere are some preparation information for the participants."
  },
  {
    "objectID": "part_1_prep.html#software",
    "href": "part_1_prep.html#software",
    "title": "Title Preparation",
    "section": "Software",
    "text": "Software\nIn this workshop we will be using R. You can either\n\nhave R and Rstudio installed on your laptop\nor, use Posit cloud (formerly Rstudio Cloud).\n\nPosit cloud is free of charge for personal users, yet you need to sign up for a new user account and have internet connection.\nThe R package we are using is glmnet."
  },
  {
    "objectID": "part_1_prep.html#data",
    "href": "part_1_prep.html#data",
    "title": "Title Preparation",
    "section": "Data",
    "text": "Data\nThe datasets we use can be found here (insert link)."
  },
  {
    "objectID": "part_1_prep.html#code",
    "href": "part_1_prep.html#code",
    "title": "Title Preparation",
    "section": "Code",
    "text": "Code\nThe R scripts used in part 1 and part 2 can be found here (insert link)."
  },
  {
    "objectID": "part_1_prep.html#resources",
    "href": "part_1_prep.html#resources",
    "title": "Title Preparation",
    "section": "Resources",
    "text": "Resources\nLecture notes (insert link)\nLab notes (insert link)"
  },
  {
    "objectID": "part_2_eda.html",
    "href": "part_2_eda.html",
    "title": "Part I",
    "section": "",
    "text": "Page with R code\n\n\n\nThis page contains an example template for a lab session, where R code and results are displayed here.\nYou can find more information on how to include code in Quarto website here.\nYou can experiment with code-fold and code-tools in the yaml header above to change how the code cells look like."
  },
  {
    "objectID": "part_2_eda.html#a-cancer-modeling-example",
    "href": "part_2_eda.html#a-cancer-modeling-example",
    "title": "Part I",
    "section": "A Cancer Modeling Example",
    "text": "A Cancer Modeling Example\nExercise on analysis of miRNA, mRNA and protein data from the paper Aure et al, Integrated analysis reveals microRNA networks coordinately expressed with key proteins in breast cancer, Genome Medicine, 2015.\nPlease run the code provided to replicate some of the analyses. Make sure you can explain what all the analysis steps do and that you understand all the results.\nIn addition, there are some extra tasks (Task 1), where no R code is provided. Please do these tasks when you have time available at the end of the lab.\n\nLoad the data\nRead the data, and convert to matrix format.\n\nmrna <- read.table(\"data/data_example.txt\", header=T, sep=\"\\t\", dec=\".\")\n\n# Convert to matrix format\n\nmrna <- as.matrix(mrna)\n\nPrint the data\n\nmrna[1:4, 1:4]\n\n      OSL2R.3002T4 OSL2R.3005T1 OSL2R.3013T1 OSL2R.3030T2\nACACA      1.60034     -0.49087     -0.26553     -0.27857\nANXA1     -2.42501     -0.05416     -0.46478     -2.18393\nAR         0.39615     -0.43348     -0.10232      0.58299\nBAK1       0.78627      0.39897      0.22598     -1.31202\n\n\nVisualise the overall distribution of expression levels by histogram\n\nhist(mrna, nclass=40, xlim=c(-5,5), col=\"lightblue\")\n\n\n\n\n\n\n\n\n\n\nTask 1\n\n\n\nThis is a callout-note, and it can be quite useful for exercises. You can find more about callout here.\nExample: Extend the above analysis to cover all genes."
  },
  {
    "objectID": "content/index.html#content-navigation",
    "href": "content/index.html#content-navigation",
    "title": "Course Content",
    "section": "Content navigation",
    "text": "Content navigation\nUse the links on the sidebar to navigate between required reading. Readings are ordered by week."
  },
  {
    "objectID": "content/Week_10/10a.html",
    "href": "content/Week_10/10a.html",
    "title": "The Bias-Variance Tradeoff",
    "section": "",
    "text": "This page.\n Chapter 2 in Introduction to Statistical Learning with Applications in R.\n\n\n\n\nWhat is the relationship between bias, variance, and mean squared error?\nWhat is the relationship between model flexibility and training error?\nWhat is the relationship between model flexibility and validation (or test) error?"
  },
  {
    "objectID": "content/Week_10/10a.html#r-setup-and-source",
    "href": "content/Week_10/10a.html#r-setup-and-source",
    "title": "The Bias-Variance Tradeoff",
    "section": "R Setup and Source",
    "text": "R Setup and Source\n\nlibrary(tibble)     # data frame printing\nlibrary(dplyr)      # data manipulation\n\nlibrary(caret)      # fitting knn\nlibrary(rpart)      # fitting trees\nlibrary(rpart.plot) # plotting trees"
  },
  {
    "objectID": "content/Week_10/10a.html#the-regression-setup",
    "href": "content/Week_10/10a.html#the-regression-setup",
    "title": "The Bias-Variance Tradeoff",
    "section": "The Regression Setup",
    "text": "The Regression Setup\nConsider the general regression setup where we are given a random pair \\((X, Y) \\in \\mathbb{R}^p \\times \\mathbb{R}\\). We would like to “predict” \\(Y\\) with some function of \\(X\\), say, \\(f(X)\\).\nTo clarify what we mean by “predict,” we specify that we would like \\(f(X)\\) to be “close” to \\(Y\\). To further clarify what we mean by “close,” we define the squared error loss of estimating \\(Y\\) using \\(f(X)\\).\n\\[\nL(Y, f(X)) \\triangleq (Y - f(X)) ^ 2\n\\]\nNow we can clarify the goal of regression, which is to minimize the above loss, on average. We call this the risk of estimating \\(Y\\) using \\(f(X)\\).\n\\[\nR(Y, f(X)) \\triangleq \\mathbb{E}[L(Y, f(X))] = \\mathbb{E}_{X, Y}[(Y - f(X)) ^ 2]\n\\]\nBefore attempting to minimize the risk, we first re-write the risk after conditioning on \\(X\\).\n\\[\n\\mathbb{E}_{X, Y} \\left[ (Y - f(X)) ^ 2 \\right] = \\mathbb{E}_{X} \\mathbb{E}_{Y \\mid X} \\left[ ( Y - f(X) ) ^ 2 \\mid X = x \\right]\n\\]\nMinimizing the right-hand side is much easier, as it simply amounts to minimizing the inner expectation with respect to \\(Y \\mid X\\), essentially minimizing the risk pointwise, for each \\(x\\).\nIt turns out, that the risk is minimized by setting \\(f(x)\\) to be equal the conditional mean of \\(Y\\) given \\(X\\),\n\\[\nf(x) = \\mathbb{E}(Y \\mid X = x)\n\\]\nwhich we call the regression function.1\nNote that the choice of squared error loss is somewhat arbitrary. Suppose instead we chose absolute error loss.\n\\[\nL(Y, f(X)) \\triangleq | Y - f(X) |\n\\]\nThe risk would then be minimized setting \\(f(x)\\) equal to the conditional median.\n\\[\nf(x) = \\text{median}(Y \\mid X = x)\n\\]\nDespite this possibility, our preference will still be for squared error loss. The reasons for this are numerous, including: historical, ease of optimization, and protecting against large deviations.\nNow, given data \\(\\mathcal{D} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}\\), our goal becomes finding some \\(\\hat{f}\\) that is a good estimate of the regression function \\(f\\). We’ll see that this amounts to minimizing what we call the reducible error."
  },
  {
    "objectID": "content/Week_10/10a.html#reducible-and-irreducible-error",
    "href": "content/Week_10/10a.html#reducible-and-irreducible-error",
    "title": "The Bias-Variance Tradeoff",
    "section": "Reducible and Irreducible Error",
    "text": "Reducible and Irreducible Error\nSuppose that we obtain some \\(\\hat{f}\\), how well does it estimate \\(f\\)? We define the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\). A good \\(\\hat{f}\\) will have a low expected prediction error.\n\\[\n\\text{EPE}\\left(Y, \\hat{f}(X)\\right) \\triangleq \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  \\left( Y - \\hat{f}(X) \\right)^2 \\right]\n\\]\nThis expectation is over \\(X\\), \\(Y\\), and also \\(\\mathcal{D}\\). The estimate \\(\\hat{f}\\) is actually random depending on the data, \\(\\mathcal{D}\\), used to estimate \\(\\hat{f}\\). We could actually write \\(\\hat{f}(X, \\mathcal{D})\\) to make this dependence explicit, but our notation will become cumbersome enough as it is.\nLike before, we’ll condition on \\(X\\). This results in the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\) when \\(X = x\\).\n\\[\n\\text{EPE}\\left(Y, \\hat{f}(x)\\right) =\n\\mathbb{E}_{Y \\mid X, \\mathcal{D}} \\left[  \\left(Y - \\hat{f}(X) \\right)^2 \\mid X = x \\right] =\n\\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left[  \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]}_\\textrm{reducible error} +\n\\underbrace{\\mathbb{V}_{Y \\mid X} \\left[ Y \\mid X = x \\right]}_\\textrm{irreducible error}\n\\]\nA number of things to note here:\n\nThe expected prediction error is for a random \\(Y\\) given a fixed \\(x\\) and a random \\(\\hat{f}\\). As such, the expectation is over \\(Y \\mid X\\) and \\(\\mathcal{D}\\). Our estimated function \\(\\hat{f}\\) is random depending on the data, \\(\\mathcal{D}\\), which is used to perform the estimation.\nThe expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\) when \\(X = x\\) has been decomposed into two errors:\n\nThe reducible error, which is the expected squared error loss of estimation \\(f(x)\\) using \\(\\hat{f}(x)\\) at a fixed point \\(x\\). The only thing that is random here is \\(\\mathcal{D}\\), the data used to obtain \\(\\hat{f}\\). (Both \\(f\\) and \\(x\\) are fixed.) We’ll often call this reducible error the mean squared error of estimating \\(f(x)\\) using \\(\\hat{f}\\) at a fixed point \\(x\\). \\[ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) \\triangleq \\mathbb{E}_{\\mathcal{D}} \\left[  \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]\\]\nThe irreducible error. This is simply the variance of \\(Y\\) given that \\(X = x\\), essentially noise that we do not want to learn. This is also called the Bayes error.\n\n\nAs the name suggests, the reducible error is the error that we have some control over. But how do we control this error?"
  },
  {
    "objectID": "content/Week_10/10a.html#bias-variance-decomposition",
    "href": "content/Week_10/10a.html#bias-variance-decomposition",
    "title": "The Bias-Variance Tradeoff",
    "section": "Bias-Variance Decomposition",
    "text": "Bias-Variance Decomposition\nAfter decomposing the expected prediction error into reducible and irreducible error, we can further decompose the reducible error.\nRecall the definition of the bias of an estimator.\n\\[\n\\text{bias}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left[\\hat{\\theta}\\right] - \\theta\n\\]\nAlso recall the definition of the variance of an estimator.\n\\[\n\\mathbb{V}(\\hat{\\theta}) = \\text{var}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left [ ( \\hat{\\theta} -\\mathbb{E}\\left[\\hat{\\theta}\\right] )^2 \\right]\n\\]\nUsing this, we further decompose the reducible error (mean squared error) into bias squared and variance.\n\\[\n\\text{MSE}\\left(f(x), \\hat{f}(x)\\right) =\n\\mathbb{E}_{\\mathcal{D}} \\left[  \\left(f(x) - \\hat{f}(x) \\right)^2 \\right] =\n\\underbrace{\\left(f(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right]  \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}(x) \\right)} +\n\\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}(x) \\right)}\n\\]\nThis is actually a common fact in estimation theory, but we have stated it here specifically for estimation of some regression function \\(f\\) using \\(\\hat{f}\\) at some point \\(x\\).\n\\[\n\\text{MSE}\\left(f(x), \\hat{f}(x)\\right) = \\text{bias}^2 \\left(\\hat{f}(x) \\right) + \\text{var} \\left(\\hat{f}(x) \\right)\n\\]\nIn a perfect world, we would be able to find some \\(\\hat{f}\\) which is unbiased, that is \\(\\text{bias}\\left(\\hat{f}(x) \\right) = 0\\), which also has low variance. In practice, this isn’t always possible.\nIt turns out, there is a bias-variance tradeoff. That is, often, the more bias in our estimation, the lesser the variance. Similarly, less variance is often accompanied by more bias. Flexible models tend to be unbiased, but highly variable. Simple models are often extremely biased, but have low variance.\nIn the context of regression, models are biased when:\n\nParametric: The form of the model does not incorporate all the necessary variables, or the form of the relationship is too simple. For example, a parametric model assumes a linear relationship, but the true relationship is quadratic.\nNon-parametric: The model provides too much smoothing.\n\nIn the context of regression, models are variable when:\n\nParametric: The form of the model incorporates too many variables, or the form of the relationship is too flexible. For example, a parametric model assumes a cubic relationship, but the true relationship is linear.\nNon-parametric: The model does not provide enough smoothing. It is very, “wiggly.” Recall our KNN model example from Content 08\n\nSo for us, to select a model that appropriately balances the tradeoff between bias and variance, and thus minimizes the reducible error, we need to select a model of the appropriate flexibility for the data.\nRecall that when fitting models, we’ve seen that train RMSE decreases as model flexibility is increasing. (Technically it is non-increasing.) For validation RMSE, we expect to see a U-shaped curve. Importantly, validation RMSE decreases, until a certain flexibility, then begins to increase.\n\n\n\nNow we can understand why this is happening. The expected test RMSE is essentially the expected prediction error, which we now known decomposes into (squared) bias, variance, and the irreducible Bayes error. The following plots show three examples of this.\n\n\n\n\n\n\n\n\n\nThe three plots show three examples of the bias-variance tradeoff. In the left panel, the variance influences the expected prediction error more than the bias. In the right panel, the opposite is true. The middle panel is somewhat neutral. In all cases, the difference between the Bayes error (the horizontal dashed grey line) and the expected prediction error (the solid black curve) is exactly the mean squared error, which is the sum of the squared bias (blue curve) and variance (orange curve). The vertical line indicates the flexibility that minimizes the prediction error.\nTo summarize, if we assume that irreducible error can be written as\n\\[\n\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2\n\\]\nthen we can write the full decomposition of the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}\\) when \\(X = x\\) as\n\\[\n\\text{EPE}\\left(Y, \\hat{f}(x)\\right) =\n\\underbrace{\\text{bias}^2\\left(\\hat{f}(x)\\right) + \\text{var}\\left(\\hat{f}(x)\\right)}_\\textrm{reducible error} + \\sigma^2.\n\\]\nAs model flexibility increases, bias decreases, while variance increases. By understanding the tradeoff between bias and variance, we can manipulate model flexibility to find a model that will predict well on unseen observations.\n\n\n\n\n\n\n\n\n\nTying this all together, the above image shows how we “expect” training and validation error to behavior in relation to model flexibility.2 In practice, we won’t always see such a nice “curve” in the validation error, but we expect to see the general trends."
  },
  {
    "objectID": "content/Week_10/10a.html#using-simulation-to-estimate-bias-and-variance",
    "href": "content/Week_10/10a.html#using-simulation-to-estimate-bias-and-variance",
    "title": "The Bias-Variance Tradeoff",
    "section": "Using Simulation to Estimate Bias and Variance",
    "text": "Using Simulation to Estimate Bias and Variance\nWe will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function \\(f(x) = x^2\\).\n\nf = function(x) {\n  x ^ 2\n}\n\nMore specifically, we’d like to predict an observation, \\(Y\\), given that \\(X = x\\) by using \\(\\hat{f}(x)\\) where\n\\[\n\\mathbb{E}[Y \\mid X = x] = f(x) = x^2\n\\]\nand\n\\[\n\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2.\n\\]\nAlternatively, we could write this as\n\\[\nY = f(X) + \\epsilon\n\\]\nwhere \\(\\mathbb{E}[\\epsilon] = 0\\) and \\(\\mathbb{V}[\\epsilon] = \\sigma ^ 2\\). In this formulation, we call \\(f(X)\\) the signal and \\(\\epsilon\\) the noise.\nTo carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following R code.\n\ngen_sim_data = function(f, sample_size = 100) {\n  x = runif(n = sample_size, min = 0, max = 1)\n  y = rnorm(n = sample_size, mean = f(x), sd = 0.3)\n  tibble(x, y)\n}\n\nAlso note that if you prefer to think of this situation using the \\(Y = f(X) + \\epsilon\\) formulation, the following code represents the same data generating process.\n\ngen_sim_data = function(f, sample_size = 100) {\n  x = runif(n = sample_size, min = 0, max = 1)\n  eps = rnorm(n = sample_size, mean = 0, sd = 0.3)\n  y = f(x) + eps\n  tibble(x, y)\n}\n\nTo completely specify the data generating process, we have made more model assumptions than simply \\(\\mathbb{E}[Y \\mid X = x] = x^2\\) and \\(\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2\\). In particular,\n\nThe \\(x_i\\) in \\(\\mathcal{D}\\) are sampled from a uniform distribution over \\([0, 1]\\).\nThe \\(x_i\\) and \\(\\epsilon\\) are independent.\nThe \\(y_i\\) in \\(\\mathcal{D}\\) are sampled from the conditional normal distribution.\n\n\\[\nY \\mid X \\sim N(f(x), \\sigma^2)\n\\]\nUsing this setup, we will generate datasets, \\(\\mathcal{D}\\), with a sample size \\(n = 100\\) and fit four models.\n\\[\n\\begin{aligned}\n\\texttt{predict(fit0, x)} &= \\hat{f}_0(x) = \\hat{\\beta}_0\\\\\n\\texttt{predict(fit1, x)} &= \\hat{f}_1(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\\\\n\\texttt{predict(fit2, x)} &= \\hat{f}_2(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 \\\\\n\\texttt{predict(fit9, x)} &= \\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9\n\\end{aligned}\n\\]\nTo get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.\n\nset.seed(1)\nsim_data = gen_sim_data(f)\n\n\nfit_0 = lm(y ~ 1,                   data = sim_data)\nfit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\nfit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\nfit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\n\nNote that technically we’re being lazy and using orthogonal polynomials, but the fitted values are the same, so this makes no difference for our purposes. These could be KNN, or decision trees just the same - the principle still applies.\nPlotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.\n\n\n\n\n\n\n\n\n\nThe following three plots were created using three additional simulated datasets. The zero predictor and ninth degree polynomial were fit to each.\n\n\n\n\n\n\n\n\n\nThis plot should make clear the difference between the bias and variance of these two models. The zero predictor model is clearly wrong, that is, biased, but nearly the same for each of the datasets, since it has very low variance.\nWhile the ninth degree model doesn’t appear to be correct for any of these three simulations, we’ll see that on average it is, and thus is performing unbiased estimation. These plots do however clearly illustrate that the ninth degree polynomial is extremely variable. Each dataset results in a very different fitted model. Correct on average isn’t the only goal we’re after, since in practice, we’ll only have a single dataset. This is why we’d also like our models to exhibit low variance.\nWe could have also fit \\(k\\)-nearest neighbors models to these three datasets.\n\n\n\n\n\n\n\n\n\nHere we see that when \\(k = 100\\) we have a biased model with very low variance.3 When \\(k = 5\\), we again have a highly variable model.\nThese two sets of plots reinforce our intuition about the bias-variance tradeoff. Flexible models (ninth degree polynomial and \\(k\\) = 5) are highly variable, and often unbiased. Simple models (zero predictor linear model and \\(k = 100\\)) are very biased, but have extremely low variance.\nWe will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates of \\(f(x)\\) given by these four models at the point \\(x = 0.90\\). We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.\n\nset.seed(1)\nn_sims = 250\nn_models = 4\nx = data.frame(x = 0.90) # fixed point at which we make predictions\npredictions = matrix(0, nrow = n_sims, ncol = n_models)\n\n\nfor (sim in 1:n_sims) {\n\n  # simulate new, random, training data\n  # this is the only random portion of the bias, var, and mse calculations\n  # this allows us to calculate the expectation over D\n  sim_data = gen_sim_data(f)\n\n  # fit models\n  fit_0 = lm(y ~ 1,                   data = sim_data)\n  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\n  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\n  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\n\n  # get predictions\n  predictions[sim, 1] = predict(fit_0, x)\n  predictions[sim, 2] = predict(fit_1, x)\n  predictions[sim, 3] = predict(fit_2, x)\n  predictions[sim, 4] = predict(fit_9, x)\n}\n\n\n\n\nNote that this is one of many ways we could have accomplished this task using R. For example we could have used a combination of replicate() and *apply() functions. Alternatively, we could have used a tidyverse approach, which likely would have used some combination of dplyr, tidyr, and purrr.\nOur approach, which would be considered a base R approach, was chosen to make it as clear as possible what is being done. The tidyverse approach is rapidly gaining popularity in the R community, but might make it more difficult to see what is happening here, unless you are already familiar with that approach.\nAlso of note, while it may seem like the output stored in predictions would meet the definition of tidy data given by Hadley Wickham since each row represents a simulation, it actually falls slightly short. For our data to be tidy, a row should store the simulation number, the model, and the resulting prediction. We’ve actually already aggregated one level above this. Our observational unit is a simulation (with four predictions), but for tidy data, it should be a single prediction.\n\n\n\n\n\n\n\n\n\nThe above plot shows the predictions for each of the 250 simulations of each of the four models of different polynomial degrees. The truth, \\(f(x = 0.90) = (0.9)^2 = 0.81\\), is given by the solid black horizontal line.\nTwo things are immediately clear:\n\nAs flexibility increases, bias decreases. The mean of a model’s predictions is closer to the truth.\nAs flexibility increases, variance increases. The variance about the mean of a model’s predictions increases.\n\nThe goal of this simulation study is to show that the following holds true for each of the four models.\n\\[\n\\text{MSE}\\left(f(0.90), \\hat{f}_k(0.90)\\right) =\n\\underbrace{\\left(\\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] - f(0.90) \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}_k(0.90) \\right)} +\n\\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}_k(0.90) - \\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}_k(0.90) \\right)}\n\\]\nWe’ll use the empirical results of our simulations to estimate these quantities. (Yes, we’re using estimation to justify facts about estimation.) Note that we’ve actually used a rather small number of simulations. In practice we should use more, but for the sake of computation time, we’ve performed just enough simulations to obtain the desired results. (Since we’re estimating estimation, the bigger the sample size, the better.)\nTo estimate the mean squared error of our predictions, we’ll use\n\\[\n\\widehat{\\text{MSE}}\\left(f(0.90), \\hat{f}_k(0.90)\\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(f(0.90) - \\hat{f}_k^{[i]}(0.90) \\right)^2\n\\]\nwhere \\(\\hat{f}_k^{[i]}(0.90)\\) is the estimate of \\(f(0.90)\\) using the \\(i\\)-th from the polynomial degree \\(k\\) model.\nWe also write an accompanying R function.\n\nget_mse = function(truth, estimate) {\n  mean((estimate - truth) ^ 2)\n}\n\nSimilarly, for the bias of our predictions we use,\n\\[\n\\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right)  = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) \\right) - f(0.90)\n\\]\nAnd again, we write an accompanying R function.\n\nget_bias = function(estimate, truth) {\n  mean(estimate) - truth\n}\n\nLastly, for the variance of our predictions we have\n\\[\n\\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k^{[i]}(0.90) \\right)^2\n\\]\nWhile there is already R function for variance, the following is more appropriate in this situation.\n\nget_var = function(estimate) {\n  mean((estimate - mean(estimate)) ^ 2)\n}\n\nTo quickly obtain these results for each of the four models, we utilize the apply() function.\n\nbias = apply(predictions, MAR = 2, get_bias, truth = f(x = 0.90))\nvariance = apply(predictions, MAR = 2, get_var)\nmse = apply(predictions, MAR = 2, get_mse, truth = f(x = 0.90))\n\nWe summarize these results in the following table.\n\n\n\n\n\nDegree\nMean Squared Error\nBias Squared\nVariance\n\n\n\n\n0\n0.22643\n0.22476\n0.00167\n\n\n1\n0.00829\n0.00508\n0.00322\n\n\n2\n0.00387\n0.00005\n0.00381\n\n\n9\n0.01019\n0.00002\n0.01017\n\n\n\n\n\nA number of things to notice here:\n\nWe use squared bias in this table. Since bias can be positive or negative, squared bias is more useful for observing the trend as flexibility increases.\nThe squared bias trend which we see here is decreasing as flexibility increases, which we expect to see in general.\nThe exact opposite is true of variance. As model flexibility increases, variance increases.\nThe mean squared error, which is a function of the bias and variance, decreases, then increases. This is a result of the bias-variance tradeoff. We can decrease bias, by increasing variance. Or, we can decrease variance by increasing bias. By striking the correct balance, we can find a good mean squared error!\n\nWe can check for these trends with the diff() function in R.\n\nall(diff(bias ^ 2) < 0)\n\n[1] TRUE\n\nall(diff(variance) > 0)\n\n[1] TRUE\n\ndiff(mse) < 0\n\n    1     2     9 \n TRUE  TRUE FALSE \n\n\nThe models with polynomial degrees 2 and 9 are both essentially unbiased. We see some bias here as a result of using simulation. If we increased the number of simulations, we would see both biases go down. Since they are both unbiased, the model with degree 2 outperforms the model with degree 9 due to its smaller variance.\nModels with degree 0 and 1 are biased because they assume the wrong form of the regression function. While the degree 9 model does this as well, it does include all the necessary polynomial degrees.\n\\[\n\\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9\n\\]\nThen, since least squares estimation is unbiased, importantly,\n\\[\n\\mathbb{E}\\left[\\hat{\\beta}_d\\right] = \\beta_d = 0\n\\]\nfor \\(d = 3, 4, \\ldots 9\\), we have\n\\[\n\\mathbb{E}\\left[\\hat{f}_9(x)\\right] = \\beta_0 + \\beta_1 x + \\beta_2 x^2\n\\]\nNow we can finally verify the bias-variance decomposition.\n\nbias ^ 2 + variance == mse\n\n    0     1     2     9 \nFALSE FALSE  TRUE  TRUE \n\n\nBut wait, this says it isn’t true, except for the degree 9 model? It turns out, this is simply a computational issue. If we allow for some very small error tolerance, we see that the bias-variance decomposition is indeed true for predictions from these for models.\n\nall.equal(bias ^ 2 + variance, mse)\n\n[1] TRUE\n\n\nSee ?all.equal() for details.\nSo far, we’ve focused our efforts on looking at the mean squared error of estimating \\(f(0.90)\\) using \\(\\hat{f}(0.90)\\). We could also look at the expected prediction error of using \\(\\hat{f}(X)\\) when \\(X = 0.90\\) to estimate \\(Y\\).\n\\[\n\\text{EPE}\\left(Y, \\hat{f}_k(0.90)\\right) =\n\\mathbb{E}_{Y \\mid X, \\mathcal{D}} \\left[  \\left(Y - \\hat{f}_k(X) \\right)^2 \\mid X = 0.90 \\right]\n\\]\nWe can estimate this quantity for each of the four models using the simulation study we already performed.\n\nget_epe = function(realized, estimate) {\n  mean((realized - estimate) ^ 2)\n}\n\n\ny = rnorm(n = nrow(predictions), mean = f(x = 0.9), sd = 0.3)\nepe = apply(predictions, 2, get_epe, realized = y)\nepe\n\n        0         1         2         9 \n0.3180470 0.1104055 0.1095955 0.1205570 \n\n\n\n\n\nWhat about the unconditional expected prediction error. That is, for any \\(X\\), not just \\(0.90\\). Specifically, the expected prediction error of estimating \\(Y\\) using \\(\\hat{f}(X)\\). The following (new) simulation study provides an estimate of\n\\[\n\\text{EPE}\\left(Y, \\hat{f}_k(X)\\right) = \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  \\left( Y - \\hat{f}_k(X) \\right)^2 \\right]\n\\]\nfor the quadratic model, that is \\(k = 2\\) as we have defined \\(k\\).\n\nset.seed(42)\nn_sims = 2500\nX = runif(n = n_sims, min = 0, max = 1)\nY = rnorm(n = n_sims, mean = f(X), sd = 0.3)\n\nf_hat_X = rep(0, length(X))\n\nfor (i in seq_along(X)) {\n  sim_data = gen_sim_data(f)\n  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\n  f_hat_X[i] = predict(fit_2, newdata = data.frame(x = X[i]))\n}\n\n\n# truth\n0.3 ^ 2\n\n[1] 0.09\n\n# via simulation\nmean((Y - f_hat_X) ^ 2)\n\n[1] 0.09566445\n\n\nNote that in practice, we should use many more simulations in this study."
  },
  {
    "objectID": "content/Week_10/10a.html#estimating-expected-prediction-error",
    "href": "content/Week_10/10a.html#estimating-expected-prediction-error",
    "title": "The Bias-Variance Tradeoff",
    "section": "Estimating Expected Prediction Error",
    "text": "Estimating Expected Prediction Error\nWhile previously, we only decomposed the expected prediction error conditionally, a similar argument holds unconditionally.\nAssuming\n\\[\n\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2.\n\\]\nwe have\n\\[\n\\text{EPE}\\left(Y, \\hat{f}(X)\\right) =\n\\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  (Y - \\hat{f}(X))^2 \\right] =\n\\underbrace{\\mathbb{E}_{X} \\left[\\text{bias}^2\\left(\\hat{f}(X)\\right)\\right] + \\mathbb{E}_{X} \\left[\\text{var}\\left(\\hat{f}(X)\\right)\\right]}_\\textrm{reducible error} + \\sigma^2\n\\]\nLastly, we note that if\n\\[\n\\mathcal{D} = \\mathcal{D}_{\\texttt{trn}} \\cup \\mathcal{D}_{\\texttt{tst}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i = 1, 2, \\ldots n\n\\]\nwhere\n\\[\n\\mathcal{D}_{\\texttt{trn}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i \\in \\texttt{trn}\n\\]\nand\n\\[\n\\mathcal{D}_{\\texttt{tst}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i \\in \\texttt{tst}\n\\]\nThen, if we have a model fit to the training data \\(\\mathcal{D}_{\\texttt{trn}}\\), we can use the test mean squared error\n\\[\n\\sum_{i \\in \\texttt{tst}}\\left(y_i - \\hat{f}(x_i)\\right) ^ 2\n\\]\nas an estimate of\n\\[\n\\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  (Y - \\hat{f}(X))^2 \\right]\n\\]\nthe expected prediction error.4\nHow good is this estimate? Well, if \\(\\mathcal{D}\\) is a random sample from \\((X, Y)\\), and the \\(\\texttt{tst}\\) data are randomly sampled observations randomly sampled from \\(i = 1, 2, \\ldots, n\\), then it is a reasonable estimate. However, it is rather variable due to the randomness of selecting the observations for the test set, if the test set is small."
  },
  {
    "objectID": "content/Week_10/10a.html#model-flexibility",
    "href": "content/Week_10/10a.html#model-flexibility",
    "title": "The Bias-Variance Tradeoff",
    "section": "Model Flexibility",
    "text": "Model Flexibility\nLet’s return to the simulated dataset we used occaisionally in the linear regression content. Recall there was a single feature \\(x\\) with the following properties:\n\n# define regression function\ncubic_mean = function(x) {\n  1 - 2 * x - 3 * x ^ 2 + 5 * x ^ 3\n}\n\nWe then generated some data around this function with some added noise:\n\n# define full data generating process\ngen_slr_data = function(sample_size = 100, mu) {\n  x = runif(n = sample_size, min = -1, max = 1)\n  y = mu(x) + rnorm(n = sample_size)\n  tibble(x, y)\n}\n\nAfter defining the data generating process, we generate and split the data.\n\n# simulate entire dataset\nset.seed(3)\nsim_slr_data = gen_slr_data(sample_size = 100, mu = cubic_mean)\n\n# test-train split\nslr_trn_idx = sample(nrow(sim_slr_data), size = 0.8 * nrow(sim_slr_data))\nslr_trn = sim_slr_data[slr_trn_idx, ]\nslr_tst = sim_slr_data[-slr_trn_idx, ]\n\n# estimation-validation split\nslr_est_idx = sample(nrow(slr_trn), size = 0.8 * nrow(slr_trn))\nslr_est = slr_trn[slr_est_idx, ]\nslr_val = slr_trn[-slr_est_idx, ]\n\n# check data\nhead(slr_trn, n = 10)\n\n# A tibble: 10 × 2\n        x      y\n    <dbl>  <dbl>\n 1  0.573 -1.18 \n 2  0.807  0.576\n 3  0.272 -0.973\n 4 -0.813 -1.78 \n 5 -0.161  0.833\n 6  0.736  1.07 \n 7 -0.242  2.97 \n 8  0.520 -1.64 \n 9 -0.664  0.269\n10 -0.777 -2.02 \n\n\nFor validating models, we will use RMSE.\n\n# helper function for calculating RMSE\ncalc_rmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\nLet’s check how linear, k-nearest neighbors, and decision tree models fit to this data make errors, while paying attention to their flexibility.\n\n\n\n\n\n\n\n\n\nThis picture is an idealized version of what we expect to see, but we’ll illustrate the sorts of validate “curves” that we might see in practice.\nNote that in the following three sub-sections, a significant portion of the code is suppressed for visual clarity. See the source document for full details.\n\nLinear Models\nFirst up, linear models. We will fit polynomial models with degree from one to nine, and then validate.\n\n# fit polynomial models\npoly_mod_est_list = list(\n  poly_mod_1_est = lm(y ~ poly(x, degree = 1), data = slr_est),\n  poly_mod_2_est = lm(y ~ poly(x, degree = 2), data = slr_est),\n  poly_mod_3_est = lm(y ~ poly(x, degree = 3), data = slr_est),\n  poly_mod_4_est = lm(y ~ poly(x, degree = 4), data = slr_est),\n  poly_mod_5_est = lm(y ~ poly(x, degree = 5), data = slr_est),\n  poly_mod_6_est = lm(y ~ poly(x, degree = 6), data = slr_est),\n  poly_mod_7_est = lm(y ~ poly(x, degree = 7), data = slr_est),\n  poly_mod_8_est = lm(y ~ poly(x, degree = 8), data = slr_est),\n  poly_mod_9_est = lm(y ~ poly(x, degree = 9), data = slr_est)\n)\n\n\n\n\nThe plot below visualizes the results.\n\n\n\n\n\n\n\n\n\nWhat do we see here? As the polynomial degree increases:\n\nThe training error decreases.\nThe validation error decreases, then increases.\n\nThis more of less matches the idealized version above, but the validation “curve” is much more jagged. This is something that we can expect in practice.\nWe have previously noted that training error isn’t particularly useful for validating models. That is still true. However, it can be useful for checking that everything is working as planned. In this case, since we known that training error decreases as model flexibility increases, we can verify our intuition that a higher degree polynomial is indeed more flexible.5\n\n\nk-Nearest Neighbors\nNext up, k-nearest neighbors. We will consider values for \\(k\\) that are odd and between \\(1\\) and \\(45\\) inclusive.\n\n# helper function for fitting knn models\nfit_knn_mod = function(neighbors) {\n  knnreg(y ~ x, data = slr_est, k = neighbors)\n}\n\n\n# define values of tuning parameter k to evaluate\nk_to_try = seq(from = 1, to = 45, by = 2)\n\n# fit knn models\nknn_mod_est_list = lapply(k_to_try, fit_knn_mod)\n\n\n\n\nThe plot below visualizes the results.\n\n\n\n\n\n\n\n\n\nHere we see the “opposite” of the usual plot. Why? Because with k-nearest neighbors, a small value of \\(k\\) generates a flexible model compared to larger values of \\(k\\). So visually, this plot is flipped. That is we see that as \\(k\\) increases:\n\nThe training error increases.\nThe validation error decreases, then increases.\n\nImportant to note here: the pattern above only holds “in general,” that is, there can be minor deviations in the validation pattern along the way. This is due to the random nature of selection the data for the validate set.\n\n\nDecision Trees\nLastly, we evaluate some decision tree models. We choose some arbitrary values of cp to evaluate, while holding minsplit constant at 5. There are arbitrary choices that produce a plot that is useful for discussion.\n\n# helper function for fitting decision tree models\ntree_knn_mod = function(flex) {\n  rpart(y ~ x, data = slr_est, cp = flex, minsplit = 5)\n}\n\n\n# define values of tuning parameter cp to evaluate\ncp_to_try = c(0.5, 0.3, 0.1, 0.05, 0.01, 0.001, 0.0001)\n\n# fit decision tree models\ntree_mod_est_list = lapply(cp_to_try, tree_knn_mod)\n\n\n\n\nThe plot below visualizes the results.\n\n\n\n\n\n\n\n\n\nBased on this plot, how is cp related to model flexibility?6"
  },
  {
    "objectID": "content/Week_01/01a.html",
    "href": "content/Week_01/01a.html",
    "title": "Welcome Back to R",
    "section": "",
    "text": "As noted in the syllabus, your readings will be assigned each week in this area. For this initial week, please read the course content. Read closely the following:\n\nThe syllabus, content, examples, and labs pages for this class.\nThis page. Yes, the whole thing.\n\n\n\nFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\nHow does this course work?\nDo you remember anything about R?\nWhat are the different data types in R?\nHow do you index specific elements of a vector? Why might you want to do that?"
  },
  {
    "objectID": "content/Week_01/01a.html#some-reminders",
    "href": "content/Week_01/01a.html#some-reminders",
    "title": "Introduction to the tidyverse",
    "section": "Some Reminders:",
    "text": "Some Reminders:\n\nStart labs early!\n\nThey are not trivial.\nThey are not short.\nThey are not easy.\nThey are not optional.\n\nYou install.packages(\"packageName\") once on your computer.\n\nAnd never ever ever in your code.\n\nYou load an already-installed package using library(packageName) in a code chunk\n\nNever in your console\nWhen RMarkdown knits, it starts a whole new, empty session that has no knowledge of what you typed into the console\n\nSlack\n\nUse it.\nI would very much prefer posting in the class-visible channels. Others can learn from your issues.\n\nWe have a channel just for labs and R. Please use that one.\n\n\n\n\nGroup Projects\nYour final is a group project. You will also have two “mini” projects. They comprise a large part of your grade. As mentioned last week, this mean that you need to start planning soon.\nTo aid in your planning, here are the required elements of your final project.\n\nYou must find existing data to analyze. Aggregating and merging data from multiple sources is encouraged.\nYou must visualize 3 interesting features of that data.\nYou must come up with some analysis—using tools from this course—which relates your data to either a prediction or a policy conclusion.\nYou must think critically about your analysis and be able to identify potential issues.\nYou must present your analysis as if presenting to a C-suite executive.\n\nYour mini-projects along the way will be more structured, but will serve to guide you towards the final project.\n\n\nTeams\nPlease form teams of 3 people. Once all agree to be on a team, have ONE PERSON email our TA Allen scovelpa@msu.edu and cc all of the members of the team so that nobody is surprised to be included on a team. Title the email [SSC442] - Group Formation. Tell us your team name (be creative), and list in the email the names of all of the team members and their email address (in addition to cc-ing those team members on the email).\nIf you opt to not form a team, you will be automatically added to the “willing to be randomly assigned” pool and will be paired with two others from the “willing to be randomly assigned” pool.\nSend this email by January 20th and we will assign un-teamed folks at the beginning of the following week. Project 1 is due in no time. See schedule for all the important project dates.\n\n\nGuiding Question\nFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\nWhy do we want tidy data?\nWhat are the challenges associated with shaping things into a tidy format?"
  },
  {
    "objectID": "content/Week_01/01a.html#tidy-data",
    "href": "content/Week_01/01a.html#tidy-data",
    "title": "Introduction to the tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\n\nWe say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\n\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nEach row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.\nTo see how the same information can be provided in different formats, consider the following example:\n\nlibrary(dslabs)\ndata(\"gapminder\") # gapminder will now be a data.frame in your \"environment\" (memory)\ntidy_data <- gapminder %>%\n  filter(country %in% c(\"South Korea\", \"Germany\") & !is.na(fertility)) %>%\n  select(country, year, fertility)\nhead(tidy_data, 6)\n\n      country year fertility\n1     Germany 1960      2.41\n2 South Korea 1960      6.16\n3     Germany 1961      2.44\n4 South Korea 1961      5.99\n5     Germany 1962      2.47\n6 South Korea 1962      5.79\n\n\nThis tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the dslabs package. Originally, the data was in the following format:\n\n\n      country 1960 1961 1962\n1     Germany 2.41 2.44 2.47\n2 South Korea 6.16 5.99 5.79\n\n\nThe same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables’ values, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into tidy format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.\nAlthough not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.\n\nTRY IT\n\nExamine the built-in dataset co2. Which of the following is true:\n\n\nco2 is tidy data: it has one year for each row.\nco2 is not tidy: we need at least one column with a character vector.\nco2 is not tidy: it is a matrix instead of a data frame.\nco2 is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.\n\n\nExamine the built-in dataset ChickWeight. Which of the following is true:\n\n\nChickWeight is not tidy: each chick has more than one row.\nChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.\nChickWeight is not tidy: we are missing the year column.\nChickWeight is tidy: it is stored in a data frame.\n\n\nExamine the built-in dataset BOD. Which of the following is true:\n\n\nBOD is not tidy: it only has six rows.\nBOD is not tidy: the first column is just an index.\nBOD is tidy: each row is an observation with two values (time and demand)\nBOD is tidy: all small datasets are tidy by definition.\n\n\nWhich of the following built-in datasets is tidy (you can pick more than one):\n\n\nBJsales\nEuStockMarkets\nDNase\nFormaldehyde\nOrange\nUCBAdmissions"
  },
  {
    "objectID": "content/Week_01/01a.html#manipulating-data-frames",
    "href": "content/Week_01/01a.html#manipulating-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Manipulating data frames",
    "text": "Manipulating data frames\nThe dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.\n\nAdding a column with mutate\nWe want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values. So, to add murder rates, we use:\n\nlibrary(dslabs)\ndata(\"murders\")\nmurders <- mutate(murders, rate = total / population * 100000)\n\nNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error?\nThis is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable.\nWe can see that the new column is added:\n\nhead(murders)\n\n       state abb region population total     rate\n1    Alabama  AL  South    4779736   135 2.824424\n2     Alaska  AK   West     710231    19 2.675186\n3    Arizona  AZ   West    6392017   232 3.629527\n4   Arkansas  AR  South    2915918    93 3.189390\n5 California  CA   West   37253956  1257 3.374138\n6   Colorado  CO   West    5029196    65 1.292453\n\n\nNote: Although we have overwritten the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will overwrite our mutated version.\n\n\nSubsetting with filter\nNow suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the filter function, which takes the data table as the first argument and then the conditional statement as the second. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace.\n\nfilter(murders, rate <= 0.71)\n\n          state abb        region population total      rate\n1        Hawaii  HI          West    1360301     7 0.5145920\n2          Iowa  IA North Central    3046355    21 0.6893484\n3 New Hampshire  NH     Northeast    1316470     5 0.3798036\n4  North Dakota  ND North Central     672591     4 0.5947151\n5       Vermont  VT     Northeast     625741     2 0.3196211\n\n\n\n\nSelecting columns with select\nAlthough our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object:\n\nnew_table <- select(murders, state, region, rate)\nfilter(new_table, rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nIn the call to select, the first argument murders is an object, but state, region, and rate are variable names.\n\nTRY IT\n\nLoad the dplyr package and the murders dataset.\n\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\n\nYou can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted:\n\nmurders <- mutate(murders, population_in_millions = population / 10^6)\n\nWe can write population rather than murders$population because mutate is part of dplyr. The function mutate knows we are grabbing columns from murders.\nUse the function mutate to add a murders column named rate with the per 100,000 murder rate as in the example code above. Make sure you redefine murders as done in the example code above ( murders <- [your code]) so we can keep using this variable.\n\nIf rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\nWith dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes:\n\n\nselect(murders, state, population) %>% head()\n\nUse select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\n\nThe dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this:\n\n\nfilter(murders, state == \"New York\")\n\nYou can use other logical vectors to filter rows.\nUse filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\n\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:\n\n\nno_florida <- filter(murders, state != \"Florida\")\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n\nWe can also use %in% to filter with dplyr. You can therefore see the data from New York and Texas like this:\n\n\nfilter(murders, state %in% c(\"New York\", \"Texas\"))\n\nCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\n\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\n\n\nfilter(murders, population < 5000000 & region == \"Northeast\")\n\nMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate, and the rank."
  },
  {
    "objectID": "content/Week_01/01a.html#the-pipe",
    "href": "content/Week_01/01a.html#the-pipe",
    "title": "Introduction to the tidyverse",
    "section": "The pipe: %>%",
    "text": "The pipe: %>%\nWith dplyr we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %>%. Some details are included below.\nWe wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects:\n\\[ \\mbox{original data }\n\\rightarrow \\mbox{ select }\n\\rightarrow \\mbox{ filter } \\]\nFor such an operation, we can use the pipe %>%. The code looks like this:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nThis line of code is equivalent to the two lines of code above. What is going on here?\nIn general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:\n\n16 %>% sqrt()\n\n[1] 4\n\n\nWe can continue to pipe values along:\n\n16 %>% sqrt() %>% log2()\n\n[1] 2\n\n\nThe above statement is equivalent to log2(sqrt(16)).\nRemember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:\n\n16 %>% sqrt() %>% log(base = 2)\n\n[1] 2\n\n\nTherefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\nmurders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function.\nNote that the pipe works well with functions where the first argument is the input data. Functions in tidyverse packages like dplyr have this format and can be used easily with the pipe. It’s worth noting that as of R 4.1, there is a base-R version of the pipe |>, though this has its disadvantages. We’ll stick with %>% for now.\n\nTRY IT\n\nThe pipe %>% can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.\n\n\nmurders <- mutate(murders, rate =  total / population * 100000,\n                  rank = rank(-rate))\n\nIn the solution to the previous exercise, we did the following:\n\nmy_states <- filter(murders, region %in% c(\"Northeast\", \"West\") &\n                      rate < 1)\n\nselect(my_states, state, rate, rank)\n\nThe pipe %>% permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this:\n\nmutate(murders, rate =  total / population * 100000,\n       rank = rank(-rate)) %>%\n  select(state, rate, rank)\n\nNotice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %>%.\nRepeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %>% to do this in just one line.\n\nReset murders to the original table by using data(murders). Use a pipe to create a new data frame called my_states that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three %>%. The code should look something like this:\n\n\nmy_states <- murders %>%\n  mutate SOMETHING %>%\n  filter SOMETHING %>%\n  select SOMETHING"
  },
  {
    "objectID": "content/Week_01/01a.html#summarizing-data",
    "href": "content/Week_01/01a.html#summarizing-data",
    "title": "Introduction to the tidyverse",
    "section": "Summarizing data",
    "text": "Summarizing data\nAn important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using the pull function.\n\n\n\n\nsummarize\nThe summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The heights dataset includes heights and sex reported by students in an in-class survey.\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(heights)\nhead(heights)\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65\n\n\nThe following code computes the average and standard deviation for females:\n\ns <- heights %>%\n  filter(sex == \"Female\") %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\ns\n\n   average standard_deviation\n1 64.93942           3.760656\n\n\nThis takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same.\nBecause the resulting table stored in s is a data frame, we can access the components with the accessor $:\n\ns$average\n\n[1] 64.93942\n\ns$standard_deviation\n\n[1] 3.760656\n\n\nAs with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(median = median(height), minimum = min(height),\n            maximum = max(height))\n\n    median minimum maximum\n1 64.98031      51      79\n\n\nWe can obtain these three values with just one line using the quantile function: for example, quantile(x, c(0,0.5,1)) returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector x. However, if we attempt to use a function like this that returns two or more values inside summarize:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nwe will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In later sections, we will learn how to deal with functions that return more than one value.\nFor another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column:\n\nmurders <- murders %>% mutate(rate = total/population*100000)\n\nRemember that the US murder rate is not the average of the state murder rates:\n\nsummarize(murders, mean(rate))\n\n  mean(rate)\n1   2.779125\n\n\nThis is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000)\nus_murder_rate\n\n      rate\n1 3.034555\n\n\nThis computation counts larger states proportionally to their size which results in a larger value.\n\n\npull\nThe us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame:\n\nclass(us_murder_rate)\n\n[1] \"data.frame\"\n\n\nsince, as most dplyr functions, summarize always returns a data frame.\nThis might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. To understand what we mean take a look at this line of code:\n\nus_murder_rate %>% pull(rate)\n\n[1] 3.034555\n\n\nThis returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate.\nTo get a number from the original data table with one line of code we can type:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000) %>%\n  pull(rate)\n\nus_murder_rate\n\n[1] 3.034555\n\n\nwhich is now a numeric:\n\nclass(us_murder_rate)\n\n[1] \"numeric\"\n\n\n\n\nGroup then summarize with group_by\nA common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this.\nIf we type this:\n\nheights %>% group_by(sex)\n\n# A tibble: 1,050 × 2\n# Groups:   sex [2]\n   sex    height\n   <fct>   <dbl>\n 1 Male       75\n 2 Male       70\n 3 Male       68\n 4 Male       74\n 5 Male       61\n 6 Female     65\n 7 Female     66\n 8 Female     62\n 9 Female     66\n10 Male       67\n# ℹ 1,040 more rows\n\n\nThe result does not look very different from heights, except we see Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:\n\nheights %>%\n  group_by(sex) %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\n\n# A tibble: 2 × 3\n  sex    average standard_deviation\n  <fct>    <dbl>              <dbl>\n1 Female    64.9               3.76\n2 Male      69.3               3.61\n\n\nThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median murder rate in the four regions of the country:\n\nmurders %>%\n  group_by(region) %>%\n  summarize(median_rate = median(rate))\n\n# A tibble: 4 × 2\n  region        median_rate\n  <fct>               <dbl>\n1 Northeast            1.80\n2 South                3.40\n3 North Central        1.97\n4 West                 1.29"
  },
  {
    "objectID": "content/Week_01/01a.html#sorting-data-frames",
    "href": "content/Week_01/01a.html#sorting-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Sorting data frames",
    "text": "Sorting data frames\nWhen examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size:\n\nmurders %>%\n  arrange(population) %>%\n  head()\n\n                 state abb        region population total       rate\n1              Wyoming  WY          West     563626     5  0.8871131\n2 District of Columbia  DC         South     601723    99 16.4527532\n3              Vermont  VT     Northeast     625741     2  0.3196211\n4         North Dakota  ND North Central     672591     4  0.5947151\n5               Alaska  AK          West     710231    19  2.6751860\n6         South Dakota  SD North Central     814180     8  0.9825837\n\n\nWith arrange we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by rate instead:\n\nmurders %>%\n  arrange(rate) %>%\n  head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102\n\n\nNote that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type:\n\nmurders %>%\n  arrange(desc(rate))\n\n\nNested sorting\nIf we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region, then within region we order by murder rate:\n\nmurders %>%\n  arrange(region, rate) %>%\n  head()\n\n          state abb    region population total      rate\n1       Vermont  VT Northeast     625741     2 0.3196211\n2 New Hampshire  NH Northeast    1316470     5 0.3798036\n3         Maine  ME Northeast    1328361    11 0.8280881\n4  Rhode Island  RI Northeast    1052567    16 1.5200933\n5 Massachusetts  MA Northeast    6547629   118 1.8021791\n6      New York  NY Northeast   19378102   517 2.6679599\n\n\n\n\nThe top \\(n\\)\nIn the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:\n\nmurders %>% top_n(5, rate)\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Maryland  MD         South    5773552   293  5.074866\n4             Missouri  MO North Central    5988927   321  5.359892\n5       South Carolina  SC         South    4625364   207  4.475323\n\n\nNote that rows are not sorted by rate, only filtered. If we want to sort, we need to use arrange. Note that if the third argument is left blank, top_n filters by the last column.\n\nTRY IT\nFor these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\n\nlibrary(NHANES)\n\nWarning: package 'NHANES' was built under R version 4.3.3\n\ndata(NHANES)\n\nThe NHANES data has many missing values. The mean and sd functions in R will return NA if any of the entries of the input vector is an NA. Here is an example:\n\nlibrary(dslabs)\ndata(na_example)\nmean(na_example)\n\n[1] NA\n\nsd(na_example)\n\n[1] NA\n\n\nTo ignore the NAs we can use the na.rm argument:\n\nmean(na_example, na.rm = TRUE)\n\n[1] 2.301754\n\nsd(na_example, na.rm = TRUE)\n\n[1] 1.22338\n\n\nLet’s now explore the NHANES data.\n\nWe will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like ” 20-29”, with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref.\n\nHint: Use filter and summarize and use the na.rm = TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter.\n\nUsing a pipe, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then pull.\nNow report the min and max values for the same group.\nCompute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age and gender, filter by Gender and then use group_by.\nRepeat exercise 4 for males.\nWe can actually combine both summaries for exercises 4 and 5 into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender).\nFor males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure."
  },
  {
    "objectID": "content/Week_01/01a.html#tibbles",
    "href": "content/Week_01/01a.html#tibbles",
    "title": "Introduction to the tidyverse",
    "section": "Tibbles",
    "text": "Tibbles\nTidy data must be stored in data frames. We have been using the murders data frame throughout the unit. In an earlier section we introduced the group_by function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?\n\nmurders %>% group_by(region)\n\n# A tibble: 51 × 6\n# Groups:   region [4]\n   state                abb   region    population total  rate\n   <chr>                <chr> <fct>          <dbl> <dbl> <dbl>\n 1 Alabama              AL    South        4779736   135  2.82\n 2 Alaska               AK    West          710231    19  2.68\n 3 Arizona              AZ    West         6392017   232  3.63\n 4 Arkansas             AR    South        2915918    93  3.19\n 5 California           CA    West        37253956  1257  3.37\n 6 Colorado             CO    West         5029196    65  1.29\n 7 Connecticut          CT    Northeast    3574097    97  2.71\n 8 Delaware             DE    South         897934    38  4.23\n 9 District of Columbia DC    South         601723    99 16.5 \n10 Florida              FL    South       19687653   669  3.40\n# ℹ 41 more rows\n\n\nNotice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble followed by dimensions. We can learn the class of the returned object using:\n\nmurders %>% group_by(region) %>% class()\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe tbl, pronounced tibble, is a special kind of data frame. The functions group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. For consistency, the dplyr manipulation verbs (select, filter, mutate, and arrange) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble.\nTibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.\n\nTibbles display better\nThe print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing murders and the output of murders if we convert it to a tibble. We can do this using as_tibble(murders). If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.\n\n\nSubsets of tibbles are tibbles\nIf you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:\n\nclass(murders[,4])\n\n[1] \"numeric\"\n\n\nis not a data frame. With tibbles this does not happen:\n\nclass(as_tibble(murders)[,4])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThis is useful in the tidyverse since functions require data frames as input.\nWith tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $:\n\nclass(as_tibble(murders)$population)\n\n[1] \"numeric\"\n\n\nA related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write Population instead of population this:\n\nmurders$Population\n\nNULL\n\n\nreturns a NULL with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:\n\nas_tibble(murders)$Population\n\nWarning: Unknown or uninitialised column: `Population`.\n\n\nNULL\n\n\n\n\nTibbles can have complex entries\nWhile data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:\n\ntibble(id = c(1, 2, 3), func = c(mean, median, sd))\n\n# A tibble: 3 × 2\n     id func  \n  <dbl> <list>\n1     1 <fn>  \n2     2 <fn>  \n3     3 <fn>  \n\n\n\n\nTibbles can be grouped\nThe function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information.\n\n\nCreate a tibble using tibble instead of data.frame\nIt is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the tibble function.\n\ngrades <- tibble(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\n\nNote that base R (without packages loaded) has a function with a very similar name, data.frame, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default data.frame coerces characters into factors without providing a warning or message:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo avoid this, we use the rather cumbersome argument stringsAsFactors:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90),\n                     stringsAsFactors = FALSE)\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo convert a regular data frame to a tibble, you can use the as_tibble function.\n\nas_tibble(grades) %>% class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\""
  },
  {
    "objectID": "content/Week_01/01a.html#the-dot-operator",
    "href": "content/Week_01/01a.html#the-dot-operator",
    "title": "Introduction to the tidyverse",
    "section": "The dot operator",
    "text": "The dot operator\nOne of the advantages of using the pipe %>% is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:\n\ntab_1 <- filter(murders, region == \"South\")\ntab_2 <- mutate(tab_1, rate = total / population * 10^5)\nrates <- tab_2$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nWe can avoid defining any new intermediate objects by instead typing:\n\nfilter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  summarize(median = median(rate)) %>%\n  pull(median)\n\n[1] 3.398069\n\n\nWe can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the pull function was not available and we wanted to access tab_2$rate? What data frame name would we use? The answer is the dot operator.\nFor example to access the rate vector without the pull function we could use\n\nrates <-   filter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  .$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nIn the next section, we will see other instances in which using the . is useful."
  },
  {
    "objectID": "content/Week_01/01a.html#do",
    "href": "content/Week_01/01a.html#do",
    "title": "Introduction to the tidyverse",
    "section": "do",
    "text": "do\nThe tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %>%, tidyverse functions consistently take data frames and return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The quantile function is an example we described earlier. The do function serves as a bridge between R functions such as quantile and the tidyverse. The do function understands grouped tibbles and always returns a data frame.\nIn the summarize section (above), we noted that if we attempt to use quantile to obtain the min, median and max in one call, we will receive something unexpected. Prior to R 4.1, we would receive an error. After R 4.1, we actually get:\n\ndata(heights)\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nWe probably wanted three columns: min, median, and max. We can use the do function to fix this.\nFirst we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame. Note that it returns a single-row data frame.\n\nmy_summary <- function(dat){\n  x <- quantile(dat$height, c(0, 0.5, 1))\n  tibble(min = x[1], median = x[2], max = x[3])\n}\n\nWe can now apply the function to the heights dataset to obtain the summaries:\n\nheights %>%\n  group_by(sex) %>%\n  my_summary\n\n# A tibble: 1 × 3\n    min median   max\n  <dbl>  <dbl> <dbl>\n1    50   68.5  82.7\n\n\nBut this is not what we want. We want a summary for each sex and the code returned just one summary. This is because my_summary is not part of the tidyverse and does not know how to handled grouped tibbles. do makes this connection:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary(.))\n\n# A tibble: 2 × 4\n# Groups:   sex [2]\n  sex      min median   max\n  <fct>  <dbl>  <dbl> <dbl>\n1 Female    51   65.0  79  \n2 Male      50   69    82.7\n\n\nNote that here we need to use the dot operator. The tibble created by group_by is piped to do. Within the call to do, the name of this tibble is . and we want to send it to my_summary. If you do not use the dot, then my_summary has no argument and returns an error telling us that argument \"dat\" is missing. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary())\n\nIf you do not use the parenthesis, then the function is not executed and instead do tries to return the function. This gives an error because do must always return a data frame. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary)\n\nSo do serves as a bridge between non-tidyverse functions and the tidyverse."
  },
  {
    "objectID": "content/Week_01/01a.html#the-purrr-package",
    "href": "content/Week_01/01a.html#the-purrr-package",
    "title": "Introduction to the tidyverse",
    "section": "The purrr package",
    "text": "The purrr package\nIn previous sections (and labs) we learned about the sapply function, which permitted us to apply the same function to each element of a vector. We constructed a function and used sapply to compute the sum of the first n integers for several values of n like this:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  sum(x)\n}\nn <- 1:25\ns_n <- sapply(n, compute_s_n)\ns_n\n\n [1]   1   3   6  10  15  21  28  36  45  55  66  78  91 105 120 136 153 171 190\n[20] 210 231 253 276 300 325\n\n\nThis type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, sapply can return several different object types; for example, we might expect a numeric result from a line of code, but sapply might convert our result to character under some circumstances. purrr functions will never do this: they will return objects of a specified type or return an error if this is not possible.\nThe first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list:\n\nlibrary(purrr) # or library(tidyverse)\nn <- 1:25\ns_n <- map(n, compute_s_n)\nclass(s_n)\n\n[1] \"list\"\n\n\nIf we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values.\n\ns_n <- map_dbl(n, compute_s_n)\nclass(s_n)\n\n[1] \"numeric\"\n\n\nThis produces the same results as the sapply call shown above.\nA particularly useful purrr function for interacting with the rest of the tidyverse is map_df, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a Argument 1 must have names error:\n\ns_n <- map_df(n, compute_s_n)\n\nWe need to change the function to make this work:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x))\n}\ns_n <- map_df(n, compute_s_n)\nhead(s_n)\n\n# A tibble: 6 × 1\n    sum\n  <int>\n1     1\n2     3\n3     6\n4    10\n5    15\n6    21\n\n\nBecause map_df returns a tibble, we can have more columns defined in our function and returned.\n\ncompute_s_n2 <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x), sumSquared = sum(x^2))\n}\ns_n <- map_df(n, compute_s_n2)\nhead(s_n)\n\n# A tibble: 6 × 2\n    sum sumSquared\n  <int>      <dbl>\n1     1          1\n2     3          5\n3     6         14\n4    10         30\n5    15         55\n6    21         91\n\n\nThe purrr package provides much more functionality not covered here. For more details you can consult this online resource."
  },
  {
    "objectID": "content/Week_01/01a.html#tidyverse-conditionals",
    "href": "content/Week_01/01a.html#tidyverse-conditionals",
    "title": "Introduction to the tidyverse",
    "section": "Tidyverse conditionals",
    "text": "Tidyverse conditionals\nA typical data analysis will often involve one or more conditional operations. In the section on Conditionals, we described the ifelse function, which we will use extensively in this book. In this section we present two dplyr functions that provide further functionality for performing conditional operations.\n\ncase_when\nThe case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0:\n\nx <- c(-2, -1, 0, 1, 2)\ncase_when(x < 0 ~ \"Negative\",\n          x > 0 ~ \"Positive\",\n          x == 0  ~ \"Zero\")\n\n[1] \"Negative\" \"Negative\" \"Zero\"     \"Positive\" \"Positive\"\n\n\nA common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this:\n\nmurders %>%\n  mutate(group = case_when(\n    abb %in% c(\"ME\", \"NH\", \"VT\", \"MA\", \"RI\", \"CT\") ~ \"New England\",\n    abb %in% c(\"WA\", \"OR\", \"CA\") ~ \"West Coast\",\n    region == \"South\" ~ \"South\",\n    TRUE ~ \"Other\")) %>%\n  group_by(group) %>%\n  summarize(rate = sum(total) / sum(population) * 10^5)\n\n# A tibble: 4 × 2\n  group        rate\n  <chr>       <dbl>\n1 New England  1.72\n2 Other        2.71\n3 South        3.63\n4 West Coast   2.90\n\n\nThat TRUE on the fourth line of case_when serves as a catch-all. As case_when steps through the conditions, if none of them are true, it comes to the last line. Since TRUE is always true, the function will return “Other”. Leaving out the last line of case_when would result in NA values for any observation that fails the first three conditionals. This may or may not be what you want.\n\n\nbetween\nA common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type\n\nx >= a & x <= b\n\nHowever, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation.\n\nbetween(x, a, b)\n\n\nTRY IT\n\nLoad the murders dataset. Which of the following is true?\n\n\nmurders is in tidy format and is stored in a tibble.\nmurders is in tidy format and is stored in a data frame.\nmurders is not in tidy format and is stored in a tibble.\nmurders is not in tidy format and is stored in a data frame.\n\n\nUse as_tibble to convert the murders data table into a tibble and save it in an object called murders_tibble.\nUse the group_by function to convert murders into a tibble that is grouped by region.\nWrite tidyverse code that is equivalent to this code:\n\n\nexp(mean(log(murders$population)))\n\nWrite it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with murders %>%.\n\nUse the map_df to create a data frame with three columns named n, s_n, and s_n_2. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through \\(n\\) with \\(n\\) the row number."
  },
  {
    "objectID": "content/Week_01/01b.html",
    "href": "content/Week_01/01b.html",
    "title": "Working with R and RStudio",
    "section": "",
    "text": "I like to use this spot to publish course announcements. Not so much for y’all, but more so I remember. If you see any announcements that don’t say “Spring 2024” there’s a good chance it’s leftover from earlier course offerings.\n\n\nA careful read of our syllabus under “class participation” will show that I do give extra credit for answering questions and (mainly) sharing completed R coding tasks. That is, we’ll walk through some examples, and when we hit a “try it”, I’ll ask you to give it a go. Then, after a few minutes, I’ll ask if anyone wants to share their answer. You get one point of participation extra credit. Five points is worth 1% of a grade boost, so these aren’t negligible points.\n\n\n\nThe Assignments page has all of our weekly lab assignments (including Week 1, due on Monday at 11:59pm). The assignments often have a preamble and some code that has to be used to set you up for the questions. The questions to be completed and turned in are under “Exercises” at the very end.\n\n\n\nI don’t like to use the DM feature of Slack. Not because I don’t like getting DMs, but because the point of Slack is so we can all learn together. If you have a question, even if you’re worried it’s a silly question, then others will, I promise, have the same question. Asking in the public channel will answer everyone’s question at once. Related to this: you should check the Slack to see if your question has already been asked.\nAnother reason is that the TA will often be faster at responding than I will be, depending on day of week and time of day (I start my day before some of you even go to bed). If you DM me, the TA can’t see it and can’t reply."
  },
  {
    "objectID": "content/Week_01/01b.html#the-components-of-a-graph",
    "href": "content/Week_01/01b.html#the-components-of-a-graph",
    "title": "Introduction to Visualization",
    "section": "The components of a graph",
    "text": "The components of a graph\nWe will eventually construct a graph that summarizes the US murders dataset that looks like this:\n\n\n\n\n\nWe can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.\nThis data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.\nThe first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main five components to note are:\n\nData: The US murders data table is being summarized. We refer to this as the data component.\nGeometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, boxplot, pie (ew!), and many, many more. We will learn about these later.\nAesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using.\nAnnotations: These are things like axis labels, axis ticks (the lines along the axis at regular intervals or specific points of interest), axis scales (e.g. log-scale), titles, legends, etc.\nStyle: An overall appearance of the graph determined by fonts, color palattes, layout, blank spaces, and more.\n\nWe also note that:\n\nThe points are labeled with the state abbreviations.\nThe range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.\nThere are labels, a title, a legend, and we use the style of The Economist magazine.\n\nAll of the flexibility and visualization power of ggplot is contained in these four elements (plus your data)"
  },
  {
    "objectID": "content/Week_01/01b.html#ggplot-objects",
    "href": "content/Week_01/01b.html#ggplot-objects",
    "title": "Introduction to Visualization",
    "section": "ggplot objects",
    "text": "ggplot objects\nWe will now construct the plot piece by piece.\nWe start by loading the dataset:\n\nlibrary(dslabs)\ndata(murders)\n\n\n\n\nThe first step in creating a ggplot2 graph is to define a ggplot object. We do this with the function ggplot, which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object:\n\nggplot(data = murders)\n\nWe can also pipe the data in as the first argument. So this line of code is equivalent to the one above:\n\nmurders %>% ggplot()\n\n\n\n\nIt renders a plot, in this case a blank slate since no geometry has been defined. The only style choice we see is a grey background.\nWhat has happened above is that the object was created and, because it was not assigned, it was automatically evaluated. But we can assign our plot to an object, for example like this:\n\np <- ggplot(data = murders)\nclass(p)\n\n[1] \"gg\"     \"ggplot\"\n\n\nTo render the plot associated with this object, we simply print the object p. The following two lines of code each produce the same plot we see above:\n\nprint(p)\np"
  },
  {
    "objectID": "content/Week_01/01b.html#geometries-briefly",
    "href": "content/Week_01/01b.html#geometries-briefly",
    "title": "Introduction to Visualization",
    "section": "Geometries (briefly)",
    "text": "Geometries (briefly)\nIn ggplot2 we create graphs by adding geometry layers. Layers can define geometries, compute summary statistics, define what scales to use, create annotations, or even change styles. To add layers, we use the symbol +. In general, a line of code will look like this:\n\nDATA %>% ggplot() + LAYER 1 + LAYER 2 + ... + LAYER N\n\nUsually, the first added layer after ggplot() + defines the geometry. After that, we may add additional geometries, we may rescale an axis, we may add annotations and labels, or we may change the style. For now, we want to make a scatterplot like the one you all created in Lab 0. What geometry do we use?\n\n\n\n\n\nTaking a quick look at the cheat sheet, we see that the ggplot2 function used to create plots with this geometry is geom_point.\nSee Here\n(Image courtesy of RStudio9. CC-BY-4.0 license10.)\n\nGeometry function names follow the pattern: geom_X where X is the name of some specific geometry. Some examples include geom_point, geom_bar, and geom_histogram. You’ve already seen a few of these. We will start with a scatterplot created using geom_point() for now, then circle back to more geometries after we cover aesthetic mappings, layers, and annotations.\nFor geom_point to run properly we need to provide data and an aesthetic mapping. The simplest mapping for a scatter plot is to say we want one variable on the X-axis, and a different one on the Y-axis, so each point is an {X,Y} pair. That is an aesthetic mapping because X and Y are aesthetics in a geom_point scatterplot.\nWe have already connected the object p with the murders data table, and if we add the layer geom_point it defaults to using this data. To find out what mappings are expected, we read the Aesthetics section of the help file ?geom_point help file:\n> Aesthetics\n>\n> geom_point understands the following aesthetics (required aesthetics are in bold):\n>\n> **x**\n>\n> **y**\n>\n> alpha\n>\n> colour\n>\n> fill\n>\n> group\n>\n> shape\n>\n> size\n>\n> stroke\nand—although it does not show in bold above—we see that at least two arguments are required: x and y. You can’t have a geom_point scatterplot unless you state what you want on the X and Y axes."
  },
  {
    "objectID": "content/Week_01/01b.html#aesthetic-mappings",
    "href": "content/Week_01/01b.html#aesthetic-mappings",
    "title": "Introduction to Visualization",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\nAesthetic mappings describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The aes function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the aes function is often used as the argument of a geometry function. This example produces a scatterplot of population in millions (x-axis) versus total murders (y-axis):\n\nmurders %>% ggplot() +\n  geom_point(aes(x = population/10^6, y = total))\n\nInstead of defining our plot from scratch, we can also add a layer to the p object that was defined above as p <- ggplot(data = murders):\n\np + geom_point(aes(x = population/10^6, y = total))\n\n\n\n\nThe scales and annotations like axis labels are defined by default when adding this layer (note the x-axis label is exactly what we wrote in the function call). Like dplyr functions, aes also uses the variable names from the object component: we can use population and total without having to call them as murders$population and murders$total. The behavior of recognizing the variables from the data component is quite specific to aes. With most functions, if you try to access the values of population or total outside of aes you receive an error.\nNote that we did some rescaling within the aes() call - we can do simple things like multiplication or division on the variable names in the ggplot call. The axis labels reflect this. We will change the axis labels later.\nThe aesthetic mappings are very powerful - changing the variable in x= or y= changes the meaning of the plot entirely. We’ll come back to additional aesthetic mappings once we talk about aesthetics in general.\n\nAesthetics in general\nEven without mappings, a plots aesthetics can be useful. Things like color, fill, alpha, and size are aesthetics that can be changed.\nLet’s say we want larger points in our scatterplot. The size aesthetic can be used to set the size. The scale of size is “multiples of the defaults” (so size = 1 is the default)\n\np + geom_point(aes(x = population/10^6, y = total), size = 3)\n\n\n\n\nsize is not a mapping so it is not in the aes() part: whereas mappings use data from specific observations and need to be inside aes(), operations we want to affect all the points the same way do not need to be included inside aes. We’ll see what happens if size is inside aes(size = xxx) in a second.\nWe can change the shape to one of the many different base-R options found here:\n\np + geom_point(aes(x = population/10^6, y = total), size = 3, shape = 17)\n\n\n\n\nWe can also change the fill and the color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 4, shape = 23, fill = '#18453B')\n\n\n\n\nfill can take a common name like 'green', or can take a hex color like '#18453B', which is MSU Green according to MSU’s branding site. You can also find UM Maize and OSU Scarlet on respective branding pages, or google “XXX color hex.” We’ll learn how to build a color palatte later on.\ncolor (or colour, same thing because ggplot creators allow both spellings) is a little tricky with points - it changes the outline of the geometry rather than the fill color, but in geom_point() most shapes are only the outline, including the default. This is more useful with, say, a barplot where the outline and the fill might be different colors. Still, shapes 21-25 have both fill and color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 5, shape = 23, fill = '#18453B', color = 'white')\n\n\n\n\nThe color = 'white' makes the outline of the shape white, which you can see if you look closely in the areas where the shapes overlap. This only works with shapes 21-25, or any other geometry that has both an outline and a fill.\n\n\nNow, back to aesthetic mappings\nNow that we’ve seen a few aesthetics (and know we can find more by looking at which aesthetics work with our geometry in the help file), let’s return to the power of aesthetic mappings.\nAn aesthetic mapping means we can vary an aesthetic (like fill or shape or size) according to some variable in our data. This opens up a world of possibilities! Let’s try adding to our x and y aesthetics with a color aesthetic (since points respond to color better than fill) that varies by region, which is a column in our data:\n\np + geom_point(aes(x = population/10^6, y = total, color = region), size = 3)\n\n\n\n\nWe include color=region inside the aes call, which tells R to find a variable called region and change color based on that. R will choose a somewhat ghastly color palatte, and every unique value in the data for region will get a different color if the variable is discrete. If the variable is a continuous value, then ggplot will automatically make a color ramp. Thus, discrete and continuous values for aesthetic mappings work differently.\nLet’s see a useful example of a continuous aesthetic mapping to color. In our data, we are making a scatterplot of population and total murders, which really just shows that states with higher populations have higher murders. What we really want is murders per capita (I think COVID taught us a lot about rates vs. levels like “cases” and “cases per 100,000 people”). We can create a variable of “murders per capita” on the fly. Since “murders per capita” is a very small number and hard to read, we’ll multiply by 100 so that we get “percent of population murdered per year”:\n\np + geom_point(aes(x = population/10^5, y = total, color = 100*total/population), size = 3)\n\n\n\n\nWhile the clear pattern of “more population means more murders” is still there, look at the outlier in light blue in the bottom left. With the color ramp, see how easy it is to see here that there is one location where murders per capita is quite high?\nNote that size is outside of aes and is set to an explicit value, not to a variable. What if we set size to a variable in the data?\n\np + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6))\n\n\n\n\n\n\nLegends for aesthetics\nHere we see yet another useful default behavior: ggplot2 automatically adds a legend that maps color to region, and size to population (which we scaled by 1,000,000). To avoid adding this legend we set the geom_point argument show.legend = FALSE. This removes both the size and the color legend.\n\np + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6), show.legend = FALSE)\n\n\n\n\nLater on, when we get to annotation layers, we’ll talk about controlling the legend text and layout. For now, we just need to know how to turn them off."
  },
  {
    "objectID": "content/Week_01/01b.html#annotation-layers",
    "href": "content/Week_01/01b.html#annotation-layers",
    "title": "Introduction to Visualization",
    "section": "Annotation Layers",
    "text": "Annotation Layers\nA second layer in the plot we wish to make involves adding a label to each point to identify the state. The geom_label and geom_text functions permit us to add text to the plot with and without a rectangle behind the text, respectively.\nBecause each point (each state in this case) has a label, we need an aesthetic mapping to make the connection between points and labels. By reading the help file ?geom_text, we learn that we supply the mapping between point and label through the label argument of aes. That is, label is an aesthetic that we can map. So the code looks like this:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\n\n\n\nWe have successfully added a second layer to the plot.\nAs an example of the unique behavior of aes mentioned above, note that this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total, label = abb))\n\nis fine, whereas this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total), label = abb)\n\nwill give you an error since abb is not found because it is outside of the aes function. The layer geom_text does not know where to find abb since it is a column name and not a global variable, and ggplot does not look for column names for non-mapped aesthetics. For a trivial example:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(population/10^6, total), label = 'abb')\n\n\n\n\n\nGlobal versus local aesthetic mappings\nIn the previous line of code, we define the mapping aes(population/10^6, total) twice, once in each geometry. We can avoid this by using a global aesthetic mapping. We can do this when we define the blank slate ggplot object. Remember that the function ggplot contains an argument that permits us to define aesthetic mappings:\n\nargs(ggplot)\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \nNULL\n\n\nIf we define a mapping in ggplot, all the geometries that are added as layers will default to this mapping. We redefine p:\n\np <- murders %>% ggplot(aes(x = population/10^6, y = total, label = abb))\n\nand then we can simply write the following code to produce the previous plot:\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 1.5) # offsets the label\n\nWe keep the size and nudge_x arguments in geom_point and geom_text, respectively, because we want to only increase the size of points and only nudge the labels. If we put those arguments in aes then they would apply to both plots. Also note that the geom_point function does not need a label argument and therefore ignores that aesthetic.\nIf necessary, we can override the global mapping by defining a new mapping within each layer. These local definitions override the global. Here is an example:\n\np + geom_point(size = 3) +\n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))\n\n\n\n\nClearly, the second call to geom_text does not use x = population and y = total."
  },
  {
    "objectID": "content/Week_01/01b.html#try-it",
    "href": "content/Week_01/01b.html#try-it",
    "title": "Introduction to Visualization",
    "section": "Try it!",
    "text": "Try it!\n\nLet’s break in to smaller groups and try playing with some of the aesthetics and aesthetic mappings. If we’re in person (woohoo!), we’ll form the same number of groups in class.\nIn each group, one person should be the main coder - someone who has the packages like dslabs installed and has successfully run the plots above. Each set of tasks ask you to learn about an aesthetic and put it into action with the murder data. We’ll leave about 5 minutes to do the task, then have you come back and share your results with the class.\nFor each group, we’ll start with the following code:\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\nThe alpha aesthetic mapping.\n\nThe alpha aesthetic can only take a number between 0 and 1. So first, in murders, create a murders_per_capita column by dividing total by population. Second, find the max(murders$murders_per_capita) and then create another new column called murders_per_capita_rescaled which divides murders_per_capita by the max value. murders_per_capita_rescaled will be between 0 and 1, with the value of 1 for the state with the max murder rate. This is a little hard to do on the fly in ggplot.\nSet the alpha aesthetic mapping to murders_per_capita_rescaled for geom_point.\nTurn off the legend using show.legend=FALSE\nInclude the geom_text labels, but make sure the aesthetic mapping does not apply to the labels.\nUse nudge_x = 1.5 as before to offset the labels.\nBe able to explain the plot.\n\nDoes the alpha aesthetic help present the data here? It’s OK if it doesn’t!\n\n\nThe stroke aesthetic mapping.\n\nThe stroke aesthetic works a bit like the size aesthetic. It must be used with a plot that has both a border and a fill, like shapes 21-25, so use one of those.\nUse the stroke aesthetic mapping (meaning the stroke will change according to a value in the data) to set a different stroke size based on murders per capita. You can create a murders per capita variable on the fly, or add it to your murders data.\n\nInclude the text labels as before and use nudge_x = 1.5.\nMake sure you’re only setting the aesthetic for the points on the scatterplot!\n\n\nThe angle aesthetic\n\nUsing the ?geom_text help, note that geom_text takes an aesthetic of angle.\nUse the angle aesthetic (not aesthetic mapping) in the appropriate place (e.g. on geom_text and not on other geometries) to adjust the labels on our plot.\nNow, try using the angle aesthetic mapping by using the total field as both the y value and the angle value in the geom_text layer.\nDoes using angle as an aesthetic help? What about as an aesthetic mapping?\n\nThe color aesthetic mapping\n\nSet the color aesthetic mapping in geom_text to total/population.\n\nUse the nudge_x = 1.5 aesthetic in geom_text still\n\nTry it with and without the legend using show.legend.\nBe able to explain the plot.\n\nDoes the color aesthetic mapping help present the data here?\n\n\ngeom_label and the fill aesthetic\n\nLooking at ?geom_label (which is the same help as geom_text), we note that “The fill aesthetic controls the backgreound colour of the label”.\nSet the fill aesthetic mapping to total/population in geom_label (replacing geom_text but still using nudge_x=1.5)\nSet the fill aesthetic (not mapping) to the color of your choice.\nBe able to explain the plots.\n\n\nDoes the fill aesthetic mapping help present the data here?\nWhat color did you choose for the non-mapped fill aesthetic?"
  },
  {
    "objectID": "content/Week_01/01a.html#starting-point-for-this-course",
    "href": "content/Week_01/01a.html#starting-point-for-this-course",
    "title": "Welcome Back to R",
    "section": "Starting point for this course",
    "text": "Starting point for this course\nBetter utilizing existing data can improve our predictive power whilst providing interpretable outputs for considering new policies.\nWARNING: Causation is tough and we will spend the entire course warning you to avoid making causal claims!\n\nNon-Social Science Approaches to Statistical Learning\nA Brief History\nSuppose you are a researcher and you want to teach a computer to recognize images of a tree.\nNote: this is an ``easy” problem. If you show pictures to a 3-year-old, that child will probably be able to tell you if there is a tree in the picture.\nComputer scientists spent about 20 years on this problem because they thought about the problem like nerds and tried to write down a series of rules.\nRules are difficult to form, and simply writing rules misses the key insight: the data can tell you something.\n\n\nSocial Science Approaches to Statistical Learning\nA Brief History\nSuppose you are a researcher and you want to know whether prisons reduce crime.\nfrom “A Call for a Moratorium on Prison Building” (1976)\n\nBetween 1955 and 1975, fifteen states increased the collective capacity of their adult prison systems by 56% (from, on average, 63,100 to 98,649).\nFifteen other states increased capacity by less than 4% (from 49,575 to 51,440).\nIn “heavy-construction” states the crime rate increased by 167%; in “low-construction” states the crime rate increased by 145%.\n\n\n\n\n\nPrison Capacity\nCrime Rate\n\n\n\n\nHigh construction\n\\(\\uparrow\\)~56%\n\\(\\uparrow\\)~167%\n\n\nLow construction\n\\(\\uparrow\\)~4%\n\\(\\uparrow\\)~145%\n\n\n\n\n\nThe Pros and Cons of Correlation\nPros:\n\nNature gives you correlations for free.\nIn principle, everyone can agree on the facts.\n\nCons:\n\nCorrelations are not very helpful.\nThey show what has happened, but not why.\nFor many things, we care about why. The social science perspective asks “why?”\n\n\nWhy a Correlation Exists Between X and Y\n\n\\(X \\rightarrow Y\\) X causes Y (causality)\n\\(X \\leftarrow Y\\) Y causes X (reverse causality)\n\\(Z \\rightarrow X\\); \\(Z \\rightarrow Y\\) Z causes X and Y (common cause)\n\\(X \\rightarrow Y\\); \\(Y \\rightarrow X\\) X causes Y and Y causes X (simultaneous equations)\n\n\n\nUniting Social Science and Computer Science\nWe will start in this course by examining situations where we do not care about why something has happened, but instead we care about our ability to predict its occurrence from existing data.\n(But of course keep in back of mind that if you are making policy, you must care about why something happened).\nWe will also borrow a few other ideas from CS:\n\nAnything is data\n\nSatellite data\nUnstructured text or audio\nFacial expressions or vocal intonations\n\nSubtle improvements on existing techniques\nAn eye towards practical implementability over “cleanliness”\n\n\n\n\nA Case Study in Prediction\nExample: a firm wishes to predict user behavior based on previous purchases or interactions.\nSmall margins \\(\\rightarrow\\) huge payoffs when scaled up.\n\\(.01\\% \\rightarrow\\) $10 million.\nNot obvious why this was true for Netflix; quite obvious why this is true in financial markets.\nFrom a computer science perspective, it only matters that you get that improvement ($$). From a social science perspective, we would want to use the predictions to learn more about why.\n\n\nMore Recent Examples of Prediction\n\nIdentify the risk factors for prostate cancer.\nClassify a tissue sample into one of several cancer classes, based on a gene expression profile.\nClassify a recorded phoneme based on a log-periodogram.\nPredict whether someone will have a heart attack on the basis of demographic, diet and clinical measurements.\nCustomize an email spam detection system.\nIdentify a hand-drawn object.\nDetermine which oscillations of stellar luminosity are likely due to exoplanets.\nIdentify food combinations that cause spikes in blood glucose level for an individual.\nEstablish the relationship between salary and demographic variables in population survey data.\n\n\n\nAn Aside: Nomenclature\nMachine learning arose as a subfield of Artificial Intelligence.\nStatistical learning arose as a subfield of Statistics.\nThere is much overlap; however, a few points of distinction:\n\nMachine learning has a greater emphasis on large scale applications and prediction accuracy.\nStatistical learning emphasizes models and their interpretability, and precision and uncertainty.\n\nBut the distinction has become more and more blurred, and there is a great deal of “cross-fertilization”.\n\n\nObviously true: machine learning has the upper hand in marketing.\n\n\nLearning from Data\nThe following are the basic requirements for statistical learning:\n\nA pattern exists.\nThis pattern is not easily expressed in a closed mathematical form.\nYou have data."
  },
  {
    "objectID": "content/Week_01/01a.html#case-study-1-global-renewable-energy-production",
    "href": "content/Week_01/01a.html#case-study-1-global-renewable-energy-production",
    "title": "Welcome Back to R",
    "section": "Case study 1: Global Renewable Energy Production",
    "text": "Case study 1: Global Renewable Energy Production\nImagine you are evaluating countries for a potential investment in renewable energy. Headlines like “Renewable Energy Capacity Growth Worldwide” have piqued your interest. Reports from various sources show diverse graphs and charts and you’re curious about the underlying data. You want to know which countries are leading the way in renewable energy production and which are lagging behind. You want to know which countries are growing their renewable energy production the fastest. In short: you want to know which countries are the best bets for investment. You might see something like the following:\n\n\n\n\n\nYou might want to look into the underlying data (in this case, fabricated) and think about what to do next. In this sense, you have learned from data."
  },
  {
    "objectID": "content/Week_01/01a.html#case-study-2-us-homicides-by-firearm",
    "href": "content/Week_01/01a.html#case-study-2-us-homicides-by-firearm",
    "title": "Welcome Back to R",
    "section": "Case study 2: US homicides by firearm",
    "text": "Case study 2: US homicides by firearm\nImagine you live in Europe (if only!) and are offered a job in a US company with many locations in every state. It is a great job, but headlines such as US Gun Homicide Rate Higher Than Other Developed Countries1 have you worried. Fox News runs a scary looking graphic, and charts like the one below only add to you anxiety:\n\n\n\n\n\n\nOr even worse, this version from everytown.org:\n\n\n\n\n\n\nBut then you remember that (1) this is a hypothetical exercise; (2) you’ll take literally any job at this point; and (3) Geographic diversity matters – the United States is a large and diverse country with 50 very different states (plus the District of Columbia and some lovely territories).2\n\n\n\n\n\nCalifornia, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial.\n\n\n\nThis is a relatively simple and straightforward problem in social science: you have options of where to live, and want to determine the safety of the various states. Your “research” is clearly policy-relevant: you will eventually have to live somewhere. In this course, we will begin to tackle the problem by examining data related to gun homicides in the US during 2010 using R as a motivating example along the way.\nBefore we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Ideally, this is a refresher. However, we are aware that your preparation in previously courses varies greatly from student to student. Moreover, we want you to be aware that the usefulness of some of these early building blocks may not be immediately obvious. Later in the class you will appreciate having these skills. Mastery will be rewarded both in this class and (of course) in life.\n\nThe Pre-Basics\nWe’ve now covered a short bit of material. The remainder of this first lecture will be covering setting up R and describing some common errors."
  },
  {
    "objectID": "content/Week_01/01b.html#installing-r-and-r-studio-posit-and-review-resources",
    "href": "content/Week_01/01b.html#installing-r-and-r-studio-posit-and-review-resources",
    "title": "Working with R and RStudio",
    "section": "Installing R and R Studio Posit and Review Resources",
    "text": "Installing R and R Studio Posit and Review Resources\nBoth R and RStudio Posit are free and available online. If you have not yet done so, you’ll need to install both R and RStudio Posit. See the Installing page of our course resources for instructions. This will be part of your assignment for this week.\nProfessor Kirkpatrick (the other instructor for this course) has created a video walkthrough for the basics of using R for another course, but it is useful here. You can see part A here (labeled “Part 2a”) here ] and part B here (labeled “Part 2b”) . You should already be at this level of familiarity with R, but if you need a review, this is a good place to start."
  },
  {
    "objectID": "content/Week_01/01b.html#the-very-basics-of-r",
    "href": "content/Week_01/01b.html#the-very-basics-of-r",
    "title": "Working with R and RStudio",
    "section": "The (very) basics of R",
    "text": "The (very) basics of R\nBefore we get started with the motivating dataset, we need to cover the very basics of R.\n\nObjects\nSuppose a relatively math unsavvy student asks us for help solving several quadratic equations of the form \\(ax^2+bx+c = 0\\). You—a savvy student—recall that the quadratic formula gives us the solutions:\n\\[\n  \\frac{-b + \\sqrt{b^2 - 4ac}}{2a}\\,\\, \\mbox{ and } \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\n\\]\nwhich of course depend on the values of \\(a\\), \\(b\\), and \\(c\\). That is, the quadratic equation represents a function with three arguments.\nOne advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve \\(x^2 + x -1 = 0\\), then we define:\n\na <- 1\nb <- 1\nc <- -1\n\nwhich stores the values for later use. We use <- to assign values to the variables.\nWe can also assign values using = instead of <-, but some recommend against using = to avoid confusion.4\n\nTRY IT\nCopy and paste the code above into your console to define the three variables. Note that R does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message. Throughout these written notes, you’ll have the most success if you continue to copy code into your own console.\n\nTo see the value stored in a variable, we simply ask R to evaluate a and it shows the stored value:\n\na\n\n[1] 1\n\n\nA more explicit way to ask R to show us the value stored in a is using print like this:\n\nprint(a)\n\n[1] 1\n\n\nWe use the term object to describe stuff that is stored in R. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.\n\n\nThe workspace\nAs we define objects in the console, we are actually changing the workspace. You can see all the variables saved in your workspace by typing:\n\nls()\n\n[1] \"a\"         \"b\"         \"BASE\"      \"c\"         \"GIS.files\" \"jLoad\"    \n\n\n(Note that one of my variables listed above comes from generating the graphs above). In RStudio Posit, the Environment tab shows the values:\n\nWe should see a, b, and c. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type x you will receive the following message: Error: object 'x' not found.\nNow since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:\n\n(-b + sqrt(b^2 - 4*a*c) ) / ( 2*a )\n\n[1] 0.618034\n\n(-b - sqrt(b^2 - 4*a*c) ) / ( 2*a )\n\n[1] -1.618034\n\n\n\n\nFunctions\nOnce you define variables, the data analysis process can usually be described as a series of functions applied to the data. R includes several zillion predefined functions and most of the analysis pipelines we construct make extensive use of the built-in functions. But R’s power comes from its scalability. We have access to (nearly) infinite functions via install.packages and library. As we go through the course, we will carefully note new functions we bring to each problem. For now, though, we will stick to the basics.\nNote that you’ve used a function already: you used the function sqrt to solve the quadratic equation above. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.\nIn general, we need to use parentheses to evaluate a function. If you type ls, the function is not evaluated and instead R shows you the code that defines the function. If you type ls() the function is evaluated and, as seen above, we see objects in the workspace.\nUnlike ls, most functions require one or more arguments. Below is an example of how we assign an object to the argument of the function log. Remember that we earlier defined a to be 1:\n\nlog(8)\n\n[1] 2.079442\n\nlog(a)\n\n[1] 0\n\n\nYou can find out what the function expects and what it does by reviewing the very useful manuals included in R. You can get help by using the help function like this:\n\nhelp(\"log\")\n\nFor most functions, we can also use this shorthand:\n\n?log\n\nThe help page will show you what arguments the function is expecting. For example, log needs x and base to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with =. Defining these is optional.5 For example, the base of the function log defaults to base = exp(1)—that is, log evaluates the natural log by default.\nIf you want a quick look at the arguments without opening the help system, you can type:\n\nargs(log)\n\nfunction (x, base = exp(1)) \nNULL\n\n\nYou can change the default values by simply assigning another object:\n\nlog(8, base = 2)\n\n[1] 3\n\n\nNote that we have not been specifying the argument x as such:\n\nlog(x = 8, base = 2)\n\n[1] 3\n\n\nThe above code works, but we can save ourselves some typing: if no argument name is used, R assumes you are entering arguments in the order shown in the help file or by args. So by not using the names, it assumes the arguments are x followed by base:\n\nlog(8,2)\n\n[1] 3\n\n\nIf using the arguments’ names, then we can include them in whatever order we want:\n\nlog(base = 2, x = 8)\n\n[1] 3\n\n\nTo specify arguments, we must use =, and cannot use <-.\nThere are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:\n\n2 ^ 3\n\n[1] 8\n\n\nYou can see the arithmetic operators by typing:\n\nhelp(\"+\")\n\nor\n\n?\"+\"\n\nand the relational operators by typing:\n\nhelp(\">\")\n\nor\n\n?\">\"\n\n\n\nOther prebuilt objects\nThere are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:\n\ndata()\n\nThis shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:\n\nco2\n\nR will show you Mauna Loa atmospheric \\(CO^2\\) concentration data.\nOther prebuilt objects are mathematical quantities, such as the constant \\(\\pi\\) and \\(\\infty\\):\n\npi\n\n[1] 3.141593\n\nInf+1\n\n[1] Inf\n\n\n\n\nVariable names\nWe have used the letters a, b, and c as variable names, but variable names can be almost anything. Some basic rules in R are that variable names have to start with a letter, can’t contain spaces, and should not be variables that are predefined in R. For example, don’t name one of your variables install.packages by typing something like install.packages <- 2. Usually, R is smart enough to prevent you from doing such nonsense, but it’s important to develop good habits.\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:\n\nsolution_1 <- (-b + sqrt(b^2 - 4*a*c)) / (2*a)\nsolution_2 <- (-b - sqrt(b^2 - 4*a*c)) / (2*a)\n\nFor more advice, we highly recommend studying (Hadley Wickham’s style guide)[http://adv-r.had.co.nz/Style.html].\n\n\nSaving your workspace\nValues remain in the workspace until you end your session or erase them with the function rm. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.\nWe actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function save or save.image. To load, use the function load. When saving a workspace, we recommend the suffix rda or RData. In RStudio, you can also do this by navigating to the Session tab and choosing Save Workspace as. You can later load it using the Load Workspace options in the same tab. You can read the help pages on save, save.image, and load to learn more.\n\n\nMotivating scripts\nTo solve another equation such as \\(3x^2 + 2x -1\\), we can copy and paste the code above and then redefine the variables and recompute the solution:\n\na <- 3\nb <- 2\nc <- -1\n(-b + sqrt(b^2 - 4*a*c)) / (2*a)\n(-b - sqrt(b^2 - 4*a*c)) / (2*a)\n\nBy creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.\nThe answer you get from the 4th and 5th lines will depend on the values of a, b, and c. If you were to type new numbers directly into your console: c = 5.33 then re-run the last two lines, you will get a different answer. Your R “environment” is affected by what is run from a script and by what you type in the console. It is good (and necessary) practice to write all your code in a script (or in your Rmarkdown document), and run from the script. Always. Periodically running a script fresh from the start (clearing everything out of the environment first) is a good idea as well.\n\n\nCommenting your code\nIf a line of R code starts with the symbol #, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:\n\n## Code to compute solution to quadratic equation of the form ax^2 + bx + c\n## define the variables\na <- 3\nb <- 2\nc <- -1\n\n## now compute the solution\n(-b + sqrt(b^2 - 4*a*c)) / (2*a)\n(-b - sqrt(b^2 - 4*a*c)) / (2*a)\n\n\nTRY IT\n\nWhat is the sum of the first 100 positive integers? The formula for the sum of integers \\(1\\) through \\(n\\) is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum?\nNow use the same formula to compute the sum of the integers from 1 through 1,000.\nLook at the result of typing the following code into R:\n\n\nn <- 1000\nx <- seq(1, n)\nsum(x)\n\nBased on the result, what do you think the functions seq and sum do? You can use help.\n\nsum creates a list of numbers and seq adds them up.\nseq creates a list of numbers and sum adds them up.\nseq creates a random list and sum computes the sum of 1 through 1,000.\nsum always returns the same number.\n\n\nIn math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.\nWhich of the following will always return the numeric value stored in x? You can try out examples and use the help system if you want.\n\n\nlog(10^x)\nlog10(x^10)\nlog(exp(x))\nexp(log(x, base = 2))\n\n\n## Data types\nVariables in R can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function class helps us determine what type of object we have:\n\na <- 2\nclass(a)\n\n[1] \"numeric\"\n\n\nTo work efficiently in R, it is important to learn the different types of variables and what we can do with these.\n\n\nData frames\nUp to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in R is in a data frame. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.\nA large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the dslabs library and loading the murders dataset using the data function:\n\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.3.3\n\n\n\nAttaching package: 'dslabs'\n\n\nThe following object is masked from 'package:gapminder':\n\n    gapminder\n\ndata(murders)\n\nTo see that this is in fact a data frame, we type:\n\nclass(murders)\n\n[1] \"data.frame\"\n\n\n\n\nExamining an object\nThe function str is useful for finding out more about the structure of an object:\n\n\n\n\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n$ state : chr \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n$ abb : chr \"AL\" \"AK\" \"AZ\" \"AR\" ...\n$ region : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n$ population: num 4779736 710231 6392017 2915918 37253956 ...\n$ total : num 135 19 232 93 1257 ...\n\n\n\n\n\nThis tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function head:\n\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nIn this dataset, each state is considered an observation and five variables are reported for each state.\nBefore we go any further in answering our original question about different states, let’s learn more about the components of this object.\n\n\nThe accessor: $\nFor our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator $ in the following way:\n\nmurders$population\n\n [1]  4779736   710231  6392017  2915918 37253956  5029196  3574097   897934\n [9]   601723 19687653  9920000  1360301  1567582 12830632  6483802  3046355\n[17]  2853118  4339367  4533372  1328361  5773552  6547629  9883640  5303925\n[25]  2967297  5988927   989415  1826341  2700551  1316470  8791894  2059179\n[33] 19378102  9535483   672591 11536504  3751351  3831074 12702379  1052567\n[41]  4625364   814180  6346105 25145561  2763885   625741  8001024  6724540\n[49]  1852994  5686986   563626\n\n\nBut how did we know to use population? Previously, by applying the function str to the object murders, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:\n\nnames(murders)\n\n[1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"     \n\n\nIt is important to know that the order of the entries in murders$population preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.\nTip: R comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing murders$p then hitting the tab key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.\n\n\nVectors: numerics, characters, and logical\nThe object murders$population is not one number but several. We call these types of objects vectors. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function length tells you how many entries are in the vector:\n\npop <- murders$population\nlength(pop)\n\n[1] 51\n\n\nThis particular vector is numeric since population sizes are numbers:\n\nclass(pop)\n\n[1] \"numeric\"\n\n\nIn a numeric vector, every entry must be a number.\nTo store character strings, vectors can also be of class character. For example, the state names are characters:\n\nclass(murders$state)\n\n[1] \"character\"\n\n\nAs with numeric vectors, all entries in a character vector need to be a character.\nAnother important type of vectors are logical vectors. These must be either TRUE or FALSE.\n\nz <- 3 == 2\nz\n\n[1] FALSE\n\nclass(z)\n\n[1] \"logical\"\n\n\nHere the == is a relational operator asking if 3 is equal to 2. In R, if you just use one =, you actually assign a variable, but if you use two == you test for equality. Yet another reason to avoid assigning via =… it can get confusing and typos can really mess things up.\nYou can see the other relational operators by typing:\n\n?Comparison\n\nIn future sections, you will see how useful relational operators can be.\nWe discuss more important features of vectors after the next set of exercises.\nAdvanced: Mathematically, the values in pop are integers and there is an integer class in R. However, by default, numbers are assigned class numeric even when they are round integers. For example, class(1) returns numeric. You can turn them into class integer with the as.integer() function or by adding an L like this: 1L. Note the class by typing: class(1L)\n\n\nFactors\nIn the murders dataset, we might expect the region to also be a character vector. However, it is not:\n\nclass(murders$region)\n\n[1] \"factor\"\n\n\nIt is a factor. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the levels function:\n\nlevels(murders$region)\n\n[1] \"Northeast\"     \"South\"         \"North Central\" \"West\"         \n\n\nIn the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters. It is also useful for computational reasons we’ll explore later.\nNote that the levels have an order that is different from the order of appearance in the factor object. The default in R is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the levels argument when creating the factor with the factor function. For example, in the murders dataset regions are ordered from east to west. The function reorder lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.\nSuppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the reorder and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.\n\nregion <- murders$region\nvalue <- murders$total\nregion <- reorder(region, value, FUN = sum)\nlevels(region)\n\n[1] \"Northeast\"     \"North Central\" \"West\"          \"South\"        \n\n\nThe new order is in agreement with the fact that the Northeast has the least murders and the South has the most.\nWarning: Factors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.\n\n\nLists\nData frames are a special case of lists. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you:\n\n\n\n\nrecord\n\n$name\n[1] \"John Doe\"\n\n$student_id\n[1] 1234\n\n$grades\n[1] 95 82 91 97 93\n\n$final_grade\n[1] \"A\"\n\nclass(record)\n\n[1] \"list\"\n\n\nAs with data frames, you can extract the components of a list with the accessor $. In fact, data frames are a type of list.\n\nrecord$student_id\n\n[1] 1234\n\n\nWe can also use double square brackets ([[) like this:\n\nrecord[[\"student_id\"]]\n\n[1] 1234\n\n\nYou should get used to the fact that in R there are often several ways to do the same thing. such as accessing entries.6\nYou might also encounter lists without variable names.\n\n\n\n\nrecord2\n\n[[1]]\n[1] \"John Doe\"\n\n[[2]]\n[1] 1234\n\n\nIf a list does not have names, you cannot extract the elements with $, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:\n\nrecord2[[1]]\n\n[1] \"John Doe\"\n\n\nWe won’t be using lists until later, but you might encounter one in your own exploration of R. For this reason, we show you some basics here.\n\n\nMatrices\nMatrices are another type of object that are common in R. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.\nYet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this class, but much of what happens in the background when you perform a data analysis involves matrices. We describe them briefly here since some of the functions we will learn return matrices.\nWe can define a matrix using the matrix function. We need to specify the number of rows and columns.\n\nmat <- matrix(1:12, 4, 3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\nYou can access specific entries in a matrix using square brackets ([). If you want the second row, third column, you use:\n\nmat[2, 3]\n\n[1] 10\n\n\nIf you want the entire second row, you leave the column spot empty:\n\nmat[2, ]\n\n[1]  2  6 10\n\n\nNotice that this returns a vector, not a matrix.\nSimilarly, if you want the entire third column, you leave the row spot empty:\n\nmat[, 3]\n\n[1]  9 10 11 12\n\n\nThis is also a vector, not a matrix.\nYou can access more than one column or more than one row if you like. This will give you a new matrix.\n\nmat[, 2:3]\n\n     [,1] [,2]\n[1,]    5    9\n[2,]    6   10\n[3,]    7   11\n[4,]    8   12\n\n\nYou can subset both rows and columns:\n\nmat[1:2, 2:3]\n\n     [,1] [,2]\n[1,]    5    9\n[2,]    6   10\n\n\nWe can convert matrices into data frames using the function as.data.frame:\n\nas.data.frame(mat)\n\n  V1 V2 V3\n1  1  5  9\n2  2  6 10\n3  3  7 11\n4  4  8 12\n\n\nYou can also use single square brackets ([) to access rows and columns of a data frame:\n\ndata(\"murders\")\nmurders[25, 1]\n\n[1] \"Mississippi\"\n\nmurders[2:3, ]\n\n    state abb region population total\n2  Alaska  AK   West     710231    19\n3 Arizona  AZ   West    6392017   232\n\n\n\nTRY IT\n\nLoad the US murders dataset.\n\n\nlibrary(dslabs)\ndata(murders)\n\nUse the function str to examine the structure of the murders object. Which of the following best describes the variables represented in this data frame?\n\nThe 51 states.\nThe murder rates for all 50 states and DC.\nThe state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010.\nstr shows no relevant information.\n\n\nWhat are the column names used by the data frame for these five variables?\nUse the accessor $ to extract the state abbreviations and assign them to the object a. What is the class of this object?\nNow use the square brackets to extract the state abbreviations and assign them to the object b. Use the identical function to determine if a and b are the same.\nWe saw that the region column stores a factor. You can corroborate this by typing:\n\n\nclass(murders$region)\n\nWith one line of code, use the function levels and length to determine the number of regions defined by this dataset.\n\nThe function table takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region."
  },
  {
    "objectID": "content/Week_01/01b.html#vectors",
    "href": "content/Week_01/01b.html#vectors",
    "title": "Working with R and RStudio",
    "section": "Vectors",
    "text": "Vectors\nIn R, the most basic objects available to store data are vectors. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.\n\nCreating vectors\nWe can create vectors using the function c, which stands for concatenate. We use c to concatenate entries in the following way:\n\ncodes <- c(380, 124, 818)\ncodes\n\n[1] 380 124 818\n\n\nWe can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.\n\ncountry <- c(\"italy\", \"canada\", \"egypt\")\n\nIn R you can also use single quotes:\n\ncountry <- c('italy', 'canada', 'egypt')\n\nBut be careful not to confuse the single quote ’ with the back quote, which shares a keyboard key with ~.\nBy now you should know that if you type:\n\ncountry <- c(italy, canada, egypt)\n\nyou receive an error because the variables italy, canada, and egypt are not defined. If we do not use the quotes, R looks for variables with those names and returns an error.\n\n\nNames\nSometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:\n\ncodes <- c(italy = 380, canada = 124, egypt = 818)\ncodes\n\n italy canada  egypt \n   380    124    818 \n\n\nThe object codes continues to be a numeric vector:\n\nclass(codes)\n\n[1] \"numeric\"\n\n\nbut with names:\n\nnames(codes)\n\n[1] \"italy\"  \"canada\" \"egypt\" \n\n\nIf the use of strings without quotes looks confusing, know that you can use the quotes as well:\n\ncodes <- c(\"italy\" = 380, \"canada\" = 124, \"egypt\" = 818)\ncodes\n\n italy canada  egypt \n   380    124    818 \n\n\nThere is no difference between this function call and the previous one. This is one of the many ways in which R is quirky compared to other languages.\n\n\nSequences\nAnother useful function for creating vectors generates sequences:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nThe first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:\n\nseq(1, 10, 2)\n\n[1] 1 3 5 7 9\n\n\nIf we want consecutive integers, we can use the following shorthand:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWhen we use these functions, R produces integers, not numerics, because they are typically used to index something:\n\nclass(1:10)\n\n[1] \"integer\"\n\n\nHowever, if we create a sequence including non-integers, the class changes:\n\nclass(seq(1, 10, 0.5))\n\n[1] \"numeric\"\n\n\n\n\nSubsetting\nWe use square brackets to access specific elements of a vector. For the vector codes we defined above, we can access the second element using:\n\ncodes[2]\n\ncanada \n   124 \n\n\nYou can get more than one entry by using a multi-entry vector as an index:\n\ncodes[c(1,3)]\n\nitaly egypt \n  380   818 \n\n\nThe sequences defined above are particularly useful if we want to access, say, the first two elements:\n\ncodes[1:2]\n\n italy canada \n   380    124 \n\n\nIf the elements have names, we can also access the entries using these names. Below are two examples.\n\ncodes[\"canada\"]\n\ncanada \n   124 \n\ncodes[c(\"egypt\",\"italy\")]\n\negypt italy \n  818   380"
  },
  {
    "objectID": "content/Week_01/01b.html#coercion",
    "href": "content/Week_01/01b.html#coercion",
    "title": "Working with R and RStudio",
    "section": "Coercion",
    "text": "Coercion\nIn general, coercion is an attempt by R to be flexible with data types. When an entry does not match the expected, some of the prebuilt R functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand coercion can drive programmers crazy when attempting to code in R since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples.\nWe said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:\n\nx <- c(1, \"canada\", 3)\n\nBut we don’t get one, not even a warning! What happened? Look at x and its class:\n\nx\n\n[1] \"1\"      \"canada\" \"3\"     \n\nclass(x)\n\n[1] \"character\"\n\n\nR coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings \"1\" and “3”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in R.\nR also offers functions to change from one type to another. For example, you can turn numbers into characters with:\n\nx <- 1:5\ny <- as.character(x)\ny\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\nYou can turn it back with as.numeric:\n\nas.numeric(y)\n\n[1] 1 2 3 4 5\n\n\nThis function is actually quite useful since datasets that include numbers as character strings are common.\n\nNot availables (NA)\nThis “topic” seems to be wholly unappreciated and it has been our experience that students often panic when encountering an NA. This often happens when a function tries to coerce one type to another and encounters an impossible case. In such circumstances, R usually gives us a warning and turns the entry into a special value called an NA (for “not available”). For example:\n\nx <- c(\"1\", \"b\", \"3\")\nas.numeric(x)\n\nWarning: NAs introduced by coercion\n\n\n[1]  1 NA  3\n\n\nR does not have any guesses for what number you want when you type b, so it does not try.\nWhile coercion is a common case leading to NAs, you’ll see them in nearly every real-world dataset. Most often, you will encounter the NAs as a stand-in for missing data. Again, this a common problem in real-world datasets and you need to be aware that it will come up."
  },
  {
    "objectID": "content/Week_01/01b.html#sorting",
    "href": "content/Week_01/01b.html#sorting",
    "title": "Working with R and RStudio",
    "section": "Sorting",
    "text": "Sorting\nNow that we have mastered some basic R knowledge (ha!), let’s try to gain some insights into the safety of different states in the context of gun murders.\n\nsort\nSay we want to rank the states from least to most gun murders. The function sort sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:\n\nlibrary(dslabs)\ndata(murders)\nsort(murders$total)\n\n [1]    2    4    5    5    7    8   11   12   12   16   19   21   22   27   32\n[16]   36   38   53   63   65   67   84   93   93   97   97   99  111  116  118\n[31]  120  135  142  207  219  232  246  250  286  293  310  321  351  364  376\n[46]  413  457  517  669  805 1257\n\n\nHowever, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257.\n\n\norder\nThe function order is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it:\n\nx <- c(31, 4, 15, 92, 65)\nsort(x)\n\n[1]  4 15 31 65 92\n\n\nRather than sort the input vector, the function order returns the index that sorts input vector:\n\nindex <- order(x)\nx[index]\n\n[1]  4 15 31 65 92\n\n\nThis is the same output as that returned by sort(x). If we look at this index, we see why it works:\n\nx\n\n[1] 31  4 15 92 65\n\norder(x)\n\n[1] 2 3 1 5 4\n\n\nThe second entry of x is the smallest, so order(x) starts with 2. The next smallest is the third entry, so the second entry is 3 and so on.\nHow does this help us order the states by murders? First, remember that the entries of vectors you access with $ follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:\n\nmurders$state[1:6]\n\n[1] \"Alabama\"    \"Alaska\"     \"Arizona\"    \"Arkansas\"   \"California\"\n[6] \"Colorado\"  \n\nmurders$abb[1:6]\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\"\n\n\nThis means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:\n\nind <- order(murders$total)\nmurders$abb[ind]\n\n [1] \"VT\" \"ND\" \"NH\" \"WY\" \"HI\" \"SD\" \"ME\" \"ID\" \"MT\" \"RI\" \"AK\" \"IA\" \"UT\" \"WV\" \"NE\"\n[16] \"OR\" \"DE\" \"MN\" \"KS\" \"CO\" \"NM\" \"NV\" \"AR\" \"WA\" \"CT\" \"WI\" \"DC\" \"OK\" \"KY\" \"MA\"\n[31] \"MS\" \"AL\" \"IN\" \"SC\" \"TN\" \"AZ\" \"NJ\" \"VA\" \"NC\" \"MD\" \"OH\" \"MO\" \"LA\" \"IL\" \"GA\"\n[46] \"MI\" \"PA\" \"NY\" \"FL\" \"TX\" \"CA\"\n\n\nAccording to the above, California had the most murders.\n\n\nmax and which.max\nIf we are only interested in the entry with the largest value, we can use max for the value:\n\nmax(murders$total)\n\n[1] 1257\n\n\nand which.max for the index of the largest value:\n\ni_max <- which.max(murders$total)\nmurders$state[i_max]\n\n[1] \"California\"\n\n\nFor the minimum, we can use min and which.min in the same way.\nDoes this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: rank.\n\n\nrank\nAlthough not as frequently used as order and sort, the function rank is also related to order and can be useful. For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:\n\nx <- c(31, 4, 15, 92, 65)\nrank(x)\n\n[1] 3 1 2 5 4\n\n\nTo summarize, let’s look at the results of the three functions we have introduced:\n\n\n\n\n \n  \n    original \n    sort \n    order \n    rank \n  \n \n\n  \n    31 \n    4 \n    2 \n    3 \n  \n  \n    4 \n    15 \n    3 \n    1 \n  \n  \n    15 \n    31 \n    1 \n    2 \n  \n  \n    92 \n    65 \n    5 \n    5 \n  \n  \n    65 \n    92 \n    4 \n    4 \n  \n\n\n\n\n\n\n\nBeware of recycling\nAnother common source of unnoticed errors in R is the use of recycling. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens:\n\nx <- c(1,2,3)\ny <- c(10, 20, 30, 40, 50, 60, 70)\nx+y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 11 22 33 41 52 63 71\n\n\nWe do get a warning, but no error. For the output, R has recycled the numbers in x. Notice the last digit of numbers in the output.\n\nTRY IT\nFor these exercises we will use the US murders dataset. Make sure you load it prior to starting.\n\nlibrary(dslabs)\ndata(\"murders\")\n\n\nUse the $ operator to access the population size data and store it as the object pop. Then use the sort function to redefine pop so that it is sorted. Finally, use the [ operator to report the smallest population size.\nNow instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use order instead of sort.\nWe can actually perform the same operation as in the previous exercise using the function which.min. Write one line of code that does this.\nNow we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable states to be the state names from the murders data frame. Report the name of the state with the smallest population.\nYou can create a data frame using the data.frame function. Here is a quick example:\n\n\ntemp <- c(35, 88, 42, 84, 81, 30)\ncity <- c(\"Beijing\", \"Lagos\", \"Paris\", \"Rio de Janeiro\",\n          \"San Juan\", \"Toronto\")\ncity_temps <- data.frame(name = city, temperature = temp)\n\nUse the rank function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called ranks, then create a data frame with the state name and its rank. Call the data frame my_df.\n\nRepeat the previous exercise, but this time order my_df so that the states are ordered from least populous to most populous. Hint: create an object ind that stores the indexes needed to order the population values. Then use the bracket operator [ to re-order each column in the data frame.\nThe na_example vector represents a series of counts. You can quickly examine the object using:\n\n\ndata(\"na_example\")\nstr(na_example)\n\n int [1:1000] 2 1 3 2 1 3 1 4 3 2 ...\n\n\nHowever, when we compute the average with the function mean, we obtain an NA:\n\nmean(na_example)\n\n[1] NA\n\n\nThe is.na function returns a logical vector that tells us which entries are NA. Assign this logical vector to an object called ind and determine how many NAs does na_example have.\n\nNow compute the average again, but only for the entries that are not NA. Hint: remember the ! operator."
  },
  {
    "objectID": "content/Week_01/01b.html#vector-arithmetics",
    "href": "content/Week_01/01b.html#vector-arithmetics",
    "title": "Working with R and RStudio",
    "section": "Vector arithmetics",
    "text": "Vector arithmetics\nCalifornia had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:\n\nlibrary(dslabs)\ndata(\"murders\")\nmurders$state[which.max(murders$population)]\n\n[1] \"California\"\n\n\nwith over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of R come in handy.\n\nRescaling a vector\nIn R, arithmetic operations on vectors occur element-wise. For a quick example, suppose we have height in inches:\n\ninches <- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\n\nand want to convert to centimeters. Notice what happens when we multiply inches by 2.54:\n\ninches * 2.54\n\n [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18 177.80\n\n\nIn the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:\n\ninches - 69\n\n [1]  0 -7 -3  1  1  4 -2  4 -2  1\n\n\n\n\nTwo vectors\nIf we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:\n\\[\n  \\begin{pmatrix}\na\\\\\nb\\\\\nc\\\\\nd\n\\end{pmatrix}\n+\n  \\begin{pmatrix}\ne\\\\\nf\\\\\ng\\\\\nh\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\na +e\\\\\nb + f\\\\\nc + g\\\\\nd + h\n\\end{pmatrix}\n\\]\nThe same holds for other mathematical operations, such as -, * and /.\nThis implies that to compute the murder rates we can simply type:\n\nmurder_rate <- murders$total / murders$population * 100000\n\nOnce we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:\n\nmurders$abb[order(murder_rate)]\n\n [1] \"VT\" \"NH\" \"HI\" \"ND\" \"IA\" \"ID\" \"UT\" \"ME\" \"WY\" \"OR\" \"SD\" \"MN\" \"MT\" \"CO\" \"WA\"\n[16] \"WV\" \"RI\" \"WI\" \"NE\" \"MA\" \"IN\" \"KS\" \"NY\" \"KY\" \"AK\" \"OH\" \"CT\" \"NJ\" \"AL\" \"IL\"\n[31] \"OK\" \"NC\" \"NV\" \"VA\" \"AR\" \"TX\" \"NM\" \"CA\" \"FL\" \"TN\" \"PA\" \"AZ\" \"GA\" \"MS\" \"MI\"\n[46] \"DE\" \"SC\" \"MD\" \"MO\" \"LA\" \"DC\"\n\n\n\nTRY IT\n\nPreviously we created this data frame:\n\n\ntemp <- c(35, 88, 42, 84, 81, 30)\ncity <- c(\"Beijing\", \"Lagos\", \"Paris\", \"Rio de Janeiro\",\n          \"San Juan\", \"Toronto\")\ncity_temps <- data.frame(name = city, temperature = temp)\n\nRemake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is \\(C = \\frac{5}{9} \\times (F - 32)\\).\n\nWrite code to compute the following sum \\(1+1/2^2 + 1/3^2 + \\dots 1/100^2\\)? Hint: thanks to Euler, we know it should be close to \\(\\pi^2/6\\).\nCompute the per 100,000 murder rate for each state and store it in the object murder_rate. Then compute the average murder rate for the US using the function mean. What is the average?"
  },
  {
    "objectID": "content/Week_01/01b.html#indexing",
    "href": "content/Week_01/01b.html#indexing",
    "title": "Working with R and RStudio",
    "section": "Indexing",
    "text": "Indexing\nIndexing is a boring name for an important tool. R provides a powerful and convenient way of referencing specific elements of vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this:\n\nlibrary(dslabs)\ndata(\"murders\")\n\n\nSubsetting with logicals\nWe have now calculated the murder rate using:\n\nmurder_rate <- murders$total / murders$population * 100000\n\nImagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of R is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:\n\nind <- murder_rate < 0.71\n\nIf we instead want to know if a value is less or equal, we can use:\n\nind <- murder_rate <= 0.71\n\nNote that we get back a logical vector with TRUE for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.\n\nmurders$state[ind]\n\n[1] \"Hawaii\"        \"Iowa\"          \"New Hampshire\" \"North Dakota\" \n[5] \"Vermont\"      \n\n\nIn order to count how many are TRUE, the function sum returns the sum of the entries of a vector and logical vectors get coerced to numeric with TRUE coded as 1 and FALSE as 0. Thus we can count the states using:\n\nsum(ind)\n\n[1] 5\n\n\n\n\nLogical operators\nSuppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator and, which in R is represented with &. This operation results in TRUE only when both logicals are TRUE. To see this, consider this example:\n\nTRUE & TRUE\n\n[1] TRUE\n\nTRUE & FALSE\n\n[1] FALSE\n\nFALSE & FALSE\n\n[1] FALSE\n\n\nFor our example, we can form two logicals:\n\nwest <- murders$region == \"West\"\nsafe <- murder_rate <= 1\n\nand we can use the & to get a vector of logicals that tells us which states satisfy both conditions:\n\nind <- safe & west\nmurders$state[ind]\n\n[1] \"Hawaii\"  \"Idaho\"   \"Oregon\"  \"Utah\"    \"Wyoming\"\n\n\n\n\nwhich\nSuppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type:\n\nind <- which(murders$state == \"California\")\nmurder_rate[ind]\n\n[1] 3.374138\n\n\n\n\n%in%\nIf rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let’s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:\n\nc(\"Boston\", \"Dakota\", \"Washington\") %in% murders$state\n\n[1] FALSE FALSE  TRUE\n\n\nNote that we will be using %in% often throughout the course\n\nTRY IT\nStart by loading the library and data.\n\nlibrary(dslabs)\ndata(murders)\n\n\nCompute the per 100,000 murder rate for each state and store it in an object called murder_rate. Then use logical operators to create a logical vector named low that tells us which entries of murder_rate are lower than 1.\nNow use the results from the previous exercise and the function which to determine the indices of murder_rate associated with values lower than 1.\nUse the results from the previous exercise to report the names of the states with murder rates lower than 1.\nNow extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector low and the logical operator &.\nIn a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?\nUse the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of murders$abb that match the three abbreviations, then use the [ operator to extract the states.\nUse the %in% operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\nExtend the code you used in exercise 7 to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index."
  },
  {
    "objectID": "content/Week_01/01b.html#further-help-with-r",
    "href": "content/Week_01/01b.html#further-help-with-r",
    "title": "Working with R and RStudio",
    "section": "Further help with R",
    "text": "Further help with R\nIf you are not comfortable with R, the earlier you seek out help, the better. Quietly letting the course pass by you because you don’t know how to fix an error will do nobody any good. Attend TA office hours or attend TA or Prof. K’s office hours see Syllabus for times and Zoom links. Also, join the course Slack (see the front page of our course website for a link) and post questions there.\nFinally, there are also primers on Rstudio.cloud that can be useful. There are many ways we can help you get used to R, but only if you reach out."
  },
  {
    "objectID": "content/Week_02/02a.html",
    "href": "content/Week_02/02a.html",
    "title": "Introduction to the tidyverse",
    "section": "",
    "text": "This page.\nChapter 1 of Introduction to Statistical Learning, available here.\nOptional: The “Tidy Your Data” tutorial on Rstudio Cloud Primers"
  },
  {
    "objectID": "content/Week_02/02a.html#some-reminders",
    "href": "content/Week_02/02a.html#some-reminders",
    "title": "Introduction to the tidyverse",
    "section": "Some Reminders:",
    "text": "Some Reminders:\n\nStart labs early!\n\nThey are not trivial.\nThey are not short.\nThey are not easy.\nThey are not optional.\n\nYou install.packages(\"packageName\") once on your computer.\n\nAnd never ever ever in your code.\n\nYou load an already-installed package using library(packageName) in a code chunk\n\nNever in your console\nWhen RMarkdown knits, it starts a whole new, empty session that has no knowledge of what you typed into the console\n\nSlack\n\nUse it.\nI would very much prefer posting in the class-visible channels. Others can learn from your issues.\n\nWe have a channel just for labs and R. Please use that one.\n\n\n\n\nGroup Projects\nYour final is a group project. You will also have two “mini” projects. They comprise a large part of your grade. As mentioned last week, this mean that you need to start planning soon.\nTo aid in your planning, here are the required elements of your final project.\n\nYou must find existing data to analyze. Aggregating and merging data from multiple sources is encouraged.\nYou must visualize 3 interesting features of that data.\nYou must come up with some analysis—using tools from this course—which relates your data to either a prediction or a policy conclusion.\nYou must think critically about your analysis and be able to identify potential issues.\nYou must present your analysis as if presenting to a C-suite executive.\n\nYour mini-projects along the way will be more structured, but will serve to guide you towards the final project.\n\n\nTeams\nPlease form teams of 3 people. Once all agree to be on a team, have ONE PERSON email our TA Allen scovelpa@msu.edu and cc all of the members of the team so that nobody is surprised to be included on a team. Title the email [SSC442] - Group Formation. Tell us your team name (be creative), and list in the email the names of all of the team members and their email address (in addition to cc-ing those team members on the email).\nIf you opt to not form a team, you will be automatically added to the “willing to be randomly assigned” pool and will be paired with two others from the “willing to be randomly assigned” pool.\nSend this email by January 20th and we will assign un-teamed folks at the beginning of the following week. Project 1 is due in no time. See schedule for all the important project dates.\n\n\nGuiding Question\nFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\nWhy do we want tidy data?\nWhat are the challenges associated with shaping things into a tidy format?"
  },
  {
    "objectID": "content/Week_02/02a.html#tidy-data",
    "href": "content/Week_02/02a.html#tidy-data",
    "title": "Introduction to the tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\n\nWe say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\n\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nEach row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.\nTo see how the same information can be provided in different formats, consider the following example:\n\nlibrary(dslabs)\ndata(\"gapminder\") # gapminder will now be a data.frame in your \"environment\" (memory)\ntidy_data <- gapminder %>%\n  filter(country %in% c(\"South Korea\", \"Germany\") & !is.na(fertility)) %>%\n  select(country, year, fertility)\nhead(tidy_data, 6)\n\n      country year fertility\n1     Germany 1960      2.41\n2 South Korea 1960      6.16\n3     Germany 1961      2.44\n4 South Korea 1961      5.99\n5     Germany 1962      2.47\n6 South Korea 1962      5.79\n\n\nThis tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the dslabs package. Originally, the data was in the following format:\n\n\n      country 1960 1961 1962\n1     Germany 2.41 2.44 2.47\n2 South Korea 6.16 5.99 5.79\n\n\nThe same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables’ values, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into tidy format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.\nAlthough not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.\n\nTRY IT\n\nExamine the built-in dataset co2. Which of the following is true:\n\n\nco2 is tidy data: it has one year for each row.\nco2 is not tidy: we need at least one column with a character vector.\nco2 is not tidy: it is a matrix instead of a data frame.\nco2 is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.\n\n\nExamine the built-in dataset ChickWeight. Which of the following is true:\n\n\nChickWeight is not tidy: each chick has more than one row.\nChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.\nChickWeight is not tidy: we are missing the year column.\nChickWeight is tidy: it is stored in a data frame.\n\n\nExamine the built-in dataset BOD. Which of the following is true:\n\n\nBOD is not tidy: it only has six rows.\nBOD is not tidy: the first column is just an index.\nBOD is tidy: each row is an observation with two values (time and demand)\nBOD is tidy: all small datasets are tidy by definition.\n\n\nWhich of the following built-in datasets is tidy (you can pick more than one):\n\n\nBJsales\nEuStockMarkets\nDNase\nFormaldehyde\nOrange\nUCBAdmissions"
  },
  {
    "objectID": "content/Week_02/02a.html#manipulating-data-frames",
    "href": "content/Week_02/02a.html#manipulating-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Manipulating data frames",
    "text": "Manipulating data frames\nThe dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.\n\nAdding a column with mutate\nWe want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values. So, to add murder rates, we use:\n\nlibrary(dslabs)\ndata(\"murders\")\nmurders <- mutate(murders, rate = total / population * 100000)\n\nNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error?\nThis is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable.\nWe can see that the new column is added:\n\nhead(murders)\n\n       state abb region population total     rate\n1    Alabama  AL  South    4779736   135 2.824424\n2     Alaska  AK   West     710231    19 2.675186\n3    Arizona  AZ   West    6392017   232 3.629527\n4   Arkansas  AR  South    2915918    93 3.189390\n5 California  CA   West   37253956  1257 3.374138\n6   Colorado  CO   West    5029196    65 1.292453\n\n\nNote: Although we have overwritten the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will overwrite our mutated version.\n\n\nSubsetting with filter\nNow suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the filter function, which takes the data table as the first argument and then the conditional statement as the second. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace.\n\nfilter(murders, rate <= 0.71)\n\n          state abb        region population total      rate\n1        Hawaii  HI          West    1360301     7 0.5145920\n2          Iowa  IA North Central    3046355    21 0.6893484\n3 New Hampshire  NH     Northeast    1316470     5 0.3798036\n4  North Dakota  ND North Central     672591     4 0.5947151\n5       Vermont  VT     Northeast     625741     2 0.3196211\n\n\n\n\nSelecting columns with select\nAlthough our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object:\n\nnew_table <- select(murders, state, region, rate)\nfilter(new_table, rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nIn the call to select, the first argument murders is an object, but state, region, and rate are variable names.\n\nTRY IT\n\nLoad the dplyr package and the murders dataset.\n\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\n\nYou can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted:\n\nmurders <- mutate(murders, population_in_millions = population / 10^6)\n\nWe can write population rather than murders$population because mutate is part of dplyr. The function mutate knows we are grabbing columns from murders.\nUse the function mutate to add a murders column named rate with the per 100,000 murder rate as in the example code above. Make sure you redefine murders as done in the example code above ( murders <- [your code]) so we can keep using this variable.\n\nIf rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\nWith dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes:\n\n\nselect(murders, state, population) %>% head()\n\nUse select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\n\nThe dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this:\n\n\nfilter(murders, state == \"New York\")\n\nYou can use other logical vectors to filter rows.\nUse filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\n\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:\n\n\nno_florida <- filter(murders, state != \"Florida\")\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n\nWe can also use %in% to filter with dplyr. You can therefore see the data from New York and Texas like this:\n\n\nfilter(murders, state %in% c(\"New York\", \"Texas\"))\n\nCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\n\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\n\n\nfilter(murders, population < 5000000 & region == \"Northeast\")\n\nMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate, and the rank."
  },
  {
    "objectID": "content/Week_02/02a.html#the-pipe",
    "href": "content/Week_02/02a.html#the-pipe",
    "title": "Introduction to the tidyverse",
    "section": "The pipe: %>%",
    "text": "The pipe: %>%\nWith dplyr we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %>%. Some details are included below.\nWe wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects:\n\\[ \\mbox{original data }\n\\rightarrow \\mbox{ select }\n\\rightarrow \\mbox{ filter } \\]\nFor such an operation, we can use the pipe %>%. The code looks like this:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nThis line of code is equivalent to the two lines of code above. What is going on here?\nIn general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:\n\n16 %>% sqrt()\n\n[1] 4\n\n\nWe can continue to pipe values along:\n\n16 %>% sqrt() %>% log2()\n\n[1] 2\n\n\nThe above statement is equivalent to log2(sqrt(16)).\nRemember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:\n\n16 %>% sqrt() %>% log(base = 2)\n\n[1] 2\n\n\nTherefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\nmurders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function.\nNote that the pipe works well with functions where the first argument is the input data. Functions in tidyverse packages like dplyr have this format and can be used easily with the pipe. It’s worth noting that as of R 4.1, there is a base-R version of the pipe |>, though this has its disadvantages. We’ll stick with %>% for now.\n\nTRY IT\n\nThe pipe %>% can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.\n\n\nmurders <- mutate(murders, rate =  total / population * 100000,\n                  rank = rank(-rate))\n\nIn the solution to the previous exercise, we did the following:\n\nmy_states <- filter(murders, region %in% c(\"Northeast\", \"West\") &\n                      rate < 1)\n\nselect(my_states, state, rate, rank)\n\nThe pipe %>% permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this:\n\nmutate(murders, rate =  total / population * 100000,\n       rank = rank(-rate)) %>%\n  select(state, rate, rank)\n\nNotice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %>%.\nRepeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %>% to do this in just one line.\n\nReset murders to the original table by using data(murders). Use a pipe to create a new data frame called my_states that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three %>%. The code should look something like this:\n\n\nmy_states <- murders %>%\n  mutate SOMETHING %>%\n  filter SOMETHING %>%\n  select SOMETHING"
  },
  {
    "objectID": "content/Week_02/02a.html#summarizing-data",
    "href": "content/Week_02/02a.html#summarizing-data",
    "title": "Introduction to the tidyverse",
    "section": "Summarizing data",
    "text": "Summarizing data\nAn important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using the pull function.\n\n\n\n\nsummarize\nThe summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The heights dataset includes heights and sex reported by students in an in-class survey.\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(heights)\nhead(heights)\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65\n\n\nThe following code computes the average and standard deviation for females:\n\ns <- heights %>%\n  filter(sex == \"Female\") %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\ns\n\n   average standard_deviation\n1 64.93942           3.760656\n\n\nThis takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same.\nBecause the resulting table stored in s is a data frame, we can access the components with the accessor $:\n\ns$average\n\n[1] 64.93942\n\ns$standard_deviation\n\n[1] 3.760656\n\n\nAs with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(median = median(height), minimum = min(height),\n            maximum = max(height))\n\n    median minimum maximum\n1 64.98031      51      79\n\n\nWe can obtain these three values with just one line using the quantile function: for example, quantile(x, c(0,0.5,1)) returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector x. However, if we attempt to use a function like this that returns two or more values inside summarize:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nwe will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In later sections, we will learn how to deal with functions that return more than one value.\nFor another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column:\n\nmurders <- murders %>% mutate(rate = total/population*100000)\n\nRemember that the US murder rate is not the average of the state murder rates:\n\nsummarize(murders, mean(rate))\n\n  mean(rate)\n1   2.779125\n\n\nThis is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000)\nus_murder_rate\n\n      rate\n1 3.034555\n\n\nThis computation counts larger states proportionally to their size which results in a larger value.\n\n\npull\nThe us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame:\n\nclass(us_murder_rate)\n\n[1] \"data.frame\"\n\n\nsince, as most dplyr functions, summarize always returns a data frame.\nThis might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. To understand what we mean take a look at this line of code:\n\nus_murder_rate %>% pull(rate)\n\n[1] 3.034555\n\n\nThis returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate.\nTo get a number from the original data table with one line of code we can type:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000) %>%\n  pull(rate)\n\nus_murder_rate\n\n[1] 3.034555\n\n\nwhich is now a numeric:\n\nclass(us_murder_rate)\n\n[1] \"numeric\"\n\n\n\n\nGroup then summarize with group_by\nA common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this.\nIf we type this:\n\nheights %>% group_by(sex)\n\n# A tibble: 1,050 × 2\n# Groups:   sex [2]\n   sex    height\n   <fct>   <dbl>\n 1 Male       75\n 2 Male       70\n 3 Male       68\n 4 Male       74\n 5 Male       61\n 6 Female     65\n 7 Female     66\n 8 Female     62\n 9 Female     66\n10 Male       67\n# ℹ 1,040 more rows\n\n\nThe result does not look very different from heights, except we see Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:\n\nheights %>%\n  group_by(sex) %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\n\n# A tibble: 2 × 3\n  sex    average standard_deviation\n  <fct>    <dbl>              <dbl>\n1 Female    64.9               3.76\n2 Male      69.3               3.61\n\n\nThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median murder rate in the four regions of the country:\n\nmurders %>%\n  group_by(region) %>%\n  summarize(median_rate = median(rate))\n\n# A tibble: 4 × 2\n  region        median_rate\n  <fct>               <dbl>\n1 Northeast            1.80\n2 South                3.40\n3 North Central        1.97\n4 West                 1.29"
  },
  {
    "objectID": "content/Week_02/02a.html#sorting-data-frames",
    "href": "content/Week_02/02a.html#sorting-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Sorting data frames",
    "text": "Sorting data frames\nWhen examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size:\n\nmurders %>%\n  arrange(population) %>%\n  head()\n\n                 state abb        region population total       rate\n1              Wyoming  WY          West     563626     5  0.8871131\n2 District of Columbia  DC         South     601723    99 16.4527532\n3              Vermont  VT     Northeast     625741     2  0.3196211\n4         North Dakota  ND North Central     672591     4  0.5947151\n5               Alaska  AK          West     710231    19  2.6751860\n6         South Dakota  SD North Central     814180     8  0.9825837\n\n\nWith arrange we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by rate instead:\n\nmurders %>%\n  arrange(rate) %>%\n  head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102\n\n\nNote that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type:\n\nmurders %>%\n  arrange(desc(rate))\n\n\nNested sorting\nIf we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region, then within region we order by murder rate:\n\nmurders %>%\n  arrange(region, rate) %>%\n  head()\n\n          state abb    region population total      rate\n1       Vermont  VT Northeast     625741     2 0.3196211\n2 New Hampshire  NH Northeast    1316470     5 0.3798036\n3         Maine  ME Northeast    1328361    11 0.8280881\n4  Rhode Island  RI Northeast    1052567    16 1.5200933\n5 Massachusetts  MA Northeast    6547629   118 1.8021791\n6      New York  NY Northeast   19378102   517 2.6679599\n\n\n\n\nThe top \\(n\\)\nIn the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:\n\nmurders %>% top_n(5, rate)\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Maryland  MD         South    5773552   293  5.074866\n4             Missouri  MO North Central    5988927   321  5.359892\n5       South Carolina  SC         South    4625364   207  4.475323\n\n\nNote that rows are not sorted by rate, only filtered. If we want to sort, we need to use arrange. Note that if the third argument is left blank, top_n filters by the last column.\n\nTRY IT\nFor these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\n\nlibrary(NHANES)\n\nWarning: package 'NHANES' was built under R version 4.3.3\n\ndata(NHANES)\n\nThe NHANES data has many missing values. The mean and sd functions in R will return NA if any of the entries of the input vector is an NA. Here is an example:\n\nlibrary(dslabs)\ndata(na_example)\nmean(na_example)\n\n[1] NA\n\nsd(na_example)\n\n[1] NA\n\n\nTo ignore the NAs we can use the na.rm argument:\n\nmean(na_example, na.rm = TRUE)\n\n[1] 2.301754\n\nsd(na_example, na.rm = TRUE)\n\n[1] 1.22338\n\n\nLet’s now explore the NHANES data.\n\nWe will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like ” 20-29”, with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref.\n\nHint: Use filter and summarize and use the na.rm = TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter.\n\nUsing a pipe, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then pull.\nNow report the min and max values for the same group.\nCompute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age and gender, filter by Gender and then use group_by.\nRepeat exercise 4 for males.\nWe can actually combine both summaries for exercises 4 and 5 into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender).\nFor males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure."
  },
  {
    "objectID": "content/Week_02/02a.html#tibbles",
    "href": "content/Week_02/02a.html#tibbles",
    "title": "Introduction to the tidyverse",
    "section": "Tibbles",
    "text": "Tibbles\nTidy data must be stored in data frames. We have been using the murders data frame throughout the unit. In an earlier section we introduced the group_by function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?\n\nmurders %>% group_by(region)\n\n# A tibble: 51 × 6\n# Groups:   region [4]\n   state                abb   region    population total  rate\n   <chr>                <chr> <fct>          <dbl> <dbl> <dbl>\n 1 Alabama              AL    South        4779736   135  2.82\n 2 Alaska               AK    West          710231    19  2.68\n 3 Arizona              AZ    West         6392017   232  3.63\n 4 Arkansas             AR    South        2915918    93  3.19\n 5 California           CA    West        37253956  1257  3.37\n 6 Colorado             CO    West         5029196    65  1.29\n 7 Connecticut          CT    Northeast    3574097    97  2.71\n 8 Delaware             DE    South         897934    38  4.23\n 9 District of Columbia DC    South         601723    99 16.5 \n10 Florida              FL    South       19687653   669  3.40\n# ℹ 41 more rows\n\n\nNotice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble followed by dimensions. We can learn the class of the returned object using:\n\nmurders %>% group_by(region) %>% class()\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe tbl, pronounced tibble, is a special kind of data frame. The functions group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. For consistency, the dplyr manipulation verbs (select, filter, mutate, and arrange) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble.\nTibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.\n\nTibbles display better\nThe print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing murders and the output of murders if we convert it to a tibble. We can do this using as_tibble(murders). If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.\n\n\nSubsets of tibbles are tibbles\nIf you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:\n\nclass(murders[,4])\n\n[1] \"numeric\"\n\n\nis not a data frame. With tibbles this does not happen:\n\nclass(as_tibble(murders)[,4])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThis is useful in the tidyverse since functions require data frames as input.\nWith tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $:\n\nclass(as_tibble(murders)$population)\n\n[1] \"numeric\"\n\n\nA related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write Population instead of population this:\n\nmurders$Population\n\nNULL\n\n\nreturns a NULL with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:\n\nas_tibble(murders)$Population\n\nWarning: Unknown or uninitialised column: `Population`.\n\n\nNULL\n\n\n\n\nTibbles can have complex entries\nWhile data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:\n\ntibble(id = c(1, 2, 3), func = c(mean, median, sd))\n\n# A tibble: 3 × 2\n     id func  \n  <dbl> <list>\n1     1 <fn>  \n2     2 <fn>  \n3     3 <fn>  \n\n\n\n\nTibbles can be grouped\nThe function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information.\n\n\nCreate a tibble using tibble instead of data.frame\nIt is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the tibble function.\n\ngrades <- tibble(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\n\nNote that base R (without packages loaded) has a function with a very similar name, data.frame, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default data.frame coerces characters into factors without providing a warning or message:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo avoid this, we use the rather cumbersome argument stringsAsFactors:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90),\n                     stringsAsFactors = FALSE)\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo convert a regular data frame to a tibble, you can use the as_tibble function.\n\nas_tibble(grades) %>% class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\""
  },
  {
    "objectID": "content/Week_02/02a.html#the-dot-operator",
    "href": "content/Week_02/02a.html#the-dot-operator",
    "title": "Introduction to the tidyverse",
    "section": "The dot operator",
    "text": "The dot operator\nOne of the advantages of using the pipe %>% is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:\n\ntab_1 <- filter(murders, region == \"South\")\ntab_2 <- mutate(tab_1, rate = total / population * 10^5)\nrates <- tab_2$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nWe can avoid defining any new intermediate objects by instead typing:\n\nfilter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  summarize(median = median(rate)) %>%\n  pull(median)\n\n[1] 3.398069\n\n\nWe can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the pull function was not available and we wanted to access tab_2$rate? What data frame name would we use? The answer is the dot operator.\nFor example to access the rate vector without the pull function we could use\n\nrates <-   filter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  .$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nIn the next section, we will see other instances in which using the . is useful."
  },
  {
    "objectID": "content/Week_02/02a.html#do",
    "href": "content/Week_02/02a.html#do",
    "title": "Introduction to the tidyverse",
    "section": "do",
    "text": "do\nThe tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %>%, tidyverse functions consistently take data frames and return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The quantile function is an example we described earlier. The do function serves as a bridge between R functions such as quantile and the tidyverse. The do function understands grouped tibbles and always returns a data frame.\nIn the summarize section (above), we noted that if we attempt to use quantile to obtain the min, median and max in one call, we will receive something unexpected. Prior to R 4.1, we would receive an error. After R 4.1, we actually get:\n\ndata(heights)\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nWe probably wanted three columns: min, median, and max. We can use the do function to fix this.\nFirst we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame. Note that it returns a single-row data frame.\n\nmy_summary <- function(dat){\n  x <- quantile(dat$height, c(0, 0.5, 1))\n  tibble(min = x[1], median = x[2], max = x[3])\n}\n\nWe can now apply the function to the heights dataset to obtain the summaries:\n\nheights %>%\n  group_by(sex) %>%\n  my_summary\n\n# A tibble: 1 × 3\n    min median   max\n  <dbl>  <dbl> <dbl>\n1    50   68.5  82.7\n\n\nBut this is not what we want. We want a summary for each sex and the code returned just one summary. This is because my_summary is not part of the tidyverse and does not know how to handled grouped tibbles. do makes this connection:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary(.))\n\n# A tibble: 2 × 4\n# Groups:   sex [2]\n  sex      min median   max\n  <fct>  <dbl>  <dbl> <dbl>\n1 Female    51   65.0  79  \n2 Male      50   69    82.7\n\n\nNote that here we need to use the dot operator. The tibble created by group_by is piped to do. Within the call to do, the name of this tibble is . and we want to send it to my_summary. If you do not use the dot, then my_summary has no argument and returns an error telling us that argument \"dat\" is missing. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary())\n\nIf you do not use the parenthesis, then the function is not executed and instead do tries to return the function. This gives an error because do must always return a data frame. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary)\n\nSo do serves as a bridge between non-tidyverse functions and the tidyverse."
  },
  {
    "objectID": "content/Week_02/02a.html#the-purrr-package",
    "href": "content/Week_02/02a.html#the-purrr-package",
    "title": "Introduction to the tidyverse",
    "section": "The purrr package",
    "text": "The purrr package\nIn previous sections (and labs) we learned about the sapply function, which permitted us to apply the same function to each element of a vector. We constructed a function and used sapply to compute the sum of the first n integers for several values of n like this:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  sum(x)\n}\nn <- 1:25\ns_n <- sapply(n, compute_s_n)\ns_n\n\n [1]   1   3   6  10  15  21  28  36  45  55  66  78  91 105 120 136 153 171 190\n[20] 210 231 253 276 300 325\n\n\nThis type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, sapply can return several different object types; for example, we might expect a numeric result from a line of code, but sapply might convert our result to character under some circumstances. purrr functions will never do this: they will return objects of a specified type or return an error if this is not possible.\nThe first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list:\n\nlibrary(purrr) # or library(tidyverse)\nn <- 1:25\ns_n <- map(n, compute_s_n)\nclass(s_n)\n\n[1] \"list\"\n\n\nIf we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values.\n\ns_n <- map_dbl(n, compute_s_n)\nclass(s_n)\n\n[1] \"numeric\"\n\n\nThis produces the same results as the sapply call shown above.\nA particularly useful purrr function for interacting with the rest of the tidyverse is map_df, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a Argument 1 must have names error:\n\ns_n <- map_df(n, compute_s_n)\n\nWe need to change the function to make this work:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x))\n}\ns_n <- map_df(n, compute_s_n)\nhead(s_n)\n\n# A tibble: 6 × 1\n    sum\n  <int>\n1     1\n2     3\n3     6\n4    10\n5    15\n6    21\n\n\nBecause map_df returns a tibble, we can have more columns defined in our function and returned.\n\ncompute_s_n2 <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x), sumSquared = sum(x^2))\n}\ns_n <- map_df(n, compute_s_n2)\nhead(s_n)\n\n# A tibble: 6 × 2\n    sum sumSquared\n  <int>      <dbl>\n1     1          1\n2     3          5\n3     6         14\n4    10         30\n5    15         55\n6    21         91\n\n\nThe purrr package provides much more functionality not covered here. For more details you can consult this online resource."
  },
  {
    "objectID": "content/Week_02/02a.html#tidyverse-conditionals",
    "href": "content/Week_02/02a.html#tidyverse-conditionals",
    "title": "Introduction to the tidyverse",
    "section": "Tidyverse conditionals",
    "text": "Tidyverse conditionals\nA typical data analysis will often involve one or more conditional operations. In the section on Conditionals, we described the ifelse function, which we will use extensively in this book. In this section we present two dplyr functions that provide further functionality for performing conditional operations.\n\ncase_when\nThe case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0:\n\nx <- c(-2, -1, 0, 1, 2)\ncase_when(x < 0 ~ \"Negative\",\n          x > 0 ~ \"Positive\",\n          x == 0  ~ \"Zero\")\n\n[1] \"Negative\" \"Negative\" \"Zero\"     \"Positive\" \"Positive\"\n\n\nA common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this:\n\nmurders %>%\n  mutate(group = case_when(\n    abb %in% c(\"ME\", \"NH\", \"VT\", \"MA\", \"RI\", \"CT\") ~ \"New England\",\n    abb %in% c(\"WA\", \"OR\", \"CA\") ~ \"West Coast\",\n    region == \"South\" ~ \"South\",\n    TRUE ~ \"Other\")) %>%\n  group_by(group) %>%\n  summarize(rate = sum(total) / sum(population) * 10^5)\n\n# A tibble: 4 × 2\n  group        rate\n  <chr>       <dbl>\n1 New England  1.72\n2 Other        2.71\n3 South        3.63\n4 West Coast   2.90\n\n\nThat TRUE on the fourth line of case_when serves as a catch-all. As case_when steps through the conditions, if none of them are true, it comes to the last line. Since TRUE is always true, the function will return “Other”. Leaving out the last line of case_when would result in NA values for any observation that fails the first three conditionals. This may or may not be what you want.\n\n\nbetween\nA common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type\n\nx >= a & x <= b\n\nHowever, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation.\n\nbetween(x, a, b)\n\n\nTRY IT\n\nLoad the murders dataset. Which of the following is true?\n\n\nmurders is in tidy format and is stored in a tibble.\nmurders is in tidy format and is stored in a data frame.\nmurders is not in tidy format and is stored in a tibble.\nmurders is not in tidy format and is stored in a data frame.\n\n\nUse as_tibble to convert the murders data table into a tibble and save it in an object called murders_tibble.\nUse the group_by function to convert murders into a tibble that is grouped by region.\nWrite tidyverse code that is equivalent to this code:\n\n\nexp(mean(log(murders$population)))\n\nWrite it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with murders %>%.\n\nUse the map_df to create a data frame with three columns named n, s_n, and s_n_2. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through \\(n\\) with \\(n\\) the row number."
  },
  {
    "objectID": "content/Week_02/02b.html",
    "href": "content/Week_02/02b.html",
    "title": "Introduction to Visualization",
    "section": "",
    "text": "Looking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table:\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nWhat do you learn from staring at this table? Even though it is a relatively straightforward table, we can’t learn anything. For starters, it is grossly abbreviated, though you could scroll through. In doing so, how quickly might you be able to determine which states have the largest populations? Which states have the smallest? How populous is a typical state? Is there a relationship between population size and total murders? How do murder rates vary across regions of the country? For most folks, it is quite difficult to extract this information just by looking at the numbers. In contrast, the answer to the questions above are readily available from examining this plot:\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(ggrepel)\n\nr <- murders %>%\n  summarize(pop=sum(population), tot=sum(total)) %>%\n  mutate(rate = tot/pop*10^6) %>% pull(rate)\n\nmurders %>% ggplot(aes(x = population/10^6, y = total, label = abb)) +\n  geom_abline(intercept = log10(r), lty=2, col=\"darkgrey\") +\n  geom_point(aes(color=region), size = 3) +\n  geom_text_repel() +\n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") +\n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\") +\n  scale_color_discrete(name=\"Region\") +\n  theme_economist_white()\n\n\n\n\nWe are reminded of the saying: “A picture is worth a thousand words”. Data visualization provides a powerful way to communicate a data-driven finding. In some cases, the visualization is so convincing that no follow-up analysis is required. You should consider visualization the most potent tool in your data analytics arsenal.\nThe growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many industries, academia, and government. A salient example is news organizations, which are increasingly embracing data journalism and including effective infographics as part of their reporting.\nA particularly salient example—given the current state of the world—is a Wall Street Journal article1 showing data related to the impact of vaccines on battling infectious diseases. One of the graphs shows measles cases by US state through the years with a vertical line demonstrating when the vaccine was introduced.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n(Source: Wall Street Journal)\nAnother striking example comes from a New York Times chart2, which summarizes scores from the NYC Regents Exams. As described in the article3, these scores are collected for several reasons, including to determine if a student graduates from high school. In New York City you need a 65 to pass. The distribution of the test scores forces us to notice something somewhat problematic:\n\n\n\n\n\n(Source: New York Times via Amanda Cox)\nThe most common test score is the minimum passing grade, with very few scores just below the threshold. This unexpected result is consistent with students close to passing having their scores bumped up.\nThis is an example of how data visualization can lead to discoveries which would otherwise be missed if we simply subjected the data to a battery of data analysis tools or procedures. Data visualization is the strongest tool of what we call exploratory data analysis (EDA). John W. Tukey4, considered the father of EDA, once said,\n\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.”\n\n\nMany widely used data analysis tools were initiated by discoveries made via EDA. EDA is perhaps the most important part of data analysis, yet it is one that is often overlooked.\nData visualization is also now pervasive in philanthropic and educational organizations. In the talks New Insights on Poverty5 and The Best Stats You’ve Ever Seen6, Hans Rosling forces us to notice the unexpected with a series of plots related to world health and economics. In his videos, he uses animated graphs to show us how the world is changing and how old narratives are no longer true.\n\n\n\n\n\nIt is also important to note that mistakes, biases, systematic errors and other unexpected problems often lead to data that should be handled with care. Failure to discover these problems can give rise to flawed analyses and false discoveries. As an example, consider that measurement devices sometimes fail and that most data analysis procedures are not designed to detect these. Yet these data analysis procedures will still give you an answer. The fact that it can be difficult or impossible to notice an error just from the reported results makes data visualization particularly important.\nToday, we will discuss the basics of data visualization and exploratory data analysis. We will use the ggplot2 package to code. To learn the very basics, we will start with a somewhat artificial example: heights reported by students. Then we will cover the two examples mentioned above: 1) world health and economics and 2) infectious disease trends in the United States.\nOf course, there is much more to data visualization than what we cover here. The following are references for those who wish to learn more:\n\nER Tufte (1983) The visual display of quantitative information. Graphics Press.\nER Tufte (1990) Envisioning information. Graphics Press.\nER Tufte (1997) Visual explanations. Graphics Press.\nWS Cleveland (1993) Visualizing data. Hobart Press.\nWS Cleveland (1994) The elements of graphing data. CRC Press.\nA Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach: Turning tables into graphs. The American Statistician 56:121-130.\nNB Robbins (2004) Creating more effective graphs. Wiley.\nA Cairo (2013) The functional art: An introduction to information graphics and visualization. New Riders.\nN Yau (2013) Data points: Visualization that means something. Wiley.\n\nWe also do not cover interactive graphics, a topic that is both too advanced for this course and too unweildy. Some useful resources for those interested in learning more can be found below, and you are encouraged to draw inspiration from those websites in your projects:\n\nhttps://shiny.rstudio.com/\nhttps://d3js.org/"
  },
  {
    "objectID": "content/Week_02/02b.html#the-components-of-a-graph",
    "href": "content/Week_02/02b.html#the-components-of-a-graph",
    "title": "Introduction to Visualization",
    "section": "The components of a graph",
    "text": "The components of a graph\nWe will eventually construct a graph that summarizes the US murders dataset that looks like this:\n\n\n\n\n\nWe can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.\nThis data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.\nThe first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main five components to note are:\n\nData: The US murders data table is being summarized. We refer to this as the data component.\nGeometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, boxplot, pie (ew!), and many, many more. We will learn about these later.\nAesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using.\nAnnotations: These are things like axis labels, axis ticks (the lines along the axis at regular intervals or specific points of interest), axis scales (e.g. log-scale), titles, legends, etc.\nStyle: An overall appearance of the graph determined by fonts, color palattes, layout, blank spaces, and more.\n\nWe also note that:\n\nThe points are labeled with the state abbreviations.\nThe range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.\nThere are labels, a title, a legend, and we use the style of The Economist magazine.\n\nAll of the flexibility and visualization power of ggplot is contained in these four elements (plus your data)"
  },
  {
    "objectID": "content/Week_02/02b.html#ggplot-objects",
    "href": "content/Week_02/02b.html#ggplot-objects",
    "title": "Introduction to Visualization",
    "section": "ggplot objects",
    "text": "ggplot objects\nWe will now construct the plot piece by piece.\nWe start by loading the dataset:\n\nlibrary(dslabs)\ndata(murders)\n\n\n\n\nThe first step in creating a ggplot2 graph is to define a ggplot object. We do this with the function ggplot, which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object:\n\nggplot(data = murders)\n\nWe can also pipe the data in as the first argument. So this line of code is equivalent to the one above:\n\nmurders %>% ggplot()\n\n\n\n\nIt renders a plot, in this case a blank slate since no geometry has been defined. The only style choice we see is a grey background.\nWhat has happened above is that the object was created and, because it was not assigned, it was automatically evaluated. But we can assign our plot to an object, for example like this:\n\np <- ggplot(data = murders)\nclass(p)\n\n[1] \"gg\"     \"ggplot\"\n\n\nTo render the plot associated with this object, we simply print the object p. The following two lines of code each produce the same plot we see above:\n\nprint(p)\np"
  },
  {
    "objectID": "content/Week_02/02b.html#geometries-briefly",
    "href": "content/Week_02/02b.html#geometries-briefly",
    "title": "Introduction to Visualization",
    "section": "Geometries (briefly)",
    "text": "Geometries (briefly)\nIn ggplot2 we create graphs by adding geometry layers. Layers can define geometries, compute summary statistics, define what scales to use, create annotations, or even change styles. To add layers, we use the symbol +. In general, a line of code will look like this:\n\nDATA %>% ggplot() + LAYER 1 + LAYER 2 + ... + LAYER N\n\nUsually, the first added layer after ggplot() + defines the geometry. After that, we may add additional geometries, we may rescale an axis, we may add annotations and labels, or we may change the style. For now, we want to make a scatterplot like the one you all created in Lab 0. What geometry do we use?\n\n\n\n\n\nTaking a quick look at the cheat sheet, we see that the ggplot2 function used to create plots with this geometry is geom_point.\nSee Here\n(Image courtesy of RStudio9. CC-BY-4.0 license10.)\n\nGeometry function names follow the pattern: geom_X where X is the name of some specific geometry. Some examples include geom_point, geom_bar, and geom_histogram. You’ve already seen a few of these. We will start with a scatterplot created using geom_point() for now, then circle back to more geometries after we cover aesthetic mappings, layers, and annotations.\nFor geom_point to run properly we need to provide data and an aesthetic mapping. The simplest mapping for a scatter plot is to say we want one variable on the X-axis, and a different one on the Y-axis, so each point is an {X,Y} pair. That is an aesthetic mapping because X and Y are aesthetics in a geom_point scatterplot.\nWe have already connected the object p with the murders data table, and if we add the layer geom_point it defaults to using this data. To find out what mappings are expected, we read the Aesthetics section of the help file ?geom_point help file:\n> Aesthetics\n>\n> geom_point understands the following aesthetics (required aesthetics are in bold):\n>\n> **x**\n>\n> **y**\n>\n> alpha\n>\n> colour\n>\n> fill\n>\n> group\n>\n> shape\n>\n> size\n>\n> stroke\nand—although it does not show in bold above—we see that at least two arguments are required: x and y. You can’t have a geom_point scatterplot unless you state what you want on the X and Y axes."
  },
  {
    "objectID": "content/Week_02/02b.html#aesthetic-mappings",
    "href": "content/Week_02/02b.html#aesthetic-mappings",
    "title": "Introduction to Visualization",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\nAesthetic mappings describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The aes function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the aes function is often used as the argument of a geometry function. This example produces a scatterplot of population in millions (x-axis) versus total murders (y-axis):\n\nmurders %>% ggplot() +\n  geom_point(aes(x = population/10^6, y = total))\n\nInstead of defining our plot from scratch, we can also add a layer to the p object that was defined above as p <- ggplot(data = murders):\n\np + geom_point(aes(x = population/10^6, y = total))\n\n\n\n\nThe scales and annotations like axis labels are defined by default when adding this layer (note the x-axis label is exactly what we wrote in the function call). Like dplyr functions, aes also uses the variable names from the object component: we can use population and total without having to call them as murders$population and murders$total. The behavior of recognizing the variables from the data component is quite specific to aes. With most functions, if you try to access the values of population or total outside of aes you receive an error.\nNote that we did some rescaling within the aes() call - we can do simple things like multiplication or division on the variable names in the ggplot call. The axis labels reflect this. We will change the axis labels later.\nThe aesthetic mappings are very powerful - changing the variable in x= or y= changes the meaning of the plot entirely. We’ll come back to additional aesthetic mappings once we talk about aesthetics in general.\n\nAesthetics in general\nEven without mappings, a plots aesthetics can be useful. Things like color, fill, alpha, and size are aesthetics that can be changed.\nLet’s say we want larger points in our scatterplot. The size aesthetic can be used to set the size. The scale of size is “multiples of the defaults” (so size = 1 is the default)\n\np + geom_point(aes(x = population/10^6, y = total), size = 3)\n\n\n\n\nsize is not a mapping so it is not in the aes() part: whereas mappings use data from specific observations and need to be inside aes(), operations we want to affect all the points the same way do not need to be included inside aes. We’ll see what happens if size is inside aes(size = xxx) in a second.\nWe can change the shape to one of the many different base-R options found here:\n\np + geom_point(aes(x = population/10^6, y = total), size = 3, shape = 17)\n\n\n\n\nWe can also change the fill and the color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 4, shape = 23, fill = '#18453B')\n\n\n\n\nfill can take a common name like 'green', or can take a hex color like '#18453B', which is MSU Green according to MSU’s branding site. You can also find UM Maize and OSU Scarlet on respective branding pages, or google “XXX color hex.” We’ll learn how to build a color palatte later on.\ncolor (or colour, same thing because ggplot creators allow both spellings) is a little tricky with points - it changes the outline of the geometry rather than the fill color, but in geom_point() most shapes are only the outline, including the default. This is more useful with, say, a barplot where the outline and the fill might be different colors. Still, shapes 21-25 have both fill and color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 5, shape = 23, fill = '#18453B', color = 'white')\n\n\n\n\nThe color = 'white' makes the outline of the shape white, which you can see if you look closely in the areas where the shapes overlap. This only works with shapes 21-25, or any other geometry that has both an outline and a fill.\n\n\nNow, back to aesthetic mappings\nNow that we’ve seen a few aesthetics (and know we can find more by looking at which aesthetics work with our geometry in the help file), let’s return to the power of aesthetic mappings.\nAn aesthetic mapping means we can vary an aesthetic (like fill or shape or size) according to some variable in our data. This opens up a world of possibilities! Let’s try adding to our x and y aesthetics with a color aesthetic (since points respond to color better than fill) that varies by region, which is a column in our data:\n\np + geom_point(aes(x = population/10^6, y = total, color = region), size = 3)\n\n\n\n\nWe include color=region inside the aes call, which tells R to find a variable called region and change color based on that. R will choose a somewhat ghastly color palatte, and every unique value in the data for region will get a different color if the variable is discrete. If the variable is a continuous value, then ggplot will automatically make a color ramp. Thus, discrete and continuous values for aesthetic mappings work differently.\nLet’s see a useful example of a continuous aesthetic mapping to color. In our data, we are making a scatterplot of population and total murders, which really just shows that states with higher populations have higher murders. What we really want is murders per capita (I think COVID taught us a lot about rates vs. levels like “cases” and “cases per 100,000 people”). We can create a variable of “murders per capita” on the fly. Since “murders per capita” is a very small number and hard to read, we’ll multiply by 100 so that we get “percent of population murdered per year”:\n\np + geom_point(aes(x = population/10^5, y = total, color = 100*total/population), size = 3)\n\n\n\n\nWhile the clear pattern of “more population means more murders” is still there, look at the outlier in light blue in the bottom left. With the color ramp, see how easy it is to see here that there is one location where murders per capita is quite high?\nNote that size is outside of aes and is set to an explicit value, not to a variable. What if we set size to a variable in the data?\n\np + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6))\n\n\n\n\n\n\nLegends for aesthetics\nHere we see yet another useful default behavior: ggplot2 automatically adds a legend that maps color to region, and size to population (which we scaled by 1,000,000). To avoid adding this legend we set the geom_point argument show.legend = FALSE. This removes both the size and the color legend.\n\np + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6), show.legend = FALSE)\n\n\n\n\nLater on, when we get to annotation layers, we’ll talk about controlling the legend text and layout. For now, we just need to know how to turn them off."
  },
  {
    "objectID": "content/Week_02/02b.html#annotation-layers",
    "href": "content/Week_02/02b.html#annotation-layers",
    "title": "Introduction to Visualization",
    "section": "Annotation Layers",
    "text": "Annotation Layers\nA second layer in the plot we wish to make involves adding a label to each point to identify the state. The geom_label and geom_text functions permit us to add text to the plot with and without a rectangle behind the text, respectively.\nBecause each point (each state in this case) has a label, we need an aesthetic mapping to make the connection between points and labels. By reading the help file ?geom_text, we learn that we supply the mapping between point and label through the label argument of aes. That is, label is an aesthetic that we can map. So the code looks like this:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\n\n\n\nWe have successfully added a second layer to the plot.\nAs an example of the unique behavior of aes mentioned above, note that this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total, label = abb))\n\nis fine, whereas this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total), label = abb)\n\nwill give you an error since abb is not found because it is outside of the aes function. The layer geom_text does not know where to find abb since it is a column name and not a global variable, and ggplot does not look for column names for non-mapped aesthetics. For a trivial example:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(population/10^6, total), label = 'abb')\n\n\n\n\n\nGlobal versus local aesthetic mappings\nIn the previous line of code, we define the mapping aes(population/10^6, total) twice, once in each geometry. We can avoid this by using a global aesthetic mapping. We can do this when we define the blank slate ggplot object. Remember that the function ggplot contains an argument that permits us to define aesthetic mappings:\n\nargs(ggplot)\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \nNULL\n\n\nIf we define a mapping in ggplot, all the geometries that are added as layers will default to this mapping. We redefine p:\n\np <- murders %>% ggplot(aes(x = population/10^6, y = total, label = abb))\n\nand then we can simply write the following code to produce the previous plot:\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 1.5) # offsets the label\n\nWe keep the size and nudge_x arguments in geom_point and geom_text, respectively, because we want to only increase the size of points and only nudge the labels. If we put those arguments in aes then they would apply to both plots. Also note that the geom_point function does not need a label argument and therefore ignores that aesthetic.\nIf necessary, we can override the global mapping by defining a new mapping within each layer. These local definitions override the global. Here is an example:\n\np + geom_point(size = 3) +\n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))\n\n\n\n\nClearly, the second call to geom_text does not use x = population and y = total."
  },
  {
    "objectID": "content/Week_02/02b.html#try-it",
    "href": "content/Week_02/02b.html#try-it",
    "title": "Introduction to Visualization",
    "section": "Try it!",
    "text": "Try it!\n\nLet’s break in to smaller groups and try playing with some of the aesthetics and aesthetic mappings. If we’re in person (woohoo!), we’ll form the same number of groups in class.\nIn each group, one person should be the main coder - someone who has the packages like dslabs installed and has successfully run the plots above. Each set of tasks ask you to learn about an aesthetic and put it into action with the murder data. We’ll leave about 5 minutes to do the task, then have you come back and share your results with the class.\nFor each group, we’ll start with the following code:\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\nThe alpha aesthetic mapping.\n\nThe alpha aesthetic can only take a number between 0 and 1. So first, in murders, create a murders_per_capita column by dividing total by population. Second, find the max(murders$murders_per_capita) and then create another new column called murders_per_capita_rescaled which divides murders_per_capita by the max value. murders_per_capita_rescaled will be between 0 and 1, with the value of 1 for the state with the max murder rate. This is a little hard to do on the fly in ggplot.\nSet the alpha aesthetic mapping to murders_per_capita_rescaled for geom_point.\nTurn off the legend using show.legend=FALSE\nInclude the geom_text labels, but make sure the aesthetic mapping does not apply to the labels.\nUse nudge_x = 1.5 as before to offset the labels.\nBe able to explain the plot.\n\nDoes the alpha aesthetic help present the data here? It’s OK if it doesn’t!\n\n\nThe stroke aesthetic mapping.\n\nThe stroke aesthetic works a bit like the size aesthetic. It must be used with a plot that has both a border and a fill, like shapes 21-25, so use one of those.\nUse the stroke aesthetic mapping (meaning the stroke will change according to a value in the data) to set a different stroke size based on murders per capita. You can create a murders per capita variable on the fly, or add it to your murders data.\n\nInclude the text labels as before and use nudge_x = 1.5.\nMake sure you’re only setting the aesthetic for the points on the scatterplot!\n\n\nThe angle aesthetic\n\nUsing the ?geom_text help, note that geom_text takes an aesthetic of angle.\nUse the angle aesthetic (not aesthetic mapping) in the appropriate place (e.g. on geom_text and not on other geometries) to adjust the labels on our plot.\nNow, try using the angle aesthetic mapping by using the total field as both the y value and the angle value in the geom_text layer.\nDoes using angle as an aesthetic help? What about as an aesthetic mapping?\n\nThe color aesthetic mapping\n\nSet the color aesthetic mapping in geom_text to total/population.\n\nUse the nudge_x = 1.5 aesthetic in geom_text still\n\nTry it with and without the legend using show.legend.\nBe able to explain the plot.\n\nDoes the color aesthetic mapping help present the data here?\n\n\ngeom_label and the fill aesthetic\n\nLooking at ?geom_label (which is the same help as geom_text), we note that “The fill aesthetic controls the backgreound colour of the label”.\nSet the fill aesthetic mapping to total/population in geom_label (replacing geom_text but still using nudge_x=1.5)\nSet the fill aesthetic (not mapping) to the color of your choice.\nBe able to explain the plots.\n\n\nDoes the fill aesthetic mapping help present the data here?\nWhat color did you choose for the non-mapped fill aesthetic?"
  }
]