[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About page\n\n\n\nThis page contains some elaborated background information about your workshop, or the instructors.\n\n\nFor example: A central problem in machine learning is how to make an algorithm perform well not just on the training data, but also on new inputs. Many strategies in machine learning are explicitly designed to reduce this test error, possibly at the expense of increased training error. These strategies are collectively known as regularisation and they are instrumental for good performance of any kind of prediction or classification model, especially in the context of small data (many features, few samples).\nIn the hands-on tutorial we will use R to perform an integrated analysis of multi-omics data with penalised regression.\n\nContact\nInstructor A: contact\nInstructor B: contact\nInstructor C: contact"
  },
  {
    "objectID": "content/Week_00/00a.html",
    "href": "content/Week_00/00a.html",
    "title": "0A - Testing",
    "section": "",
    "text": "Page with R code\n\n\n\nThis page contains an example template for a lab session, where R code and results are displayed here.\nYou can find more information on how to include code in Quarto website here.\nYou can experiment with code-fold and code-tools in the yaml header above to change how the code cells look like."
  },
  {
    "objectID": "content/Week_00/00a.html#a-cancer-modeling-example",
    "href": "content/Week_00/00a.html#a-cancer-modeling-example",
    "title": "0A - Testing",
    "section": "A Cancer Modeling Example",
    "text": "A Cancer Modeling Example\nExercise on analysis of miRNA, mRNA and protein data from the paper Aure et al, Integrated analysis reveals microRNA networks coordinately expressed with key proteins in breast cancer, Genome Medicine, 2015.\nPlease run the code provided to replicate some of the analyses. Make sure you can explain what all the analysis steps do and that you understand all the results.\nIn addition, there are some extra tasks (Task 1), where no R code is provided. Please do these tasks when you have time available at the end of the lab.\n\nLoad the data\nRead the data, and convert to matrix format.\n\nmrna <- read.table(here(\"data/data_example.txt\"), header=T, sep=\"\\t\", dec=\".\")\n\n# Convert to matrix format\n\nmrna <- as.matrix(mrna)\n\nPrint the data\n\nmrna[1:4, 1:4]\n\n      OSL2R.3002T4 OSL2R.3005T1 OSL2R.3013T1 OSL2R.3030T2\nACACA      1.60034     -0.49087     -0.26553     -0.27857\nANXA1     -2.42501     -0.05416     -0.46478     -2.18393\nAR         0.39615     -0.43348     -0.10232      0.58299\nBAK1       0.78627      0.39897      0.22598     -1.31202\n\n\nVisualise the overall distribution of expression levels by histogram\n\nhist(mrna, nclass=40, xlim=c(-5,5), col=\"lightblue\")\n\n\n\n\n\n\n\n\n\n\nTask 1\n\n\n\nThis is a callout-note, and it can be quite useful for exercises. You can find more about callout here.\nExample: Extend the above analysis to cover all genes."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html",
    "title": "Introduction to the tidyverse",
    "section": "",
    "text": "This page.\nChapter 1 of Introduction to Statistical Learning, available here.\nOptional: The “Tidy Your Data” tutorial on Rstudio Cloud Primers"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#some-reminders",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#some-reminders",
    "title": "Introduction to the tidyverse",
    "section": "Some Reminders:",
    "text": "Some Reminders:\n\nStart labs early!\n\nThey are not trivial.\nThey are not short.\nThey are not easy.\nThey are not optional.\n\nYou install.packages(\"packageName\") once on your computer.\n\nAnd never ever ever in your code.\n\nYou load an already-installed package using library(packageName) in a code chunk\n\nNever in your console\nWhen RMarkdown knits, it starts a whole new, empty session that has no knowledge of what you typed into the console\n\nSlack\n\nUse it.\nI would very much prefer posting in the class-visible channels. Others can learn from your issues.\n\nWe have a channel just for labs and R. Please use that one.\n\n\n\n\nGroup Projects\nYour final is a group project. You will also have two “mini” projects. They comprise a large part of your grade. As mentioned last week, this mean that you need to start planning soon.\nTo aid in your planning, here are the required elements of your final project.\n\nYou must find existing data to analyze. Aggregating and merging data from multiple sources is encouraged.\nYou must visualize 3 interesting features of that data.\nYou must come up with some analysis—using tools from this course—which relates your data to either a prediction or a policy conclusion.\nYou must think critically about your analysis and be able to identify potential issues.\nYou must present your analysis as if presenting to a C-suite executive.\n\nYour mini-projects along the way will be more structured, but will serve to guide you towards the final project.\n\n\nTeams\nPlease form teams of 3 people. Once all agree to be on a team, have ONE PERSON email our TA Allen scovelpa@msu.edu and cc all of the members of the team so that nobody is surprised to be included on a team. Title the email [SSC442] - Group Formation. Tell us your team name (be creative), and list in the email the names of all of the team members and their email address (in addition to cc-ing those team members on the email).\nIf you opt to not form a team, you will be automatically added to the “willing to be randomly assigned” pool and will be paired with two others from the “willing to be randomly assigned” pool.\nSend this email by January 20th and we will assign un-teamed folks at the beginning of the following week. Project 1 is due in no time. See schedule for all the important project dates.\n\n\nGuiding Question\nFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\nWhy do we want tidy data?\nWhat are the challenges associated with shaping things into a tidy format?"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#tidy-data",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#tidy-data",
    "title": "Introduction to the tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\n\nWe say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\n\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nEach row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.\nTo see how the same information can be provided in different formats, consider the following example:\n\nlibrary(dslabs)\ndata(\"gapminder\") # gapminder will now be a data.frame in your \"environment\" (memory)\ntidy_data <- gapminder %>%\n  filter(country %in% c(\"South Korea\", \"Germany\") & !is.na(fertility)) %>%\n  select(country, year, fertility)\nhead(tidy_data, 6)\n\n      country year fertility\n1     Germany 1960      2.41\n2 South Korea 1960      6.16\n3     Germany 1961      2.44\n4 South Korea 1961      5.99\n5     Germany 1962      2.47\n6 South Korea 1962      5.79\n\n\nThis tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the dslabs package. Originally, the data was in the following format:\n\n\n      country 1960 1961 1962\n1     Germany 2.41 2.44 2.47\n2 South Korea 6.16 5.99 5.79\n\n\nThe same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables’ values, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into tidy format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.\nAlthough not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.\n\nTRY IT\n\nExamine the built-in dataset co2. Which of the following is true:\n\n\nco2 is tidy data: it has one year for each row.\nco2 is not tidy: we need at least one column with a character vector.\nco2 is not tidy: it is a matrix instead of a data frame.\nco2 is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.\n\n\nExamine the built-in dataset ChickWeight. Which of the following is true:\n\n\nChickWeight is not tidy: each chick has more than one row.\nChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.\nChickWeight is not tidy: we are missing the year column.\nChickWeight is tidy: it is stored in a data frame.\n\n\nExamine the built-in dataset BOD. Which of the following is true:\n\n\nBOD is not tidy: it only has six rows.\nBOD is not tidy: the first column is just an index.\nBOD is tidy: each row is an observation with two values (time and demand)\nBOD is tidy: all small datasets are tidy by definition.\n\n\nWhich of the following built-in datasets is tidy (you can pick more than one):\n\n\nBJsales\nEuStockMarkets\nDNase\nFormaldehyde\nOrange\nUCBAdmissions"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#manipulating-data-frames",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#manipulating-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Manipulating data frames",
    "text": "Manipulating data frames\nThe dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.\n\nAdding a column with mutate\nWe want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values. So, to add murder rates, we use:\n\nlibrary(dslabs)\ndata(\"murders\")\nmurders <- mutate(murders, rate = total / population * 100000)\n\nNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error?\nThis is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable.\nWe can see that the new column is added:\n\nhead(murders)\n\n       state abb region population total     rate\n1    Alabama  AL  South    4779736   135 2.824424\n2     Alaska  AK   West     710231    19 2.675186\n3    Arizona  AZ   West    6392017   232 3.629527\n4   Arkansas  AR  South    2915918    93 3.189390\n5 California  CA   West   37253956  1257 3.374138\n6   Colorado  CO   West    5029196    65 1.292453\n\n\nNote: Although we have overwritten the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will overwrite our mutated version.\n\n\nSubsetting with filter\nNow suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the filter function, which takes the data table as the first argument and then the conditional statement as the second. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace.\n\nfilter(murders, rate <= 0.71)\n\n          state abb        region population total      rate\n1        Hawaii  HI          West    1360301     7 0.5145920\n2          Iowa  IA North Central    3046355    21 0.6893484\n3 New Hampshire  NH     Northeast    1316470     5 0.3798036\n4  North Dakota  ND North Central     672591     4 0.5947151\n5       Vermont  VT     Northeast     625741     2 0.3196211\n\n\n\n\nSelecting columns with select\nAlthough our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object:\n\nnew_table <- select(murders, state, region, rate)\nfilter(new_table, rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nIn the call to select, the first argument murders is an object, but state, region, and rate are variable names.\n\nTRY IT\n\nLoad the dplyr package and the murders dataset.\n\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\n\nYou can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted:\n\nmurders <- mutate(murders, population_in_millions = population / 10^6)\n\nWe can write population rather than murders$population because mutate is part of dplyr. The function mutate knows we are grabbing columns from murders.\nUse the function mutate to add a murders column named rate with the per 100,000 murder rate as in the example code above. Make sure you redefine murders as done in the example code above ( murders <- [your code]) so we can keep using this variable.\n\nIf rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\nWith dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes:\n\n\nselect(murders, state, population) %>% head()\n\nUse select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\n\nThe dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this:\n\n\nfilter(murders, state == \"New York\")\n\nYou can use other logical vectors to filter rows.\nUse filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\n\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:\n\n\nno_florida <- filter(murders, state != \"Florida\")\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n\nWe can also use %in% to filter with dplyr. You can therefore see the data from New York and Texas like this:\n\n\nfilter(murders, state %in% c(\"New York\", \"Texas\"))\n\nCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\n\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\n\n\nfilter(murders, population < 5000000 & region == \"Northeast\")\n\nMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate, and the rank."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#the-pipe",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#the-pipe",
    "title": "Introduction to the tidyverse",
    "section": "The pipe: %>%",
    "text": "The pipe: %>%\nWith dplyr we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %>%. Some details are included below.\nWe wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects:\n\\[ \\mbox{original data }\n\\rightarrow \\mbox{ select }\n\\rightarrow \\mbox{ filter } \\]\nFor such an operation, we can use the pipe %>%. The code looks like this:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nThis line of code is equivalent to the two lines of code above. What is going on here?\nIn general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:\n\n16 %>% sqrt()\n\n[1] 4\n\n\nWe can continue to pipe values along:\n\n16 %>% sqrt() %>% log2()\n\n[1] 2\n\n\nThe above statement is equivalent to log2(sqrt(16)).\nRemember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:\n\n16 %>% sqrt() %>% log(base = 2)\n\n[1] 2\n\n\nTherefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\nmurders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function.\nNote that the pipe works well with functions where the first argument is the input data. Functions in tidyverse packages like dplyr have this format and can be used easily with the pipe. It’s worth noting that as of R 4.1, there is a base-R version of the pipe |>, though this has its disadvantages. We’ll stick with %>% for now.\n\nTRY IT\n\nThe pipe %>% can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.\n\n\nmurders <- mutate(murders, rate =  total / population * 100000,\n                  rank = rank(-rate))\n\nIn the solution to the previous exercise, we did the following:\n\nmy_states <- filter(murders, region %in% c(\"Northeast\", \"West\") &\n                      rate < 1)\n\nselect(my_states, state, rate, rank)\n\nThe pipe %>% permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this:\n\nmutate(murders, rate =  total / population * 100000,\n       rank = rank(-rate)) %>%\n  select(state, rate, rank)\n\nNotice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %>%.\nRepeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %>% to do this in just one line.\n\nReset murders to the original table by using data(murders). Use a pipe to create a new data frame called my_states that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three %>%. The code should look something like this:\n\n\nmy_states <- murders %>%\n  mutate SOMETHING %>%\n  filter SOMETHING %>%\n  select SOMETHING"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#summarizing-data",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#summarizing-data",
    "title": "Introduction to the tidyverse",
    "section": "Summarizing data",
    "text": "Summarizing data\nAn important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using the pull function.\n\n\n\n\nsummarize\nThe summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The heights dataset includes heights and sex reported by students in an in-class survey.\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(heights)\nhead(heights)\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65\n\n\nThe following code computes the average and standard deviation for females:\n\ns <- heights %>%\n  filter(sex == \"Female\") %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\ns\n\n   average standard_deviation\n1 64.93942           3.760656\n\n\nThis takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same.\nBecause the resulting table stored in s is a data frame, we can access the components with the accessor $:\n\ns$average\n\n[1] 64.93942\n\ns$standard_deviation\n\n[1] 3.760656\n\n\nAs with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(median = median(height), minimum = min(height),\n            maximum = max(height))\n\n    median minimum maximum\n1 64.98031      51      79\n\n\nWe can obtain these three values with just one line using the quantile function: for example, quantile(x, c(0,0.5,1)) returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector x. However, if we attempt to use a function like this that returns two or more values inside summarize:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nwe will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In later sections, we will learn how to deal with functions that return more than one value.\nFor another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column:\n\nmurders <- murders %>% mutate(rate = total/population*100000)\n\nRemember that the US murder rate is not the average of the state murder rates:\n\nsummarize(murders, mean(rate))\n\n  mean(rate)\n1   2.779125\n\n\nThis is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000)\nus_murder_rate\n\n      rate\n1 3.034555\n\n\nThis computation counts larger states proportionally to their size which results in a larger value.\n\n\npull\nThe us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame:\n\nclass(us_murder_rate)\n\n[1] \"data.frame\"\n\n\nsince, as most dplyr functions, summarize always returns a data frame.\nThis might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. To understand what we mean take a look at this line of code:\n\nus_murder_rate %>% pull(rate)\n\n[1] 3.034555\n\n\nThis returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate.\nTo get a number from the original data table with one line of code we can type:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000) %>%\n  pull(rate)\n\nus_murder_rate\n\n[1] 3.034555\n\n\nwhich is now a numeric:\n\nclass(us_murder_rate)\n\n[1] \"numeric\"\n\n\n\n\nGroup then summarize with group_by\nA common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this.\nIf we type this:\n\nheights %>% group_by(sex)\n\n# A tibble: 1,050 × 2\n# Groups:   sex [2]\n   sex    height\n   <fct>   <dbl>\n 1 Male       75\n 2 Male       70\n 3 Male       68\n 4 Male       74\n 5 Male       61\n 6 Female     65\n 7 Female     66\n 8 Female     62\n 9 Female     66\n10 Male       67\n# ℹ 1,040 more rows\n\n\nThe result does not look very different from heights, except we see Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:\n\nheights %>%\n  group_by(sex) %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\n\n# A tibble: 2 × 3\n  sex    average standard_deviation\n  <fct>    <dbl>              <dbl>\n1 Female    64.9               3.76\n2 Male      69.3               3.61\n\n\nThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median murder rate in the four regions of the country:\n\nmurders %>%\n  group_by(region) %>%\n  summarize(median_rate = median(rate))\n\n# A tibble: 4 × 2\n  region        median_rate\n  <fct>               <dbl>\n1 Northeast            1.80\n2 South                3.40\n3 North Central        1.97\n4 West                 1.29"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#sorting-data-frames",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#sorting-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Sorting data frames",
    "text": "Sorting data frames\nWhen examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size:\n\nmurders %>%\n  arrange(population) %>%\n  head()\n\n                 state abb        region population total       rate\n1              Wyoming  WY          West     563626     5  0.8871131\n2 District of Columbia  DC         South     601723    99 16.4527532\n3              Vermont  VT     Northeast     625741     2  0.3196211\n4         North Dakota  ND North Central     672591     4  0.5947151\n5               Alaska  AK          West     710231    19  2.6751860\n6         South Dakota  SD North Central     814180     8  0.9825837\n\n\nWith arrange we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by rate instead:\n\nmurders %>%\n  arrange(rate) %>%\n  head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102\n\n\nNote that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type:\n\nmurders %>%\n  arrange(desc(rate))\n\n\nNested sorting\nIf we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region, then within region we order by murder rate:\n\nmurders %>%\n  arrange(region, rate) %>%\n  head()\n\n          state abb    region population total      rate\n1       Vermont  VT Northeast     625741     2 0.3196211\n2 New Hampshire  NH Northeast    1316470     5 0.3798036\n3         Maine  ME Northeast    1328361    11 0.8280881\n4  Rhode Island  RI Northeast    1052567    16 1.5200933\n5 Massachusetts  MA Northeast    6547629   118 1.8021791\n6      New York  NY Northeast   19378102   517 2.6679599\n\n\n\n\nThe top \\(n\\)\nIn the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:\n\nmurders %>% top_n(5, rate)\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Maryland  MD         South    5773552   293  5.074866\n4             Missouri  MO North Central    5988927   321  5.359892\n5       South Carolina  SC         South    4625364   207  4.475323\n\n\nNote that rows are not sorted by rate, only filtered. If we want to sort, we need to use arrange. Note that if the third argument is left blank, top_n filters by the last column.\n\nTRY IT\nFor these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\n\nlibrary(NHANES)\n\nWarning: package 'NHANES' was built under R version 4.3.3\n\ndata(NHANES)\n\nThe NHANES data has many missing values. The mean and sd functions in R will return NA if any of the entries of the input vector is an NA. Here is an example:\n\nlibrary(dslabs)\ndata(na_example)\nmean(na_example)\n\n[1] NA\n\nsd(na_example)\n\n[1] NA\n\n\nTo ignore the NAs we can use the na.rm argument:\n\nmean(na_example, na.rm = TRUE)\n\n[1] 2.301754\n\nsd(na_example, na.rm = TRUE)\n\n[1] 1.22338\n\n\nLet’s now explore the NHANES data.\n\nWe will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like ” 20-29”, with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref.\n\nHint: Use filter and summarize and use the na.rm = TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter.\n\nUsing a pipe, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then pull.\nNow report the min and max values for the same group.\nCompute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age and gender, filter by Gender and then use group_by.\nRepeat exercise 4 for males.\nWe can actually combine both summaries for exercises 4 and 5 into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender).\nFor males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#tibbles",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#tibbles",
    "title": "Introduction to the tidyverse",
    "section": "Tibbles",
    "text": "Tibbles\nTidy data must be stored in data frames. We have been using the murders data frame throughout the unit. In an earlier section we introduced the group_by function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?\n\nmurders %>% group_by(region)\n\n# A tibble: 51 × 6\n# Groups:   region [4]\n   state                abb   region    population total  rate\n   <chr>                <chr> <fct>          <dbl> <dbl> <dbl>\n 1 Alabama              AL    South        4779736   135  2.82\n 2 Alaska               AK    West          710231    19  2.68\n 3 Arizona              AZ    West         6392017   232  3.63\n 4 Arkansas             AR    South        2915918    93  3.19\n 5 California           CA    West        37253956  1257  3.37\n 6 Colorado             CO    West         5029196    65  1.29\n 7 Connecticut          CT    Northeast    3574097    97  2.71\n 8 Delaware             DE    South         897934    38  4.23\n 9 District of Columbia DC    South         601723    99 16.5 \n10 Florida              FL    South       19687653   669  3.40\n# ℹ 41 more rows\n\n\nNotice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble followed by dimensions. We can learn the class of the returned object using:\n\nmurders %>% group_by(region) %>% class()\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe tbl, pronounced tibble, is a special kind of data frame. The functions group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. For consistency, the dplyr manipulation verbs (select, filter, mutate, and arrange) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble.\nTibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.\n\nTibbles display better\nThe print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing murders and the output of murders if we convert it to a tibble. We can do this using as_tibble(murders). If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.\n\n\nSubsets of tibbles are tibbles\nIf you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:\n\nclass(murders[,4])\n\n[1] \"numeric\"\n\n\nis not a data frame. With tibbles this does not happen:\n\nclass(as_tibble(murders)[,4])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThis is useful in the tidyverse since functions require data frames as input.\nWith tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $:\n\nclass(as_tibble(murders)$population)\n\n[1] \"numeric\"\n\n\nA related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write Population instead of population this:\n\nmurders$Population\n\nNULL\n\n\nreturns a NULL with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:\n\nas_tibble(murders)$Population\n\nWarning: Unknown or uninitialised column: `Population`.\n\n\nNULL\n\n\n\n\nTibbles can have complex entries\nWhile data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:\n\ntibble(id = c(1, 2, 3), func = c(mean, median, sd))\n\n# A tibble: 3 × 2\n     id func  \n  <dbl> <list>\n1     1 <fn>  \n2     2 <fn>  \n3     3 <fn>  \n\n\n\n\nTibbles can be grouped\nThe function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information.\n\n\nCreate a tibble using tibble instead of data.frame\nIt is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the tibble function.\n\ngrades <- tibble(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\n\nNote that base R (without packages loaded) has a function with a very similar name, data.frame, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default data.frame coerces characters into factors without providing a warning or message:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo avoid this, we use the rather cumbersome argument stringsAsFactors:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90),\n                     stringsAsFactors = FALSE)\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo convert a regular data frame to a tibble, you can use the as_tibble function.\n\nas_tibble(grades) %>% class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\""
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#the-dot-operator",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#the-dot-operator",
    "title": "Introduction to the tidyverse",
    "section": "The dot operator",
    "text": "The dot operator\nOne of the advantages of using the pipe %>% is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:\n\ntab_1 <- filter(murders, region == \"South\")\ntab_2 <- mutate(tab_1, rate = total / population * 10^5)\nrates <- tab_2$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nWe can avoid defining any new intermediate objects by instead typing:\n\nfilter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  summarize(median = median(rate)) %>%\n  pull(median)\n\n[1] 3.398069\n\n\nWe can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the pull function was not available and we wanted to access tab_2$rate? What data frame name would we use? The answer is the dot operator.\nFor example to access the rate vector without the pull function we could use\n\nrates <-   filter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  .$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nIn the next section, we will see other instances in which using the . is useful."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#do",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#do",
    "title": "Introduction to the tidyverse",
    "section": "do",
    "text": "do\nThe tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %>%, tidyverse functions consistently take data frames and return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The quantile function is an example we described earlier. The do function serves as a bridge between R functions such as quantile and the tidyverse. The do function understands grouped tibbles and always returns a data frame.\nIn the summarize section (above), we noted that if we attempt to use quantile to obtain the min, median and max in one call, we will receive something unexpected. Prior to R 4.1, we would receive an error. After R 4.1, we actually get:\n\ndata(heights)\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nWe probably wanted three columns: min, median, and max. We can use the do function to fix this.\nFirst we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame. Note that it returns a single-row data frame.\n\nmy_summary <- function(dat){\n  x <- quantile(dat$height, c(0, 0.5, 1))\n  tibble(min = x[1], median = x[2], max = x[3])\n}\n\nWe can now apply the function to the heights dataset to obtain the summaries:\n\nheights %>%\n  group_by(sex) %>%\n  my_summary\n\n# A tibble: 1 × 3\n    min median   max\n  <dbl>  <dbl> <dbl>\n1    50   68.5  82.7\n\n\nBut this is not what we want. We want a summary for each sex and the code returned just one summary. This is because my_summary is not part of the tidyverse and does not know how to handled grouped tibbles. do makes this connection:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary(.))\n\n# A tibble: 2 × 4\n# Groups:   sex [2]\n  sex      min median   max\n  <fct>  <dbl>  <dbl> <dbl>\n1 Female    51   65.0  79  \n2 Male      50   69    82.7\n\n\nNote that here we need to use the dot operator. The tibble created by group_by is piped to do. Within the call to do, the name of this tibble is . and we want to send it to my_summary. If you do not use the dot, then my_summary has no argument and returns an error telling us that argument \"dat\" is missing. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary())\n\nIf you do not use the parenthesis, then the function is not executed and instead do tries to return the function. This gives an error because do must always return a data frame. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary)\n\nSo do serves as a bridge between non-tidyverse functions and the tidyverse."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#the-purrr-package",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#the-purrr-package",
    "title": "Introduction to the tidyverse",
    "section": "The purrr package",
    "text": "The purrr package\nIn previous sections (and labs) we learned about the sapply function, which permitted us to apply the same function to each element of a vector. We constructed a function and used sapply to compute the sum of the first n integers for several values of n like this:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  sum(x)\n}\nn <- 1:25\ns_n <- sapply(n, compute_s_n)\ns_n\n\n [1]   1   3   6  10  15  21  28  36  45  55  66  78  91 105 120 136 153 171 190\n[20] 210 231 253 276 300 325\n\n\nThis type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, sapply can return several different object types; for example, we might expect a numeric result from a line of code, but sapply might convert our result to character under some circumstances. purrr functions will never do this: they will return objects of a specified type or return an error if this is not possible.\nThe first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list:\n\nlibrary(purrr) # or library(tidyverse)\nn <- 1:25\ns_n <- map(n, compute_s_n)\nclass(s_n)\n\n[1] \"list\"\n\n\nIf we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values.\n\ns_n <- map_dbl(n, compute_s_n)\nclass(s_n)\n\n[1] \"numeric\"\n\n\nThis produces the same results as the sapply call shown above.\nA particularly useful purrr function for interacting with the rest of the tidyverse is map_df, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a Argument 1 must have names error:\n\ns_n <- map_df(n, compute_s_n)\n\nWe need to change the function to make this work:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x))\n}\ns_n <- map_df(n, compute_s_n)\nhead(s_n)\n\n# A tibble: 6 × 1\n    sum\n  <int>\n1     1\n2     3\n3     6\n4    10\n5    15\n6    21\n\n\nBecause map_df returns a tibble, we can have more columns defined in our function and returned.\n\ncompute_s_n2 <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x), sumSquared = sum(x^2))\n}\ns_n <- map_df(n, compute_s_n2)\nhead(s_n)\n\n# A tibble: 6 × 2\n    sum sumSquared\n  <int>      <dbl>\n1     1          1\n2     3          5\n3     6         14\n4    10         30\n5    15         55\n6    21         91\n\n\nThe purrr package provides much more functionality not covered here. For more details you can consult this online resource."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#tidyverse-conditionals",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01a.html#tidyverse-conditionals",
    "title": "Introduction to the tidyverse",
    "section": "Tidyverse conditionals",
    "text": "Tidyverse conditionals\nA typical data analysis will often involve one or more conditional operations. In the section on Conditionals, we described the ifelse function, which we will use extensively in this book. In this section we present two dplyr functions that provide further functionality for performing conditional operations.\n\ncase_when\nThe case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0:\n\nx <- c(-2, -1, 0, 1, 2)\ncase_when(x < 0 ~ \"Negative\",\n          x > 0 ~ \"Positive\",\n          x == 0  ~ \"Zero\")\n\n[1] \"Negative\" \"Negative\" \"Zero\"     \"Positive\" \"Positive\"\n\n\nA common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this:\n\nmurders %>%\n  mutate(group = case_when(\n    abb %in% c(\"ME\", \"NH\", \"VT\", \"MA\", \"RI\", \"CT\") ~ \"New England\",\n    abb %in% c(\"WA\", \"OR\", \"CA\") ~ \"West Coast\",\n    region == \"South\" ~ \"South\",\n    TRUE ~ \"Other\")) %>%\n  group_by(group) %>%\n  summarize(rate = sum(total) / sum(population) * 10^5)\n\n# A tibble: 4 × 2\n  group        rate\n  <chr>       <dbl>\n1 New England  1.72\n2 Other        2.71\n3 South        3.63\n4 West Coast   2.90\n\n\nThat TRUE on the fourth line of case_when serves as a catch-all. As case_when steps through the conditions, if none of them are true, it comes to the last line. Since TRUE is always true, the function will return “Other”. Leaving out the last line of case_when would result in NA values for any observation that fails the first three conditionals. This may or may not be what you want.\n\n\nbetween\nA common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type\n\nx >= a & x <= b\n\nHowever, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation.\n\nbetween(x, a, b)\n\n\nTRY IT\n\nLoad the murders dataset. Which of the following is true?\n\n\nmurders is in tidy format and is stored in a tibble.\nmurders is in tidy format and is stored in a data frame.\nmurders is not in tidy format and is stored in a tibble.\nmurders is not in tidy format and is stored in a data frame.\n\n\nUse as_tibble to convert the murders data table into a tibble and save it in an object called murders_tibble.\nUse the group_by function to convert murders into a tibble that is grouped by region.\nWrite tidyverse code that is equivalent to this code:\n\n\nexp(mean(log(murders$population)))\n\nWrite it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with murders %>%.\n\nUse the map_df to create a data frame with three columns named n, s_n, and s_n_2. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through \\(n\\) with \\(n\\) the row number."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html",
    "title": "Introduction to Visualization",
    "section": "",
    "text": "Looking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table:\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nWhat do you learn from staring at this table? Even though it is a relatively straightforward table, we can’t learn anything. For starters, it is grossly abbreviated, though you could scroll through. In doing so, how quickly might you be able to determine which states have the largest populations? Which states have the smallest? How populous is a typical state? Is there a relationship between population size and total murders? How do murder rates vary across regions of the country? For most folks, it is quite difficult to extract this information just by looking at the numbers. In contrast, the answer to the questions above are readily available from examining this plot:\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(ggrepel)\n\nr <- murders %>%\n  summarize(pop=sum(population), tot=sum(total)) %>%\n  mutate(rate = tot/pop*10^6) %>% pull(rate)\n\nmurders %>% ggplot(aes(x = population/10^6, y = total, label = abb)) +\n  geom_abline(intercept = log10(r), lty=2, col=\"darkgrey\") +\n  geom_point(aes(color=region), size = 3) +\n  geom_text_repel() +\n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") +\n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\") +\n  scale_color_discrete(name=\"Region\") +\n  theme_economist_white()\n\n\n\n\nWe are reminded of the saying: “A picture is worth a thousand words”. Data visualization provides a powerful way to communicate a data-driven finding. In some cases, the visualization is so convincing that no follow-up analysis is required. You should consider visualization the most potent tool in your data analytics arsenal.\nThe growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many industries, academia, and government. A salient example is news organizations, which are increasingly embracing data journalism and including effective infographics as part of their reporting.\nA particularly salient example—given the current state of the world—is a Wall Street Journal article1 showing data related to the impact of vaccines on battling infectious diseases. One of the graphs shows measles cases by US state through the years with a vertical line demonstrating when the vaccine was introduced.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n(Source: Wall Street Journal)\nAnother striking example comes from a New York Times chart2, which summarizes scores from the NYC Regents Exams. As described in the article3, these scores are collected for several reasons, including to determine if a student graduates from high school. In New York City you need a 65 to pass. The distribution of the test scores forces us to notice something somewhat problematic:\n\n\n\n\n\n(Source: New York Times via Amanda Cox)\nThe most common test score is the minimum passing grade, with very few scores just below the threshold. This unexpected result is consistent with students close to passing having their scores bumped up.\nThis is an example of how data visualization can lead to discoveries which would otherwise be missed if we simply subjected the data to a battery of data analysis tools or procedures. Data visualization is the strongest tool of what we call exploratory data analysis (EDA). John W. Tukey4, considered the father of EDA, once said,\n\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.”\n\n\nMany widely used data analysis tools were initiated by discoveries made via EDA. EDA is perhaps the most important part of data analysis, yet it is one that is often overlooked.\nData visualization is also now pervasive in philanthropic and educational organizations. In the talks New Insights on Poverty5 and The Best Stats You’ve Ever Seen6, Hans Rosling forces us to notice the unexpected with a series of plots related to world health and economics. In his videos, he uses animated graphs to show us how the world is changing and how old narratives are no longer true.\n\n\n\n\n\nIt is also important to note that mistakes, biases, systematic errors and other unexpected problems often lead to data that should be handled with care. Failure to discover these problems can give rise to flawed analyses and false discoveries. As an example, consider that measurement devices sometimes fail and that most data analysis procedures are not designed to detect these. Yet these data analysis procedures will still give you an answer. The fact that it can be difficult or impossible to notice an error just from the reported results makes data visualization particularly important.\nToday, we will discuss the basics of data visualization and exploratory data analysis. We will use the ggplot2 package to code. To learn the very basics, we will start with a somewhat artificial example: heights reported by students. Then we will cover the two examples mentioned above: 1) world health and economics and 2) infectious disease trends in the United States.\nOf course, there is much more to data visualization than what we cover here. The following are references for those who wish to learn more:\n\nER Tufte (1983) The visual display of quantitative information. Graphics Press.\nER Tufte (1990) Envisioning information. Graphics Press.\nER Tufte (1997) Visual explanations. Graphics Press.\nWS Cleveland (1993) Visualizing data. Hobart Press.\nWS Cleveland (1994) The elements of graphing data. CRC Press.\nA Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach: Turning tables into graphs. The American Statistician 56:121-130.\nNB Robbins (2004) Creating more effective graphs. Wiley.\nA Cairo (2013) The functional art: An introduction to information graphics and visualization. New Riders.\nN Yau (2013) Data points: Visualization that means something. Wiley.\n\nWe also do not cover interactive graphics, a topic that is both too advanced for this course and too unweildy. Some useful resources for those interested in learning more can be found below, and you are encouraged to draw inspiration from those websites in your projects:\n\nhttps://shiny.rstudio.com/\nhttps://d3js.org/"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#the-components-of-a-graph",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#the-components-of-a-graph",
    "title": "Introduction to Visualization",
    "section": "The components of a graph",
    "text": "The components of a graph\nWe will eventually construct a graph that summarizes the US murders dataset that looks like this:\n\n\n\n\n\nWe can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.\nThis data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.\nThe first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main five components to note are:\n\nData: The US murders data table is being summarized. We refer to this as the data component.\nGeometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, boxplot, pie (ew!), and many, many more. We will learn about these later.\nAesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using.\nAnnotations: These are things like axis labels, axis ticks (the lines along the axis at regular intervals or specific points of interest), axis scales (e.g. log-scale), titles, legends, etc.\nStyle: An overall appearance of the graph determined by fonts, color palattes, layout, blank spaces, and more.\n\nWe also note that:\n\nThe points are labeled with the state abbreviations.\nThe range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.\nThere are labels, a title, a legend, and we use the style of The Economist magazine.\n\nAll of the flexibility and visualization power of ggplot is contained in these four elements (plus your data)"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#ggplot-objects",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#ggplot-objects",
    "title": "Introduction to Visualization",
    "section": "ggplot objects",
    "text": "ggplot objects\nWe will now construct the plot piece by piece.\nWe start by loading the dataset:\n\nlibrary(dslabs)\ndata(murders)\n\n\n\n\nThe first step in creating a ggplot2 graph is to define a ggplot object. We do this with the function ggplot, which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object:\n\nggplot(data = murders)\n\nWe can also pipe the data in as the first argument. So this line of code is equivalent to the one above:\n\nmurders %>% ggplot()\n\n\n\n\nIt renders a plot, in this case a blank slate since no geometry has been defined. The only style choice we see is a grey background.\nWhat has happened above is that the object was created and, because it was not assigned, it was automatically evaluated. But we can assign our plot to an object, for example like this:\n\np <- ggplot(data = murders)\nclass(p)\n\n[1] \"gg\"     \"ggplot\"\n\n\nTo render the plot associated with this object, we simply print the object p. The following two lines of code each produce the same plot we see above:\n\nprint(p)\np"
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#geometries-briefly",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#geometries-briefly",
    "title": "Introduction to Visualization",
    "section": "Geometries (briefly)",
    "text": "Geometries (briefly)\nIn ggplot2 we create graphs by adding geometry layers. Layers can define geometries, compute summary statistics, define what scales to use, create annotations, or even change styles. To add layers, we use the symbol +. In general, a line of code will look like this:\n\nDATA %>% ggplot() + LAYER 1 + LAYER 2 + ... + LAYER N\n\nUsually, the first added layer after ggplot() + defines the geometry. After that, we may add additional geometries, we may rescale an axis, we may add annotations and labels, or we may change the style. For now, we want to make a scatterplot like the one you all created in Lab 0. What geometry do we use?\n\n\n\n\n\nTaking a quick look at the cheat sheet, we see that the ggplot2 function used to create plots with this geometry is geom_point.\nSee Here\n(Image courtesy of RStudio9. CC-BY-4.0 license10.)\n\nGeometry function names follow the pattern: geom_X where X is the name of some specific geometry. Some examples include geom_point, geom_bar, and geom_histogram. You’ve already seen a few of these. We will start with a scatterplot created using geom_point() for now, then circle back to more geometries after we cover aesthetic mappings, layers, and annotations.\nFor geom_point to run properly we need to provide data and an aesthetic mapping. The simplest mapping for a scatter plot is to say we want one variable on the X-axis, and a different one on the Y-axis, so each point is an {X,Y} pair. That is an aesthetic mapping because X and Y are aesthetics in a geom_point scatterplot.\nWe have already connected the object p with the murders data table, and if we add the layer geom_point it defaults to using this data. To find out what mappings are expected, we read the Aesthetics section of the help file ?geom_point help file:\n> Aesthetics\n>\n> geom_point understands the following aesthetics (required aesthetics are in bold):\n>\n> **x**\n>\n> **y**\n>\n> alpha\n>\n> colour\n>\n> fill\n>\n> group\n>\n> shape\n>\n> size\n>\n> stroke\nand—although it does not show in bold above—we see that at least two arguments are required: x and y. You can’t have a geom_point scatterplot unless you state what you want on the X and Y axes."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#aesthetic-mappings",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#aesthetic-mappings",
    "title": "Introduction to Visualization",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\nAesthetic mappings describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The aes function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the aes function is often used as the argument of a geometry function. This example produces a scatterplot of population in millions (x-axis) versus total murders (y-axis):\n\nmurders %>% ggplot() +\n  geom_point(aes(x = population/10^6, y = total))\n\nInstead of defining our plot from scratch, we can also add a layer to the p object that was defined above as p <- ggplot(data = murders):\n\np + geom_point(aes(x = population/10^6, y = total))\n\n\n\n\nThe scales and annotations like axis labels are defined by default when adding this layer (note the x-axis label is exactly what we wrote in the function call). Like dplyr functions, aes also uses the variable names from the object component: we can use population and total without having to call them as murders$population and murders$total. The behavior of recognizing the variables from the data component is quite specific to aes. With most functions, if you try to access the values of population or total outside of aes you receive an error.\nNote that we did some rescaling within the aes() call - we can do simple things like multiplication or division on the variable names in the ggplot call. The axis labels reflect this. We will change the axis labels later.\nThe aesthetic mappings are very powerful - changing the variable in x= or y= changes the meaning of the plot entirely. We’ll come back to additional aesthetic mappings once we talk about aesthetics in general.\n\nAesthetics in general\nEven without mappings, a plots aesthetics can be useful. Things like color, fill, alpha, and size are aesthetics that can be changed.\nLet’s say we want larger points in our scatterplot. The size aesthetic can be used to set the size. The scale of size is “multiples of the defaults” (so size = 1 is the default)\n\np + geom_point(aes(x = population/10^6, y = total), size = 3)\n\n\n\n\nsize is not a mapping so it is not in the aes() part: whereas mappings use data from specific observations and need to be inside aes(), operations we want to affect all the points the same way do not need to be included inside aes. We’ll see what happens if size is inside aes(size = xxx) in a second.\nWe can change the shape to one of the many different base-R options found here:\n\np + geom_point(aes(x = population/10^6, y = total), size = 3, shape = 17)\n\n\n\n\nWe can also change the fill and the color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 4, shape = 23, fill = '#18453B')\n\n\n\n\nfill can take a common name like 'green', or can take a hex color like '#18453B', which is MSU Green according to MSU’s branding site. You can also find UM Maize and OSU Scarlet on respective branding pages, or google “XXX color hex.” We’ll learn how to build a color palatte later on.\ncolor (or colour, same thing because ggplot creators allow both spellings) is a little tricky with points - it changes the outline of the geometry rather than the fill color, but in geom_point() most shapes are only the outline, including the default. This is more useful with, say, a barplot where the outline and the fill might be different colors. Still, shapes 21-25 have both fill and color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 5, shape = 23, fill = '#18453B', color = 'white')\n\n\n\n\nThe color = 'white' makes the outline of the shape white, which you can see if you look closely in the areas where the shapes overlap. This only works with shapes 21-25, or any other geometry that has both an outline and a fill.\n\n\nNow, back to aesthetic mappings\nNow that we’ve seen a few aesthetics (and know we can find more by looking at which aesthetics work with our geometry in the help file), let’s return to the power of aesthetic mappings.\nAn aesthetic mapping means we can vary an aesthetic (like fill or shape or size) according to some variable in our data. This opens up a world of possibilities! Let’s try adding to our x and y aesthetics with a color aesthetic (since points respond to color better than fill) that varies by region, which is a column in our data:\n\np + geom_point(aes(x = population/10^6, y = total, color = region), size = 3)\n\n\n\n\nWe include color=region inside the aes call, which tells R to find a variable called region and change color based on that. R will choose a somewhat ghastly color palatte, and every unique value in the data for region will get a different color if the variable is discrete. If the variable is a continuous value, then ggplot will automatically make a color ramp. Thus, discrete and continuous values for aesthetic mappings work differently.\nLet’s see a useful example of a continuous aesthetic mapping to color. In our data, we are making a scatterplot of population and total murders, which really just shows that states with higher populations have higher murders. What we really want is murders per capita (I think COVID taught us a lot about rates vs. levels like “cases” and “cases per 100,000 people”). We can create a variable of “murders per capita” on the fly. Since “murders per capita” is a very small number and hard to read, we’ll multiply by 100 so that we get “percent of population murdered per year”:\n\np + geom_point(aes(x = population/10^5, y = total, color = 100*total/population), size = 3)\n\n\n\n\nWhile the clear pattern of “more population means more murders” is still there, look at the outlier in light blue in the bottom left. With the color ramp, see how easy it is to see here that there is one location where murders per capita is quite high?\nNote that size is outside of aes and is set to an explicit value, not to a variable. What if we set size to a variable in the data?\n\np + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6))\n\n\n\n\n\n\nLegends for aesthetics\nHere we see yet another useful default behavior: ggplot2 automatically adds a legend that maps color to region, and size to population (which we scaled by 1,000,000). To avoid adding this legend we set the geom_point argument show.legend = FALSE. This removes both the size and the color legend.\n\np + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6), show.legend = FALSE)\n\n\n\n\nLater on, when we get to annotation layers, we’ll talk about controlling the legend text and layout. For now, we just need to know how to turn them off."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#annotation-layers",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#annotation-layers",
    "title": "Introduction to Visualization",
    "section": "Annotation Layers",
    "text": "Annotation Layers\nA second layer in the plot we wish to make involves adding a label to each point to identify the state. The geom_label and geom_text functions permit us to add text to the plot with and without a rectangle behind the text, respectively.\nBecause each point (each state in this case) has a label, we need an aesthetic mapping to make the connection between points and labels. By reading the help file ?geom_text, we learn that we supply the mapping between point and label through the label argument of aes. That is, label is an aesthetic that we can map. So the code looks like this:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\n\n\n\nWe have successfully added a second layer to the plot.\nAs an example of the unique behavior of aes mentioned above, note that this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total, label = abb))\n\nis fine, whereas this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total), label = abb)\n\nwill give you an error since abb is not found because it is outside of the aes function. The layer geom_text does not know where to find abb since it is a column name and not a global variable, and ggplot does not look for column names for non-mapped aesthetics. For a trivial example:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(population/10^6, total), label = 'abb')\n\n\n\n\n\nGlobal versus local aesthetic mappings\nIn the previous line of code, we define the mapping aes(population/10^6, total) twice, once in each geometry. We can avoid this by using a global aesthetic mapping. We can do this when we define the blank slate ggplot object. Remember that the function ggplot contains an argument that permits us to define aesthetic mappings:\n\nargs(ggplot)\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \nNULL\n\n\nIf we define a mapping in ggplot, all the geometries that are added as layers will default to this mapping. We redefine p:\n\np <- murders %>% ggplot(aes(x = population/10^6, y = total, label = abb))\n\nand then we can simply write the following code to produce the previous plot:\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 1.5) # offsets the label\n\nWe keep the size and nudge_x arguments in geom_point and geom_text, respectively, because we want to only increase the size of points and only nudge the labels. If we put those arguments in aes then they would apply to both plots. Also note that the geom_point function does not need a label argument and therefore ignores that aesthetic.\nIf necessary, we can override the global mapping by defining a new mapping within each layer. These local definitions override the global. Here is an example:\n\np + geom_point(size = 3) +\n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))\n\n\n\n\nClearly, the second call to geom_text does not use x = population and y = total."
  },
  {
    "objectID": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#try-it",
    "href": "content/Week_01_-_The_Tidyverse_and_Vis/01b.html#try-it",
    "title": "Introduction to Visualization",
    "section": "Try it!",
    "text": "Try it!\n\nLet’s break in to smaller groups and try playing with some of the aesthetics and aesthetic mappings. If we’re in person (woohoo!), we’ll form the same number of groups in class.\nIn each group, one person should be the main coder - someone who has the packages like dslabs installed and has successfully run the plots above. Each set of tasks ask you to learn about an aesthetic and put it into action with the murder data. We’ll leave about 5 minutes to do the task, then have you come back and share your results with the class.\nFor each group, we’ll start with the following code:\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\nThe alpha aesthetic mapping.\n\nThe alpha aesthetic can only take a number between 0 and 1. So first, in murders, create a murders_per_capita column by dividing total by population. Second, find the max(murders$murders_per_capita) and then create another new column called murders_per_capita_rescaled which divides murders_per_capita by the max value. murders_per_capita_rescaled will be between 0 and 1, with the value of 1 for the state with the max murder rate. This is a little hard to do on the fly in ggplot.\nSet the alpha aesthetic mapping to murders_per_capita_rescaled for geom_point.\nTurn off the legend using show.legend=FALSE\nInclude the geom_text labels, but make sure the aesthetic mapping does not apply to the labels.\nUse nudge_x = 1.5 as before to offset the labels.\nBe able to explain the plot.\n\nDoes the alpha aesthetic help present the data here? It’s OK if it doesn’t!\n\n\nThe stroke aesthetic mapping.\n\nThe stroke aesthetic works a bit like the size aesthetic. It must be used with a plot that has both a border and a fill, like shapes 21-25, so use one of those.\nUse the stroke aesthetic mapping (meaning the stroke will change according to a value in the data) to set a different stroke size based on murders per capita. You can create a murders per capita variable on the fly, or add it to your murders data.\n\nInclude the text labels as before and use nudge_x = 1.5.\nMake sure you’re only setting the aesthetic for the points on the scatterplot!\n\n\nThe angle aesthetic\n\nUsing the ?geom_text help, note that geom_text takes an aesthetic of angle.\nUse the angle aesthetic (not aesthetic mapping) in the appropriate place (e.g. on geom_text and not on other geometries) to adjust the labels on our plot.\nNow, try using the angle aesthetic mapping by using the total field as both the y value and the angle value in the geom_text layer.\nDoes using angle as an aesthetic help? What about as an aesthetic mapping?\n\nThe color aesthetic mapping\n\nSet the color aesthetic mapping in geom_text to total/population.\n\nUse the nudge_x = 1.5 aesthetic in geom_text still\n\nTry it with and without the legend using show.legend.\nBe able to explain the plot.\n\nDoes the color aesthetic mapping help present the data here?\n\n\ngeom_label and the fill aesthetic\n\nLooking at ?geom_label (which is the same help as geom_text), we note that “The fill aesthetic controls the backgreound colour of the label”.\nSet the fill aesthetic mapping to total/population in geom_label (replacing geom_text but still using nudge_x=1.5)\nSet the fill aesthetic (not mapping) to the color of your choice.\nBe able to explain the plots.\n\n\nDoes the fill aesthetic mapping help present the data here?\nWhat color did you choose for the non-mapped fill aesthetic?"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Course Content",
    "section": "",
    "text": "Each week has two sets of required readings (pages in the sidebar) that you should complete before coming to lecture. Read the first before our first meeting of the week, and read the second before the second meetings. That is, you should complete the reading, attend Tuesday class, then do the associated “exercises” (contained within the reading) before Thursday. You will be working each week’s lab between Thursday afternoon and Monday at 11:59 PM (when the labs are due). Don’t forget your weekly writing in between, due Saturday at 11:59pm.\nThe course content is structured as follows. For each topic, we begin with a set of questions that might guide your reading and help frame your thoughts. These questions can serve as helpful starting places for your thinking; they are not representative of the totality of the content and are not intended to be limiting. You should not try to respond to all of these (or any of them if you don’t want to)—they’ll just help you know what to look for and think about as you read."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EC242 - Social Science Data Analytics",
    "section": "",
    "text": "Date: Date and time\nRoom: Location\nInstructors: Instructor A, B, C\n\n\n\n\n\n\n\nThis workshop template\n\n\n\nThis workshop template contains 4 pages:\n\nHome: index.qmd (this page)\nAbout: about.qmd\n\nTwo content pages\n\nPage without code: part_1_prep.qmd\nPage with R code: part_2_eda.qmd\nAnd another: content/Week 00/00a.qmd\n\nIt is straightforward to add more content pages: you need to create a new .qmd file (or copy/paste the existing ones), then link the new page inside _quarto.yml.\n\n\n\n\n\n\n\n\nHomepage of your workshop\n\n\n\nThis is the homepage index.qmd for your workshop, so ideally it should contain some key information such as time and place, instructors and information of the course/workshop.\nIt should be easy to navigate.\n\n\n\nWelcome!\n\nThe goal of the workshop is to … (insert your message)\nFor example, introduce kep concepts in machine learning, such as regularisation.\nWorkshop material can be found in the workshop github repository.\n\n\nLearning Objectives\nAt the end of the tutorial, participants will be able to\n\nunderstand key concepts for … (insert your message)\nFor example, training machine learning models such as regularisation.\nanother objective\n\n\n\nPre-requisites\n\nBasic familiarity with R\nSome other knowledge\n\n\n\n\nSchedule\n\n\n\n\n\n\nTabular schedule\n\n\n\nIt can be useful to include a tabular schedule with links.\n\n\n\n\n\nTime\nTopic\nPresenter\n\n\n\n\n9:00 - 10:30\nSession 1: Preparation\nInstructor A\n\n\n10:45 - 12:00\nSession 2: Exploratory Analysis\nInstructor B, C"
  },
  {
    "objectID": "part_1_prep.html",
    "href": "part_1_prep.html",
    "title": "Title Preparation",
    "section": "",
    "text": "Page without code\n\n\n\nThis page contains an example for some structured preparation information for a workshop. No code is executed here.\nHere are some preparation information for the participants."
  },
  {
    "objectID": "part_1_prep.html#software",
    "href": "part_1_prep.html#software",
    "title": "Title Preparation",
    "section": "Software",
    "text": "Software\nIn this workshop we will be using R. You can either\n\nhave R and Rstudio installed on your laptop\nor, use Posit cloud (formerly Rstudio Cloud).\n\nPosit cloud is free of charge for personal users, yet you need to sign up for a new user account and have internet connection.\nThe R package we are using is glmnet."
  },
  {
    "objectID": "part_1_prep.html#data",
    "href": "part_1_prep.html#data",
    "title": "Title Preparation",
    "section": "Data",
    "text": "Data\nThe datasets we use can be found here (insert link)."
  },
  {
    "objectID": "part_1_prep.html#code",
    "href": "part_1_prep.html#code",
    "title": "Title Preparation",
    "section": "Code",
    "text": "Code\nThe R scripts used in part 1 and part 2 can be found here (insert link)."
  },
  {
    "objectID": "part_1_prep.html#resources",
    "href": "part_1_prep.html#resources",
    "title": "Title Preparation",
    "section": "Resources",
    "text": "Resources\nLecture notes (insert link)\nLab notes (insert link)"
  },
  {
    "objectID": "part_2_eda.html",
    "href": "part_2_eda.html",
    "title": "Part I",
    "section": "",
    "text": "Page with R code\n\n\n\nThis page contains an example template for a lab session, where R code and results are displayed here.\nYou can find more information on how to include code in Quarto website here.\nYou can experiment with code-fold and code-tools in the yaml header above to change how the code cells look like."
  },
  {
    "objectID": "part_2_eda.html#a-cancer-modeling-example",
    "href": "part_2_eda.html#a-cancer-modeling-example",
    "title": "Part I",
    "section": "A Cancer Modeling Example",
    "text": "A Cancer Modeling Example\nExercise on analysis of miRNA, mRNA and protein data from the paper Aure et al, Integrated analysis reveals microRNA networks coordinately expressed with key proteins in breast cancer, Genome Medicine, 2015.\nPlease run the code provided to replicate some of the analyses. Make sure you can explain what all the analysis steps do and that you understand all the results.\nIn addition, there are some extra tasks (Task 1), where no R code is provided. Please do these tasks when you have time available at the end of the lab.\n\nLoad the data\nRead the data, and convert to matrix format.\n\nmrna <- read.table(\"data/data_example.txt\", header=T, sep=\"\\t\", dec=\".\")\n\n# Convert to matrix format\n\nmrna <- as.matrix(mrna)\n\nPrint the data\n\nmrna[1:4, 1:4]\n\n      OSL2R.3002T4 OSL2R.3005T1 OSL2R.3013T1 OSL2R.3030T2\nACACA      1.60034     -0.49087     -0.26553     -0.27857\nANXA1     -2.42501     -0.05416     -0.46478     -2.18393\nAR         0.39615     -0.43348     -0.10232      0.58299\nBAK1       0.78627      0.39897      0.22598     -1.31202\n\n\nVisualise the overall distribution of expression levels by histogram\n\nhist(mrna, nclass=40, xlim=c(-5,5), col=\"lightblue\")\n\n\n\n\n\n\n\n\n\n\nTask 1\n\n\n\nThis is a callout-note, and it can be quite useful for exercises. You can find more about callout here.\nExample: Extend the above analysis to cover all genes."
  },
  {
    "objectID": "content/index.html#content-navigation",
    "href": "content/index.html#content-navigation",
    "title": "Course Content",
    "section": "Content navigation",
    "text": "Content navigation\nUse the links on the sidebar to navigate between required reading. Readings are ordered by week."
  },
  {
    "objectID": "content/Week_10/10a.html",
    "href": "content/Week_10/10a.html",
    "title": "The Bias-Variance Tradeoff",
    "section": "",
    "text": "This page.\n Chapter 2 in Introduction to Statistical Learning with Applications in R.\n\n\n\n\nWhat is the relationship between bias, variance, and mean squared error?\nWhat is the relationship between model flexibility and training error?\nWhat is the relationship between model flexibility and validation (or test) error?"
  },
  {
    "objectID": "content/Week_10/10a.html#r-setup-and-source",
    "href": "content/Week_10/10a.html#r-setup-and-source",
    "title": "The Bias-Variance Tradeoff",
    "section": "R Setup and Source",
    "text": "R Setup and Source\n\nlibrary(tibble)     # data frame printing\nlibrary(dplyr)      # data manipulation\n\nlibrary(caret)      # fitting knn\nlibrary(rpart)      # fitting trees\nlibrary(rpart.plot) # plotting trees"
  },
  {
    "objectID": "content/Week_10/10a.html#the-regression-setup",
    "href": "content/Week_10/10a.html#the-regression-setup",
    "title": "The Bias-Variance Tradeoff",
    "section": "The Regression Setup",
    "text": "The Regression Setup\nConsider the general regression setup where we are given a random pair \\((X, Y) \\in \\mathbb{R}^p \\times \\mathbb{R}\\). We would like to “predict” \\(Y\\) with some function of \\(X\\), say, \\(f(X)\\).\nTo clarify what we mean by “predict,” we specify that we would like \\(f(X)\\) to be “close” to \\(Y\\). To further clarify what we mean by “close,” we define the squared error loss of estimating \\(Y\\) using \\(f(X)\\).\n\\[\nL(Y, f(X)) \\triangleq (Y - f(X)) ^ 2\n\\]\nNow we can clarify the goal of regression, which is to minimize the above loss, on average. We call this the risk of estimating \\(Y\\) using \\(f(X)\\).\n\\[\nR(Y, f(X)) \\triangleq \\mathbb{E}[L(Y, f(X))] = \\mathbb{E}_{X, Y}[(Y - f(X)) ^ 2]\n\\]\nBefore attempting to minimize the risk, we first re-write the risk after conditioning on \\(X\\).\n\\[\n\\mathbb{E}_{X, Y} \\left[ (Y - f(X)) ^ 2 \\right] = \\mathbb{E}_{X} \\mathbb{E}_{Y \\mid X} \\left[ ( Y - f(X) ) ^ 2 \\mid X = x \\right]\n\\]\nMinimizing the right-hand side is much easier, as it simply amounts to minimizing the inner expectation with respect to \\(Y \\mid X\\), essentially minimizing the risk pointwise, for each \\(x\\).\nIt turns out, that the risk is minimized by setting \\(f(x)\\) to be equal the conditional mean of \\(Y\\) given \\(X\\),\n\\[\nf(x) = \\mathbb{E}(Y \\mid X = x)\n\\]\nwhich we call the regression function.1\nNote that the choice of squared error loss is somewhat arbitrary. Suppose instead we chose absolute error loss.\n\\[\nL(Y, f(X)) \\triangleq | Y - f(X) |\n\\]\nThe risk would then be minimized setting \\(f(x)\\) equal to the conditional median.\n\\[\nf(x) = \\text{median}(Y \\mid X = x)\n\\]\nDespite this possibility, our preference will still be for squared error loss. The reasons for this are numerous, including: historical, ease of optimization, and protecting against large deviations.\nNow, given data \\(\\mathcal{D} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}\\), our goal becomes finding some \\(\\hat{f}\\) that is a good estimate of the regression function \\(f\\). We’ll see that this amounts to minimizing what we call the reducible error."
  },
  {
    "objectID": "content/Week_10/10a.html#reducible-and-irreducible-error",
    "href": "content/Week_10/10a.html#reducible-and-irreducible-error",
    "title": "The Bias-Variance Tradeoff",
    "section": "Reducible and Irreducible Error",
    "text": "Reducible and Irreducible Error\nSuppose that we obtain some \\(\\hat{f}\\), how well does it estimate \\(f\\)? We define the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\). A good \\(\\hat{f}\\) will have a low expected prediction error.\n\\[\n\\text{EPE}\\left(Y, \\hat{f}(X)\\right) \\triangleq \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  \\left( Y - \\hat{f}(X) \\right)^2 \\right]\n\\]\nThis expectation is over \\(X\\), \\(Y\\), and also \\(\\mathcal{D}\\). The estimate \\(\\hat{f}\\) is actually random depending on the data, \\(\\mathcal{D}\\), used to estimate \\(\\hat{f}\\). We could actually write \\(\\hat{f}(X, \\mathcal{D})\\) to make this dependence explicit, but our notation will become cumbersome enough as it is.\nLike before, we’ll condition on \\(X\\). This results in the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\) when \\(X = x\\).\n\\[\n\\text{EPE}\\left(Y, \\hat{f}(x)\\right) =\n\\mathbb{E}_{Y \\mid X, \\mathcal{D}} \\left[  \\left(Y - \\hat{f}(X) \\right)^2 \\mid X = x \\right] =\n\\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left[  \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]}_\\textrm{reducible error} +\n\\underbrace{\\mathbb{V}_{Y \\mid X} \\left[ Y \\mid X = x \\right]}_\\textrm{irreducible error}\n\\]\nA number of things to note here:\n\nThe expected prediction error is for a random \\(Y\\) given a fixed \\(x\\) and a random \\(\\hat{f}\\). As such, the expectation is over \\(Y \\mid X\\) and \\(\\mathcal{D}\\). Our estimated function \\(\\hat{f}\\) is random depending on the data, \\(\\mathcal{D}\\), which is used to perform the estimation.\nThe expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\) when \\(X = x\\) has been decomposed into two errors:\n\nThe reducible error, which is the expected squared error loss of estimation \\(f(x)\\) using \\(\\hat{f}(x)\\) at a fixed point \\(x\\). The only thing that is random here is \\(\\mathcal{D}\\), the data used to obtain \\(\\hat{f}\\). (Both \\(f\\) and \\(x\\) are fixed.) We’ll often call this reducible error the mean squared error of estimating \\(f(x)\\) using \\(\\hat{f}\\) at a fixed point \\(x\\). \\[ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) \\triangleq \\mathbb{E}_{\\mathcal{D}} \\left[  \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]\\]\nThe irreducible error. This is simply the variance of \\(Y\\) given that \\(X = x\\), essentially noise that we do not want to learn. This is also called the Bayes error.\n\n\nAs the name suggests, the reducible error is the error that we have some control over. But how do we control this error?"
  },
  {
    "objectID": "content/Week_10/10a.html#bias-variance-decomposition",
    "href": "content/Week_10/10a.html#bias-variance-decomposition",
    "title": "The Bias-Variance Tradeoff",
    "section": "Bias-Variance Decomposition",
    "text": "Bias-Variance Decomposition\nAfter decomposing the expected prediction error into reducible and irreducible error, we can further decompose the reducible error.\nRecall the definition of the bias of an estimator.\n\\[\n\\text{bias}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left[\\hat{\\theta}\\right] - \\theta\n\\]\nAlso recall the definition of the variance of an estimator.\n\\[\n\\mathbb{V}(\\hat{\\theta}) = \\text{var}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left [ ( \\hat{\\theta} -\\mathbb{E}\\left[\\hat{\\theta}\\right] )^2 \\right]\n\\]\nUsing this, we further decompose the reducible error (mean squared error) into bias squared and variance.\n\\[\n\\text{MSE}\\left(f(x), \\hat{f}(x)\\right) =\n\\mathbb{E}_{\\mathcal{D}} \\left[  \\left(f(x) - \\hat{f}(x) \\right)^2 \\right] =\n\\underbrace{\\left(f(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right]  \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}(x) \\right)} +\n\\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}(x) \\right)}\n\\]\nThis is actually a common fact in estimation theory, but we have stated it here specifically for estimation of some regression function \\(f\\) using \\(\\hat{f}\\) at some point \\(x\\).\n\\[\n\\text{MSE}\\left(f(x), \\hat{f}(x)\\right) = \\text{bias}^2 \\left(\\hat{f}(x) \\right) + \\text{var} \\left(\\hat{f}(x) \\right)\n\\]\nIn a perfect world, we would be able to find some \\(\\hat{f}\\) which is unbiased, that is \\(\\text{bias}\\left(\\hat{f}(x) \\right) = 0\\), which also has low variance. In practice, this isn’t always possible.\nIt turns out, there is a bias-variance tradeoff. That is, often, the more bias in our estimation, the lesser the variance. Similarly, less variance is often accompanied by more bias. Flexible models tend to be unbiased, but highly variable. Simple models are often extremely biased, but have low variance.\nIn the context of regression, models are biased when:\n\nParametric: The form of the model does not incorporate all the necessary variables, or the form of the relationship is too simple. For example, a parametric model assumes a linear relationship, but the true relationship is quadratic.\nNon-parametric: The model provides too much smoothing.\n\nIn the context of regression, models are variable when:\n\nParametric: The form of the model incorporates too many variables, or the form of the relationship is too flexible. For example, a parametric model assumes a cubic relationship, but the true relationship is linear.\nNon-parametric: The model does not provide enough smoothing. It is very, “wiggly.” Recall our KNN model example from Content 08\n\nSo for us, to select a model that appropriately balances the tradeoff between bias and variance, and thus minimizes the reducible error, we need to select a model of the appropriate flexibility for the data.\nRecall that when fitting models, we’ve seen that train RMSE decreases as model flexibility is increasing. (Technically it is non-increasing.) For validation RMSE, we expect to see a U-shaped curve. Importantly, validation RMSE decreases, until a certain flexibility, then begins to increase.\n\n\n\nNow we can understand why this is happening. The expected test RMSE is essentially the expected prediction error, which we now known decomposes into (squared) bias, variance, and the irreducible Bayes error. The following plots show three examples of this.\n\n\n\n\n\n\n\n\n\nThe three plots show three examples of the bias-variance tradeoff. In the left panel, the variance influences the expected prediction error more than the bias. In the right panel, the opposite is true. The middle panel is somewhat neutral. In all cases, the difference between the Bayes error (the horizontal dashed grey line) and the expected prediction error (the solid black curve) is exactly the mean squared error, which is the sum of the squared bias (blue curve) and variance (orange curve). The vertical line indicates the flexibility that minimizes the prediction error.\nTo summarize, if we assume that irreducible error can be written as\n\\[\n\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2\n\\]\nthen we can write the full decomposition of the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}\\) when \\(X = x\\) as\n\\[\n\\text{EPE}\\left(Y, \\hat{f}(x)\\right) =\n\\underbrace{\\text{bias}^2\\left(\\hat{f}(x)\\right) + \\text{var}\\left(\\hat{f}(x)\\right)}_\\textrm{reducible error} + \\sigma^2.\n\\]\nAs model flexibility increases, bias decreases, while variance increases. By understanding the tradeoff between bias and variance, we can manipulate model flexibility to find a model that will predict well on unseen observations.\n\n\n\n\n\n\n\n\n\nTying this all together, the above image shows how we “expect” training and validation error to behavior in relation to model flexibility.2 In practice, we won’t always see such a nice “curve” in the validation error, but we expect to see the general trends."
  },
  {
    "objectID": "content/Week_10/10a.html#using-simulation-to-estimate-bias-and-variance",
    "href": "content/Week_10/10a.html#using-simulation-to-estimate-bias-and-variance",
    "title": "The Bias-Variance Tradeoff",
    "section": "Using Simulation to Estimate Bias and Variance",
    "text": "Using Simulation to Estimate Bias and Variance\nWe will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function \\(f(x) = x^2\\).\n\nf = function(x) {\n  x ^ 2\n}\n\nMore specifically, we’d like to predict an observation, \\(Y\\), given that \\(X = x\\) by using \\(\\hat{f}(x)\\) where\n\\[\n\\mathbb{E}[Y \\mid X = x] = f(x) = x^2\n\\]\nand\n\\[\n\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2.\n\\]\nAlternatively, we could write this as\n\\[\nY = f(X) + \\epsilon\n\\]\nwhere \\(\\mathbb{E}[\\epsilon] = 0\\) and \\(\\mathbb{V}[\\epsilon] = \\sigma ^ 2\\). In this formulation, we call \\(f(X)\\) the signal and \\(\\epsilon\\) the noise.\nTo carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following R code.\n\ngen_sim_data = function(f, sample_size = 100) {\n  x = runif(n = sample_size, min = 0, max = 1)\n  y = rnorm(n = sample_size, mean = f(x), sd = 0.3)\n  tibble(x, y)\n}\n\nAlso note that if you prefer to think of this situation using the \\(Y = f(X) + \\epsilon\\) formulation, the following code represents the same data generating process.\n\ngen_sim_data = function(f, sample_size = 100) {\n  x = runif(n = sample_size, min = 0, max = 1)\n  eps = rnorm(n = sample_size, mean = 0, sd = 0.3)\n  y = f(x) + eps\n  tibble(x, y)\n}\n\nTo completely specify the data generating process, we have made more model assumptions than simply \\(\\mathbb{E}[Y \\mid X = x] = x^2\\) and \\(\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2\\). In particular,\n\nThe \\(x_i\\) in \\(\\mathcal{D}\\) are sampled from a uniform distribution over \\([0, 1]\\).\nThe \\(x_i\\) and \\(\\epsilon\\) are independent.\nThe \\(y_i\\) in \\(\\mathcal{D}\\) are sampled from the conditional normal distribution.\n\n\\[\nY \\mid X \\sim N(f(x), \\sigma^2)\n\\]\nUsing this setup, we will generate datasets, \\(\\mathcal{D}\\), with a sample size \\(n = 100\\) and fit four models.\n\\[\n\\begin{aligned}\n\\texttt{predict(fit0, x)} &= \\hat{f}_0(x) = \\hat{\\beta}_0\\\\\n\\texttt{predict(fit1, x)} &= \\hat{f}_1(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\\\\n\\texttt{predict(fit2, x)} &= \\hat{f}_2(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 \\\\\n\\texttt{predict(fit9, x)} &= \\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9\n\\end{aligned}\n\\]\nTo get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.\n\nset.seed(1)\nsim_data = gen_sim_data(f)\n\n\nfit_0 = lm(y ~ 1,                   data = sim_data)\nfit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\nfit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\nfit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\n\nNote that technically we’re being lazy and using orthogonal polynomials, but the fitted values are the same, so this makes no difference for our purposes. These could be KNN, or decision trees just the same - the principle still applies.\nPlotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.\n\n\n\n\n\n\n\n\n\nThe following three plots were created using three additional simulated datasets. The zero predictor and ninth degree polynomial were fit to each.\n\n\n\n\n\n\n\n\n\nThis plot should make clear the difference between the bias and variance of these two models. The zero predictor model is clearly wrong, that is, biased, but nearly the same for each of the datasets, since it has very low variance.\nWhile the ninth degree model doesn’t appear to be correct for any of these three simulations, we’ll see that on average it is, and thus is performing unbiased estimation. These plots do however clearly illustrate that the ninth degree polynomial is extremely variable. Each dataset results in a very different fitted model. Correct on average isn’t the only goal we’re after, since in practice, we’ll only have a single dataset. This is why we’d also like our models to exhibit low variance.\nWe could have also fit \\(k\\)-nearest neighbors models to these three datasets.\n\n\n\n\n\n\n\n\n\nHere we see that when \\(k = 100\\) we have a biased model with very low variance.3 When \\(k = 5\\), we again have a highly variable model.\nThese two sets of plots reinforce our intuition about the bias-variance tradeoff. Flexible models (ninth degree polynomial and \\(k\\) = 5) are highly variable, and often unbiased. Simple models (zero predictor linear model and \\(k = 100\\)) are very biased, but have extremely low variance.\nWe will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates of \\(f(x)\\) given by these four models at the point \\(x = 0.90\\). We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.\n\nset.seed(1)\nn_sims = 250\nn_models = 4\nx = data.frame(x = 0.90) # fixed point at which we make predictions\npredictions = matrix(0, nrow = n_sims, ncol = n_models)\n\n\nfor (sim in 1:n_sims) {\n\n  # simulate new, random, training data\n  # this is the only random portion of the bias, var, and mse calculations\n  # this allows us to calculate the expectation over D\n  sim_data = gen_sim_data(f)\n\n  # fit models\n  fit_0 = lm(y ~ 1,                   data = sim_data)\n  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\n  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\n  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\n\n  # get predictions\n  predictions[sim, 1] = predict(fit_0, x)\n  predictions[sim, 2] = predict(fit_1, x)\n  predictions[sim, 3] = predict(fit_2, x)\n  predictions[sim, 4] = predict(fit_9, x)\n}\n\n\n\n\nNote that this is one of many ways we could have accomplished this task using R. For example we could have used a combination of replicate() and *apply() functions. Alternatively, we could have used a tidyverse approach, which likely would have used some combination of dplyr, tidyr, and purrr.\nOur approach, which would be considered a base R approach, was chosen to make it as clear as possible what is being done. The tidyverse approach is rapidly gaining popularity in the R community, but might make it more difficult to see what is happening here, unless you are already familiar with that approach.\nAlso of note, while it may seem like the output stored in predictions would meet the definition of tidy data given by Hadley Wickham since each row represents a simulation, it actually falls slightly short. For our data to be tidy, a row should store the simulation number, the model, and the resulting prediction. We’ve actually already aggregated one level above this. Our observational unit is a simulation (with four predictions), but for tidy data, it should be a single prediction.\n\n\n\n\n\n\n\n\n\nThe above plot shows the predictions for each of the 250 simulations of each of the four models of different polynomial degrees. The truth, \\(f(x = 0.90) = (0.9)^2 = 0.81\\), is given by the solid black horizontal line.\nTwo things are immediately clear:\n\nAs flexibility increases, bias decreases. The mean of a model’s predictions is closer to the truth.\nAs flexibility increases, variance increases. The variance about the mean of a model’s predictions increases.\n\nThe goal of this simulation study is to show that the following holds true for each of the four models.\n\\[\n\\text{MSE}\\left(f(0.90), \\hat{f}_k(0.90)\\right) =\n\\underbrace{\\left(\\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] - f(0.90) \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}_k(0.90) \\right)} +\n\\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}_k(0.90) - \\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}_k(0.90) \\right)}\n\\]\nWe’ll use the empirical results of our simulations to estimate these quantities. (Yes, we’re using estimation to justify facts about estimation.) Note that we’ve actually used a rather small number of simulations. In practice we should use more, but for the sake of computation time, we’ve performed just enough simulations to obtain the desired results. (Since we’re estimating estimation, the bigger the sample size, the better.)\nTo estimate the mean squared error of our predictions, we’ll use\n\\[\n\\widehat{\\text{MSE}}\\left(f(0.90), \\hat{f}_k(0.90)\\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(f(0.90) - \\hat{f}_k^{[i]}(0.90) \\right)^2\n\\]\nwhere \\(\\hat{f}_k^{[i]}(0.90)\\) is the estimate of \\(f(0.90)\\) using the \\(i\\)-th from the polynomial degree \\(k\\) model.\nWe also write an accompanying R function.\n\nget_mse = function(truth, estimate) {\n  mean((estimate - truth) ^ 2)\n}\n\nSimilarly, for the bias of our predictions we use,\n\\[\n\\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right)  = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) \\right) - f(0.90)\n\\]\nAnd again, we write an accompanying R function.\n\nget_bias = function(estimate, truth) {\n  mean(estimate) - truth\n}\n\nLastly, for the variance of our predictions we have\n\\[\n\\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k^{[i]}(0.90) \\right)^2\n\\]\nWhile there is already R function for variance, the following is more appropriate in this situation.\n\nget_var = function(estimate) {\n  mean((estimate - mean(estimate)) ^ 2)\n}\n\nTo quickly obtain these results for each of the four models, we utilize the apply() function.\n\nbias = apply(predictions, MAR = 2, get_bias, truth = f(x = 0.90))\nvariance = apply(predictions, MAR = 2, get_var)\nmse = apply(predictions, MAR = 2, get_mse, truth = f(x = 0.90))\n\nWe summarize these results in the following table.\n\n\n\n\n\nDegree\nMean Squared Error\nBias Squared\nVariance\n\n\n\n\n0\n0.22643\n0.22476\n0.00167\n\n\n1\n0.00829\n0.00508\n0.00322\n\n\n2\n0.00387\n0.00005\n0.00381\n\n\n9\n0.01019\n0.00002\n0.01017\n\n\n\n\n\nA number of things to notice here:\n\nWe use squared bias in this table. Since bias can be positive or negative, squared bias is more useful for observing the trend as flexibility increases.\nThe squared bias trend which we see here is decreasing as flexibility increases, which we expect to see in general.\nThe exact opposite is true of variance. As model flexibility increases, variance increases.\nThe mean squared error, which is a function of the bias and variance, decreases, then increases. This is a result of the bias-variance tradeoff. We can decrease bias, by increasing variance. Or, we can decrease variance by increasing bias. By striking the correct balance, we can find a good mean squared error!\n\nWe can check for these trends with the diff() function in R.\n\nall(diff(bias ^ 2) < 0)\n\n[1] TRUE\n\nall(diff(variance) > 0)\n\n[1] TRUE\n\ndiff(mse) < 0\n\n    1     2     9 \n TRUE  TRUE FALSE \n\n\nThe models with polynomial degrees 2 and 9 are both essentially unbiased. We see some bias here as a result of using simulation. If we increased the number of simulations, we would see both biases go down. Since they are both unbiased, the model with degree 2 outperforms the model with degree 9 due to its smaller variance.\nModels with degree 0 and 1 are biased because they assume the wrong form of the regression function. While the degree 9 model does this as well, it does include all the necessary polynomial degrees.\n\\[\n\\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9\n\\]\nThen, since least squares estimation is unbiased, importantly,\n\\[\n\\mathbb{E}\\left[\\hat{\\beta}_d\\right] = \\beta_d = 0\n\\]\nfor \\(d = 3, 4, \\ldots 9\\), we have\n\\[\n\\mathbb{E}\\left[\\hat{f}_9(x)\\right] = \\beta_0 + \\beta_1 x + \\beta_2 x^2\n\\]\nNow we can finally verify the bias-variance decomposition.\n\nbias ^ 2 + variance == mse\n\n    0     1     2     9 \nFALSE FALSE  TRUE  TRUE \n\n\nBut wait, this says it isn’t true, except for the degree 9 model? It turns out, this is simply a computational issue. If we allow for some very small error tolerance, we see that the bias-variance decomposition is indeed true for predictions from these for models.\n\nall.equal(bias ^ 2 + variance, mse)\n\n[1] TRUE\n\n\nSee ?all.equal() for details.\nSo far, we’ve focused our efforts on looking at the mean squared error of estimating \\(f(0.90)\\) using \\(\\hat{f}(0.90)\\). We could also look at the expected prediction error of using \\(\\hat{f}(X)\\) when \\(X = 0.90\\) to estimate \\(Y\\).\n\\[\n\\text{EPE}\\left(Y, \\hat{f}_k(0.90)\\right) =\n\\mathbb{E}_{Y \\mid X, \\mathcal{D}} \\left[  \\left(Y - \\hat{f}_k(X) \\right)^2 \\mid X = 0.90 \\right]\n\\]\nWe can estimate this quantity for each of the four models using the simulation study we already performed.\n\nget_epe = function(realized, estimate) {\n  mean((realized - estimate) ^ 2)\n}\n\n\ny = rnorm(n = nrow(predictions), mean = f(x = 0.9), sd = 0.3)\nepe = apply(predictions, 2, get_epe, realized = y)\nepe\n\n        0         1         2         9 \n0.3180470 0.1104055 0.1095955 0.1205570 \n\n\n\n\n\nWhat about the unconditional expected prediction error. That is, for any \\(X\\), not just \\(0.90\\). Specifically, the expected prediction error of estimating \\(Y\\) using \\(\\hat{f}(X)\\). The following (new) simulation study provides an estimate of\n\\[\n\\text{EPE}\\left(Y, \\hat{f}_k(X)\\right) = \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  \\left( Y - \\hat{f}_k(X) \\right)^2 \\right]\n\\]\nfor the quadratic model, that is \\(k = 2\\) as we have defined \\(k\\).\n\nset.seed(42)\nn_sims = 2500\nX = runif(n = n_sims, min = 0, max = 1)\nY = rnorm(n = n_sims, mean = f(X), sd = 0.3)\n\nf_hat_X = rep(0, length(X))\n\nfor (i in seq_along(X)) {\n  sim_data = gen_sim_data(f)\n  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\n  f_hat_X[i] = predict(fit_2, newdata = data.frame(x = X[i]))\n}\n\n\n# truth\n0.3 ^ 2\n\n[1] 0.09\n\n# via simulation\nmean((Y - f_hat_X) ^ 2)\n\n[1] 0.09566445\n\n\nNote that in practice, we should use many more simulations in this study."
  },
  {
    "objectID": "content/Week_10/10a.html#estimating-expected-prediction-error",
    "href": "content/Week_10/10a.html#estimating-expected-prediction-error",
    "title": "The Bias-Variance Tradeoff",
    "section": "Estimating Expected Prediction Error",
    "text": "Estimating Expected Prediction Error\nWhile previously, we only decomposed the expected prediction error conditionally, a similar argument holds unconditionally.\nAssuming\n\\[\n\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2.\n\\]\nwe have\n\\[\n\\text{EPE}\\left(Y, \\hat{f}(X)\\right) =\n\\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  (Y - \\hat{f}(X))^2 \\right] =\n\\underbrace{\\mathbb{E}_{X} \\left[\\text{bias}^2\\left(\\hat{f}(X)\\right)\\right] + \\mathbb{E}_{X} \\left[\\text{var}\\left(\\hat{f}(X)\\right)\\right]}_\\textrm{reducible error} + \\sigma^2\n\\]\nLastly, we note that if\n\\[\n\\mathcal{D} = \\mathcal{D}_{\\texttt{trn}} \\cup \\mathcal{D}_{\\texttt{tst}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i = 1, 2, \\ldots n\n\\]\nwhere\n\\[\n\\mathcal{D}_{\\texttt{trn}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i \\in \\texttt{trn}\n\\]\nand\n\\[\n\\mathcal{D}_{\\texttt{tst}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i \\in \\texttt{tst}\n\\]\nThen, if we have a model fit to the training data \\(\\mathcal{D}_{\\texttt{trn}}\\), we can use the test mean squared error\n\\[\n\\sum_{i \\in \\texttt{tst}}\\left(y_i - \\hat{f}(x_i)\\right) ^ 2\n\\]\nas an estimate of\n\\[\n\\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  (Y - \\hat{f}(X))^2 \\right]\n\\]\nthe expected prediction error.4\nHow good is this estimate? Well, if \\(\\mathcal{D}\\) is a random sample from \\((X, Y)\\), and the \\(\\texttt{tst}\\) data are randomly sampled observations randomly sampled from \\(i = 1, 2, \\ldots, n\\), then it is a reasonable estimate. However, it is rather variable due to the randomness of selecting the observations for the test set, if the test set is small."
  },
  {
    "objectID": "content/Week_10/10a.html#model-flexibility",
    "href": "content/Week_10/10a.html#model-flexibility",
    "title": "The Bias-Variance Tradeoff",
    "section": "Model Flexibility",
    "text": "Model Flexibility\nLet’s return to the simulated dataset we used occaisionally in the linear regression content. Recall there was a single feature \\(x\\) with the following properties:\n\n# define regression function\ncubic_mean = function(x) {\n  1 - 2 * x - 3 * x ^ 2 + 5 * x ^ 3\n}\n\nWe then generated some data around this function with some added noise:\n\n# define full data generating process\ngen_slr_data = function(sample_size = 100, mu) {\n  x = runif(n = sample_size, min = -1, max = 1)\n  y = mu(x) + rnorm(n = sample_size)\n  tibble(x, y)\n}\n\nAfter defining the data generating process, we generate and split the data.\n\n# simulate entire dataset\nset.seed(3)\nsim_slr_data = gen_slr_data(sample_size = 100, mu = cubic_mean)\n\n# test-train split\nslr_trn_idx = sample(nrow(sim_slr_data), size = 0.8 * nrow(sim_slr_data))\nslr_trn = sim_slr_data[slr_trn_idx, ]\nslr_tst = sim_slr_data[-slr_trn_idx, ]\n\n# estimation-validation split\nslr_est_idx = sample(nrow(slr_trn), size = 0.8 * nrow(slr_trn))\nslr_est = slr_trn[slr_est_idx, ]\nslr_val = slr_trn[-slr_est_idx, ]\n\n# check data\nhead(slr_trn, n = 10)\n\n# A tibble: 10 × 2\n        x      y\n    <dbl>  <dbl>\n 1  0.573 -1.18 \n 2  0.807  0.576\n 3  0.272 -0.973\n 4 -0.813 -1.78 \n 5 -0.161  0.833\n 6  0.736  1.07 \n 7 -0.242  2.97 \n 8  0.520 -1.64 \n 9 -0.664  0.269\n10 -0.777 -2.02 \n\n\nFor validating models, we will use RMSE.\n\n# helper function for calculating RMSE\ncalc_rmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\nLet’s check how linear, k-nearest neighbors, and decision tree models fit to this data make errors, while paying attention to their flexibility.\n\n\n\n\n\n\n\n\n\nThis picture is an idealized version of what we expect to see, but we’ll illustrate the sorts of validate “curves” that we might see in practice.\nNote that in the following three sub-sections, a significant portion of the code is suppressed for visual clarity. See the source document for full details.\n\nLinear Models\nFirst up, linear models. We will fit polynomial models with degree from one to nine, and then validate.\n\n# fit polynomial models\npoly_mod_est_list = list(\n  poly_mod_1_est = lm(y ~ poly(x, degree = 1), data = slr_est),\n  poly_mod_2_est = lm(y ~ poly(x, degree = 2), data = slr_est),\n  poly_mod_3_est = lm(y ~ poly(x, degree = 3), data = slr_est),\n  poly_mod_4_est = lm(y ~ poly(x, degree = 4), data = slr_est),\n  poly_mod_5_est = lm(y ~ poly(x, degree = 5), data = slr_est),\n  poly_mod_6_est = lm(y ~ poly(x, degree = 6), data = slr_est),\n  poly_mod_7_est = lm(y ~ poly(x, degree = 7), data = slr_est),\n  poly_mod_8_est = lm(y ~ poly(x, degree = 8), data = slr_est),\n  poly_mod_9_est = lm(y ~ poly(x, degree = 9), data = slr_est)\n)\n\n\n\n\nThe plot below visualizes the results.\n\n\n\n\n\n\n\n\n\nWhat do we see here? As the polynomial degree increases:\n\nThe training error decreases.\nThe validation error decreases, then increases.\n\nThis more of less matches the idealized version above, but the validation “curve” is much more jagged. This is something that we can expect in practice.\nWe have previously noted that training error isn’t particularly useful for validating models. That is still true. However, it can be useful for checking that everything is working as planned. In this case, since we known that training error decreases as model flexibility increases, we can verify our intuition that a higher degree polynomial is indeed more flexible.5\n\n\nk-Nearest Neighbors\nNext up, k-nearest neighbors. We will consider values for \\(k\\) that are odd and between \\(1\\) and \\(45\\) inclusive.\n\n# helper function for fitting knn models\nfit_knn_mod = function(neighbors) {\n  knnreg(y ~ x, data = slr_est, k = neighbors)\n}\n\n\n# define values of tuning parameter k to evaluate\nk_to_try = seq(from = 1, to = 45, by = 2)\n\n# fit knn models\nknn_mod_est_list = lapply(k_to_try, fit_knn_mod)\n\n\n\n\nThe plot below visualizes the results.\n\n\n\n\n\n\n\n\n\nHere we see the “opposite” of the usual plot. Why? Because with k-nearest neighbors, a small value of \\(k\\) generates a flexible model compared to larger values of \\(k\\). So visually, this plot is flipped. That is we see that as \\(k\\) increases:\n\nThe training error increases.\nThe validation error decreases, then increases.\n\nImportant to note here: the pattern above only holds “in general,” that is, there can be minor deviations in the validation pattern along the way. This is due to the random nature of selection the data for the validate set.\n\n\nDecision Trees\nLastly, we evaluate some decision tree models. We choose some arbitrary values of cp to evaluate, while holding minsplit constant at 5. There are arbitrary choices that produce a plot that is useful for discussion.\n\n# helper function for fitting decision tree models\ntree_knn_mod = function(flex) {\n  rpart(y ~ x, data = slr_est, cp = flex, minsplit = 5)\n}\n\n\n# define values of tuning parameter cp to evaluate\ncp_to_try = c(0.5, 0.3, 0.1, 0.05, 0.01, 0.001, 0.0001)\n\n# fit decision tree models\ntree_mod_est_list = lapply(cp_to_try, tree_knn_mod)\n\n\n\n\nThe plot below visualizes the results.\n\n\n\n\n\n\n\n\n\nBased on this plot, how is cp related to model flexibility?6"
  },
  {
    "objectID": "content/Week_01/01a.html",
    "href": "content/Week_01/01a.html",
    "title": "Introduction to the tidyverse",
    "section": "",
    "text": "This page.\nChapter 1 of Introduction to Statistical Learning, available here.\nOptional: The “Tidy Your Data” tutorial on Rstudio Cloud Primers"
  },
  {
    "objectID": "content/Week_01/01a.html#some-reminders",
    "href": "content/Week_01/01a.html#some-reminders",
    "title": "Introduction to the tidyverse",
    "section": "Some Reminders:",
    "text": "Some Reminders:\n\nStart labs early!\n\nThey are not trivial.\nThey are not short.\nThey are not easy.\nThey are not optional.\n\nYou install.packages(\"packageName\") once on your computer.\n\nAnd never ever ever in your code.\n\nYou load an already-installed package using library(packageName) in a code chunk\n\nNever in your console\nWhen RMarkdown knits, it starts a whole new, empty session that has no knowledge of what you typed into the console\n\nSlack\n\nUse it.\nI would very much prefer posting in the class-visible channels. Others can learn from your issues.\n\nWe have a channel just for labs and R. Please use that one.\n\n\n\n\nGroup Projects\nYour final is a group project. You will also have two “mini” projects. They comprise a large part of your grade. As mentioned last week, this mean that you need to start planning soon.\nTo aid in your planning, here are the required elements of your final project.\n\nYou must find existing data to analyze. Aggregating and merging data from multiple sources is encouraged.\nYou must visualize 3 interesting features of that data.\nYou must come up with some analysis—using tools from this course—which relates your data to either a prediction or a policy conclusion.\nYou must think critically about your analysis and be able to identify potential issues.\nYou must present your analysis as if presenting to a C-suite executive.\n\nYour mini-projects along the way will be more structured, but will serve to guide you towards the final project.\n\n\nTeams\nPlease form teams of 3 people. Once all agree to be on a team, have ONE PERSON email our TA Allen scovelpa@msu.edu and cc all of the members of the team so that nobody is surprised to be included on a team. Title the email [SSC442] - Group Formation. Tell us your team name (be creative), and list in the email the names of all of the team members and their email address (in addition to cc-ing those team members on the email).\nIf you opt to not form a team, you will be automatically added to the “willing to be randomly assigned” pool and will be paired with two others from the “willing to be randomly assigned” pool.\nSend this email by January 20th and we will assign un-teamed folks at the beginning of the following week. Project 1 is due in no time. See schedule for all the important project dates.\n\n\nGuiding Question\nFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\nWhy do we want tidy data?\nWhat are the challenges associated with shaping things into a tidy format?"
  },
  {
    "objectID": "content/Week_01/01a.html#tidy-data",
    "href": "content/Week_01/01a.html#tidy-data",
    "title": "Introduction to the tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\n\nWe say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\n\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nEach row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.\nTo see how the same information can be provided in different formats, consider the following example:\n\nlibrary(dslabs)\ndata(\"gapminder\") # gapminder will now be a data.frame in your \"environment\" (memory)\ntidy_data <- gapminder %>%\n  filter(country %in% c(\"South Korea\", \"Germany\") & !is.na(fertility)) %>%\n  select(country, year, fertility)\nhead(tidy_data, 6)\n\n      country year fertility\n1     Germany 1960      2.41\n2 South Korea 1960      6.16\n3     Germany 1961      2.44\n4 South Korea 1961      5.99\n5     Germany 1962      2.47\n6 South Korea 1962      5.79\n\n\nThis tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the dslabs package. Originally, the data was in the following format:\n\n\n      country 1960 1961 1962\n1     Germany 2.41 2.44 2.47\n2 South Korea 6.16 5.99 5.79\n\n\nThe same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables’ values, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into tidy format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.\nAlthough not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.\n\nTRY IT\n\nExamine the built-in dataset co2. Which of the following is true:\n\n\nco2 is tidy data: it has one year for each row.\nco2 is not tidy: we need at least one column with a character vector.\nco2 is not tidy: it is a matrix instead of a data frame.\nco2 is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.\n\n\nExamine the built-in dataset ChickWeight. Which of the following is true:\n\n\nChickWeight is not tidy: each chick has more than one row.\nChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.\nChickWeight is not tidy: we are missing the year column.\nChickWeight is tidy: it is stored in a data frame.\n\n\nExamine the built-in dataset BOD. Which of the following is true:\n\n\nBOD is not tidy: it only has six rows.\nBOD is not tidy: the first column is just an index.\nBOD is tidy: each row is an observation with two values (time and demand)\nBOD is tidy: all small datasets are tidy by definition.\n\n\nWhich of the following built-in datasets is tidy (you can pick more than one):\n\n\nBJsales\nEuStockMarkets\nDNase\nFormaldehyde\nOrange\nUCBAdmissions"
  },
  {
    "objectID": "content/Week_01/01a.html#manipulating-data-frames",
    "href": "content/Week_01/01a.html#manipulating-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Manipulating data frames",
    "text": "Manipulating data frames\nThe dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.\n\nAdding a column with mutate\nWe want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values. So, to add murder rates, we use:\n\nlibrary(dslabs)\ndata(\"murders\")\nmurders <- mutate(murders, rate = total / population * 100000)\n\nNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error?\nThis is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable.\nWe can see that the new column is added:\n\nhead(murders)\n\n       state abb region population total     rate\n1    Alabama  AL  South    4779736   135 2.824424\n2     Alaska  AK   West     710231    19 2.675186\n3    Arizona  AZ   West    6392017   232 3.629527\n4   Arkansas  AR  South    2915918    93 3.189390\n5 California  CA   West   37253956  1257 3.374138\n6   Colorado  CO   West    5029196    65 1.292453\n\n\nNote: Although we have overwritten the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will overwrite our mutated version.\n\n\nSubsetting with filter\nNow suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the filter function, which takes the data table as the first argument and then the conditional statement as the second. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace.\n\nfilter(murders, rate <= 0.71)\n\n          state abb        region population total      rate\n1        Hawaii  HI          West    1360301     7 0.5145920\n2          Iowa  IA North Central    3046355    21 0.6893484\n3 New Hampshire  NH     Northeast    1316470     5 0.3798036\n4  North Dakota  ND North Central     672591     4 0.5947151\n5       Vermont  VT     Northeast     625741     2 0.3196211\n\n\n\n\nSelecting columns with select\nAlthough our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object:\n\nnew_table <- select(murders, state, region, rate)\nfilter(new_table, rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nIn the call to select, the first argument murders is an object, but state, region, and rate are variable names.\n\nTRY IT\n\nLoad the dplyr package and the murders dataset.\n\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\n\nYou can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted:\n\nmurders <- mutate(murders, population_in_millions = population / 10^6)\n\nWe can write population rather than murders$population because mutate is part of dplyr. The function mutate knows we are grabbing columns from murders.\nUse the function mutate to add a murders column named rate with the per 100,000 murder rate as in the example code above. Make sure you redefine murders as done in the example code above ( murders <- [your code]) so we can keep using this variable.\n\nIf rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\nWith dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes:\n\n\nselect(murders, state, population) %>% head()\n\nUse select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\n\nThe dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this:\n\n\nfilter(murders, state == \"New York\")\n\nYou can use other logical vectors to filter rows.\nUse filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\n\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:\n\n\nno_florida <- filter(murders, state != \"Florida\")\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n\nWe can also use %in% to filter with dplyr. You can therefore see the data from New York and Texas like this:\n\n\nfilter(murders, state %in% c(\"New York\", \"Texas\"))\n\nCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\n\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\n\n\nfilter(murders, population < 5000000 & region == \"Northeast\")\n\nMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate, and the rank."
  },
  {
    "objectID": "content/Week_01/01a.html#the-pipe",
    "href": "content/Week_01/01a.html#the-pipe",
    "title": "Introduction to the tidyverse",
    "section": "The pipe: %>%",
    "text": "The pipe: %>%\nWith dplyr we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %>%. Some details are included below.\nWe wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects:\n\\[ \\mbox{original data }\n\\rightarrow \\mbox{ select }\n\\rightarrow \\mbox{ filter } \\]\nFor such an operation, we can use the pipe %>%. The code looks like this:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nThis line of code is equivalent to the two lines of code above. What is going on here?\nIn general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:\n\n16 %>% sqrt()\n\n[1] 4\n\n\nWe can continue to pipe values along:\n\n16 %>% sqrt() %>% log2()\n\n[1] 2\n\n\nThe above statement is equivalent to log2(sqrt(16)).\nRemember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:\n\n16 %>% sqrt() %>% log(base = 2)\n\n[1] 2\n\n\nTherefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\nmurders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function.\nNote that the pipe works well with functions where the first argument is the input data. Functions in tidyverse packages like dplyr have this format and can be used easily with the pipe. It’s worth noting that as of R 4.1, there is a base-R version of the pipe |>, though this has its disadvantages. We’ll stick with %>% for now.\n\nTRY IT\n\nThe pipe %>% can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.\n\n\nmurders <- mutate(murders, rate =  total / population * 100000,\n                  rank = rank(-rate))\n\nIn the solution to the previous exercise, we did the following:\n\nmy_states <- filter(murders, region %in% c(\"Northeast\", \"West\") &\n                      rate < 1)\n\nselect(my_states, state, rate, rank)\n\nThe pipe %>% permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this:\n\nmutate(murders, rate =  total / population * 100000,\n       rank = rank(-rate)) %>%\n  select(state, rate, rank)\n\nNotice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %>%.\nRepeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %>% to do this in just one line.\n\nReset murders to the original table by using data(murders). Use a pipe to create a new data frame called my_states that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three %>%. The code should look something like this:\n\n\nmy_states <- murders %>%\n  mutate SOMETHING %>%\n  filter SOMETHING %>%\n  select SOMETHING"
  },
  {
    "objectID": "content/Week_01/01a.html#summarizing-data",
    "href": "content/Week_01/01a.html#summarizing-data",
    "title": "Introduction to the tidyverse",
    "section": "Summarizing data",
    "text": "Summarizing data\nAn important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using the pull function.\n\n\n\n\nsummarize\nThe summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The heights dataset includes heights and sex reported by students in an in-class survey.\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(heights)\nhead(heights)\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65\n\n\nThe following code computes the average and standard deviation for females:\n\ns <- heights %>%\n  filter(sex == \"Female\") %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\ns\n\n   average standard_deviation\n1 64.93942           3.760656\n\n\nThis takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same.\nBecause the resulting table stored in s is a data frame, we can access the components with the accessor $:\n\ns$average\n\n[1] 64.93942\n\ns$standard_deviation\n\n[1] 3.760656\n\n\nAs with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(median = median(height), minimum = min(height),\n            maximum = max(height))\n\n    median minimum maximum\n1 64.98031      51      79\n\n\nWe can obtain these three values with just one line using the quantile function: for example, quantile(x, c(0,0.5,1)) returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector x. However, if we attempt to use a function like this that returns two or more values inside summarize:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nwe will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In later sections, we will learn how to deal with functions that return more than one value.\nFor another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column:\n\nmurders <- murders %>% mutate(rate = total/population*100000)\n\nRemember that the US murder rate is not the average of the state murder rates:\n\nsummarize(murders, mean(rate))\n\n  mean(rate)\n1   2.779125\n\n\nThis is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000)\nus_murder_rate\n\n      rate\n1 3.034555\n\n\nThis computation counts larger states proportionally to their size which results in a larger value.\n\n\npull\nThe us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame:\n\nclass(us_murder_rate)\n\n[1] \"data.frame\"\n\n\nsince, as most dplyr functions, summarize always returns a data frame.\nThis might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. To understand what we mean take a look at this line of code:\n\nus_murder_rate %>% pull(rate)\n\n[1] 3.034555\n\n\nThis returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate.\nTo get a number from the original data table with one line of code we can type:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000) %>%\n  pull(rate)\n\nus_murder_rate\n\n[1] 3.034555\n\n\nwhich is now a numeric:\n\nclass(us_murder_rate)\n\n[1] \"numeric\"\n\n\n\n\nGroup then summarize with group_by\nA common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this.\nIf we type this:\n\nheights %>% group_by(sex)\n\n# A tibble: 1,050 × 2\n# Groups:   sex [2]\n   sex    height\n   <fct>   <dbl>\n 1 Male       75\n 2 Male       70\n 3 Male       68\n 4 Male       74\n 5 Male       61\n 6 Female     65\n 7 Female     66\n 8 Female     62\n 9 Female     66\n10 Male       67\n# ℹ 1,040 more rows\n\n\nThe result does not look very different from heights, except we see Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:\n\nheights %>%\n  group_by(sex) %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\n\n# A tibble: 2 × 3\n  sex    average standard_deviation\n  <fct>    <dbl>              <dbl>\n1 Female    64.9               3.76\n2 Male      69.3               3.61\n\n\nThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median murder rate in the four regions of the country:\n\nmurders %>%\n  group_by(region) %>%\n  summarize(median_rate = median(rate))\n\n# A tibble: 4 × 2\n  region        median_rate\n  <fct>               <dbl>\n1 Northeast            1.80\n2 South                3.40\n3 North Central        1.97\n4 West                 1.29"
  },
  {
    "objectID": "content/Week_01/01a.html#sorting-data-frames",
    "href": "content/Week_01/01a.html#sorting-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Sorting data frames",
    "text": "Sorting data frames\nWhen examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size:\n\nmurders %>%\n  arrange(population) %>%\n  head()\n\n                 state abb        region population total       rate\n1              Wyoming  WY          West     563626     5  0.8871131\n2 District of Columbia  DC         South     601723    99 16.4527532\n3              Vermont  VT     Northeast     625741     2  0.3196211\n4         North Dakota  ND North Central     672591     4  0.5947151\n5               Alaska  AK          West     710231    19  2.6751860\n6         South Dakota  SD North Central     814180     8  0.9825837\n\n\nWith arrange we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by rate instead:\n\nmurders %>%\n  arrange(rate) %>%\n  head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102\n\n\nNote that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type:\n\nmurders %>%\n  arrange(desc(rate))\n\n\nNested sorting\nIf we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region, then within region we order by murder rate:\n\nmurders %>%\n  arrange(region, rate) %>%\n  head()\n\n          state abb    region population total      rate\n1       Vermont  VT Northeast     625741     2 0.3196211\n2 New Hampshire  NH Northeast    1316470     5 0.3798036\n3         Maine  ME Northeast    1328361    11 0.8280881\n4  Rhode Island  RI Northeast    1052567    16 1.5200933\n5 Massachusetts  MA Northeast    6547629   118 1.8021791\n6      New York  NY Northeast   19378102   517 2.6679599\n\n\n\n\nThe top \\(n\\)\nIn the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:\n\nmurders %>% top_n(5, rate)\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Maryland  MD         South    5773552   293  5.074866\n4             Missouri  MO North Central    5988927   321  5.359892\n5       South Carolina  SC         South    4625364   207  4.475323\n\n\nNote that rows are not sorted by rate, only filtered. If we want to sort, we need to use arrange. Note that if the third argument is left blank, top_n filters by the last column.\n\nTRY IT\nFor these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\n\nlibrary(NHANES)\n\nWarning: package 'NHANES' was built under R version 4.3.3\n\ndata(NHANES)\n\nThe NHANES data has many missing values. The mean and sd functions in R will return NA if any of the entries of the input vector is an NA. Here is an example:\n\nlibrary(dslabs)\ndata(na_example)\nmean(na_example)\n\n[1] NA\n\nsd(na_example)\n\n[1] NA\n\n\nTo ignore the NAs we can use the na.rm argument:\n\nmean(na_example, na.rm = TRUE)\n\n[1] 2.301754\n\nsd(na_example, na.rm = TRUE)\n\n[1] 1.22338\n\n\nLet’s now explore the NHANES data.\n\nWe will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like ” 20-29”, with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref.\n\nHint: Use filter and summarize and use the na.rm = TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter.\n\nUsing a pipe, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then pull.\nNow report the min and max values for the same group.\nCompute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age and gender, filter by Gender and then use group_by.\nRepeat exercise 4 for males.\nWe can actually combine both summaries for exercises 4 and 5 into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender).\nFor males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure."
  },
  {
    "objectID": "content/Week_01/01a.html#tibbles",
    "href": "content/Week_01/01a.html#tibbles",
    "title": "Introduction to the tidyverse",
    "section": "Tibbles",
    "text": "Tibbles\nTidy data must be stored in data frames. We have been using the murders data frame throughout the unit. In an earlier section we introduced the group_by function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?\n\nmurders %>% group_by(region)\n\n# A tibble: 51 × 6\n# Groups:   region [4]\n   state                abb   region    population total  rate\n   <chr>                <chr> <fct>          <dbl> <dbl> <dbl>\n 1 Alabama              AL    South        4779736   135  2.82\n 2 Alaska               AK    West          710231    19  2.68\n 3 Arizona              AZ    West         6392017   232  3.63\n 4 Arkansas             AR    South        2915918    93  3.19\n 5 California           CA    West        37253956  1257  3.37\n 6 Colorado             CO    West         5029196    65  1.29\n 7 Connecticut          CT    Northeast    3574097    97  2.71\n 8 Delaware             DE    South         897934    38  4.23\n 9 District of Columbia DC    South         601723    99 16.5 \n10 Florida              FL    South       19687653   669  3.40\n# ℹ 41 more rows\n\n\nNotice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble followed by dimensions. We can learn the class of the returned object using:\n\nmurders %>% group_by(region) %>% class()\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe tbl, pronounced tibble, is a special kind of data frame. The functions group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. For consistency, the dplyr manipulation verbs (select, filter, mutate, and arrange) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble.\nTibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.\n\nTibbles display better\nThe print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing murders and the output of murders if we convert it to a tibble. We can do this using as_tibble(murders). If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.\n\n\nSubsets of tibbles are tibbles\nIf you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:\n\nclass(murders[,4])\n\n[1] \"numeric\"\n\n\nis not a data frame. With tibbles this does not happen:\n\nclass(as_tibble(murders)[,4])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThis is useful in the tidyverse since functions require data frames as input.\nWith tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $:\n\nclass(as_tibble(murders)$population)\n\n[1] \"numeric\"\n\n\nA related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write Population instead of population this:\n\nmurders$Population\n\nNULL\n\n\nreturns a NULL with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:\n\nas_tibble(murders)$Population\n\nWarning: Unknown or uninitialised column: `Population`.\n\n\nNULL\n\n\n\n\nTibbles can have complex entries\nWhile data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:\n\ntibble(id = c(1, 2, 3), func = c(mean, median, sd))\n\n# A tibble: 3 × 2\n     id func  \n  <dbl> <list>\n1     1 <fn>  \n2     2 <fn>  \n3     3 <fn>  \n\n\n\n\nTibbles can be grouped\nThe function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information.\n\n\nCreate a tibble using tibble instead of data.frame\nIt is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the tibble function.\n\ngrades <- tibble(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\n\nNote that base R (without packages loaded) has a function with a very similar name, data.frame, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default data.frame coerces characters into factors without providing a warning or message:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo avoid this, we use the rather cumbersome argument stringsAsFactors:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90),\n                     stringsAsFactors = FALSE)\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo convert a regular data frame to a tibble, you can use the as_tibble function.\n\nas_tibble(grades) %>% class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\""
  },
  {
    "objectID": "content/Week_01/01a.html#the-dot-operator",
    "href": "content/Week_01/01a.html#the-dot-operator",
    "title": "Introduction to the tidyverse",
    "section": "The dot operator",
    "text": "The dot operator\nOne of the advantages of using the pipe %>% is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:\n\ntab_1 <- filter(murders, region == \"South\")\ntab_2 <- mutate(tab_1, rate = total / population * 10^5)\nrates <- tab_2$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nWe can avoid defining any new intermediate objects by instead typing:\n\nfilter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  summarize(median = median(rate)) %>%\n  pull(median)\n\n[1] 3.398069\n\n\nWe can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the pull function was not available and we wanted to access tab_2$rate? What data frame name would we use? The answer is the dot operator.\nFor example to access the rate vector without the pull function we could use\n\nrates <-   filter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  .$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nIn the next section, we will see other instances in which using the . is useful."
  },
  {
    "objectID": "content/Week_01/01a.html#do",
    "href": "content/Week_01/01a.html#do",
    "title": "Introduction to the tidyverse",
    "section": "do",
    "text": "do\nThe tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %>%, tidyverse functions consistently take data frames and return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The quantile function is an example we described earlier. The do function serves as a bridge between R functions such as quantile and the tidyverse. The do function understands grouped tibbles and always returns a data frame.\nIn the summarize section (above), we noted that if we attempt to use quantile to obtain the min, median and max in one call, we will receive something unexpected. Prior to R 4.1, we would receive an error. After R 4.1, we actually get:\n\ndata(heights)\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nWe probably wanted three columns: min, median, and max. We can use the do function to fix this.\nFirst we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame. Note that it returns a single-row data frame.\n\nmy_summary <- function(dat){\n  x <- quantile(dat$height, c(0, 0.5, 1))\n  tibble(min = x[1], median = x[2], max = x[3])\n}\n\nWe can now apply the function to the heights dataset to obtain the summaries:\n\nheights %>%\n  group_by(sex) %>%\n  my_summary\n\n# A tibble: 1 × 3\n    min median   max\n  <dbl>  <dbl> <dbl>\n1    50   68.5  82.7\n\n\nBut this is not what we want. We want a summary for each sex and the code returned just one summary. This is because my_summary is not part of the tidyverse and does not know how to handled grouped tibbles. do makes this connection:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary(.))\n\n# A tibble: 2 × 4\n# Groups:   sex [2]\n  sex      min median   max\n  <fct>  <dbl>  <dbl> <dbl>\n1 Female    51   65.0  79  \n2 Male      50   69    82.7\n\n\nNote that here we need to use the dot operator. The tibble created by group_by is piped to do. Within the call to do, the name of this tibble is . and we want to send it to my_summary. If you do not use the dot, then my_summary has no argument and returns an error telling us that argument \"dat\" is missing. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary())\n\nIf you do not use the parenthesis, then the function is not executed and instead do tries to return the function. This gives an error because do must always return a data frame. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary)\n\nSo do serves as a bridge between non-tidyverse functions and the tidyverse."
  },
  {
    "objectID": "content/Week_01/01a.html#the-purrr-package",
    "href": "content/Week_01/01a.html#the-purrr-package",
    "title": "Introduction to the tidyverse",
    "section": "The purrr package",
    "text": "The purrr package\nIn previous sections (and labs) we learned about the sapply function, which permitted us to apply the same function to each element of a vector. We constructed a function and used sapply to compute the sum of the first n integers for several values of n like this:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  sum(x)\n}\nn <- 1:25\ns_n <- sapply(n, compute_s_n)\ns_n\n\n [1]   1   3   6  10  15  21  28  36  45  55  66  78  91 105 120 136 153 171 190\n[20] 210 231 253 276 300 325\n\n\nThis type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, sapply can return several different object types; for example, we might expect a numeric result from a line of code, but sapply might convert our result to character under some circumstances. purrr functions will never do this: they will return objects of a specified type or return an error if this is not possible.\nThe first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list:\n\nlibrary(purrr) # or library(tidyverse)\nn <- 1:25\ns_n <- map(n, compute_s_n)\nclass(s_n)\n\n[1] \"list\"\n\n\nIf we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values.\n\ns_n <- map_dbl(n, compute_s_n)\nclass(s_n)\n\n[1] \"numeric\"\n\n\nThis produces the same results as the sapply call shown above.\nA particularly useful purrr function for interacting with the rest of the tidyverse is map_df, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a Argument 1 must have names error:\n\ns_n <- map_df(n, compute_s_n)\n\nWe need to change the function to make this work:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x))\n}\ns_n <- map_df(n, compute_s_n)\nhead(s_n)\n\n# A tibble: 6 × 1\n    sum\n  <int>\n1     1\n2     3\n3     6\n4    10\n5    15\n6    21\n\n\nBecause map_df returns a tibble, we can have more columns defined in our function and returned.\n\ncompute_s_n2 <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x), sumSquared = sum(x^2))\n}\ns_n <- map_df(n, compute_s_n2)\nhead(s_n)\n\n# A tibble: 6 × 2\n    sum sumSquared\n  <int>      <dbl>\n1     1          1\n2     3          5\n3     6         14\n4    10         30\n5    15         55\n6    21         91\n\n\nThe purrr package provides much more functionality not covered here. For more details you can consult this online resource."
  },
  {
    "objectID": "content/Week_01/01a.html#tidyverse-conditionals",
    "href": "content/Week_01/01a.html#tidyverse-conditionals",
    "title": "Introduction to the tidyverse",
    "section": "Tidyverse conditionals",
    "text": "Tidyverse conditionals\nA typical data analysis will often involve one or more conditional operations. In the section on Conditionals, we described the ifelse function, which we will use extensively in this book. In this section we present two dplyr functions that provide further functionality for performing conditional operations.\n\ncase_when\nThe case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0:\n\nx <- c(-2, -1, 0, 1, 2)\ncase_when(x < 0 ~ \"Negative\",\n          x > 0 ~ \"Positive\",\n          x == 0  ~ \"Zero\")\n\n[1] \"Negative\" \"Negative\" \"Zero\"     \"Positive\" \"Positive\"\n\n\nA common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this:\n\nmurders %>%\n  mutate(group = case_when(\n    abb %in% c(\"ME\", \"NH\", \"VT\", \"MA\", \"RI\", \"CT\") ~ \"New England\",\n    abb %in% c(\"WA\", \"OR\", \"CA\") ~ \"West Coast\",\n    region == \"South\" ~ \"South\",\n    TRUE ~ \"Other\")) %>%\n  group_by(group) %>%\n  summarize(rate = sum(total) / sum(population) * 10^5)\n\n# A tibble: 4 × 2\n  group        rate\n  <chr>       <dbl>\n1 New England  1.72\n2 Other        2.71\n3 South        3.63\n4 West Coast   2.90\n\n\nThat TRUE on the fourth line of case_when serves as a catch-all. As case_when steps through the conditions, if none of them are true, it comes to the last line. Since TRUE is always true, the function will return “Other”. Leaving out the last line of case_when would result in NA values for any observation that fails the first three conditionals. This may or may not be what you want.\n\n\nbetween\nA common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type\n\nx >= a & x <= b\n\nHowever, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation.\n\nbetween(x, a, b)\n\n\nTRY IT\n\nLoad the murders dataset. Which of the following is true?\n\n\nmurders is in tidy format and is stored in a tibble.\nmurders is in tidy format and is stored in a data frame.\nmurders is not in tidy format and is stored in a tibble.\nmurders is not in tidy format and is stored in a data frame.\n\n\nUse as_tibble to convert the murders data table into a tibble and save it in an object called murders_tibble.\nUse the group_by function to convert murders into a tibble that is grouped by region.\nWrite tidyverse code that is equivalent to this code:\n\n\nexp(mean(log(murders$population)))\n\nWrite it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with murders %>%.\n\nUse the map_df to create a data frame with three columns named n, s_n, and s_n_2. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through \\(n\\) with \\(n\\) the row number."
  },
  {
    "objectID": "content/Week_01/01b.html",
    "href": "content/Week_01/01b.html",
    "title": "Introduction to Visualization",
    "section": "",
    "text": "Looking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table:\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nWhat do you learn from staring at this table? Even though it is a relatively straightforward table, we can’t learn anything. For starters, it is grossly abbreviated, though you could scroll through. In doing so, how quickly might you be able to determine which states have the largest populations? Which states have the smallest? How populous is a typical state? Is there a relationship between population size and total murders? How do murder rates vary across regions of the country? For most folks, it is quite difficult to extract this information just by looking at the numbers. In contrast, the answer to the questions above are readily available from examining this plot:\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(ggrepel)\n\nr <- murders %>%\n  summarize(pop=sum(population), tot=sum(total)) %>%\n  mutate(rate = tot/pop*10^6) %>% pull(rate)\n\nmurders %>% ggplot(aes(x = population/10^6, y = total, label = abb)) +\n  geom_abline(intercept = log10(r), lty=2, col=\"darkgrey\") +\n  geom_point(aes(color=region), size = 3) +\n  geom_text_repel() +\n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") +\n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\") +\n  scale_color_discrete(name=\"Region\") +\n  theme_economist_white()\n\n\n\n\nWe are reminded of the saying: “A picture is worth a thousand words”. Data visualization provides a powerful way to communicate a data-driven finding. In some cases, the visualization is so convincing that no follow-up analysis is required. You should consider visualization the most potent tool in your data analytics arsenal.\nThe growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many industries, academia, and government. A salient example is news organizations, which are increasingly embracing data journalism and including effective infographics as part of their reporting.\nA particularly salient example—given the current state of the world—is a Wall Street Journal article1 showing data related to the impact of vaccines on battling infectious diseases. One of the graphs shows measles cases by US state through the years with a vertical line demonstrating when the vaccine was introduced.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n(Source: Wall Street Journal)\nAnother striking example comes from a New York Times chart2, which summarizes scores from the NYC Regents Exams. As described in the article3, these scores are collected for several reasons, including to determine if a student graduates from high school. In New York City you need a 65 to pass. The distribution of the test scores forces us to notice something somewhat problematic:\n\n\n\n\n\n(Source: New York Times via Amanda Cox)\nThe most common test score is the minimum passing grade, with very few scores just below the threshold. This unexpected result is consistent with students close to passing having their scores bumped up.\nThis is an example of how data visualization can lead to discoveries which would otherwise be missed if we simply subjected the data to a battery of data analysis tools or procedures. Data visualization is the strongest tool of what we call exploratory data analysis (EDA). John W. Tukey4, considered the father of EDA, once said,\n\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.”\n\n\nMany widely used data analysis tools were initiated by discoveries made via EDA. EDA is perhaps the most important part of data analysis, yet it is one that is often overlooked.\nData visualization is also now pervasive in philanthropic and educational organizations. In the talks New Insights on Poverty5 and The Best Stats You’ve Ever Seen6, Hans Rosling forces us to notice the unexpected with a series of plots related to world health and economics. In his videos, he uses animated graphs to show us how the world is changing and how old narratives are no longer true.\n\n\n\n\n\nIt is also important to note that mistakes, biases, systematic errors and other unexpected problems often lead to data that should be handled with care. Failure to discover these problems can give rise to flawed analyses and false discoveries. As an example, consider that measurement devices sometimes fail and that most data analysis procedures are not designed to detect these. Yet these data analysis procedures will still give you an answer. The fact that it can be difficult or impossible to notice an error just from the reported results makes data visualization particularly important.\nToday, we will discuss the basics of data visualization and exploratory data analysis. We will use the ggplot2 package to code. To learn the very basics, we will start with a somewhat artificial example: heights reported by students. Then we will cover the two examples mentioned above: 1) world health and economics and 2) infectious disease trends in the United States.\nOf course, there is much more to data visualization than what we cover here. The following are references for those who wish to learn more:\n\nER Tufte (1983) The visual display of quantitative information. Graphics Press.\nER Tufte (1990) Envisioning information. Graphics Press.\nER Tufte (1997) Visual explanations. Graphics Press.\nWS Cleveland (1993) Visualizing data. Hobart Press.\nWS Cleveland (1994) The elements of graphing data. CRC Press.\nA Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach: Turning tables into graphs. The American Statistician 56:121-130.\nNB Robbins (2004) Creating more effective graphs. Wiley.\nA Cairo (2013) The functional art: An introduction to information graphics and visualization. New Riders.\nN Yau (2013) Data points: Visualization that means something. Wiley.\n\nWe also do not cover interactive graphics, a topic that is both too advanced for this course and too unweildy. Some useful resources for those interested in learning more can be found below, and you are encouraged to draw inspiration from those websites in your projects:\n\nhttps://shiny.rstudio.com/\nhttps://d3js.org/"
  },
  {
    "objectID": "content/Week_01/01b.html#the-components-of-a-graph",
    "href": "content/Week_01/01b.html#the-components-of-a-graph",
    "title": "Introduction to Visualization",
    "section": "The components of a graph",
    "text": "The components of a graph\nWe will eventually construct a graph that summarizes the US murders dataset that looks like this:\n\n\n\n\n\nWe can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.\nThis data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.\nThe first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main five components to note are:\n\nData: The US murders data table is being summarized. We refer to this as the data component.\nGeometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, boxplot, pie (ew!), and many, many more. We will learn about these later.\nAesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using.\nAnnotations: These are things like axis labels, axis ticks (the lines along the axis at regular intervals or specific points of interest), axis scales (e.g. log-scale), titles, legends, etc.\nStyle: An overall appearance of the graph determined by fonts, color palattes, layout, blank spaces, and more.\n\nWe also note that:\n\nThe points are labeled with the state abbreviations.\nThe range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.\nThere are labels, a title, a legend, and we use the style of The Economist magazine.\n\nAll of the flexibility and visualization power of ggplot is contained in these four elements (plus your data)"
  },
  {
    "objectID": "content/Week_01/01b.html#ggplot-objects",
    "href": "content/Week_01/01b.html#ggplot-objects",
    "title": "Introduction to Visualization",
    "section": "ggplot objects",
    "text": "ggplot objects\nWe will now construct the plot piece by piece.\nWe start by loading the dataset:\n\nlibrary(dslabs)\ndata(murders)\n\n\n\n\nThe first step in creating a ggplot2 graph is to define a ggplot object. We do this with the function ggplot, which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object:\n\nggplot(data = murders)\n\nWe can also pipe the data in as the first argument. So this line of code is equivalent to the one above:\n\nmurders %>% ggplot()\n\n\n\n\nIt renders a plot, in this case a blank slate since no geometry has been defined. The only style choice we see is a grey background.\nWhat has happened above is that the object was created and, because it was not assigned, it was automatically evaluated. But we can assign our plot to an object, for example like this:\n\np <- ggplot(data = murders)\nclass(p)\n\n[1] \"gg\"     \"ggplot\"\n\n\nTo render the plot associated with this object, we simply print the object p. The following two lines of code each produce the same plot we see above:\n\nprint(p)\np"
  },
  {
    "objectID": "content/Week_01/01b.html#geometries-briefly",
    "href": "content/Week_01/01b.html#geometries-briefly",
    "title": "Introduction to Visualization",
    "section": "Geometries (briefly)",
    "text": "Geometries (briefly)\nIn ggplot2 we create graphs by adding geometry layers. Layers can define geometries, compute summary statistics, define what scales to use, create annotations, or even change styles. To add layers, we use the symbol +. In general, a line of code will look like this:\n\nDATA %>% ggplot() + LAYER 1 + LAYER 2 + ... + LAYER N\n\nUsually, the first added layer after ggplot() + defines the geometry. After that, we may add additional geometries, we may rescale an axis, we may add annotations and labels, or we may change the style. For now, we want to make a scatterplot like the one you all created in Lab 0. What geometry do we use?\n\n\n\n\n\nTaking a quick look at the cheat sheet, we see that the ggplot2 function used to create plots with this geometry is geom_point.\nSee Here\n(Image courtesy of RStudio9. CC-BY-4.0 license10.)\n\nGeometry function names follow the pattern: geom_X where X is the name of some specific geometry. Some examples include geom_point, geom_bar, and geom_histogram. You’ve already seen a few of these. We will start with a scatterplot created using geom_point() for now, then circle back to more geometries after we cover aesthetic mappings, layers, and annotations.\nFor geom_point to run properly we need to provide data and an aesthetic mapping. The simplest mapping for a scatter plot is to say we want one variable on the X-axis, and a different one on the Y-axis, so each point is an {X,Y} pair. That is an aesthetic mapping because X and Y are aesthetics in a geom_point scatterplot.\nWe have already connected the object p with the murders data table, and if we add the layer geom_point it defaults to using this data. To find out what mappings are expected, we read the Aesthetics section of the help file ?geom_point help file:\n> Aesthetics\n>\n> geom_point understands the following aesthetics (required aesthetics are in bold):\n>\n> **x**\n>\n> **y**\n>\n> alpha\n>\n> colour\n>\n> fill\n>\n> group\n>\n> shape\n>\n> size\n>\n> stroke\nand—although it does not show in bold above—we see that at least two arguments are required: x and y. You can’t have a geom_point scatterplot unless you state what you want on the X and Y axes."
  },
  {
    "objectID": "content/Week_01/01b.html#aesthetic-mappings",
    "href": "content/Week_01/01b.html#aesthetic-mappings",
    "title": "Introduction to Visualization",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\nAesthetic mappings describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The aes function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the aes function is often used as the argument of a geometry function. This example produces a scatterplot of population in millions (x-axis) versus total murders (y-axis):\n\nmurders %>% ggplot() +\n  geom_point(aes(x = population/10^6, y = total))\n\nInstead of defining our plot from scratch, we can also add a layer to the p object that was defined above as p <- ggplot(data = murders):\n\np + geom_point(aes(x = population/10^6, y = total))\n\n\n\n\nThe scales and annotations like axis labels are defined by default when adding this layer (note the x-axis label is exactly what we wrote in the function call). Like dplyr functions, aes also uses the variable names from the object component: we can use population and total without having to call them as murders$population and murders$total. The behavior of recognizing the variables from the data component is quite specific to aes. With most functions, if you try to access the values of population or total outside of aes you receive an error.\nNote that we did some rescaling within the aes() call - we can do simple things like multiplication or division on the variable names in the ggplot call. The axis labels reflect this. We will change the axis labels later.\nThe aesthetic mappings are very powerful - changing the variable in x= or y= changes the meaning of the plot entirely. We’ll come back to additional aesthetic mappings once we talk about aesthetics in general.\n\nAesthetics in general\nEven without mappings, a plots aesthetics can be useful. Things like color, fill, alpha, and size are aesthetics that can be changed.\nLet’s say we want larger points in our scatterplot. The size aesthetic can be used to set the size. The scale of size is “multiples of the defaults” (so size = 1 is the default)\n\np + geom_point(aes(x = population/10^6, y = total), size = 3)\n\n\n\n\nsize is not a mapping so it is not in the aes() part: whereas mappings use data from specific observations and need to be inside aes(), operations we want to affect all the points the same way do not need to be included inside aes. We’ll see what happens if size is inside aes(size = xxx) in a second.\nWe can change the shape to one of the many different base-R options found here:\n\np + geom_point(aes(x = population/10^6, y = total), size = 3, shape = 17)\n\n\n\n\nWe can also change the fill and the color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 4, shape = 23, fill = '#18453B')\n\n\n\n\nfill can take a common name like 'green', or can take a hex color like '#18453B', which is MSU Green according to MSU’s branding site. You can also find UM Maize and OSU Scarlet on respective branding pages, or google “XXX color hex.” We’ll learn how to build a color palatte later on.\ncolor (or colour, same thing because ggplot creators allow both spellings) is a little tricky with points - it changes the outline of the geometry rather than the fill color, but in geom_point() most shapes are only the outline, including the default. This is more useful with, say, a barplot where the outline and the fill might be different colors. Still, shapes 21-25 have both fill and color:\n\np + geom_point(aes(x = population/10^6, y = total), size = 5, shape = 23, fill = '#18453B', color = 'white')\n\n\n\n\nThe color = 'white' makes the outline of the shape white, which you can see if you look closely in the areas where the shapes overlap. This only works with shapes 21-25, or any other geometry that has both an outline and a fill.\n\n\nNow, back to aesthetic mappings\nNow that we’ve seen a few aesthetics (and know we can find more by looking at which aesthetics work with our geometry in the help file), let’s return to the power of aesthetic mappings.\nAn aesthetic mapping means we can vary an aesthetic (like fill or shape or size) according to some variable in our data. This opens up a world of possibilities! Let’s try adding to our x and y aesthetics with a color aesthetic (since points respond to color better than fill) that varies by region, which is a column in our data:\n\np + geom_point(aes(x = population/10^6, y = total, color = region), size = 3)\n\n\n\n\nWe include color=region inside the aes call, which tells R to find a variable called region and change color based on that. R will choose a somewhat ghastly color palatte, and every unique value in the data for region will get a different color if the variable is discrete. If the variable is a continuous value, then ggplot will automatically make a color ramp. Thus, discrete and continuous values for aesthetic mappings work differently.\nLet’s see a useful example of a continuous aesthetic mapping to color. In our data, we are making a scatterplot of population and total murders, which really just shows that states with higher populations have higher murders. What we really want is murders per capita (I think COVID taught us a lot about rates vs. levels like “cases” and “cases per 100,000 people”). We can create a variable of “murders per capita” on the fly. Since “murders per capita” is a very small number and hard to read, we’ll multiply by 100 so that we get “percent of population murdered per year”:\n\np + geom_point(aes(x = population/10^5, y = total, color = 100*total/population), size = 3)\n\n\n\n\nWhile the clear pattern of “more population means more murders” is still there, look at the outlier in light blue in the bottom left. With the color ramp, see how easy it is to see here that there is one location where murders per capita is quite high?\nNote that size is outside of aes and is set to an explicit value, not to a variable. What if we set size to a variable in the data?\n\np + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6))\n\n\n\n\n\n\nLegends for aesthetics\nHere we see yet another useful default behavior: ggplot2 automatically adds a legend that maps color to region, and size to population (which we scaled by 1,000,000). To avoid adding this legend we set the geom_point argument show.legend = FALSE. This removes both the size and the color legend.\n\np + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6), show.legend = FALSE)\n\n\n\n\nLater on, when we get to annotation layers, we’ll talk about controlling the legend text and layout. For now, we just need to know how to turn them off."
  },
  {
    "objectID": "content/Week_01/01b.html#annotation-layers",
    "href": "content/Week_01/01b.html#annotation-layers",
    "title": "Introduction to Visualization",
    "section": "Annotation Layers",
    "text": "Annotation Layers\nA second layer in the plot we wish to make involves adding a label to each point to identify the state. The geom_label and geom_text functions permit us to add text to the plot with and without a rectangle behind the text, respectively.\nBecause each point (each state in this case) has a label, we need an aesthetic mapping to make the connection between points and labels. By reading the help file ?geom_text, we learn that we supply the mapping between point and label through the label argument of aes. That is, label is an aesthetic that we can map. So the code looks like this:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\n\n\n\nWe have successfully added a second layer to the plot.\nAs an example of the unique behavior of aes mentioned above, note that this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total, label = abb))\n\nis fine, whereas this call:\n\np + geom_point(aes(x = population/10^6, y = total)) + \n  geom_text(aes(population/10^6, total), label = abb)\n\nwill give you an error since abb is not found because it is outside of the aes function. The layer geom_text does not know where to find abb since it is a column name and not a global variable, and ggplot does not look for column names for non-mapped aesthetics. For a trivial example:\n\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(population/10^6, total), label = 'abb')\n\n\n\n\n\nGlobal versus local aesthetic mappings\nIn the previous line of code, we define the mapping aes(population/10^6, total) twice, once in each geometry. We can avoid this by using a global aesthetic mapping. We can do this when we define the blank slate ggplot object. Remember that the function ggplot contains an argument that permits us to define aesthetic mappings:\n\nargs(ggplot)\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \nNULL\n\n\nIf we define a mapping in ggplot, all the geometries that are added as layers will default to this mapping. We redefine p:\n\np <- murders %>% ggplot(aes(x = population/10^6, y = total, label = abb))\n\nand then we can simply write the following code to produce the previous plot:\n\np + geom_point(size = 3) +\n  geom_text(nudge_x = 1.5) # offsets the label\n\nWe keep the size and nudge_x arguments in geom_point and geom_text, respectively, because we want to only increase the size of points and only nudge the labels. If we put those arguments in aes then they would apply to both plots. Also note that the geom_point function does not need a label argument and therefore ignores that aesthetic.\nIf necessary, we can override the global mapping by defining a new mapping within each layer. These local definitions override the global. Here is an example:\n\np + geom_point(size = 3) +\n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))\n\n\n\n\nClearly, the second call to geom_text does not use x = population and y = total."
  },
  {
    "objectID": "content/Week_01/01b.html#try-it",
    "href": "content/Week_01/01b.html#try-it",
    "title": "Introduction to Visualization",
    "section": "Try it!",
    "text": "Try it!\n\nLet’s break in to smaller groups and try playing with some of the aesthetics and aesthetic mappings. If we’re in person (woohoo!), we’ll form the same number of groups in class.\nIn each group, one person should be the main coder - someone who has the packages like dslabs installed and has successfully run the plots above. Each set of tasks ask you to learn about an aesthetic and put it into action with the murder data. We’ll leave about 5 minutes to do the task, then have you come back and share your results with the class.\nFor each group, we’ll start with the following code:\np + geom_point(aes(x = population/10^6, y = total)) +\n  geom_text(aes(x = population/10^6, y = total, label = abb))\n\nThe alpha aesthetic mapping.\n\nThe alpha aesthetic can only take a number between 0 and 1. So first, in murders, create a murders_per_capita column by dividing total by population. Second, find the max(murders$murders_per_capita) and then create another new column called murders_per_capita_rescaled which divides murders_per_capita by the max value. murders_per_capita_rescaled will be between 0 and 1, with the value of 1 for the state with the max murder rate. This is a little hard to do on the fly in ggplot.\nSet the alpha aesthetic mapping to murders_per_capita_rescaled for geom_point.\nTurn off the legend using show.legend=FALSE\nInclude the geom_text labels, but make sure the aesthetic mapping does not apply to the labels.\nUse nudge_x = 1.5 as before to offset the labels.\nBe able to explain the plot.\n\nDoes the alpha aesthetic help present the data here? It’s OK if it doesn’t!\n\n\nThe stroke aesthetic mapping.\n\nThe stroke aesthetic works a bit like the size aesthetic. It must be used with a plot that has both a border and a fill, like shapes 21-25, so use one of those.\nUse the stroke aesthetic mapping (meaning the stroke will change according to a value in the data) to set a different stroke size based on murders per capita. You can create a murders per capita variable on the fly, or add it to your murders data.\n\nInclude the text labels as before and use nudge_x = 1.5.\nMake sure you’re only setting the aesthetic for the points on the scatterplot!\n\n\nThe angle aesthetic\n\nUsing the ?geom_text help, note that geom_text takes an aesthetic of angle.\nUse the angle aesthetic (not aesthetic mapping) in the appropriate place (e.g. on geom_text and not on other geometries) to adjust the labels on our plot.\nNow, try using the angle aesthetic mapping by using the total field as both the y value and the angle value in the geom_text layer.\nDoes using angle as an aesthetic help? What about as an aesthetic mapping?\n\nThe color aesthetic mapping\n\nSet the color aesthetic mapping in geom_text to total/population.\n\nUse the nudge_x = 1.5 aesthetic in geom_text still\n\nTry it with and without the legend using show.legend.\nBe able to explain the plot.\n\nDoes the color aesthetic mapping help present the data here?\n\n\ngeom_label and the fill aesthetic\n\nLooking at ?geom_label (which is the same help as geom_text), we note that “The fill aesthetic controls the backgreound colour of the label”.\nSet the fill aesthetic mapping to total/population in geom_label (replacing geom_text but still using nudge_x=1.5)\nSet the fill aesthetic (not mapping) to the color of your choice.\nBe able to explain the plots.\n\n\nDoes the fill aesthetic mapping help present the data here?\nWhat color did you choose for the non-mapped fill aesthetic?"
  }
]