{
  "hash": "6a11839d40ceea878ee5a3770825030d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Applied Logistic Regression - Classification\"\nduedate: \"{{< var duedates.lab12 >}}\"\n---\n\n\n\n\n<!-- Updated 8.30.22, but classdata/bank.csv may need to be updated still -- it also uses 'y' instead of 'default' (though data/bank23 was fixed; this is maybe different data?) -->\n\n\n\n::: {.callout-warning}\n## Due Date\nThis assignment is due on **{{< meta duedate >}}**\n:::\n\n\n\n\n{{< var blurbs.homeworkdue >}}\n\n\n\n\n\n\n\n\n\n## Backstory and Set Up\nYou work for a bank. This bank is trying to predict defaults on loans (a relatively uncommon occurence, but one that costs the bank a great deal of money when it does happen.) They've given you a dataset on defaults (encoded as the variable `y`, and not the column *default*). You're going to try to predict this.\n\nThis is some new data. The snippet below loads it.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbank <- read.table(\"https://ec242.netlify.app/data/bank.csv\",\n                 header = TRUE,\n                 sep = \",\")\n```\n:::\n\n\n\n\nThere's not going to be a whole lot of wind-up here. You should be well-versed in doing these sorts of things by now (if not, look back at the previous lab for sample code).\n\n::: {.callout-note}\n\n__EXERCISE 1__\n\n0. Encode the outcome we're trying to predict (`y`, and not `default`) as a binary.\n\n1. Split the data into an 80/20 train vs. test split. Make sure you explicitly set the seed for replicability.\n\n2. Run a series of logistic regressions with between 1 and 4 predictors of your choice (you can use interactions).\n\n3. Create eight total confusion matrices: four by applying your models to the training data, and four by applying your models to the test data. **Briefly discuss your findings.** How does the error rate, sensitivity, and specificity change as the number of predictors increases? \n\nA few hints:\n\n- If you are not getting a 2x2 confusion matrix, you might need to adjust your cutoff probability.\n- It might be the case that your model perfectly predicts the outcome variable when the setup cutoff probability is too high.\n- You need to make sure your predictions take the same possible values as the `actual` data (which, remember, you had to convert to a binary 0/1 variable)\n\n:::\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}