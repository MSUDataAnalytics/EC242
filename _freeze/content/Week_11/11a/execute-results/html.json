{
  "hash": "646d69a9a1efe73cc08e1725df764687",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bias-Variance Tradeoff\"\ntype: docs\noutput:\n  blogdown::html_page:\n    toc: true\n---\n\n\n\n\n## Required Reading\n\n- This page.\n- <i class=\"fas fa-book\"></i> [Chapter 2](https://trevorhastie.github.io/ISLR/ISLR%20Seventh%20Printing.pdf) in *Introduction to Statistical Learning with Applications in R*.\n- <i class=\"fas fa-book\"></i> [Chapter 8](https://trevorhastie.github.io/ISLR/ISLR%20Seventh%20Printing.pdf) (briefly) in *Introduction to Statistical Learning with Applications in R*.\n\n### Guiding Questions\n\n- What is the relationship between **bias**, **variance**, and **mean squared error?**\n- What is the relationship between **model flexibility** and training error?\n- What is the relationship between **model flexibility** and validation (or test) error?\n- What is a random forest?\n\n# The Bias–Variance Tradeoff\n\n\n\n\n\n\n\n\n\nThis lecture will begin to dig into some theoretical details of estimating regression functions, in particular how the **bias-variance tradeoff** helps explain the relationship between **model flexibility** and the errors a model makes.\n\nDon't freak out if this seems mathematically overwhelming. We'll walk through relatively slowly. It's not super important to follow the nitty-gritty details; but the broad takeaways are quite important.\n\n\n## Illustration of Bias vs. Variance\nBias is about how close you are on average to the correct answer. Variance is about how scattered your estimates would be if you repeated your experiment with new data. \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Image from MachineLearningPlus.com](https://www.machinelearningplus.com/wp-content/uploads/2020/10/output_31_0.png){fig-align='center'}\n:::\n:::\n\n\n\n\nWe care about these things because we usually only have our one dataset (when we're not creating simulated data, that is), but need to know something about how bias and variance tend to look when we change our model complexity.\n\n\n## R Setup and Source\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tibble)     # data frame printing\nlibrary(dplyr)      # data manipulation\n\nlibrary(caret)      # fitting knn\nlibrary(rpart)      # fitting trees\nlibrary(rpart.plot) # plotting trees\n```\n:::\n\n\n\n\n## The Regression Setup\n\nConsider the general regression setup where we are given a random pair $(X, Y) \\in \\mathbb{R}^p \\times \\mathbb{R}$. We would like to \"predict\" $Y$ with some function of $X$, say, $f(X)$.\n\nTo clarify what we mean by \"predict,\" we specify that we would like $f(X)$ to be \"close\" to $Y$. To further clarify what we mean by \"close,\" we define the **squared error loss** of estimating $Y$ using $f(X)$.\n\n$$\nL(Y, f(X)) \\triangleq (Y - f(X)) ^ 2\n$$\n\nNow we can clarify the goal of regression, which is to minimize the above loss, on average. We call this the **risk** of estimating $Y$ using $f(X)$.\n\n$$\nR(Y, f(X)) \\triangleq \\mathbb{E}[L(Y, f(X))] = \\mathbb{E}_{X, Y}[(Y - f(X)) ^ 2]\n$$\n\nBefore attempting to minimize the risk, we first re-write the risk after conditioning on $X$.\n\n$$\n\\mathbb{E}_{X, Y} \\left[ (Y - f(X)) ^ 2 \\right] = \\mathbb{E}_{X} \\mathbb{E}_{Y \\mid X} \\left[ ( Y - f(X) ) ^ 2 \\mid X = x \\right]\n$$\n\nMinimizing the right-hand side is much easier, as it simply amounts to minimizing the inner expectation with respect to $Y \\mid X$, essentially minimizing the risk pointwise, for each $x$.\n\nIt turns out, that the risk is minimized by setting $f(x)$ to be equal the conditional mean of $Y$ given $X$,\n\n$$\nf(x) = \\mathbb{E}(Y \\mid X = x)\n$$\n\nwhich we call the **regression function**.^[Note that in this section, we will refer to $f(x)$ as the regression function instead of $\\mu(x)$ for unimportant and arbitrary reasons.]\n\nNote that the choice of squared error loss is somewhat arbitrary. Suppose instead we chose absolute error loss.\n\n$$\nL(Y, f(X)) \\triangleq | Y - f(X) |\n$$\n\nThe risk would then be minimized setting $f(x)$ equal to the conditional median.\n\n$$\nf(x) = \\text{median}(Y \\mid X = x)\n$$\n\nDespite this possibility, our preference will still be for squared error loss. The reasons for this are numerous, including: historical, ease of optimization, and protecting against large deviations.\n\nNow, given data $\\mathcal{D} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}$, our goal becomes finding some $\\hat{f}$ that is a good estimate of the regression function $f$. We'll see that this amounts to minimizing what we call the reducible error.\n\n## Reducible and Irreducible Error\n\nSuppose that we obtain some $\\hat{f}$, how well does it estimate $f$? We define the **expected prediction error** of predicting $Y$ using $\\hat{f}(X)$. A good $\\hat{f}$ will have a low expected prediction error.\n\n$$\n\\text{EPE}\\left(Y, \\hat{f}(X)\\right) \\triangleq \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[  \\left( Y - \\hat{f}(X) \\right)^2 \\right]\n$$\n\nThis expectation is over $X$, $Y$, and also $\\mathcal{D}$. The estimate $\\hat{f}$ is actually random depending on the data, $\\mathcal{D}$, used to estimate $\\hat{f}$. We could actually write $\\hat{f}(X, \\mathcal{D})$ to make this dependence explicit, but our notation will become cumbersome enough as it is.\n\nLike before, we'll condition on $X$. This results in the expected prediction error of predicting $Y$ using $\\hat{f}(X)$ when $X = x$.\n\n$$\n\\text{EPE}\\left(Y, \\hat{f}(x)\\right) =\n\\mathbb{E}_{Y \\mid X, \\mathcal{D}} \\left[  \\left(Y - \\hat{f}(X) \\right)^2 \\mid X = x \\right] =\n\\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left[  \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]}_\\textrm{reducible error} +\n\\underbrace{\\mathbb{V}_{Y \\mid X} \\left[ Y \\mid X = x \\right]}_\\textrm{irreducible error}\n$$\n\nA number of things to note here:\n\n- The expected prediction error is for a random $Y$ given a fixed $x$ and a random $\\hat{f}$. As such, the expectation is over $Y \\mid X$ and $\\mathcal{D}$. Our estimated function $\\hat{f}$ is random depending on the data, $\\mathcal{D}$, which is used to perform the estimation.\n- The expected prediction error of predicting $Y$ using $\\hat{f}(X)$ when $X = x$ has been decomposed into two errors:\n    - The **reducible error**, which is the expected squared error loss of estimation $f(x)$ using $\\hat{f}(x)$ at a fixed point $x$. The only thing that is random here is $\\mathcal{D}$, the data used to obtain $\\hat{f}$. (Both $f$ and $x$ are fixed.) We'll often call this reducible error the **mean squared error** of estimating $f(x)$ using $\\hat{f}$ at a fixed point $x$.\n    $$ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) \\triangleq \\mathbb{E}_{\\mathcal{D}} \\left[  \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]$$\n    - The **irreducible error**. This is simply the variance of $Y$ given that $X = x$, essentially noise that we do not want to learn. This is also called the **Bayes error**.\n\nAs the name suggests, the reducible error is the error that we have some control over. But how do we control this error?\n\n## Bias-Variance Decomposition\n\nAfter decomposing the expected prediction error into reducible and irreducible error, we can further decompose the reducible error.\n\nRecall the definition of the **bias** of an estimator.\n\n$$\n\\text{bias}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left[\\hat{\\theta}\\right] - \\theta\n$$\n\nAlso recall the definition of the **variance** of an estimator.\n\n$$\n\\mathbb{V}(\\hat{\\theta}) = \\text{var}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left [ ( \\hat{\\theta} -\\mathbb{E}\\left[\\hat{\\theta}\\right] )^2 \\right]\n$$\n\nUsing this, we further decompose the reducible error (mean squared error) into bias squared and variance.\n\n$$\n\\text{MSE}\\left(f(x), \\hat{f}(x)\\right) =\n\\mathbb{E}_{\\mathcal{D}} \\left[  \\left(f(x) - \\hat{f}(x) \\right)^2 \\right] =\n\\underbrace{\\left(f(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right]  \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}(x) \\right)} +\n\\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}(x) \\right)}\n$$\n\nThis is actually a common fact in estimation theory, but we have stated it here specifically for estimation of some regression function $f$ using $\\hat{f}$ at some point $x$.\n\n$$\n\\text{MSE}\\left(f(x), \\hat{f}(x)\\right) = \\text{bias}^2 \\left(\\hat{f}(x) \\right) + \\text{var} \\left(\\hat{f}(x) \\right)\n$$\n\nIn a perfect world, we would be able to find some $\\hat{f}$ which is **unbiased**, that is $\\text{bias}\\left(\\hat{f}(x) \\right) = 0$, which also has low variance. In practice, this isn't always possible.\n\nIt turns out, there is a **bias-variance tradeoff**. That is, often, the more bias in our estimation, the lesser the variance. Similarly, less variance is often accompanied by more bias. <span style=\"color: red;\">Flexible models tend to be unbiased, but highly variable. Simple models are often extremely biased, but have low variance.</span>\n\nIn the context of regression, models are biased when:\n\n- Parametric: The form of the model [does not incorporate all the necessary variables](https://en.wikipedia.org/wiki/Omitted-variable_bias), or the form of the relationship is too simple. For example, a parametric model assumes a linear relationship, but the true relationship is quadratic.\n- Non-parametric: The model provides too much smoothing.\n\nIn the context of regression, models are variable when:\n\n- Parametric: The form of the model incorporates too many variables, or the form of the relationship is too flexible. For example, a parametric model assumes a cubic relationship, but the true relationship is linear.\n- Non-parametric: The model does not provide enough smoothing. It is very, \"wiggly.\" [Recall our KNN model example from Content 08](https://ssc442kirkpatrick.netlify.app/content/08-content/#k-nearest-neighbors-1)\n\nSo for us, to select a model that appropriately balances the tradeoff between bias and variance, and thus minimizes the reducible error, we need to select a model of the appropriate flexibility for the data.\n\nRecall that when fitting models, we've seen that train RMSE decreases as model flexibility is increasing. (Technically it is non-increasing.) For validation RMSE, we expect to see a U-shaped curve. Importantly, validation RMSE decreases, until a certain flexibility, then begins to increase.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nNow we can understand why this is happening. The expected test RMSE is essentially the expected prediction error, which we now known decomposes into (squared) bias, variance, and the irreducible Bayes error. The following plots show three examples of this.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11a_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n\nThe three plots show three examples of the bias-variance tradeoff. In the left panel, the variance influences the expected prediction error more than the bias. In the right panel, the opposite is true. The middle panel is somewhat neutral. In all cases, the difference between the Bayes error (the horizontal dashed grey line) and the expected prediction error (the solid black curve) is exactly the mean squared error, which is the sum of the squared bias (blue curve) and variance (orange curve). The vertical line indicates the flexibility that minimizes the prediction error.\n\nTo summarize, if we assume that irreducible error can be written as\n\n$$\n\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2\n$$\n\nthen we can write the full decomposition of the expected prediction error of predicting $Y$ using $\\hat{f}$ when $X = x$ as\n\n$$\n\\text{EPE}\\left(Y, \\hat{f}(x)\\right) =\n\\underbrace{\\text{bias}^2\\left(\\hat{f}(x)\\right) + \\text{var}\\left(\\hat{f}(x)\\right)}_\\textrm{reducible error} + \\sigma^2.\n$$\n\nAs model flexibility increases, bias decreases, while variance increases. By understanding the tradeoff between bias and variance, we can manipulate model flexibility to find a model that will predict well on unseen observations.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11a_files/figure-html/error-vs-flex-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\nTying this all together, the above image shows how we \"expect\" training and validation error to behavior in relation to model flexibility.^[Someday, someone will tell you this is a lie. They aren't wrong. In modern deep learning, there is a concept called [Deep Double Descent](https://openai.com/blog/deep-double-descent/). See also @belkin2018reconciling.] In practice, we won't always see such a nice \"curve\" in the validation error, but we expect to see the general trends.\n\n## Using Simulation to Estimate Bias and Variance\n\nWe will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function $f(x) = x^2$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf = function(x) {\n  x ^ 2\n}\n```\n:::\n\n\n\n\nMore specifically, we'd like to predict an observation, $Y$, given that $X = x$ by using $\\hat{f}(x)$ where\n\n$$\n\\mathbb{E}[Y \\mid X = x] = f(x) = x^2\n$$\n\nand\n\n$$\n\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2.\n$$\n\nAlternatively, we could write this as\n\n$$\nY = f(X) + \\epsilon\n$$\n\nwhere $\\mathbb{E}[\\epsilon] = 0$ and $\\mathbb{V}[\\epsilon] = \\sigma ^ 2$. In this formulation, we call $f(X)$ the **signal** and $\\epsilon$ the **noise**.\n\nTo carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following `R` code.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngen_sim_data = function(f, sample_size = 100) {\n  x = runif(n = sample_size, min = 0, max = 1)\n  y = rnorm(n = sample_size, mean = f(x), sd = 0.3)\n  tibble(x, y)\n}\n```\n:::\n\n\n\n\nAlso note that if you prefer to think of this situation using the $Y = f(X) + \\epsilon$ formulation, the following code represents the same data generating process.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngen_sim_data = function(f, sample_size = 100) {\n  x = runif(n = sample_size, min = 0, max = 1)\n  eps = rnorm(n = sample_size, mean = 0, sd = 0.3)\n  y = f(x) + eps\n  tibble(x, y)\n}\n```\n:::\n\n\n\n\nTo completely specify the data generating process, we have made more model assumptions than simply $\\mathbb{E}[Y \\mid X = x] = x^2$ and $\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2$. In particular,\n\n- The $x_i$ in $\\mathcal{D}$ are sampled from a uniform distribution over $[0, 1]$.\n- The $x_i$ and $\\epsilon$ are independent.\n- The $y_i$ in $\\mathcal{D}$ are sampled from the conditional normal distribution.\n\n$$\nY \\mid X \\sim N(f(x), \\sigma^2)\n$$\n\nUsing this setup, we will generate datasets, $\\mathcal{D}$, with a sample size $n = 100$ and fit four models.\n\n$$\n\\begin{aligned}\n\\texttt{predict(fit0, x)} &= \\hat{f}_0(x) = \\hat{\\beta}_0\\\\\n\\texttt{predict(fit1, x)} &= \\hat{f}_1(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\\\\n\\texttt{predict(fit2, x)} &= \\hat{f}_2(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 \\\\\n\\texttt{predict(fit9, x)} &= \\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9\n\\end{aligned}\n$$\n\nTo get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1)\nsim_data = gen_sim_data(f)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_0 = lm(y ~ 1,                   data = sim_data)\nfit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\nfit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\nfit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\n```\n:::\n\n\n\n\nNote that technically we're being lazy and using orthogonal polynomials, but the fitted values are the same, so this makes no difference for our purposes. These could be KNN, or decision trees just the same - the principle still applies.\n\nPlotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11a_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=864}\n:::\n:::\n\n\n\n\nThe following three plots were created using three additional simulated datasets. The zero predictor and ninth degree polynomial were fit to each.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11a_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n\nThis plot should make clear the difference between the bias and variance of these two models. The zero predictor model is clearly wrong, that is, biased, but nearly the same for each of the datasets, since it has very low variance.\n\nWhile the ninth degree model doesn't appear to be correct for any of these three simulations, we'll see that on average it is, and thus is performing unbiased estimation. These plots do however clearly illustrate that the ninth degree polynomial is extremely variable. Each dataset results in a very different fitted model. Correct on average isn't the only goal we're after, since in practice, we'll only have a single dataset. This is why we'd also like our models to exhibit low variance.\n\nWe could have also fit $k$-nearest neighbors models to these three datasets.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11a_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n\nHere we see that when $k = 100$ we have a biased model with very low variance.^[It's actually the same as the 0 predictor linear model. Can you see why?] When $k = 5$, we again have a highly variable model.\n\nThese two sets of plots reinforce our intuition about the bias-variance tradeoff. Flexible models (ninth degree polynomial and $k$ = 5) are highly variable, and often unbiased. Simple models (zero predictor linear model and $k = 100$) are very biased, but have extremely low variance.\n\nWe will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates of $f(x)$ given by these four models at the point $x = 0.90$. We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1)\nn_sims = 250\nn_models = 4\nx = data.frame(x = 0.90) # fixed point at which we make predictions\npredictions = matrix(0, nrow = n_sims, ncol = n_models)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfor (sim in 1:n_sims) {\n\n  # simulate new, random, training data\n  # this is the only random portion of the bias, var, and mse calculations\n  # this allows us to calculate the expectation over D\n  sim_data = gen_sim_data(f)\n\n  # fit models\n  fit_0 = lm(y ~ 1,                   data = sim_data)\n  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\n  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\n  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\n\n  # get predictions\n  predictions[sim, 1] = predict(fit_0, x)\n  predictions[sim, 2] = predict(fit_1, x)\n  predictions[sim, 3] = predict(fit_2, x)\n  predictions[sim, 4] = predict(fit_9, x)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11a_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=864}\n:::\n:::\n\n\n\n\nThe above plot shows the predictions for each of the 250 simulations of each of the four models of different polynomial degrees. The truth, $f(x = 0.90) = (0.9)^2 = 0.81$, is given by the solid black horizontal line.\n\nTwo things are immediately clear:\n\n- As flexibility *increases*, **bias decreases**. The mean of a model's predictions is closer to the truth.\n- As flexibility *increases*, **variance increases**. The variance about the mean of a model's predictions increases.\n\nThe goal of this simulation study is to show that the following holds true for each of the four models.\n\n$$\n\\text{MSE}\\left(f(0.90), \\hat{f}_k(0.90)\\right) =\n\\underbrace{\\left(\\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] - f(0.90) \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}_k(0.90) \\right)} +\n\\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}_k(0.90) - \\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}_k(0.90) \\right)}\n$$\n\nWe'll use the empirical results of our simulations to estimate these quantities. (Yes, we're using estimation to justify facts about estimation.) Note that we've actually used a rather small number of simulations. In practice we should use more, but for the sake of computation time, we've performed just enough simulations to obtain the desired results. (Since we're estimating estimation, the bigger the sample size, the better.)\n\nTo estimate the mean squared error of our predictions, we'll use\n\n$$\n\\widehat{\\text{MSE}}\\left(f(0.90), \\hat{f}_k(0.90)\\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(f(0.90) - \\hat{f}_k^{[i]}(0.90) \\right)^2\n$$\n\nwhere $\\hat{f}_k^{[i]}(0.90)$ is the estimate of $f(0.90)$ using the $i$-th from the polynomial degree $k$ model.\n\nWe also write an accompanying `R` function.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_mse = function(truth, estimate) {\n  mean((estimate - truth) ^ 2)\n}\n```\n:::\n\n\n\n\nSimilarly, for the bias of our predictions we use,\n\n$$\n\\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right)  = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) \\right) - f(0.90)\n$$\n\nAnd again, we write an accompanying `R` function.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_bias = function(estimate, truth) {\n  mean(estimate) - truth\n}\n```\n:::\n\n\n\n\nLastly, for the variance of our predictions we have\n\n$$\n\\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k^{[i]}(0.90) \\right)^2\n$$\n\nWhile there is already `R` function for variance, the following is more appropriate in this situation.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_var = function(estimate) {\n  mean((estimate - mean(estimate)) ^ 2)\n}\n```\n:::\n\n\n\n\nTo quickly obtain these results for each of the four models, we utilize the `apply()` function.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbias = apply(predictions, MAR = 2, get_bias, truth = f(x = 0.90))\nvariance = apply(predictions, MAR = 2, get_var)\nmse = apply(predictions, MAR = 2, get_mse, truth = f(x = 0.90))\n```\n:::\n\n\n\n\nWe summarize these results in the following table.\n\n\n\n\n::: {.cell layout-align=\"center\" asis='true'}\n::: {.cell-output-display}\n\n\n| Degree | Mean Squared Error | Bias Squared | Variance |\n|:------:|:------------------:|:------------:|:--------:|\n|   0    |      0.22643       |   0.22476    | 0.00167  |\n|   1    |      0.00829       |   0.00508    | 0.00322  |\n|   2    |      0.00387       |   0.00005    | 0.00381  |\n|   9    |      0.01019       |   0.00002    | 0.01017  |\n\n\n:::\n:::\n\n\n\n\nA number of things to notice here:\n\n- We use squared bias in this table. Since bias can be positive or negative, squared bias is more useful for observing the trend as flexibility increases.\n- The squared bias trend which we see here is **decreasing** as flexibility increases, which we expect to see in general.\n- The exact opposite is true of variance. As model flexibility increases, variance **increases**.\n- The mean squared error, which is a function of the bias and variance, decreases, then increases. This is a result of the bias-variance tradeoff. We can decrease bias, by increasing variance. Or, we can decrease variance by increasing bias. By striking the correct balance, we can find a good mean squared error!\n\nWe can check for these trends with the `diff()` function in `R`.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nall(diff(bias ^ 2) < 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nall(diff(variance) > 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\ndiff(mse) < 0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    1     2     9 \n TRUE  TRUE FALSE \n```\n\n\n:::\n:::\n\n\n\n\nThe models with polynomial degrees 2 and 9 are both essentially unbiased. We see some bias here as a result of using simulation. If we increased the number of simulations, we would see both biases go down. Since they are both unbiased, the model with degree 2 outperforms the model with degree 9 due to its smaller variance.\n\nModels with degree 0 and 1 are biased because they assume the wrong form of the regression function. While the degree 9 model does this as well, it does include all the necessary polynomial degrees.\n\n$$\n\\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9\n$$\n\nThen, since least squares estimation is unbiased, importantly,\n\n$$\n\\mathbb{E}\\left[\\hat{\\beta}_d\\right] = \\beta_d = 0\n$$\n\nfor $d = 3, 4, \\ldots 9$, we have\n\n$$\n\\mathbb{E}\\left[\\hat{f}_9(x)\\right] = \\beta_0 + \\beta_1 x + \\beta_2 x^2\n$$\n\nNow we can finally verify the bias-variance decomposition.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbias ^ 2 + variance == mse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    0     1     2     9 \nFALSE FALSE FALSE  TRUE \n```\n\n\n:::\n:::\n\n\n\n\nBut wait, this says it isn't true, except for the degree 9 model? It turns out, this is simply a computational issue. If we allow for some very small error tolerance, we see that the bias-variance decomposition is indeed true for predictions from these for models.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nall.equal(bias ^ 2 + variance, mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n\nSee `?all.equal()` for details.\n\n\n\n# Random Forests\n\nThe decision trees (or \"regression trees\") from last week were an intuitive (hopefully) way of partitioning out the predictor variable space into prediction areas. By \"partitioning\", I mean chopping up the predictor space in a way that explains the data best, then using the resulting regions as the prediction for any value of the predictors. From ISLR Chapter 8:\n\n\n![Source:ISLR](11b_insertimage_1.png)\n\n\nWe can change the two tuning parameters - `cp` and `minsplit` to make the partition more or less detailed (have more or fewer terminal nodes), and there are actually other parameters we can play with in regression trees, though we won't address those here. If we tune those parameters by cross-validation, then we can generate a pretty good predictor with our chosen tree. But...we can still do better.\n\nRegression trees still have quite a bit of variance, and we'd love to reduce that variance. One way of doing this is by constructing a **random forest**. As the name indicates, a random forest is a whole bunch of trees put together. But how do we combine multiple trees? And how do we make multiple trees if we have one dataset that leads to one optimal tree?\n\n### Multiple trees by *bootstrap*\nThe \"bootstrap\" is a statistical term that means, essentially, randomly resampling. We can \"bootstrap\" our existing (training) data by randomly drawing observations from the data. Usually, if we have $n$ observations, we'll draw $n$ observations *with replacement*, which gives us a slightly different dataset: some observations will not be in the bootstrapped sample, and some will be represented 2+ times. All of the bootstrapped samples follow the *same* distribution of the predictors, but are different realizations. It's like getting more random samples for free!\n\nSo let's say you draw sample $b=1$ from the $n$ observations. It is also of size $n$, but is different from the original sample. Then, you draw sample number $b=2$, all the way to, say, $B = 250$. Now you have 250 different samples of data, and each one will generate a *different* regression tree, even using the same tuning parameter values.\n\n### Combining multiple trees\n\nWe need a way of generating a RMSE for any given tuning parameter value, but now we have $B$ different trees. We can take our test data and put it into each of the $B$ trees and get a predicted value, right? And when the tree is different, then the prediction will be different. If we take the *average* over all $B$ predictions, we'll get a prediction that has less variance, even when each tree has variance $\\sigma$, because variance of the mean scales to $\\frac{\\sigma}{\\sqrt{n}}$. \n\nSo the rmse is:\n\n$$RMSE = \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{1}{B} \\sum_{b=1}^B (\\hat{f}_{b}(x_i) - y_i)^2\\right)$$\n\nWhile it takes a bit longer to estimate this (especially when $B$ is large), you get much more stable results that often predict better on an evaluation hold-out sample.\n\n### OK, technically that's \"bagging\"\n\nRandom Forest has one other wrinkle -- when choosing amongst the candidate predictors for each split, the Random Forest method will choose from a randomly selected subset of the predictors. The reasoning for this given in ISLR is intuitive: if there is one predictor that predicts quite well for the sample, it will always be selected and little emphasis will be on predictors aside from this one. By randomly leaving it out, some trees are forced to fit without that predictor, so they won't all look the same. When you don't take the subset, you are *bagging*. When you do take a subset, you are estimating a *random forest*\n\n\nYou won't need to use this on your Project 2, it's purely for your own info. Random forests are common out in the world, and are useful to know especially since you already know the fundamental element of it -- the tree.\n\n\n# More examples of bias, variance, and model flexibility\n\nIf we have time, we can dive back into a little more about model flexibility, bias, and variance....\n\n## Model Flexibility\n\nLet's return to the simulated dataset we used occaisionally in the linear regression content. Recall there was a single feature $x$ with the following properties:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# define regression function\ncubic_mean = function(x) {\n  1 - 2 * x - 3 * x ^ 2 + 5 * x ^ 3\n}\n```\n:::\n\n\n\n\nWe then generated some data around this function with some added noise:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# define full data generating process\ngen_slr_data = function(sample_size = 100, mu) {\n  x = runif(n = sample_size, min = -1, max = 1)\n  y = mu(x) + rnorm(n = sample_size)\n  tibble(x, y)\n}\n```\n:::\n\n\n\n\nAfter defining the data generating process, we generate and split the data.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# simulate entire dataset\nset.seed(3)\nsim_slr_data = gen_slr_data(sample_size = 100, mu = cubic_mean)\n\n# test-train split\nslr_trn_idx = sample(nrow(sim_slr_data), size = 0.8 * nrow(sim_slr_data))\nslr_trn = sim_slr_data[slr_trn_idx, ]\nslr_tst = sim_slr_data[-slr_trn_idx, ]\n\n# estimation-validation split\nslr_est_idx = sample(nrow(slr_trn), size = 0.8 * nrow(slr_trn))\nslr_est = slr_trn[slr_est_idx, ]\nslr_val = slr_trn[-slr_est_idx, ]\n\n# check data\nhead(slr_trn, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 2\n        x      y\n    <dbl>  <dbl>\n 1  0.573 -1.18 \n 2  0.807  0.576\n 3  0.272 -0.973\n 4 -0.813 -1.78 \n 5 -0.161  0.833\n 6  0.736  1.07 \n 7 -0.242  2.97 \n 8  0.520 -1.64 \n 9 -0.664  0.269\n10 -0.777 -2.02 \n```\n\n\n:::\n:::\n\n\n\n\nFor validating models, we will use RMSE.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# helper function for calculating RMSE\ncalc_rmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n```\n:::\n\n\n\n\nLet's check how linear, k-nearest neighbors, and decision tree models fit to this data make errors, while paying attention to their flexibility.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11a_files/figure-html/error-vs-flex-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\nThis picture is an idealized version of what we expect to see, but we'll illustrate the sorts of validate \"curves\" that we might see in practice.\n\nNote that in the following three sub-sections, a significant portion of the code is suppressed for visual clarity. See the source document for full details.\n\n### Linear Models\n\nFirst up, linear models. We will fit polynomial models with degree from one to nine, and then validate.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fit polynomial models\npoly_mod_est_list = list(\n  poly_mod_1_est = lm(y ~ poly(x, degree = 1), data = slr_est),\n  poly_mod_2_est = lm(y ~ poly(x, degree = 2), data = slr_est),\n  poly_mod_3_est = lm(y ~ poly(x, degree = 3), data = slr_est),\n  poly_mod_4_est = lm(y ~ poly(x, degree = 4), data = slr_est),\n  poly_mod_5_est = lm(y ~ poly(x, degree = 5), data = slr_est),\n  poly_mod_6_est = lm(y ~ poly(x, degree = 6), data = slr_est),\n  poly_mod_7_est = lm(y ~ poly(x, degree = 7), data = slr_est),\n  poly_mod_8_est = lm(y ~ poly(x, degree = 8), data = slr_est),\n  poly_mod_9_est = lm(y ~ poly(x, degree = 9), data = slr_est)\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nThe plot below visualizes the results.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11a_files/figure-html/unnamed-chunk-31-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nWhat do we see here? As the polynomial degree *increases*:\n\n- The training error *decreases*.\n- The validation error *decreases*, then *increases*.\n\nThis more of less matches the idealized version above, but the validation \"curve\" is much more jagged. This is something that we can expect in practice.\n\nWe have previously noted that training error isn't particularly useful for validating models. That is still true. However, it can be useful for checking that everything is working as planned. In this case, since we known that training error decreases as model flexibility increases, we can verify our intuition that a higher degree polynomial is indeed more flexible.^[In practice, if you already know how your model's flexibility works, by checking that the training error goes down as you increase flexibility, you can check that you have done your coding and model training correctly.]\n\n### k-Nearest Neighbors\n\nNext up, k-nearest neighbors. We will consider values for $k$ that are odd and between $1$ and $45$ inclusive.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# helper function for fitting knn models\nfit_knn_mod = function(neighbors) {\n  knnreg(y ~ x, data = slr_est, k = neighbors)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# define values of tuning parameter k to evaluate\nk_to_try = seq(from = 1, to = 45, by = 2)\n\n# fit knn models\nknn_mod_est_list = lapply(k_to_try, fit_knn_mod)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nThe plot below visualizes the results.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11a_files/figure-html/unnamed-chunk-35-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nHere we see the \"opposite\" of the usual plot. Why? Because with k-nearest neighbors, a small value of $k$ generates a flexible model compared to larger values of $k$. So visually, this plot is flipped. That is we see that as $k$ *increases*:\n\n- The training error *increases*.\n- The validation error *decreases*, then *increases*.\n\nImportant to note here: the pattern above only holds \"in general,\" that is, there can be minor deviations in the validation pattern along the way. This is due to the random nature of selection the data for the validate set.\n\n### Decision Trees\n\nLastly, we evaluate some decision tree models. We choose some arbitrary values of `cp` to evaluate, while holding `minsplit` constant at `5`. There are arbitrary choices that produce a plot that is useful for discussion.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# helper function for fitting decision tree models\ntree_knn_mod = function(flex) {\n  rpart(y ~ x, data = slr_est, cp = flex, minsplit = 5)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# define values of tuning parameter cp to evaluate\ncp_to_try = c(0.5, 0.3, 0.1, 0.05, 0.01, 0.001, 0.0001)\n\n# fit decision tree models\ntree_mod_est_list = lapply(cp_to_try, tree_knn_mod)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nThe plot below visualizes the results.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11a_files/figure-html/unnamed-chunk-39-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nBased on this plot, how is `cp` related to model flexibility?^[As `cp` increases, model flexibility decreases.]\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}