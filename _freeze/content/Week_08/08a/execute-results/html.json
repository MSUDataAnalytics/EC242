{
  "hash": "0a1246b9e116f28c7d40a602adf9a038",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Regression III\"\noutput:\n  blogdown::html_page:\n    toc: true\n---\n\n\n\n\n<!-- Spring 2025: Shorten this -- the causality part is redundant to others, and the conditioning is repetitive of moneyball, though it makes an OK review. The spurious correlation helps to motivate overfitting with pure noise, though, which is nice. Keep p-hacking part. Move beginning of 8b through the TRY IT onto here. -->\n\n\n\n\n\n\n\n\n\n## Required Reading\n\n- This page. \n\n### Guiding Question\n\n- When can we make causal claims about the relationship between variables?\n\nAs with recent weeks, we will work with real data during the lecture. Please download the following dataset and load it into `R`.\n\n- [<i class=\"fas fa-file-csv\"></i> `Ames.csv`](/projects/data/ames.csv)\n\n## A quick `R` note:\nWe learned that we can run a regression with all variables in a dataset using, for example, `SalePrice ~ .`\n\nIn your lab this week, you are asked to run 15 models (!) of increasing complexity, where \"complexity\" is defined in one question as adding an additional linear term. So if you had $X1$, $X2$, and $X3$ as explanatory variables, you might want to have a model that is:\n\n$$Y = \\beta_0 + \\beta_1 X1$$\n\nand then\n\n$$Y = \\beta_0 + \\beta_1 X1 + \\beta_2 X2$$\n\nand so on. How you do so is up to you -- you could do a list of formulas constructed with `paste`, then use `lapply` to that list. Or you could wisely use the `~ .` part of the formula and change the columns in the data you pass to `lm`. Here are a few tips pertaining to some of the ways of doing this.\n\nIf you have a dataset called `DesMoines` and a character vector of all of your columns in `DesMoines` like so `varnames = c('Y','X1','X2','X3')`, and it's in the order in which you'd like to add the variables to the model, then:\n\n```\nDesMoines %>% dplyr::select(varnames[1:N])\n```\n\nwould select the first `N` variables. A loop on `N` from 2 to 4 would give you `c('Y','X1')` then `c('Y','X1','X2')` and so on. \n\nAnother handy tip is using `paste` with `collapse = '+'`. **This will let you include interactions**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvarnames = c('Y','X1','X2','X3','X1:X3','X4*X5')\n\npaste(varnames[2:4], collapse = ' + ')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"X1 + X2 + X3\"\n```\n\n\n:::\n:::\n\n\n\n\nThis will put all of the things in the vector together with `+` between them. Wisely pasting `Y ~ ` onto this, then using `as.formula()` lets you make a formula object from a character vector. Handy!\n\n# Association is not causation\n\n_Association is not causation_ is perhaps the most important lesson one learns in a statistics class. _Correlation is not causation_ is another way to say this. Throughout the previous parts of this class, we have described tools useful for quantifying associations between variables. However, we must be careful not to over-interpret these associations.\n\nThere are many reasons that a variable $X$ can be correlated with a variable $Y$ without having any direct effect on $Y$. Here we examine four common ways that can lead to misinterpreting data.\n\n## Spurious correlation\n\nThe following comical example underscores that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08a_files/figure-html/divorce-versus-margarine-1.png){width=672}\n:::\n:::\n\n\n\n\nDoes this mean that margarine causes divorces? Or do divorces cause people to eat more margarine? Of course the answer to both these questions is no. This is just an example of what we call a _spurious correlation_.\n\nYou can see many more absurd examples on the Spurious Correlations website^[http://tylervigen.com/spurious-correlations].\n\nThe cases presented in the spurious correlation site are all instances of what is generally called _data dredging_, _data fishing_, or _data snooping_. It's basically a form of what in the US we call _cherry picking_. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.\n\nA Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 25\ng <- 1000000\nsim_data <- tibble(group = rep(1:g, each=N),\n                   x = rnorm(N * g),\n                   y = rnorm(N * g))\n```\n:::\n\n\n\n\nThe first column denotes group. We created  groups and for each one we generated a pair of independent vectors, $X$ and $Y$, with 25 observations each, stored in the second and third columns. Because we constructed the simulation, we know that $X$ and $Y$ are not correlated.\n\nNext, we compute the correlation between `X` and `Y` for each group and look at the max:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- sim_data %>%\n  group_by(group) %>%\n  summarize(r = cor(x, y)) %>%\n  arrange(desc(r))\nres\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,000,000 × 2\n    group     r\n    <int> <dbl>\n 1 648278 0.852\n 2 404745 0.831\n 3 239061 0.768\n 4 979172 0.766\n 5 565886 0.763\n 6 496951 0.759\n 7 165110 0.756\n 8 916022 0.751\n 9 635400 0.749\n10 655316 0.749\n# ℹ 999,990 more rows\n```\n\n\n:::\n:::\n\n\n\n\nWe see a maximum correlation of 0.852 and if you just plot the data from the group achieving this correlation, it shows a convincing plot that $X$ and $Y$ are in fact correlated:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_data %>% filter(group == res$group[which.max(res$r)]) %>%\n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](08a_files/figure-html/dredging-1.png){width=672}\n:::\n:::\n\n\n\n\nRemember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres %>% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = \"black\")\n```\n\n::: {.cell-output-display}\n![](08a_files/figure-html/null-corr-hist-1.png){width=672}\n:::\n:::\n\n\n\n\nIt's just a mathematical fact that if we observe  random correlations that are expected to be 0, but have a standard error of 0.2041623, the largest one will be close to 1.\n\nIf we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nsim_data %>%\n  filter(group == res$group[which.max(res$r)]) %>%\n  do(tidy(lm(y ~ x, data = .))) %>%\n  filter(term == \"x\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  term  estimate std.error statistic      p.value\n  <chr>    <dbl>     <dbl>     <dbl>        <dbl>\n1 x        0.602    0.0773      7.79 0.0000000670\n```\n\n\n:::\n:::\n\n\n\n\nNow, imagine that instead of a whole lot of simulated data, you had a whole lot of actual data and waded through enough of it to find two unrelated variables that happened to show up as correlated (like divorce rates and pounds of margarine consumed). This particular form of data dredging is referred to as _p-hacking_. P-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results. In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and a lot of different variables that represent exposures and report only the one exposure that resulted in a small p-value. Furthermore, they might try fitting several different models to account for confounding and pick the one that yields the smallest p-value. In experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported. This does not necessarily happen due to unethical behavior, but rather as a result of statistical ignorance or wishful thinking. In advanced statistics courses, you can learn methods to adjust for these multiple comparisons.\n\n\n## Outliers\n\nSuppose we take measurements from two independent outcomes, $X$ and $Y$, and we standardize the measurements. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1985)\nx <- rnorm(100,100,1)\ny <- rnorm(100,84,1)\nx[-23] <- scale(x[-23])\ny[-23] <- scale(y[-23])\n```\n:::\n\n\n\n\nThe data look like this:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(aes(x,y))\n```\n\n::: {.cell-output-display}\n![](08a_files/figure-html/outlier-1.png){width=672}\n:::\n:::\n\n\n\n\nNot surprisingly, the correlation is very high:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(x,y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9878382\n```\n\n\n:::\n:::\n\n\n\n\nBut this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(x[-23], y[-23])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.04419032\n```\n\n\n:::\n:::\n\n\n\n\nPreviously, we (briefly) described alternatives to the average and standard deviation that are robust to outliers. There is also an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called _Spearman correlation_. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(aes(x = rank(x), y = rank(y)))\n```\n\n::: {.cell-output-display}\n![](08a_files/figure-html/scatter-plot-of-ranks-1.png){width=672}\n:::\n:::\n\n\n\n\nThe outlier is no longer associated with a very large value and the correlation comes way down:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(rank(x), rank(y))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.002508251\n```\n\n\n:::\n:::\n\n\n\n\nSpearman correlation can also be calculated like this:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(x, y, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.002508251\n```\n\n\n:::\n:::\n\n\n\n\nThere are also methods for robust fitting of linear models which you can learn about in, for instance, this book: Robust Statistics: Edition 2 by Peter J. Huber & Elvezio M. Ronchetti.\n\n\n## Reversing cause and effect\n\nAnother way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.\n\nA form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated^[https://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated]. Consider this quote from the article:\n\n> When we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades... Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.\n\nA very likely possibility is that the children needing regular parental help, receive this help because they don't perform well in school.\n\n\nWe can easily construct an example of cause and effect reversal using the father and son height data. If we fit the model:\n\n$$X_i = \\beta_0 + \\beta_1 y_i + \\varepsilon_i, i=1, \\dots, N$$\n\nto the father and son height data, with $X_i$ the father height and $y_i$ the son height, we do get a statistically significant result:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(HistData)\ndata(\"GaltonFamilies\")\nGaltonFamilies %>%\n  filter(childNum == 1 & gender == \"male\") %>%\n  select(father, childHeight) %>%\n  rename(son = childHeight) %>%\n  do(tidy(lm(father ~ son, data = .)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   34.0      4.57        7.44 4.31e-12\n2 son            0.499    0.0648      7.70 9.47e-13\n```\n\n\n:::\n:::\n\n\n\n\nThe model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted so as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it's the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the interpretation.\n\n\n## Confounders\n\nConfounders are perhaps the most common reason that leads to associations begin misinterpreted.\n\nIf $X$ and $Y$ are correlated, we call $Z$ a _confounder_ if changes in $Z$ causes changes in both $X$ and $Y$. Earlier, when studying baseball data, we saw how Home Runs was a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs. In some cases, we can use linear models to account for confounders. However, this is not always the case.\n\nIncorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions.\n\n### Example: UC Berkeley admissions\n\nAdmission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O'Connell. Science (1975). We can load the data and calculate the \"headline\" number:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dslabs)\ndata(admissions)\nadmissions %>% group_by(gender) %>%\n  summarize(percentage =\n              round(sum(admitted*applicants)/sum(applicants),1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  gender percentage\n  <chr>       <dbl>\n1 men          44.5\n2 women        30.3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Note: \"admitted\" is PERCENT admitted\n```\n:::\n\n\n\n\nThe chi-squared test compares two groups with binary outcomes (like \"admit\" and \"nonadmit\"). The null hypothesis is that the groups are not differently distributed between the outcomes. Here's what the data looks like going in -- its in counts of \"admitted\" and \"not admitted\".\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmissions %>% group_by(gender) %>%\n  summarize(total_admitted = round(sum(admitted / 100 * applicants)),\n            not_admitted = sum(applicants) - sum(total_admitted)) %>%\n  select(-gender) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  total_admitted not_admitted\n           <dbl>        <dbl>\n1           1198         1493\n2            557         1278\n```\n\n\n:::\n:::\n\n\n\n\n\nA low p-value rejects this hypothesis. Here, the test clearly rejects the hypothesis that gender and admission are independent:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmissions %>% group_by(gender) %>%\n  summarize(total_admitted = round(sum(admitted / 100 * applicants)),\n            not_admitted = sum(applicants) - sum(total_admitted)) %>%\n  select(-gender) %>%\n  do(tidy(chisq.test(.))) %>% .$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.055797e-21\n```\n\n\n:::\n:::\n\n\n\n\n\nBut closer inspection shows a paradoxical result. Here are the percent admissions by major:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmissions %>% select(major, gender, admitted) %>%\n  spread(gender, admitted) %>%\n  mutate(women_minus_men_pct = women - men)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  major men women women_minus_men_pct\n1     A  62    82                  20\n2     B  63    68                   5\n3     C  37    34                  -3\n4     D  33    35                   2\n5     E  28    24                  -4\n6     F   6     7                   1\n```\n\n\n:::\n:::\n\n\n\n\nFour out of the six majors favor women. More importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals.\n\nThe paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear.  What's going on? This actually can happen if an uncounted confounder is driving most of the variability.\n\nSo let's define three variables: $X$ is 1 for men and 0 for women, $Y$ is 1 for those admitted and 0 otherwise, and $Z$ quantifies the selectivity of the major. A gender bias claim would be based on the fact that $\\mbox{Pr}(Y=1 | X = x)$ is higher for $x=1$ than $x=0$. However, $Z$ is an important confounder to consider. Clearly $Z$ is associated with  $Y$, as the more selective a major, the lower $\\mbox{Pr}(Y=1 | Z = z)$. But is major selectivity $Z$ associated with gender $X$?\n\nOne way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmissions %>%\n  group_by(major) %>%\n  summarize(major_selectivity = sum(admitted * applicants)/sum(applicants),\n            percent_women_applicants = sum(applicants * (gender==\"women\")) /\n                                             sum(applicants) * 100) %>%\n  ggplot(aes(major_selectivity, percent_women_applicants, label = major)) +\n  geom_text()\n```\n\n::: {.cell-output-display}\n![](08a_files/figure-html/uc-berkeley-majors-1.png){width=672}\n:::\n:::\n\n\n\n\nThere seems to be association. The plot suggests that women were much more likely to apply to the four \"hard\" majors: gender and major selectivity are confounded. Compare, for example, major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women, while less than 30% of the applicants of major B were women.\n\n\n### Confounding explained graphically\n\nThe following plot shows the number of applicants that were admitted and those that were not by major and gender:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08a_files/figure-html/confounding-1.png){width=672}\n:::\n:::\n\n\n\n\n<!--\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmissions %>%\n  mutate(percent_admitted = admitted * applicants/sum(applicants)) %>%\n  ggplot(aes(gender, y = percent_admitted, fill = major)) +\n  geom_bar(stat = \"identity\", position = \"stack\")\n```\n\n::: {.cell-output-display}\n![](08a_files/figure-html/confounding-2-1.png){width=672}\n:::\n:::\n\n\n\n-->\nIt also breaks down the acceptances by major. This breakdown allows us to see that the majority of accepted men came from two majors: A and B. It also lets us see that few women applied to these majors.\n\n\n### Average after stratifying\n\nIn this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmissions %>%\n  ggplot(aes(major, admitted, col = gender, size = applicants)) +\n  geom_point() + \n  labs(x = 'Percent Admitted')\n```\n\n::: {.cell-output-display}\n![](08a_files/figure-html/admission-by-major-1.png){width=672}\n:::\n:::\n\n\n\n\nNow we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest (least selective) majors, A and B.\n\nIf we average the difference by major, we find that the percent is actually 3.5% higher for women.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmissions %>%  group_by(gender) %>% summarize(average = mean(admitted))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  gender average\n  <chr>    <dbl>\n1 men       38.2\n2 women     41.7\n```\n\n\n:::\n:::\n\n\n\n\n\n## Simpson's paradox\n\nThe case we have just covered is an example of Simpson's paradox. It is called a paradox because we see the sign of the correlation flip when comparing the entire publication and specific strata. As an illustrative example, suppose you have three random variables $X$, $Y$, and $Z$ and that we observe realizations of these. Here is a plot of simulated observations for $X$ and $Y$ along with the sample correlation:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08a_files/figure-html/simpsons-paradox-1.png){width=672}\n:::\n:::\n\n\n\n\nYou can see that $X$ and $Y$ are negatively correlated. However, once we stratify by $Z$ (shown in different colors below) another pattern emerges:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08a_files/figure-html/simpsons-paradox-explained-1.png){width=672}\n:::\n:::\n\n\n\n\nIt is really $Z$ that is negatively correlated with $X$. If we stratify by $Z$, the $X$ and $Y$ are actually positively correlated as seen in the plot above.\n\nHow do we stratify in regression? Depending on the relationship we think the variables have (conditional on $Z$). In the above example, it looks like an intercept shift would account for the differences across $Z$ (note that the plot shows the correlation between $Y$ and $X$ for each value of $Z$):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(y ~ x + as.factor(z), data = dat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x + as.factor(z), data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6120 -0.5354  0.0269  0.5750  1.9138 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -5.78432    0.34026  -17.00   <2e-16 ***\nx              0.78805    0.02999   26.28   <2e-16 ***\nas.factor(z)2  3.66771    0.12976   28.27   <2e-16 ***\nas.factor(z)3  7.24256    0.16855   42.97   <2e-16 ***\nas.factor(z)4 10.73651    0.21520   49.89   <2e-16 ***\nas.factor(z)5 14.43156    0.26816   53.82   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.824 on 494 degrees of freedom\nMultiple R-squared:  0.9301,\tAdjusted R-squared:  0.9294 \nF-statistic:  1315 on 5 and 494 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\nIf we thought that the relationship between $Y$ and $X$ were *different* for each $Z$ (not just the intercept), we could try allowing a slope-shift using interactions:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(y ~ x*as.factor(z), data = dat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x * as.factor(z), data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6383 -0.5431  0.0197  0.5581  1.9655 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -6.22140    0.72249  -8.611  < 2e-16 ***\nx                0.82776    0.06520  12.696  < 2e-16 ***\nas.factor(z)2    4.55366    0.95253   4.781 2.31e-06 ***\nas.factor(z)3    8.28606    0.88257   9.389  < 2e-16 ***\nas.factor(z)4   11.02669    0.78302  14.082  < 2e-16 ***\nas.factor(z)5   14.70047    0.76182  19.296  < 2e-16 ***\nx:as.factor(z)2 -0.08900    0.09390  -0.948    0.344    \nx:as.factor(z)3 -0.12699    0.09712  -1.307    0.192    \nx:as.factor(z)4 -0.01018    0.08751  -0.116    0.907    \nx:as.factor(z)5  0.01720    0.10079   0.171    0.865    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8247 on 490 degrees of freedom\nMultiple R-squared:  0.9306,\tAdjusted R-squared:  0.9293 \nF-statistic: 729.8 on 9 and 490 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}